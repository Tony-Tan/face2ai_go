<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【信息论、推理与学习算法】本系列博客介绍 | 谭升的博客</title><meta name=keywords content="Information Theory,信息论,Inference,推理,Learning Algorithms,算法"><meta name=description content="Abstract: 本文是本系列的第一篇，介绍本系列的主要内容
Keywords: Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/math/itila-introduction-to-this-series/><link crossorigin=anonymous href=../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="【信息论、推理与学习算法】本系列博客介绍"><meta property="og:description" content="Abstract: 本文是本系列的第一篇，介绍本系列的主要内容
Keywords: Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/math/itila-introduction-to-this-series/"><meta property="article:section" content="math"><meta property="article:published_time" content="2018-09-21T11:53:59+00:00"><meta property="article:modified_time" content="2023-04-04T15:19:02+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【信息论、推理与学习算法】本系列博客介绍"><meta name=twitter:description content="Abstract: 本文是本系列的第一篇，介绍本系列的主要内容
Keywords: Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Maths","item":"https://go.face2ai.com/math/"},{"@type":"ListItem","position":3,"name":"【信息论、推理与学习算法】本系列博客介绍","item":"https://go.face2ai.com/math/itila-introduction-to-this-series/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【信息论、推理与学习算法】本系列博客介绍","name":"【信息论、推理与学习算法】本系列博客介绍","description":"Abstract: 本文是本系列的第一篇，介绍本系列的主要内容 Keywords: Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法\n","keywords":["Information Theory","信息论","Inference","推理","Learning Algorithms","算法"],"articleBody":"Abstract: 本文是本系列的第一篇，介绍本系列的主要内容 Keywords: Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法\n信息论、推理与学习算法介绍 这个系列是信息论相关内容的介绍，信息论是什么可能有些做机器学习或者AI的同学们不太了解，而做通信的同学应该是非常清楚的，如何准确的定义信息论是什么，不在我的能力范围内，但是我们平时接触到的图像，或者简单点说灰度图像，一个8bit的像素点能有多少阶灰度，为什么有256，而不是258或者其他的，这个其实就属于信息论的知识范围，而信息论和机器学习有关系么？答案是肯定的，凡是处理信息，传递信息的过程，都多多少少跟信息论有那么点关系。 本系列主要面对的读者是： 从事机器学习，人工智能类内容研究的同学，工程师，或者爱好者 需要的背景知识：工程专业，科学类专业，或者数学类本科1，2年级的数学知识，包括，微积分，概率论，线性代数的基本知识（本站已经完成这些基础知识的全部博客，可以随时查阅）。 本书封面： 机器学习，信息论 传统的信息论课程不仅包括Shannon的信息化思想，也有实际解决问题的现实应用，我们这个系列更加进一步的包括了：\n Bayesian Data Modelling Monte Carlo Methods Variational Methods Clustering ALgorithms Neural Networks  为什么要把信息论和机器学习弄到一起？ 信息论和机器学习是一个硬币的两面！ 60年代一个领域 —— 控制理论（cybernetics）在信息论，计算机科学，和神经科学等学科中非常火爆，这些科学家们都在研究一个相同的问题，那时候信息论和机器学习还是属于同一类。大脑是一个压缩信息，进行沟通的系统，而在数据压缩（data conpression）和纠错码上（error-correcting code）表现最好的（state-of-the-art）的算法上使用的工具，在机器学习中也会使用。 这些种种迹象都表明，机器学习和信息论有着密切的关联，而我们本系列更关注的就是信息论在机器学习方面的应用，或者帮助我们理解一些算法的特点和局限。\n学习地图 本书的目录如下,当然这些课不是我们所有要学的，我画了个地图，大概应该是按照这个地图来完成我们的博客的：\nPreface 1 Introduction to Information Theory 2 Probability, Entropy, and Inference 3 More about Inference\nI Data Compression 4 The Source Coding Theorem 5 Symbol Codes 6 Stream Codes 7 Codes for Integers\nII Noisy-Channel Coding 8 Dependent Random Variables 9 Communication over a Noisy Channel 10 The Noisy-Channel Coding Theorem 11 Error-Correcting Codes and Real Channels\nIII Further Topics in Information Theory 12 Hash Codes: Codes for E\u000ecient Information Retrieval 13 Binary Codes 14 Very Good Linear Codes Exist 15 Further Exercises on Information Theory 16 Message Passing 17 Communication over Constrained Noiseless Channels 18 Crosswords and Codebreaking 19 Why have Sex? Information Acquisition and Evolution\nIV Probabilities and Inference 20 An Example Inference Task: Clustering 21 Exact Inference by Complete Enumeration 22 Maximum Likelihood and Clustering 23 Useful Probability Distributions 24 Exact Marginalization 25 Exact Marginalization in Trellises 26 Exact Marginalization in Graphs 27 Laplace’s Method 28 Model Comparison and Occam’s Razor 29 Monte Carlo Methods 30 E\u000ecient Monte Carlo Methods 31 Ising Models 32 Exact Monte Carlo Sampling 33 Variational Methods 34 Independent Component Analysis and Latent Variable Modelling 35 Random Inference Topics 36 Decision Theory 37 Bayesian Inference and Sampling Theory\nV Neural networks 38 Introduction to Neural Networks 39 The Single Neuron as a Classi\u000cer 40 Capacity of a Single Neuron 41 Learning as Inference 42 Hop\u000celd Networks 43 Boltzmann Machines 44 Supervised Learning in Multilayer Networks 45 Gaussian Processes 46 Deconvolution\nVI Sparse Graph Codes 47 Low-Density Parity-Check Codes 48 Convolutional Codes and Turbo Codes 49 Repeat{Accumulate Codes 50 Digital Fountain Codes\n地图 根据我们的目标和书上给出的建议，我们要学习下面这些章节，箭头之间表示先后关系，箭头指向的课程需要在前面的课程完成后才能进行： github: https://github.com/Tony-Tan/MachineLearningMath 上有高清大图\n总结 本文是信息论的第一课，后续就围绕上图展开，对于基础不是了解的同学可以去看前面的其他博客，谢谢支持。\n","wordCount":"303","inLanguage":"en","datePublished":"2018-09-21T11:53:59Z","dateModified":"2023-04-04T15:19:02+08:00","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/math/itila-introduction-to-this-series/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a>&nbsp;»&nbsp;<a href=https://go.face2ai.com/math/>Maths</a></div><h1 class=post-title>【信息论、推理与学习算法】本系列博客介绍</h1><div class=post-meta><span title="2018-09-21 11:53:59 +0000 UTC">September 21, 2018</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%bf%a1%e6%81%af%e8%ae%ba%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e4%bb%8b%e7%bb%8d aria-label=信息论、推理与学习算法介绍>信息论、推理与学习算法介绍</a><ul><li><a href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e4%bf%a1%e6%81%af%e8%ae%ba aria-label=机器学习，信息论>机器学习，信息论</a></li><li><a href=#%e5%ad%a6%e4%b9%a0%e5%9c%b0%e5%9b%be aria-label=学习地图>学习地图</a><ul><li><a href=#preface aria-label=Preface>Preface</a></li><li><a href=#i-data-compression aria-label="I Data Compression">I Data Compression</a></li><li><a href=#ii-noisy-channel-coding aria-label="II Noisy-Channel Coding">II Noisy-Channel Coding</a></li><li><a href=#iii-further-topics-in-information-theory aria-label="III Further Topics in Information Theory">III Further Topics in Information Theory</a></li><li><a href=#iv-probabilities-and-inference aria-label="IV Probabilities and Inference">IV Probabilities and Inference</a></li><li><a href=#v-neural-networks aria-label="V Neural networks">V Neural networks</a></li><li><a href=#vi-sparse-graph-codes aria-label="VI Sparse Graph Codes">VI Sparse Graph Codes</a></li><li><a href=#%e5%9c%b0%e5%9b%be aria-label=地图>地图</a></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 本文是本系列的第一篇，介绍本系列的主要内容
<strong>Keywords:</strong> Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法</p><h1 id=信息论推理与学习算法介绍>信息论、推理与学习算法介绍<a hidden class=anchor aria-hidden=true href=#信息论推理与学习算法介绍>#</a></h1><p>这个系列是信息论相关内容的介绍，信息论是什么可能有些做机器学习或者AI的同学们不太了解，而做通信的同学应该是非常清楚的，如何准确的定义信息论是什么，不在我的能力范围内，但是我们平时接触到的图像，或者简单点说灰度图像，一个8bit的像素点能有多少阶灰度，为什么有256，而不是258或者其他的，这个其实就属于信息论的知识范围，而信息论和机器学习有关系么？答案是肯定的，凡是处理信息，传递信息的过程，都多多少少跟信息论有那么点关系。
本系列主要面对的读者是： <strong>从事机器学习，人工智能类内容研究的同学，工程师，或者爱好者</strong>
需要的背景知识：工程专业，科学类专业，或者数学类本科1，2年级的数学知识，包括，微积分，概率论，线性代数的基本知识（本站已经完成这些基础知识的全部博客，可以随时查阅）。
本书封面：
<img loading=lazy src=./cover.jpg alt></p><h2 id=机器学习信息论>机器学习，信息论<a hidden class=anchor aria-hidden=true href=#机器学习信息论>#</a></h2><p>传统的信息论课程不仅包括Shannon的信息化思想，也有实际解决问题的现实应用，我们这个系列更加进一步的包括了：</p><ul><li>Bayesian Data Modelling</li><li>Monte Carlo Methods</li><li>Variational Methods</li><li>Clustering ALgorithms</li><li>Neural Networks</li></ul><p>为什么要把信息论和机器学习弄到一起？
信息论和机器学习是一个硬币的两面！
60年代一个领域 —— 控制理论（cybernetics）在信息论，计算机科学，和神经科学等学科中非常火爆，这些科学家们都在研究一个相同的问题，那时候信息论和机器学习还是属于同一类。大脑是一个压缩信息，进行沟通的系统，而在数据压缩（data conpression）和纠错码上（error-correcting code）表现最好的（state-of-the-art）的算法上使用的工具，在机器学习中也会使用。
这些种种迹象都表明，机器学习和信息论有着密切的关联，而我们本系列更关注的就是信息论在机器学习方面的应用，或者帮助我们理解一些算法的特点和局限。</p><h2 id=学习地图>学习地图<a hidden class=anchor aria-hidden=true href=#学习地图>#</a></h2><p>本书的目录如下,当然这些课不是我们所有要学的，我画了个地图，大概应该是按照这个地图来完成我们的博客的：</p><h3 id=preface>Preface<a hidden class=anchor aria-hidden=true href=#preface>#</a></h3><p>1 Introduction to Information Theory
2 Probability, Entropy, and Inference
3 More about Inference</p><h3 id=i-data-compression>I Data Compression<a hidden class=anchor aria-hidden=true href=#i-data-compression>#</a></h3><p>4 The Source Coding Theorem
5 Symbol Codes
6 Stream Codes
7 Codes for Integers</p><h3 id=ii-noisy-channel-coding>II Noisy-Channel Coding<a hidden class=anchor aria-hidden=true href=#ii-noisy-channel-coding>#</a></h3><p>8 Dependent Random Variables
9 Communication over a Noisy Channel
10 The Noisy-Channel Coding Theorem
11 Error-Correcting Codes and Real Channels</p><h3 id=iii-further-topics-in-information-theory>III Further Topics in Information Theory<a hidden class=anchor aria-hidden=true href=#iii-further-topics-in-information-theory>#</a></h3><p>12 Hash Codes: Codes for Ecient Information Retrieval
13 Binary Codes
14 Very Good Linear Codes Exist
15 Further Exercises on Information Theory
16 Message Passing
17 Communication over Constrained Noiseless Channels
18 Crosswords and Codebreaking
19 Why have Sex? Information Acquisition and Evolution</p><h3 id=iv-probabilities-and-inference>IV Probabilities and Inference<a hidden class=anchor aria-hidden=true href=#iv-probabilities-and-inference>#</a></h3><p>20 An Example Inference Task: Clustering
21 Exact Inference by Complete Enumeration
22 Maximum Likelihood and Clustering
23 Useful Probability Distributions
24 Exact Marginalization
25 Exact Marginalization in Trellises
26 Exact Marginalization in Graphs
27 Laplace&rsquo;s Method
28 Model Comparison and Occam&rsquo;s Razor
29 Monte Carlo Methods
30 Ecient Monte Carlo Methods
31 Ising Models
32 Exact Monte Carlo Sampling
33 Variational Methods
34 Independent Component Analysis and Latent Variable Modelling
35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference and Sampling Theory</p><h3 id=v-neural-networks>V Neural networks<a hidden class=anchor aria-hidden=true href=#v-neural-networks>#</a></h3><p>38 Introduction to Neural Networks
39 The Single Neuron as a Classi er
40 Capacity of a Single Neuron
41 Learning as Inference
42 Hop eld Networks
43 Boltzmann Machines
44 Supervised Learning in Multilayer Networks
45 Gaussian Processes
46 Deconvolution</p><h3 id=vi-sparse-graph-codes>VI Sparse Graph Codes<a hidden class=anchor aria-hidden=true href=#vi-sparse-graph-codes>#</a></h3><p>47 Low-Density Parity-Check Codes
48 Convolutional Codes and Turbo Codes
49 Repeat{Accumulate Codes
50 Digital Fountain Codes</p><h3 id=地图>地图<a hidden class=anchor aria-hidden=true href=#地图>#</a></h3><p>根据我们的目标和书上给出的建议，我们要学习下面这些章节，箭头之间表示先后关系，箭头指向的课程需要在前面的课程完成后才能进行：
<img loading=lazy src=https://raw.githubusercontent.com/Tony-Tan/MachineLearningMath/master/information.png alt></p><p>github: <a href=https://github.com/Tony-Tan/MachineLearningMath>https://github.com/Tony-Tan/MachineLearningMath</a> 上有高清大图</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>本文是信息论的第一课，后续就围绕上图展开，对于基础不是了解的同学可以去看前面的其他博客，谢谢支持。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/information-theory/>Information Theory</a></li><li><a href=https://go.face2ai.com/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/>信息论</a></li><li><a href=https://go.face2ai.com/tags/inference/>Inference</a></li><li><a href=https://go.face2ai.com/tags/%E6%8E%A8%E7%90%86/>推理</a></li><li><a href=https://go.face2ai.com/tags/learning-algorithms/>Learning Algorithms</a></li><li><a href=https://go.face2ai.com/tags/%E7%AE%97%E6%B3%95/>算法</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 【信息论、推理与学习算法】本系列博客介绍 on twitter" href="https://twitter.com/intent/tweet/?text=%e3%80%90%e4%bf%a1%e6%81%af%e8%ae%ba%e3%80%81%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e3%80%91%e6%9c%ac%e7%b3%bb%e5%88%97%e5%8d%9a%e5%ae%a2%e4%bb%8b%e7%bb%8d&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fitila-introduction-to-this-series%2f&hashtags=InformationTheory%2c%e4%bf%a1%e6%81%af%e8%ae%ba%2cInference%2c%e6%8e%a8%e7%90%86%2cLearningAlgorithms%2c%e7%ae%97%e6%b3%95"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【信息论、推理与学习算法】本系列博客介绍 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fitila-introduction-to-this-series%2f&title=%e3%80%90%e4%bf%a1%e6%81%af%e8%ae%ba%e3%80%81%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e3%80%91%e6%9c%ac%e7%b3%bb%e5%88%97%e5%8d%9a%e5%ae%a2%e4%bb%8b%e7%bb%8d&summary=%e3%80%90%e4%bf%a1%e6%81%af%e8%ae%ba%e3%80%81%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e3%80%91%e6%9c%ac%e7%b3%bb%e5%88%97%e5%8d%9a%e5%ae%a2%e4%bb%8b%e7%bb%8d&source=https%3a%2f%2fgo.face2ai.com%2fmath%2fitila-introduction-to-this-series%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【信息论、推理与学习算法】本系列博客介绍 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2fmath%2fitila-introduction-to-this-series%2f&title=%e3%80%90%e4%bf%a1%e6%81%af%e8%ae%ba%e3%80%81%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e3%80%91%e6%9c%ac%e7%b3%bb%e5%88%97%e5%8d%9a%e5%ae%a2%e4%bb%8b%e7%bb%8d"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【信息论、推理与学习算法】本系列博客介绍 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2fmath%2fitila-introduction-to-this-series%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【信息论、推理与学习算法】本系列博客介绍 on whatsapp" href="https://api.whatsapp.com/send?text=%e3%80%90%e4%bf%a1%e6%81%af%e8%ae%ba%e3%80%81%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e3%80%91%e6%9c%ac%e7%b3%bb%e5%88%97%e5%8d%9a%e5%ae%a2%e4%bb%8b%e7%bb%8d%20-%20https%3a%2f%2fgo.face2ai.com%2fmath%2fitila-introduction-to-this-series%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【信息论、推理与学习算法】本系列博客介绍 on telegram" href="https://telegram.me/share/url?text=%e3%80%90%e4%bf%a1%e6%81%af%e8%ae%ba%e3%80%81%e6%8e%a8%e7%90%86%e4%b8%8e%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95%e3%80%91%e6%9c%ac%e7%b3%bb%e5%88%97%e5%8d%9a%e5%ae%a2%e4%bb%8b%e7%bb%8d&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fitila-introduction-to-this-series%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>