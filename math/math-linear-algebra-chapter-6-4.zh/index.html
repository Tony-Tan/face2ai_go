<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【线性代数】6-4:对称矩阵(Symmetric Matrices) | 谭升的博客</title><meta name=keywords content="特征值,特征向量,对称矩阵,投影矩阵,谱定理"><meta name=description content="Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质
Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/math/math-linear-algebra-chapter-6-4.zh/><link crossorigin=anonymous href=../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="【线性代数】6-4:对称矩阵(Symmetric Matrices)"><meta property="og:description" content="Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质
Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/math/math-linear-algebra-chapter-6-4.zh/"><meta property="article:section" content="math"><meta property="article:published_time" content="2017-11-22T15:18:03+00:00"><meta property="article:modified_time" content="2023-04-04T15:19:02+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【线性代数】6-4:对称矩阵(Symmetric Matrices)"><meta name=twitter:description content="Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质
Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Maths","item":"https://go.face2ai.com/math/"},{"@type":"ListItem","position":3,"name":"【线性代数】6-4:对称矩阵(Symmetric Matrices)","item":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-4.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【线性代数】6-4:对称矩阵(Symmetric Matrices)","name":"【线性代数】6-4:对称矩阵(Symmetric Matrices)","description":"Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质 Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem\n","keywords":["特征值","特征向量","对称矩阵","投影矩阵","谱定理"],"articleBody":"Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质 Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem\n对称矩阵 这几篇在难度上确实要比前面的内容大很多，所以看书理解和总结都变得不那么流畅了，但是慢慢看下来收获还是有很大的，而且我发现不管学的多认真，还是会有遗漏，所以我觉得之前的想法就是一次性把什么什么学会是不可能的，只能学到自己觉得达到自己能发现的最大限度，等到应用之时还是要回来查阅，这样往往会有进一步的更大发现，\n对称矩阵 (Symmetric Matrices) 对称矩阵我们在最早的知识里面就学过 $A^T=A$ 的矩阵叫做对称矩阵，我们也学过投影矩阵,但是当时我们并没有强调过一点就是投影矩阵都是对称的，这个性质今天在这里会有很大的用途。 我们继续说投影矩阵，所谓投影矩阵，就是在和向量 $\\vec{c}$ 相乘的时候，投影到矩阵A的列空间内，那么其中，投影 $p$ 和 原向量 $\\vec{c}$ 的差 $\\vec{e} =\\vec{c}-\\vec{p}$ 与子空间正交。\n举个例子，在三维空间内，A的列空间是一个二维平面那么，A对应的投影矩阵P能够把任何方向的向量投影到平面上，那么如果向量本身属于平面那么 $Px=x$ 显然是不用质疑的（我们之前在投影那篇文章中也讲过） 但是，同志们，看看这个有木有很面熟啊，这个明显就是投影矩阵 $P$ 的特征值和特征向量么？没错，$P$ 有一个平面的特征向量，可以随便选！能选多少个呢、当然是无数个，但是问题又来了，这无数多个并不是独立的，因为一共就二维，选出来三个线性独立的向量都是不可能的，所以这个平面能选出两个线性独立的特征向量，并且对应的特征值都是1，这里有人可能疑惑为啥要选两个，因为我们6-2的时候说过只有特诊向量足够的情况下才能对角化，投影矩阵明显是个3x3的矩阵，那么特征向量也应该有三个呀！我们的子空间是二维的，所以理论上应该有两个特征向量在上面，剩下一维存在一个，那么这一个也能很好找，$\\vec{e}$ 就是 也就是和子空间正交的向量都行 $Px=0x$ 表明 $\\vec{x}$ 和子空间正交，那么这是个特征值为0的特征向量，这样我们又进一步规范一下，选择三个特征向量相互正交，这个也是可以做到的，也就是对于矩阵P我们找到了三个相互正交的特征向量，并且长度缩放到单位长度。\n以上三维投影到二维平面可以通过几何来解释，但为了能让大家从线性空间来理解，就没用几何方法，大家可以自己脑补。\n得出结论，对称矩阵 $P^T=P$ 的特征向量相互正交并且为单位向量。 对称矩阵\"It is no exaggeration to say that these are the most important matrices the world will ever see – in the theory of linear algebra and alos in the applications\" 翻译成中文：“对称矩阵是史上最牛B的矩阵，无论在理论还是应用” 这个我们目前还无法考证，还没做过应用呢？不是么，但是我知道PCA中确实用了对称矩阵，SVD等一些列相关技术。 一个矩阵能被如此称赞，不外乎几点原因，首先是其本身拥有较好的性质，其次这个矩阵在自然生活中经常出现，就像正态分布，那么难的公式，却能准确的描述自然届的现象。最后就是如果表现形式简单，那么这个就是非常有用的东西啦。\n下面我们开始探索对称矩阵的性质。 如果一个对称矩阵满足： $$ suppose:\\ A^T=A\\ A=S\\Lambda S^{-1}\\ then:\\ A^T=(S^{-1})^T\\Lambda^T S^T $$ 这种情况下就有下面这种可能了，也就是对应的 $S=(S^{-1})^T$ 注意我们这里说的是可能，并不排除不可能的情况，原文书上用的也是possibly，也就是说我们目前假设: $$ S^T=S^{-1}\\ S^TS=I $$ 这里我们可以预报一下：\n  对称矩阵只有实数特征值 对称矩阵特征向量可以选择正交单位向量 orthonormal   对于 $S^TS=I$ 面熟么？还有印象么？我们认识啊，正交矩阵  $Q^TQ=I$ 矩阵Q中每列之间相互正交，也就是我们对于对称矩阵可以写成： $$ A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T\\ with:\\ Q^{-1}=Q^T $$\n这个就是著名的普定理 “Spectral Theorem”:\n Every symmetric matrix has the factorization $A=Q\\Lambda Q^T$ with real eigenvalues in $\\Lambda$ and orthonormal eigenvectors in $S=Q$\n 对于所有对称矩阵都能分解成 $A=Q\\Lambda Q^T$ 的形式并且在 $\\Lambda$ 中的所有特征值都是实数，其对应的特征向量是正交单位矩阵，即 $S=Q$\n普定理在数学中很重要，对应的主轴定理 “Principle Axis Theorem”在集合物理中有重要地位。\n这个定理对于从后到前 $A=Q\\Lambda Q^T$ 很好证明，但是对于对称矩阵的特征值都是实数和特征向量相互正交比较难证明，也就是说，对角化时，特征向量矩阵是正交矩阵的时候很容易证明原始矩阵是对称的，但是对称矩阵不太容易证明，可以对角化，并且对角化的特征矩阵 $S$ 是正交矩阵 $Q$\n插播一句，orthonormal，翻译成中文是正交，而且normal，因为特征向量是可以随意缩放的，所以主要强调正交性 我们接下来要做的伟大的证明，分为下面三步来证明：\n 首先举个例子，来展示下特征值都是实数，特征向量orthonormal 当特征值不重复的时候的证明 当特征值重复的时候的证明   我们来先看个🌰： $$ A=\\begin{bmatrix}1\u00262\\2\u00264\\end{bmatrix}\\ \\begin{vmatrix}1-\\lambda \u00262\\2\u00264-\\lambda\\end{vmatrix}=0\\ \\lambda^2-5\\lambda=0\\ \\lambda_1=0\\ \\lambda_2=5\\ x_1=\\begin{bmatrix}2\\newline -1\\end{bmatrix}\\ x_2=\\begin{bmatrix}1\\newline 2\\end{bmatrix} $$ 可以看出$x_1$ 属于矩阵的nullspace，而 $x_2$ 属于矩阵的column space，但是我们学四个子空间的时候有nullspace和rowspace是正交的，但是这里的 $x_2$ 属于矩阵的column space，为什么呢？因为矩阵实对称的，所以对称矩阵的rowspace和columnspace一致。 $$ Q^{-1}AQ= \\frac{1}{\\sqrt{5}} \\begin{bmatrix}2\u00261\\newline -1\u00262\\end{bmatrix} \\begin{bmatrix}1\u00262\\2\u00264\\end{bmatrix} \\frac{1}{\\sqrt{5}} \\begin{bmatrix}2\u00261\\newline -1\u00262\\end{bmatrix}= \\begin{bmatrix}0\u00260\\newline 0\u00265\\end{bmatrix}= \\Lambda $$ 这就是个简单的例子，但是证明全体元素成立不能靠举一个例子来证明，但证明不成立可以靠举个反例来证明不成立。\n 下面证明 所有实数对称矩阵特征值都是实数\n怎么证明一个数是实数呢，只能用点实数的性质，实数的性质不少但是能证明实数是实数的不多，可以想到一个就是实数的共轭是其本身，这也可以说是复数的性质，证明是实数也就是说证明不是复数。 复数的共轭 $$ \\lambda=a+bi\\ \\bar{\\lambda}=a-bi $$\n根据复数的性质，以及向量的基本计算，对于实数矩阵A，满足：\n$$ Ax=\\lambda x\\ \\bar{Ax}=\\bar{\\lambda x}\\ A\\bar{x}=\\bar{\\lambda}\\bar{x}\\ Transpose:\\ \\bar{x}^TA=\\bar{x}^T\\bar{\\lambda}\\ for:\\ \\bar{x}^TAx=\\bar{x}^T\\lambda x\\ \\bar{x}^TAx=\\bar{x}^T\\bar{\\lambda}x\\ and:\\ \\bar{x}^Tx=|x|^20\\ so:\\ \\bar{\\lambda}=\\lambda $$\nQED\n整个过程证明了特征值的共轭等于原特征值，故特征值是实数被证明了。 证明思路就是通过构造出 $\\bar{\\lambda}=\\lambda$ 的结构来证明，主要用到了A是实数矩阵的性质，通过转置和乘法等来完成这个过程。 $(A-\\lambda I)x=0$ 可以得出既然特征值是实数，A也是实数，那么x肯定是实数（实数没办法仅通过乘法加法得出复数）\n 下面证明 所有实数对称矩阵特征向量相互正交,当特征值不相等的时候\n$$ suppose:\\ Ax=\\lambda_1x\\ Ay=\\lambda_2y\\ A^T=A\\ (Ax)^T=\\lambda_1x^T\\ (Ax)^Ty=x^TAy=\\lambda_1x^Ty\\ x^TAy=x^T\\lambda_2 y\\ then:\\ \\lambda_1x^Ty=x^T\\lambda_2 y\\ for:\\ \\lambda_1\\neq \\lambda_2\\ so:\\ x^Ty=0 $$ QED\n 插播一个小栗子： $$ A=Q\\Lambda Q^T= \\begin{bmatrix}x_1 \u0026x_2\\end{bmatrix} \\begin{bmatrix}\\lambda_1 \u0026\\\u0026\\lambda_2\\end{bmatrix} \\begin{bmatrix}x_1^T \\x_2^T\\end{bmatrix}=\\lambda_1x_1x_1^T+\\lambda_2x_2x_2^T $$ $A=\\lambda_1x_1x_1^T+\\lambda_2x_2x_2^T$ 把A写成了两个rank=1的矩阵的线性组合，并且这个rank=1的矩阵还是投影矩阵，当然也是对称矩阵，是不是很神奇，这个矩阵分解是比较有意思的，基是矩阵，而且基实对称的rank=1的投影矩阵，是不是很多头衔啊，头衔越多越厉害，不信你去看看大大有多少头衔。这个投影矩阵可以理解为投影到特征向量组成的空间(Eigenspace)\n 其实写到这我有点疑惑了，本来写博客一个是自己总结，另一个是给大家一个参考，但是当我写了上面那个头衔了以后，我发现，如果没有基础或者不从头看起，直接看本文可能会感到疑惑，这也是知识的一个性质，就是连续性和扩展性，那么没有基础，基本都是空中楼阁（这种大牛太多了，基础没用，直接上算法的比比皆是，我以前也是，我现在改邪归正了）\n实数矩阵的复特征值(Complex Eigenvalues of Real Matrix) 本文主要说对称矩阵的特征值，特征向量，对称矩阵的特征值和特征向量一定是实数，我们上面基本都为证明这个结论，那么什么样的矩阵会产生复数特征值特征向量呢？ 如果一个矩阵包含一个复数特征值，那么一定是成对出现的，所谓成对就是如果 $\\lambda=a+bi$ 是一个特征值，那么 $\\bar{\\lambda}=a-bi$ 也一定是A的一个特征值，证明： $$ Ax=\\lambda x\\ A\\bar{x}=\\bar{\\lambda x}=\\bar{\\lambda} \\bar{x} $$ 上面的取共轭操作hexo渲染有点问题，在两个字母中间的$\\bar{\\lambda x}$ 其实是个长的,他画的有点短 我们后面有一章专门介绍复数矩阵，所以这里有点迷糊的不要紧，放过自己，继续看下面。\nEigenvalues 和 Pivots Pivots是我们这章之前主要研究的对象，因为我们主要研究的是矩阵用于方程组的求解，而本章开始Eigenvalues的研究，那么我们的惯性思维就是，既然是一个体系下的知识重点，那么他们有联系么？\n product of pivots = determinant = product of Eigenvalues\n 这个是个特殊的性质，结合前面trace的性质我们知道了特征值和矩阵相关的两个算数性质，但是这个具体的证明我还没学会，包括下面的这个结论，证明我也没看懂。\n The number of positive eigenvalues of $A=A^T$ equals the number of positive Pivots\n 这个结论书上给出了证明，但是说实话，我没明白，所以这个地方必须先留个空白，把书上的东西抄过来没意义\n上述两个结论不会证明，后续补上，此处留坑\n这就是pivots和eigenvalues可以通过上述两个结论产生联系，不过联系应该也就这么多了。\n所有对称矩阵可以对角化(All Symmetric Matrices are Diagonalizable) 接下来我们要证明最后一个结论，就是在有重复特征值的情况下，对称矩阵依然可以被对角化，其实本文第一个例子中就包含相同的特征值，两个 $\\lambda=1$ 虽然如此我们还是在平面中找到了两个相互正交的特征向量，一个特殊的例子没办法证明全部情况，下面我们系统证明一下： 正式的证明之前有个小trick,就是给矩阵对角线上的每个元素加一个扰动 $nc$ 这样所有的特征值不同（具体为啥我也没想明白,有明白人请指教一下）所有有不同的特征向量，当 $c\\to 0$ 得到原始特征值，和一组不同特征向量（这个思路是Prof. Strang写在书上的，他说这个有点不严谨，但是I am sure this is true） 接下来将正规的证明方法 首先用到一个理论：\n Schur’s Theorem: Every square matrix factors into $A=QTQ^{-1}$ where T is upper trangular and $\\bar{Q}^T=Q^{-1}$ If A has real eigenvalues the Q and T can be chosen real:$Q^TQ=I$\n 这个定理给出了详细的证明，但比较复杂，我试着简单的证明了一下（如果有不严谨的地方，请各位指出）: pf: 对于一个矩阵A，它代表的是A的列空间的基的矩阵，我们可以把它进行 $QR$ 分解，得到以Q为基（正交基）的同样的子空间，那么 $AQ$ 将还是这个子空间的基，所以我们可以得到： $$ AQ=QT\\ so: A=QTQ^{-1} $$ 上述过程中T是和R类似的上三角矩阵,对于所有方阵成立。 也就是说如果我们把Schur 定理稍微进行变形: $$ A^T=Q^TT^TQ\\ when:\\ A^T=A\\ T^T=T $$ 因为T是三角矩阵，所以当它也是对称矩阵的时候，必然是对角的。QED 以上是我刚发明的简单的证明方法，对于Schur’s Theorem 我们需要找的就是是否存在T使得 $AQ=QT$ 成立，其中Q是正交矩阵（研究证明题必须先把目标搞明白，别看了一路都是对的，就是不知道要干嘛，我刚才就犯了这个毛病，看哪句都是真命题，然后迷迷糊糊不知道要证明啥），下面描述下书上的方法： 我们要寻找Q，满足$AQ=QT$ 观察T是上三角矩阵，所以第一列只有一个元素$t_{11}$ 其中 $q_1$ 是Q的第一列,也就是说 $Aq_1=t_{11}q_1$ (这个要是不明白就自己拿笔画个矩阵比划一下子，就知道了)\n$$ \\bar{Q}^T_1AQ_1= \\begin{bmatrix} \\bar{q}^T_1\\ \\vdots \\ \\bar{q}n^T \\end{bmatrix} \\begin{bmatrix} \u0026\u0026\\ Aq_1\u0026\\dots\u0026Aq_n\\ \u0026\u0026 \\end{bmatrix}= \\begin{bmatrix} t{11}\u0026\\dots\u0026\\dots\u0026\\dots \\ 0\u0026\u0026\u0026 \\ 0\u0026\u0026A_2\u0026\\ 0\u0026\u0026\u0026 \\end{bmatrix} $$ 观察上面的整个过程$Aq_1=t_{11}q_1$ 其实是A的特征值和特征向量，那么$q_2\\dots q_n$这些值怎么确定？答案是无所谓，只要找一组和$q_1$ 都正交的基就可以填充出其他部分（因为$t_{12}\\dots t_{1n}$ $q_1\\dots q_n$ 相乘得到A的其他部分，使得等式成立），我们的目的是为了找T，所以第一步我们算是迈出去了，下一步就是找$A_2$ 中的T了，根据假设，我们可以认为 $A_2Q_2=Q_2T_2$ 这里面的矩阵都比上一步少小一号(size=n-1)递归调用上面的证明方法，可以得出$Q_2$ , $t_{22}$ 以及$A_3$ 如此递归下去可以找到所有A的第一个特征向量，然后组合成Q，T $$ Q=Q_1\\begin{bmatrix}1\u00260\\0\u0026Q_2\\end{bmatrix}\\ T=\\begin{bmatrix}t_{11}\u00260\\0\u0026T_2\\end{bmatrix}\\ AQ=QT $$ 以上可证存在T使得 $AQ=QT$ 成立，那么如果A是symmetric的，那么: $$ A=QTQ^T\\ A^T=Q^TT^TQ\\ A=A^T\\ so:\\ T=T^T $$ T是上三角矩阵，得出结论，T是对角矩阵 QED 上面这一小段其实在证明Schur定理，因为Schur定理一旦得到证明，那么自然可以得到我们想要的结论，所有对称矩阵可以被对角化，Schur矩阵以复数形式给出，因为我们前面已经证明了对称矩阵的特征值都是实数，所以这里可以用实数表达，当然复数是对于非对称矩阵的，因为非对称实数矩阵可能得到复数特征值和特征向量。\nConclusion 这篇文章扎扎实实写了24小时，而且中间确实有不太清楚的地方，至今没动，所以都高亮标注了，提醒读者也提醒自己要来填坑，schur定理的证明方法还有别的，这个是Prof. Strang 书上的方法，后面如果有新发现继续补充。\n","wordCount":"455","inLanguage":"en","datePublished":"2017-11-22T15:18:03Z","dateModified":"2023-04-04T15:19:02+08:00","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-4.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a>&nbsp;»&nbsp;<a href=https://go.face2ai.com/math/>Maths</a></div><h1 class=post-title>【线性代数】6-4:对称矩阵(Symmetric Matrices)</h1><div class=post-meta><span title="2017-11-22 15:18:03 +0000 UTC">November 22, 2017</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5 aria-label=对称矩阵>对称矩阵</a><ul><li><a href=#%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5-symmetric-matrices aria-label="对称矩阵 (Symmetric Matrices)">对称矩阵 (Symmetric Matrices)</a></li><li><a href=#%e5%ae%9e%e6%95%b0%e7%9f%a9%e9%98%b5%e7%9a%84%e5%a4%8d%e7%89%b9%e5%be%81%e5%80%bccomplex-eigenvalues-of-real-matrix aria-label="实数矩阵的复特征值(Complex Eigenvalues of Real Matrix)">实数矩阵的复特征值(Complex Eigenvalues of Real Matrix)</a></li><li><a href=#eigenvalues-%e5%92%8c-pivots aria-label="Eigenvalues 和 Pivots">Eigenvalues 和 Pivots</a></li><li><a href=#%e6%89%80%e6%9c%89%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%e5%8f%af%e4%bb%a5%e5%af%b9%e8%a7%92%e5%8c%96all-symmetric-matrices-are-diagonalizable aria-label="所有对称矩阵可以对角化(All Symmetric Matrices are Diagonalizable)">所有对称矩阵可以对角化(All Symmetric Matrices are Diagonalizable)</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质
<strong>Keywords:</strong> Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem</p><h1 id=对称矩阵>对称矩阵<a hidden class=anchor aria-hidden=true href=#对称矩阵>#</a></h1><p>这几篇在难度上确实要比前面的内容大很多，所以看书理解和总结都变得不那么流畅了，但是慢慢看下来收获还是有很大的，而且我发现不管学的多认真，还是会有遗漏，所以我觉得之前的想法就是一次性把什么什么学会是不可能的，只能学到自己觉得达到自己能发现的最大限度，等到应用之时还是要回来查阅，这样往往会有进一步的更大发现，</p><h2 id=对称矩阵-symmetric-matrices>对称矩阵 (Symmetric Matrices)<a hidden class=anchor aria-hidden=true href=#对称矩阵-symmetric-matrices>#</a></h2><p>对称矩阵我们在最早的知识里面就学过 $A^T=A$ 的矩阵叫做<a href=http://face2ai.com/Math-Linear-Algebra-Chapter-4-2/>对称矩阵</a>，我们也学过<a href=http://face2ai.com/Math-Linear-Algebra-Chapter-4-2/>投影矩阵</a>,但是当时我们并没有强调过一点就是投影矩阵都是对称的，这个性质今天在这里会有很大的用途。
我们继续说投影矩阵，所谓投影矩阵，就是在和向量 $\vec{c}$ 相乘的时候，投影到矩阵A的列空间内，那么其中，投影 $p$ 和 原向量 $\vec{c}$ 的差 $\vec{e} =\vec{c}-\vec{p}$ 与子空间正交。</p><p>举个例子，在三维空间内，A的列空间是一个二维平面那么，A对应的投影矩阵P能够把任何方向的向量投影到平面上，那么如果向量本身属于平面那么 $Px=x$ 显然是不用质疑的（我们之前在投影那篇文章中也讲过） 但是，同志们，看看这个有木有很面熟啊，这个明显就是投影矩阵 $P$ 的特征值和特征向量么？没错，$P$ 有一个平面的特征向量，可以随便选！能选多少个呢、当然是无数个，但是问题又来了，这无数多个并不是独立的，因为一共就二维，选出来三个线性独立的向量都是不可能的，所以这个平面能选出两个线性独立的特征向量，并且对应的特征值都是1，这里有人可能疑惑为啥要选两个，因为我们<a href=http://face2ai.com/Math-Linear-Algebra-Chapter-6-2/>6-2</a>的时候说过只有特诊向量足够的情况下才能对角化，投影矩阵明显是个3x3的矩阵，那么特征向量也应该有三个呀！我们的子空间是二维的，所以理论上应该有两个特征向量在上面，剩下一维存在一个，那么这一个也能很好找，$\vec{e}$ 就是 也就是和子空间正交的向量都行 $Px=0x$ 表明 $\vec{x}$ 和子空间正交，那么这是个特征值为0的特征向量，这样我们又进一步规范一下，选择三个特征向量相互正交，这个也是可以做到的，也就是对于矩阵P我们找到了三个相互正交的特征向量，并且长度缩放到单位长度。</p><p>以上三维投影到二维平面可以通过几何来解释，但为了能让大家从线性空间来理解，就没用几何方法，大家可以自己脑补。</p><p>得出结论，对称矩阵 $P^T=P$ 的特征向量相互正交并且为单位向量。
对称矩阵"It is no exaggeration to say that these are the most important matrices the world will ever see &ndash; in the theory of linear algebra and alos in the applications" 翻译成中文：“对称矩阵是史上最牛B的矩阵，无论在理论还是应用”
这个我们目前还无法考证，还没做过应用呢？不是么，但是我知道PCA中确实用了对称矩阵，SVD等一些列相关技术。
一个矩阵能被如此称赞，不外乎几点原因，首先是其本身拥有较好的性质，其次这个矩阵在自然生活中经常出现，就像正态分布，那么难的公式，却能准确的描述自然届的现象。最后就是如果表现形式简单，那么这个就是非常有用的东西啦。</p><p>下面我们开始探索对称矩阵的性质。
如果一个对称矩阵满足：
$$
suppose:\
A^T=A\
A=S\Lambda S^{-1}\
then:\
A^T=(S^{-1})^T\Lambda^T S^T
$$
这种情况下就有下面这种<em><strong>可能</strong></em>了，也就是对应的 $S=(S^{-1})^T$ 注意我们这里说的是可能，并不排除不可能的情况，原文书上用的也是possibly，也就是说我们目前假设:
$$
S^T=S^{-1}\
S^TS=I
$$
这里我们可以预报一下：</p><blockquote><ol><li>对称矩阵只有实数特征值</li><li>对称矩阵特征向量可以选择正交单位向量 <em>orthonormal</em></li></ol></blockquote><p>对于 $S^TS=I$ 面熟么？还有印象么？我们认识啊，<a href=http://face2ai.com/Math-Linear-Algebra-Chapter-4-4/>正交矩阵</a>
$Q^TQ=I$ 矩阵Q中每列之间相互正交，也就是我们对于对称矩阵可以写成：
$$
A=Q\Lambda Q^{-1}=Q\Lambda Q^T\
with:\
Q^{-1}=Q^T
$$</p><p>这个就是著名的普定理 &ldquo;Spectral Theorem&rdquo;:</p><blockquote><p>Every symmetric matrix has the factorization $A=Q\Lambda Q^T$ with real eigenvalues in $\Lambda$ and orthonormal eigenvectors in $S=Q$</p></blockquote><p>对于所有对称矩阵都能分解成 $A=Q\Lambda Q^T$ 的形式并且在 $\Lambda$ 中的所有特征值都是实数，其对应的特征向量是正交单位矩阵，即 $S=Q$</p><p>普定理在数学中很重要，对应的主轴定理 “Principle Axis Theorem”在集合物理中有重要地位。</p><p>这个定理对于从后到前 $A=Q\Lambda Q^T$ 很好证明，但是对于对称矩阵的特征值都是实数和特征向量相互正交比较难证明，也就是说，对角化时，特征向量矩阵是正交矩阵的时候很容易证明原始矩阵是对称的，但是对称矩阵不太容易证明，可以对角化，并且对角化的特征矩阵 $S$ 是正交矩阵 $Q$</p><p><em><strong>插播一句，orthonormal，翻译成中文是正交，而且normal，因为特征向量是可以随意缩放的，所以主要强调正交性</strong></em>
我们接下来要做的伟大的证明，分为下面三步来证明：</p><ol><li>首先举个例子，来展示下特征值都是实数，特征向量orthonormal</li><li>当特征值不重复的时候的证明</li><li>当特征值重复的时候的证明</li></ol><hr><p>我们来先看个🌰：
$$
A=\begin{bmatrix}1&2\2&4\end{bmatrix}\
\begin{vmatrix}1-\lambda &2\2&4-\lambda\end{vmatrix}=0\
\lambda^2-5\lambda=0\
\lambda_1=0\
\lambda_2=5\
x_1=\begin{bmatrix}2\newline -1\end{bmatrix}\
x_2=\begin{bmatrix}1\newline 2\end{bmatrix}
$$
可以看出$x_1$ 属于矩阵的nullspace，而 $x_2$ 属于矩阵的column space，但是我们学四个子空间的时候有nullspace和rowspace是正交的，但是这里的 $x_2$ 属于矩阵的column space，为什么呢？因为矩阵实对称的，所以对称矩阵的rowspace和columnspace一致。
$$
Q^{-1}AQ=
\frac{1}{\sqrt{5}}
\begin{bmatrix}2&1\newline -1&2\end{bmatrix}
\begin{bmatrix}1&2\2&4\end{bmatrix}
\frac{1}{\sqrt{5}}
\begin{bmatrix}2&1\newline -1&2\end{bmatrix}=
\begin{bmatrix}0&0\newline 0&5\end{bmatrix}=
\Lambda
$$
这就是个简单的例子，但是证明全体元素成立不能靠举一个例子来证明，但证明不成立可以靠举个反例来证明不成立。</p><hr><p>下面证明 <em>所有实数对称矩阵特征值都是实数</em></p><p>怎么证明一个数是实数呢，只能用点实数的性质，实数的性质不少但是能证明实数是实数的不多，可以想到一个就是实数的共轭是其本身，这也可以说是复数的性质，证明是实数也就是说证明不是复数。
复数的共轭
$$
\lambda=a+bi\
\bar{\lambda}=a-bi
$$</p><p>根据复数的性质，以及向量的基本计算，对于实数矩阵A，满足：</p><p>$$
Ax=\lambda x\
\bar{Ax}=\bar{\lambda x}\
A\bar{x}=\bar{\lambda}\bar{x}\
Transpose:\
\bar{x}^TA=\bar{x}^T\bar{\lambda}\
for:\
\bar{x}^TAx=\bar{x}^T\lambda x\
\bar{x}^TAx=\bar{x}^T\bar{\lambda}x\
and:\
\bar{x}^Tx=|x|^2>0\
so:\
\bar{\lambda}=\lambda
$$</p><p>QED</p><p>整个过程证明了特征值的共轭等于原特征值，故特征值是实数被证明了。
证明思路就是通过构造出 $\bar{\lambda}=\lambda$ 的结构来证明，主要用到了A是实数矩阵的性质，通过转置和乘法等来完成这个过程。
$(A-\lambda I)x=0$ 可以得出既然特征值是实数，A也是实数，那么x肯定是实数（实数没办法仅通过乘法加法得出复数）</p><hr><p>下面证明 <em>所有实数对称矩阵特征向量相互正交,当特征值不相等的时候</em></p><p>$$
suppose:\
Ax=\lambda_1x\
Ay=\lambda_2y\
A^T=A\
(Ax)^T=\lambda_1x^T\
(Ax)^Ty=x^TAy=\lambda_1x^Ty\
x^TAy=x^T\lambda_2 y\
then:\
\lambda_1x^Ty=x^T\lambda_2 y\
for:\
\lambda_1\neq \lambda_2\
so:\
x^Ty=0
$$
QED</p><hr><p>插播一个小栗子：
$$
A=Q\Lambda Q^T=
\begin{bmatrix}x_1 &x_2\end{bmatrix}
\begin{bmatrix}\lambda_1 &\&\lambda_2\end{bmatrix}
\begin{bmatrix}x_1^T \x_2^T\end{bmatrix}=\lambda_1x_1x_1^T+\lambda_2x_2x_2^T
$$
$A=\lambda_1x_1x_1^T+\lambda_2x_2x_2^T$ 把A写成了两个rank=1的矩阵的线性组合，并且这个rank=1的矩阵还是投影矩阵，当然也是对称矩阵，是不是很神奇，这个矩阵分解是比较有意思的，基是矩阵，而且基实对称的rank=1的投影矩阵，是不是很多头衔啊，头衔越多越厉害，不信你去看看大大有多少头衔。这个投影矩阵可以理解为投影到特征向量组成的空间(Eigenspace)</p><hr><p>其实写到这我有点疑惑了，本来写博客一个是自己总结，另一个是给大家一个参考，但是当我写了上面那个头衔了以后，我发现，如果没有基础或者不从头看起，直接看本文可能会感到疑惑，这也是知识的一个性质，就是连续性和扩展性，那么没有基础，基本都是空中楼阁（这种大牛太多了，基础没用，直接上算法的比比皆是，我以前也是，我现在改邪归正了）</p><h2 id=实数矩阵的复特征值complex-eigenvalues-of-real-matrix>实数矩阵的复特征值(Complex Eigenvalues of Real Matrix)<a hidden class=anchor aria-hidden=true href=#实数矩阵的复特征值complex-eigenvalues-of-real-matrix>#</a></h2><p>本文主要说对称矩阵的特征值，特征向量，对称矩阵的特征值和特征向量一定是实数，我们上面基本都为证明这个结论，那么什么样的矩阵会产生复数特征值特征向量呢？
如果一个矩阵包含一个复数特征值，那么一定是成对出现的，所谓成对就是如果 $\lambda=a+bi$ 是一个特征值，那么 $\bar{\lambda}=a-bi$ 也一定是A的一个特征值，证明：
$$
Ax=\lambda x\
A\bar{x}=\bar{\lambda x}=\bar{\lambda} \bar{x}
$$
上面的取共轭操作hexo渲染有点问题，在两个字母中间的$\bar{\lambda x}$ 其实是个长的,他画的有点短
我们后面有一章专门介绍复数矩阵，所以这里有点迷糊的不要紧，放过自己，继续看下面。</p><h2 id=eigenvalues-和-pivots>Eigenvalues 和 Pivots<a hidden class=anchor aria-hidden=true href=#eigenvalues-和-pivots>#</a></h2><p>Pivots是我们这章之前主要研究的对象，因为我们主要研究的是矩阵用于方程组的求解，而本章开始Eigenvalues的研究，那么我们的惯性思维就是，既然是一个体系下的知识重点，那么他们有联系么？</p><blockquote><p>product of pivots = determinant = product of Eigenvalues</p></blockquote><p>这个是个特殊的性质，结合前面trace的性质我们知道了特征值和矩阵相关的两个算数性质，但是这个具体的证明我还没学会，包括下面的这个结论，证明我也没看懂。</p><blockquote><p>The number of positive eigenvalues of $A=A^T$ equals the number of positive Pivots</p></blockquote><p>这个结论书上给出了证明，但是说实话，我没明白，所以这个地方必须先留个空白，把书上的东西抄过来没意义</p><p><em><strong>上述两个结论不会证明，后续补上，此处留坑</strong></em></p><p>这就是pivots和eigenvalues可以通过上述两个结论产生联系，不过联系应该也就这么多了。</p><h2 id=所有对称矩阵可以对角化all-symmetric-matrices-are-diagonalizable>所有对称矩阵可以对角化(All Symmetric Matrices are Diagonalizable)<a hidden class=anchor aria-hidden=true href=#所有对称矩阵可以对角化all-symmetric-matrices-are-diagonalizable>#</a></h2><p>接下来我们要证明最后一个结论，就是在有重复特征值的情况下，对称矩阵依然可以被对角化，其实本文第一个例子中就包含相同的特征值，两个 $\lambda=1$ 虽然如此我们还是在平面中找到了两个相互正交的特征向量，一个特殊的例子没办法证明全部情况，下面我们系统证明一下：
正式的证明之前有个小trick,就是给矩阵对角线上的每个元素加一个扰动 $nc$ 这样所有的特征值不同（<em><strong>具体为啥我也没想明白,有明白人请指教一下</strong></em>）所有有不同的特征向量，当 $c\to 0$ 得到原始特征值，和一组不同特征向量（这个思路是Prof. Strang写在书上的，他说这个有点不严谨，但是I am sure this is true）
接下来将正规的证明方法
首先用到一个理论：</p><blockquote><p><strong>Schur&rsquo;s Theorem</strong>: Every square matrix factors into $A=QTQ^{-1}$ where T is upper trangular and $\bar{Q}^T=Q^{-1}$ If A has real eigenvalues the Q and T can be chosen real:$Q^TQ=I$</p></blockquote><p>这个定理给出了详细的证明，但比较复杂，我试着简单的证明了一下（如果有不严谨的地方，请各位指出）:
pf:
对于一个矩阵A，它代表的是A的列空间的基的矩阵，我们可以把它进行 $QR$ 分解，得到以Q为基（正交基）的同样的子空间，那么 $AQ$ 将还是这个子空间的基，所以我们可以得到：
$$
AQ=QT\
so:
A=QTQ^{-1}
$$
上述过程中T是和R类似的上三角矩阵,对于所有方阵成立。
也就是说如果我们把Schur 定理稍微进行变形:
$$
A^T=Q^TT^TQ\
when:\
A^T=A\
T^T=T
$$
因为T是三角矩阵，所以当它也是对称矩阵的时候，必然是对角的。QED
以上是我刚发明的简单的证明方法，对于Schur&rsquo;s Theorem 我们需要找的就是是否存在T使得 $AQ=QT$ 成立，其中Q是正交矩阵（研究证明题必须先把目标搞明白，别看了一路都是对的，就是不知道要干嘛，我刚才就犯了这个毛病，看哪句都是真命题，然后迷迷糊糊不知道要证明啥），下面描述下书上的方法：
我们要寻找Q，满足$AQ=QT$
观察T是上三角矩阵，所以第一列只有一个元素$t_{11}$ 其中 $q_1$ 是Q的第一列,也就是说 $Aq_1=t_{11}q_1$ (这个要是不明白就自己拿笔画个矩阵比划一下子，就知道了)</p><p>$$
\bar{Q}^T_1AQ_1=
\begin{bmatrix}
\bar{q}^T_1\
\vdots \
\bar{q}<em>n^T
\end{bmatrix}
\begin{bmatrix}
&&\
Aq_1&\dots&Aq_n\
&&
\end{bmatrix}=
\begin{bmatrix}
t</em>{11}&\dots&\dots&\dots \
0&&& \
0&&A_2&\
0&&&
\end{bmatrix}
$$
观察上面的整个过程$Aq_1=t_{11}q_1$ 其实是A的特征值和特征向量，那么$q_2\dots q_n$这些值怎么确定？答案是无所谓，只要找一组和$q_1$ 都正交的基就可以填充出其他部分（因为$t_{12}\dots t_{1n}$ $q_1\dots q_n$ 相乘得到A的其他部分，使得等式成立），我们的目的是为了找T，所以第一步我们算是迈出去了，下一步就是找$A_2$ 中的T了，根据假设，我们可以认为 $A_2Q_2=Q_2T_2$ 这里面的矩阵都比上一步少小一号(size=n-1)递归调用上面的证明方法，可以得出$Q_2$ , $t_{22}$ 以及$A_3$
如此递归下去可以找到所有A的第一个特征向量，然后组合成Q，T
$$
Q=Q_1\begin{bmatrix}1&0\0&Q_2\end{bmatrix}\
T=\begin{bmatrix}t_{11}&0\0&T_2\end{bmatrix}\
AQ=QT
$$
以上可证存在T使得 $AQ=QT$ 成立，那么如果A是symmetric的，那么:
$$
A=QTQ^T\
A^T=Q^TT^TQ\
A=A^T\
so:\
T=T^T
$$
T是上三角矩阵，得出结论，T是对角矩阵
QED
上面这一小段其实在证明Schur定理，因为Schur定理一旦得到证明，那么自然可以得到我们想要的结论，所有对称矩阵可以被对角化，Schur矩阵以复数形式给出，因为我们前面已经证明了对称矩阵的特征值都是实数，所以这里可以用实数表达，当然复数是对于非对称矩阵的，因为非对称实数矩阵可能得到复数特征值和特征向量。</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>这篇文章扎扎实实写了24小时，而且中间确实有不太清楚的地方，至今没动，所以都高亮标注了，提醒读者也提醒自己要来填坑，schur定理的证明方法还有别的，这个是Prof. Strang 书上的方法，后面如果有新发现继续补充。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/%E7%89%B9%E5%BE%81%E5%80%BC/>特征值</a></li><li><a href=https://go.face2ai.com/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/>特征向量</a></li><li><a href=https://go.face2ai.com/tags/%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5/>对称矩阵</a></li><li><a href=https://go.face2ai.com/tags/%E6%8A%95%E5%BD%B1%E7%9F%A9%E9%98%B5/>投影矩阵</a></li><li><a href=https://go.face2ai.com/tags/%E8%B0%B1%E5%AE%9A%E7%90%86/>谱定理</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-4:对称矩阵(Symmetric Matrices) on twitter" href="https://twitter.com/intent/tweet/?text=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-4%3a%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%28Symmetric%20Matrices%29&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-4.zh%2f&hashtags=%e7%89%b9%e5%be%81%e5%80%bc%2c%e7%89%b9%e5%be%81%e5%90%91%e9%87%8f%2c%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%2c%e6%8a%95%e5%bd%b1%e7%9f%a9%e9%98%b5%2c%e8%b0%b1%e5%ae%9a%e7%90%86"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-4:对称矩阵(Symmetric Matrices) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-4.zh%2f&title=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-4%3a%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%28Symmetric%20Matrices%29&summary=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-4%3a%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%28Symmetric%20Matrices%29&source=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-4.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-4:对称矩阵(Symmetric Matrices) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-4.zh%2f&title=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-4%3a%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%28Symmetric%20Matrices%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-4:对称矩阵(Symmetric Matrices) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-4.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-4:对称矩阵(Symmetric Matrices) on whatsapp" href="https://api.whatsapp.com/send?text=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-4%3a%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%28Symmetric%20Matrices%29%20-%20https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-4.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-4:对称矩阵(Symmetric Matrices) on telegram" href="https://telegram.me/share/url?text=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-4%3a%e5%af%b9%e7%a7%b0%e7%9f%a9%e9%98%b5%28Symmetric%20Matrices%29&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-4.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>