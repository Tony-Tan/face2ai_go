<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) | 谭升的博客</title><meta name=keywords content="JPEG,奇异值分解,特征值,特征向量"><meta name=description content="Abstract: 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 Keywords: Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/math/math-linear-algebra-chapter-6-7.zh/><link crossorigin=anonymous href=../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)"><meta property="og:description" content="Abstract: 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 Keywords: Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/math/math-linear-algebra-chapter-6-7.zh/"><meta property="article:section" content="math"><meta property="article:published_time" content="2017-11-30T09:02:19+00:00"><meta property="article:modified_time" content="2023-04-04T15:19:02+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)"><meta name=twitter:description content="Abstract: 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 Keywords: Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Maths","item":"https://go.face2ai.com/math/"},{"@type":"ListItem","position":3,"name":"【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)","item":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-7.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)","name":"【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)","description":"Abstract: 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 Keywords: Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors\n","keywords":["JPEG","奇异值分解","特征值","特征向量"],"articleBody":"Abstract: 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 Keywords: Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors\nSVD分解 今天的废话关于学习知识，最近看到一种说法，我觉的非常的形象，有个大神（是谁我忘了），他说已知的知识像一个圆圈，而自己能感受的未知就是紧邻圆圈，圆外部的区域，当你知道的知识越来越多，圆圈不断扩大，圆周也随之扩大，所以你会越来越发现自己无知，那么就会更努力的去学习，所以越有知识的人越谦逊，尤其是对待知识上，尊重知识，探索未知领域是人类文明存在的根本动力。 ## 奇异值分解 (Singular Value Decomposition) SVD，熟悉的名字，如果不学习线性代数，直接机器学习，可能最先接触的就是SVD，所以我记得在写上个系列的博客的时候（CSDN，图像处理算法）就说到过SVD，当时还调侃了下百度，每次搜SVD出来的都是一把枪（报告政府，这个枪是穿越火线里面的，没超过1.7J） 这张分解图是我无意中发现的，ak47的发明人说过，如果一把枪，零件已经精简到最少了，那么这个才是精品，类似的意思上篇博客也说过，矩阵变换到最简单的形式，能够体现出其最重要的性质。 SVD，奇异值分解，与QR，LU，\\(S\\Lambda S^{-1}\\) 等变换类似，其经过变换后会得到一个结构特异性质非凡的矩阵，SVD分解的结果和形式与对角化都非常相似，只是在形式和思路上更复杂，或者说如果说Jordan 是矩阵的对角化的扩展，因为有些矩阵特征向量不完全，那么SVD也是对角化的扩展，因为有些矩阵并不是方的。 所以SVD也是对角化，并且拥有比 \\(A=S\\Lambda S^{-1}\\) 更完美的性质，但却是也复杂了一些，\\(A=S\\Lambda S^{-1}\\) 有以下几个问题，需要完善： 1. S中特征向量一般不是正交的，除非A是对称矩阵 2. A并不是总有足够的特征值，这个是Jordan解决的问题，多个特征值相等，其对应于一个特征向量的时候，Jordan可以写成一块一块的对角矩阵 3. A必须是方的方的方的\nSingular Vectors作为eigenvectors 的替代品，可以完美解决上述问题，但是作为代价，我们的计算过程会变得复杂，并且Singular Vectors有两组，\\(u\\) 和 \\(v\\)\n\\(u\\) 对应的是\\(AA^T\\) 的特征向量，因为 \\(AA^T\\) 对称，所以 \\(u\\) 们可以选择相互正交的一组。 同理 \\(v\\) 对应 \\(A^TA\\) 的特征向量，因为\\(A^TA\\) 对称，所以 \\(v\\) 们也可以选择相互正交的一组。 这里注意是选择，因为你也可以选择不正交的，但是不正交的可能就会很麻烦了。\n铺垫的差不多 ，然后我们有下面的这条重要性质，为什么会成立后面有证明，现在就告诉你SVD究竟是个啥子鬼： \\[ Av_1=\\sigma_1u_1\\\\ Av_2=\\sigma_2u_2\\\\ \\vdots\\\\ Av_n=\\sigma_nu_n\\\\ \\]\n\\(v_1,\\dots,v_n\\) 是\\(A^TA\\) 的特征向量，所以 \\(v\\) 是矩阵A的Row Space \\(u_1,\\dots,u_n\\) 是\\(AA^T\\) 的特征向量，所以 \\(u\\) 是矩阵A的Column Space \\(\\sigma_1,\\dots,\\sigma_n\\) 全部为正数，称为矩阵A的奇异值。\n然后下面我们把 \\(u\\) 和 \\(v\\) 组合成矩阵 \\(U\\) 和 \\(V\\) ,那么根据对称矩阵的性质，\\(U^TU=I\\) 同理 \\(V^TV=I\\) 那么接下来我们来组合一下：\n\\[ AV=U\\Sigma \\\\ A \\begin{bmatrix} \u0026\u0026\\\\ v_1\u0026\\dots\u0026v_r\\\\ \u0026\u0026 \\end{bmatrix}= \\begin{bmatrix} \u0026\u0026\\\\ u_1\u0026\\dots\u0026u_r\\\\ \u0026\u0026 \\end{bmatrix} \\begin{bmatrix} \\sigma_1\u0026\u0026\\\\ \u0026\\ddots\u0026\\\\ \u0026\u0026\\sigma_r \\end{bmatrix} \\]\n矩阵形式就是这样喽，没什么解释的，就是上面计算的组合形式，但是注意这里有个很重要的参数，\\(r\\) 没错，就是矩阵的rank，这里rank表示了矩阵A的Singular Values的数量，所以上面计算从规模上是： \\[ (m\\times n)(n\\times r)=(m\\times r)(r\\times r)\\\\ m\\times r=m\\times r \\] 从矩阵相乘的规模上也能看出等式没有问题，但是这个r有的问题，可以肯定的是，有效的Singular vector有r组，但是这样与原始矩阵形状差的有点多，那么就补一补，虽然补的都是没用的，但是也算是整齐划一了，首先 \\(\\Sigma\\) 中缺少的只能补0 ，所以对应的V就只能补A的Nullspace了，因为这样 \\(AV\\) 的补充部分是0,同理，为了配合V，U添加的是left nullspace，并且这些添加的无用值也要选择orthonormal的，以保证\\(U^TU=I\\) 和\\(V^TV=I\\)。\n其实这里隐藏了一个重要的知识点，就是四个空间的那课，矩阵的rowspace和nullspace正交column space与left nullspace正交，而V本来是A的行空间正交基，那么添加的一定是Nullspace中的正交基，以保证矩阵正交，所以完美结合，（如果忘了四个空间点击查看）\n所以更一般化的表示： \\[ AV=U\\Sigma \\\\ A \\begin{bmatrix} \u0026\u0026\\\\ v_1\u0026\\dots\u0026v_n\\\\ \u0026\u0026 \\end{bmatrix}= \\begin{bmatrix} \u0026\u0026\\\\ u_1\u0026\\dots\u0026u_m\\\\ \u0026\u0026 \\end{bmatrix} \\begin{bmatrix} \\sigma_1\u0026\u0026\u0026\\\\ \u0026\\ddots\u0026\u0026\\\\ \u0026\u0026\\sigma_r\u0026\\\\ \u0026\u0026\u0026 \\end{bmatrix} \\] 规模上是，注意 \\(\\Sigma\\) 不是方阵： \\[ (m\\times n)(n\\times n)=(m\\times m)(m\\times n)\\\\ m\\times n=m\\times n \\] \\(\\Sigma\\) 被填充成立 \\(m\\times n\\) 通过在矩阵中加入0来实现，新的矩阵U和V依旧满足 \\(V^TV=I\\)以及 \\(U^TU=I\\)\n那么我们的A就可以分解了 \\[ AV=U\\Sigma\\\\ for:\\;VV^T=I\\\\ so:\\\\ A=U\\Sigma V^T\\\\ SVD\\,\\,\\, is:\\\\ A=u_1\\sigma_1 v_1^T+\\dots+u_r\\sigma_r v_r^T \\] 其中\\(u\\) 是\\(m\\times 1\\) 的 \\(v^T\\) 是 \\(1\\times n\\) 的，所以A是 \\(m\\times n\\) 的没有问题，并且所有 \\(u_i\\sigma_r v_i^T\\) d的rank都是1，这就是Sigular Values Decomposition了，这里反复的验证规模的原因是因为A不是方阵，所以，在做乘法的时候要非常小心矩阵规模。那个小的只有r个有用值的SVD我们叫他reduced SVD（其实我觉得这个更有实际意义，毕竟这里面才有最重要的信息，新增的那些最后奇异值都是0了，也就没有啥作用了）可以表示为: \\[ A=U_r\\Sigma_r V_r^T \\] 写了这么多，我们到现在还不知道Singular是怎么计算出来的，那么我们先给出结论，后面继续证明： \\[ \\sigma_i^2=\\lambda_i \\] 其中\\(\\lambda_i\\) 是\\(A^TA\\) 和\\(AA^T\\) 的特征值。 那么要问\\(A^TA\\) 和\\(AA^T\\) 拥有相同的特征值，为什么？ 这个我真没想明白怎么证明，所以这个地方算个坑，会了再回来填\n然后我们得到Singular Values后，我们把他们按照从大到小的顺序排列，然后写成上面SVD的形式： \\[ \\sigma_1 \\geq \\sigma_2 \\geq \\sigma_3 \\dots \\geq \\sigma_n \\]\n下面举个小🌰 ： 什么时候SVD和对角化相等？ 当A是半正定或者正定矩阵的时候，\\(S=U\\) 并且 \\(S^T=V^T\\) 此时 \\(\\Lambda=\\Sigma\\) ,因为正定矩阵特征值为正，而且是对称的，所以 \\(U=V=Q\\)\n下面介绍一个应用，大应用，为什么我会把这个应用写出来呢？因为他和图像有关，所以我们可以简单实践一下，还是从感性上认识一下SVD，然后再理论上完整的证明一下。\n图像压缩 (Image Compression) 在介绍SVD之前，我们先来分析一个问题：图像的存储空间，一个图像假如是512x512的大小，灰度图像，每个像素占一个字节的话，这张图片那么会占据硬盘262144个字节，也就260多k个字节，如果按照23帧每秒，十秒钟大概要60兆，一分钟大概3.6G，在想想这个尺寸这么小，我们看的一般都是720P的，这样算的话硬盘根本不够用，一个东京热以后就再也不能一本道了，所以必须要压缩一下子，怎么压缩呢，这里介绍下JPEG的一个大致思路，就是矩阵分解： \\[ A=u_1\\sigma_1 v_1^T+\\dots+u_r\\sigma_r v_r^T\\\\ A=\\sigma_1 u_1 v_1^T+\\dots+\\sigma_r u_r v_r^T\\\\ A=\\sigma_1 S_1+\\dots+\\sigma_r S_r\\\\ where:\\,S_i=u_i v_i^T \\] 这样就是按照singular的大小，给所有singular vector排序，奇异值越大证明这个奇异值向量对原始数据影响越大，所以这两奇异向量组成的这个基就越重要，当然要提前加进去，所以我们如果选择一部分奇异值和奇异向量，比如\\(N=100\\) 也就是200个奇异向量和100个奇异值，那么一共是\\(200\\times 512+100=102500\\) 比原来的260k减少了一半，如果用的更少那么减少的更多，当然图像质量也就有所损失了，写了个python程序，可以观察一下： 使用多组S来还原原始数据，使用S越多还原度越高，但需要的存储空间就越大，S就是上面\\(u_iv_i\\)的结果，可以看做图像的一个切片，视频中第一幅为原图，第二幅为若干张S相加得到的结果，最后一张是他们之间的差，我们可以暂时理解为压缩误差。\n视频演示地址http://player.youku.com/embed/XMzE5NTIyNjQ4MA==\n代码\nimport cv2 import numpy as np from numpy import linalg import copy N=500 image=cv2.imread('lena.jpg',0) cv2.imshow('src',image) U,Sigma,V=linalg.svd(image) sigma_size=len(Sigma) image_s=[] waittime=0 for i in range(N):   if isigma_size:  break  image_s.append(U[:,i].reshape(len(U[:,i]),1)*V[i]*Sigma[i])  if i0:  image_s[i]+=image_s[i-1]  print 'total: ',i,' Singular Value'  show_image=copy.deepcopy(np.uint8(image_s[i]))  font = cv2.FONT_HERSHEY_TRIPLEX  cv2.putText(show_image, `i`, (10, 500), font, 4, (255, 255, 0), 1, False)  cv2.imshow('Compression',show_image)  cv2.imshow('Different',np.uint8(image-image_s[i]))  cv2.waitKey(waittime)  waittime=100 上面公式中每个S都是一个rank=1的矩阵，如果两个这样的矩阵相加，rank就变成了2，n个相加就是rank=n，所以SVD分解后再组合有点像切片，每一片都是有规律的，同样的道理，SVD换成小波，那就有了JPEG2000，小波的基矩阵也是满足一些特殊性质的，后面可能会将，但是不确定，所以数据压缩可以理解为最关键的一步就是两个向量相乘，能够得到一个矩阵，这个矩阵组成了基础的片，然后多个片加权求和就得到了还原数据。 SVD的应用应该非常多这里就写了一个，比较重要直观的，下面还是要继续研究原理。\n基和SVD(The Bases and the SVD) 书中并没有给出严格的证明和推到过程，老爷子还是走的启发式套路，我们来从基开始看，假设一个矩阵A是个2x2的矩阵，这样比较好计算，并且A是非奇异矩阵，也就是A可逆，rank=2，span成整个 \\(\\Re^2\\) 空间，那么我们应该可以找到两个向量正交的单位向量 \\(v_1,v_2\\) 满足 \\(u_1=\\frac{Av_1}{|Av_1|},u_2=\\frac{Av_2}{|Av_2|}\\) 使得\\(u_2\\)和\\(u_1\\) 正交，并且是单位向量，那么就有： \\[ A \\begin{bmatrix}v_1\u0026v_2\\end{bmatrix}= \\begin{bmatrix}Av_1\u0026Av_2\\end{bmatrix}= \\begin{bmatrix}|Av_1|\\cdot u_1\u0026|Av_2|\\cdot u_2\\end{bmatrix}= \\begin{bmatrix}u_1\u0026u_2\\end{bmatrix} \\begin{bmatrix}|Av_1|\u0026\\\\\u0026|Av_2|\\end{bmatrix}\\\\ AV=U\\Sigma \\] 总结下，这个构造基的过程是瞄准了目标去的，也就是说目标就是类似于构造 \\(Ax=\\alpha y\\) 形状的一种形式，但是x和y要满足不同的关系，如果相等那么就是特征值，如果不相等就可以构造出多组相互正交形成奇异值，并且通过上面的构造过程，我们可以得知奇异值等于缩放u到单位向量的缩放比例 \\(|Av|\\) 其实可以用一种更直观的方法解答SVD的存在： 假设对任意矩阵A存在分解 \\(A=U\\Sigma V^T\\) 并且其中\\(U^TU=I\\) \\(V^TV=I\\) 那么我们要证明U和V的存在即可： \\[ A^TA=(U\\Sigma V^T)^T(U\\Sigma V^T)\\\\ A^TA=V\\Sigma (U^TU) \\Sigma V^T\\\\ A^TA=V\\Sigma^2 V^T \\] 虽然A不是对称的，但因为\\(A^TA\\) 是对称矩阵，存在正交矩阵Q使得 \\(A^TA=Q\\Lambda Q^T\\)，那么这时候的V就是 \\(A^TA\\) 的Q这个是没问题的，至于 \\(\\lambda_i=\\sigma_i^2 \\geq 0\\) 成立的原因是 \\(A^TA\\) 是个正定矩阵（A中各列线性无关），或者半正定矩阵（A中各列线性相关），所以其特征值 \\(\\lambda\\) 必然非负数，所以根号后能得到奇异值，根据\\(Av_i=\\sigma u_i\\) 可以求出剩下的 \\(u_i\\) ,当然这是理论上的方法，实际上的数值计算过程中可以避免 \\(A^TA\\) 这种大规模矩阵乘法。 回忆一下正定矩阵关于椭圆的那个例子 一个2x2正定矩阵对应二维空间一个椭圆（或者圆），其正交特征矩阵Q矩阵是对椭圆轴的旋转，特征值矩阵 \\(\\Lambda\\) 是对轴的拉伸，那么我们的SVD有同样的功效，而且有过之无不及，思考： 作为\\(A^TA\\) 的正交特征矩阵\\(V\\)也是一个旋转矩阵，旋转的是圆的轴，\\(V^T\\) 当然就是反方向旋转，\\(\\Sigma\\) 是对图形的拉伸，圆的拉成长的，接着 \\(AA^T\\) 的正交特征矩阵也是旋转，整个过程如下图：\n所以一个2x2的可逆矩阵，对一个圆的操作就是先拉伸，然后旋转。图中来自Cliff Long and Tom Herm\n上面讲解了如何求V，同样的道理也可以求U， \\[ AA^T=(U\\Sigma V^T)(U\\Sigma V^T)^T\\\\ AA^T=(U\\Sigma V^T)(V\\Sigma U)\\\\ AA^T=U\\Sigma^2 U^T \\] U是\\(AA^T\\) 的正交特征矩阵，也就是说同一个 \\(\\Sigma^2\\) 既是 \\(AA^T\\) 又是 \\(A^TA\\) 的特征值，所以上面那个疑问也得到了证明。 我们重新梳理一下这个证明过程，我们首先假设结论成立，来找到使结论成立的条件，也就是V和U矩阵，结果很理想的，我们找到了，所以原来结论成立，成立的条件就是我们刚找到的这两个矩阵（如果算上奇异值，可以说是三个矩阵），思考过程可以通过2x2构造那里来推出简单情况下，来验证我们的结论，然后再推广到任意矩阵。 至此SVD的基本来路我们已经算是摸着门了，所以可以深入开发下V和U矩阵，这两个矩阵是方阵，那么组成这些矩阵的向量门也是很有来历的，总结个表格，前面在第一部分有说过，就是reduced SVD那部分，这里在啰嗦一边：\n  Number in which Matrix(column) in which Subspace    r V rowspcae of A  n-r V nullspace of A  r U column space of A  m-r U nullspace of \\(A^T\\)    前r列对应的是 \\(A^TA\\) 和 \\(AA^T\\) 的特征向量，因为其间的正交关系，所以SVD是没有什么问题的。 严格的证明： \\[ A^TAv_i=\\sigma_i^2v_i \\] 这个是可以作为条件使用的，其成立的必然原因是 \\(A^TA\\) 是正定或半正定矩阵，所以必然存在n个大于等于0的实数特征值，这样可以得到\\(v_i,\\sigma_i\\) ，在这个条件下我们目标是证明：存在\\(u_i\\) 使得\\(Av_i=\\sigma_i u_i\\) 成立\n对条件两边同时乘上 \\(v_i^T\\) 后： \\[ v_i^TA^TAv_i=\\sigma_i^2v_i^Tv_i\\\\ ||Av_i||^2=\\sigma_i^2 so:\\\\ ||Av_i||=\\sigma_i\\\\ \\] 对条件两边同时乘A得到： \\[ AA^TAv_i=\\sigma_i^2Av_i\\\\ set\\,unit\\,vector:\\;u_i=\\frac{Av_i}{\\sigma}\\\\ AA^Tu_i=\\sigma_i^2 u_i\\\\ \\] 上面两个过程及其精妙，尤其是下面这个，用\\(u_i=\\frac{Av_i}{\\sigma}\\) 进行置换后，得到\\(u_i\\) 是\\(AA^T\\) 的特征向量，然后得出结论，存在 \\(u_i=\\frac{Av_i}{\\sigma}\\) 使得命题成立，而且u就是\\(AA^T\\)的特征向量，并且相互正交： 下面证明u相互正交: \\[ u_i^Tu_j=(Av_i)^T(Av_j)=v_i^T(A^TAv_j)=v_i^T(\\sigma^2v_j)=0 \\] QED\n然后老爷子在书上发话了，这是这本书最高潮的部分了，也是基础理论的最后一步了，因为他用到了所有的前面的理论： 1. 四个子空间的维度（我们上面那个表） 2. 正交 3. 正交基来对角化矩阵A 4. 最后得到SVD \\(A=U\\Sigma V^T\\)\n于是我写了一天这篇博客，比之前看书收获了更多，也可能有写纰漏，难免的因为水平有限，而且这个是比较精髓的部分，自然难度也比较大\n搜索网络(Searching the Web) 这里讲了个应用，但是我实在不想写了，已经精疲力尽了，所以我打算后面选几个应用写，这个作为候选，这里略 ## Conclusion 线性代数的高潮算是来了，但是后面还有一个比较有意思的主题，也是很常用的，叫做线性变换，也可以作为切入线性代数的一个切入点，我们这个系列的博客是从线性方程组开始的，当然也可以通过线性变换引出矩阵，我们下一篇来讲解这个，待续。。\n","wordCount":"521","inLanguage":"en","datePublished":"2017-11-30T09:02:19Z","dateModified":"2023-04-04T15:19:02+08:00","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-7.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a>&nbsp;»&nbsp;<a href=https://go.face2ai.com/math/>Maths</a></div><h1 class=post-title>【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)</h1><div class=post-meta><span title="2017-11-30 09:02:19 +0000 UTC">November 30, 2017</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#svd%e5%88%86%e8%a7%a3 aria-label=SVD分解>SVD分解</a><ul><li><a href=#%e5%9b%be%e5%83%8f%e5%8e%8b%e7%bc%a9-image-compression aria-label="图像压缩 (Image Compression)">图像压缩 (Image Compression)</a></li><li><a href=#%e5%9f%ba%e5%92%8csvdthe-bases-and-the-svd aria-label="基和SVD(The Bases and the SVD)">基和SVD(The Bases and the SVD)</a></li><li><a href=#%e6%90%9c%e7%b4%a2%e7%bd%91%e7%bb%9csearching-the-web aria-label="搜索网络(Searching the Web)">搜索网络(Searching the Web)</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 <strong>Keywords:</strong> Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors</p><h1 id=svd分解>SVD分解<a hidden class=anchor aria-hidden=true href=#svd分解>#</a></h1><p>今天的废话关于学习知识，最近看到一种说法，我觉的非常的形象，有个大神（是谁我忘了），他说已知的知识像一个圆圈，而自己能感受的未知就是紧邻圆圈，圆外部的区域，当你知道的知识越来越多，圆圈不断扩大，圆周也随之扩大，所以你会越来越发现自己无知，那么就会更努力的去学习，所以越有知识的人越谦逊，尤其是对待知识上，尊重知识，探索未知领域是人类文明存在的根本动力。 ## 奇异值分解 (Singular Value Decomposition) SVD，熟悉的名字，如果不学习线性代数，直接机器学习，可能最先接触的就是SVD，所以我记得在写上个系列的博客的时候（CSDN，图像处理算法）就说到过SVD，当时还调侃了下百度，每次搜SVD出来的都是一把枪（报告政府，这个枪是穿越火线里面的，没超过1.7J） <img src=./SVD-1.jpg></p><p>这张分解图是我无意中发现的，ak47的发明人说过，如果一把枪，零件已经精简到最少了，那么这个才是精品，类似的意思上篇博客也说过，矩阵变换到最简单的形式，能够体现出其最重要的性质。 SVD，奇异值分解，与QR，LU，<span class="math inline">\(S\Lambda S^{-1}\)</span> 等变换类似，其经过变换后会得到一个结构特异性质非凡的矩阵，SVD分解的结果和形式与对角化都非常相似，只是在形式和思路上更复杂，或者说如果说Jordan 是矩阵的对角化的扩展，因为有些矩阵特征向量不完全，那么SVD也是对角化的扩展，因为有些矩阵并不是方的。 所以SVD也是对角化，并且拥有比 <span class="math inline">\(A=S\Lambda S^{-1}\)</span> 更完美的性质，但却是也复杂了一些，<span class="math inline">\(A=S\Lambda S^{-1}\)</span> 有以下几个问题，需要完善： 1. S中特征向量一般不是正交的，除非A是对称矩阵 2. A并不是总有足够的特征值，这个是Jordan解决的问题，多个特征值相等，其对应于一个特征向量的时候，Jordan可以写成一块一块的对角矩阵 3. A必须是方的方的方的</p><p>Singular Vectors作为eigenvectors 的替代品，可以完美解决上述问题，但是作为代价，我们的计算过程会变得复杂，并且Singular Vectors有两组，<span class="math inline">\(u\)</span> 和 <span class="math inline">\(v\)</span></p><p><span class="math inline">\(u\)</span> 对应的是<span class="math inline">\(AA^T\)</span> 的特征向量，因为 <span class="math inline">\(AA^T\)</span> 对称，所以 <span class="math inline">\(u\)</span> 们可以选择相互正交的一组。 同理 <span class="math inline">\(v\)</span> 对应 <span class="math inline">\(A^TA\)</span> 的特征向量，因为<span class="math inline">\(A^TA\)</span> 对称，所以 <span class="math inline">\(v\)</span> 们也可以选择相互正交的一组。 这里注意是选择，因为你也可以选择不正交的，但是不正交的可能就会很麻烦了。</p><p>铺垫的差不多 ，然后我们有下面的这条重要性质，为什么会成立后面有证明，现在就告诉你SVD究竟是个啥子鬼： <span class="math display">\[
Av_1=\sigma_1u_1\\
Av_2=\sigma_2u_2\\
\vdots\\
Av_n=\sigma_nu_n\\
\]</span></p><p><span class="math inline">\(v_1,\dots,v_n\)</span> 是<span class="math inline">\(A^TA\)</span> 的特征向量，所以 <span class="math inline">\(v\)</span> 是矩阵A的Row Space <span class="math inline">\(u_1,\dots,u_n\)</span> 是<span class="math inline">\(AA^T\)</span> 的特征向量，所以 <span class="math inline">\(u\)</span> 是矩阵A的Column Space <span class="math inline">\(\sigma_1,\dots,\sigma_n\)</span> 全部为正数，称为矩阵A的奇异值。</p><p>然后下面我们把 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(v\)</span> 组合成矩阵 <span class="math inline">\(U\)</span> 和 <span class="math inline">\(V\)</span> ,那么根据对称矩阵的性质，<span class="math inline">\(U^TU=I\)</span> 同理 <span class="math inline">\(V^TV=I\)</span> 那么接下来我们来组合一下：</p><p><span class="math display">\[
AV=U\Sigma \\
A
\begin{bmatrix}
&&\\
v_1&\dots&v_r\\
&&
\end{bmatrix}=
\begin{bmatrix}
&&\\
u_1&\dots&u_r\\
&&
\end{bmatrix}
\begin{bmatrix}
\sigma_1&&\\
&\ddots&\\
&&\sigma_r
\end{bmatrix}
\]</span></p><p>矩阵形式就是这样喽，没什么解释的，就是上面计算的组合形式，但是注意这里有个很重要的参数，<span class="math inline">\(r\)</span> 没错，就是矩阵的rank，这里rank表示了矩阵A的Singular Values的数量，所以上面计算从规模上是： <span class="math display">\[
(m\times n)(n\times r)=(m\times r)(r\times r)\\
m\times r=m\times r
\]</span> 从矩阵相乘的规模上也能看出等式没有问题，但是这个r有的问题，可以肯定的是，有效的Singular vector有r组，但是这样与原始矩阵形状差的有点多，那么就补一补，虽然补的都是没用的，但是也算是整齐划一了，首先 <span class="math inline">\(\Sigma\)</span> 中缺少的只能补0 ，所以对应的V就只能补A的Nullspace了，因为这样 <span class="math inline">\(AV\)</span> 的补充部分是0,同理，为了配合V，U添加的是left nullspace，并且这些添加的无用值也要选择orthonormal的，以保证<span class="math inline">\(U^TU=I\)</span> 和<span class="math inline">\(V^TV=I\)</span>。</p><p>其实这里隐藏了一个重要的知识点，就是四个空间的那课，矩阵的rowspace和nullspace正交column space与left nullspace正交，而V本来是A的行空间正交基，那么添加的一定是Nullspace中的正交基，以保证矩阵正交，所以完美结合，（如果忘了四个空间<a href=http://face2ai.com/Math-Linear-Algebra-Chapter-4-1/>点击查看</a>）</p><p>所以更一般化的表示： <span class="math display">\[
AV=U\Sigma \\
A
\begin{bmatrix}
&&\\
v_1&\dots&v_n\\
&&
\end{bmatrix}=
\begin{bmatrix}
&&\\
u_1&\dots&u_m\\
&&
\end{bmatrix}
\begin{bmatrix}
\sigma_1&&&\\
&\ddots&&\\
&&\sigma_r&\\
&&&
\end{bmatrix}
\]</span> 规模上是，注意 <span class="math inline">\(\Sigma\)</span> 不是方阵： <span class="math display">\[
(m\times n)(n\times n)=(m\times m)(m\times n)\\
m\times n=m\times n
\]</span> <span class="math inline">\(\Sigma\)</span> 被填充成立 <span class="math inline">\(m\times n\)</span> 通过在矩阵中加入0来实现，新的矩阵U和V依旧满足 <span class="math inline">\(V^TV=I\)</span>以及 <span class="math inline">\(U^TU=I\)</span></p><p>那么我们的A就可以分解了 <span class="math display">\[
AV=U\Sigma\\
for:\;VV^T=I\\
so:\\
A=U\Sigma V^T\\
SVD\,\,\, is:\\
A=u_1\sigma_1 v_1^T+\dots+u_r\sigma_r v_r^T
\]</span> 其中<span class="math inline">\(u\)</span> 是<span class="math inline">\(m\times 1\)</span> 的 <span class="math inline">\(v^T\)</span> 是 <span class="math inline">\(1\times n\)</span> 的，所以A是 <span class="math inline">\(m\times n\)</span> 的没有问题，并且所有 <span class="math inline">\(u_i\sigma_r v_i^T\)</span> d的rank都是1，这就是Sigular Values Decomposition了，这里反复的验证规模的原因是因为A不是方阵，所以，在做乘法的时候要非常小心矩阵规模。那个小的只有r个有用值的SVD我们叫他reduced SVD（其实我觉得这个更有实际意义，毕竟这里面才有最重要的信息，新增的那些最后奇异值都是0了，也就没有啥作用了）可以表示为: <span class="math display">\[
A=U_r\Sigma_r V_r^T
\]</span> 写了这么多，我们到现在还不知道Singular是怎么计算出来的，那么我们先给出结论，后面继续证明： <span class="math display">\[
\sigma_i^2=\lambda_i
\]</span> 其中<span class="math inline">\(\lambda_i\)</span> 是<span class="math inline">\(A^TA\)</span> 和<span class="math inline">\(AA^T\)</span> 的特征值。 那么要问<span class="math inline">\(A^TA\)</span> 和<span class="math inline">\(AA^T\)</span> 拥有相同的特征值，为什么？ <strong><em>这个我真没想明白怎么证明，所以这个地方算个坑，会了再回来填</em></strong></p><p>然后我们得到Singular Values后，我们把他们按照从大到小的顺序排列，然后写成上面SVD的形式： <span class="math display">\[
\sigma_1 \geq \sigma_2 \geq \sigma_3 \dots \geq \sigma_n
\]</span></p><p>下面举个小🌰 ： 什么时候SVD和对角化相等？ 当A是半正定或者正定矩阵的时候，<span class="math inline">\(S=U\)</span> 并且 <span class="math inline">\(S^T=V^T\)</span> 此时 <span class="math inline">\(\Lambda=\Sigma\)</span> ,因为正定矩阵特征值为正，而且是对称的，所以 <span class="math inline">\(U=V=Q\)</span></p><p>下面介绍一个应用，大应用，为什么我会把这个应用写出来呢？因为他和图像有关，所以我们可以简单实践一下，还是从感性上认识一下SVD，然后再理论上完整的证明一下。</p><h2 id=图像压缩-image-compression>图像压缩 (Image Compression)<a hidden class=anchor aria-hidden=true href=#图像压缩-image-compression>#</a></h2><p>在介绍SVD之前，我们先来分析一个问题：图像的存储空间，一个图像假如是512x512的大小，灰度图像，每个像素占一个字节的话，这张图片那么会占据硬盘262144个字节，也就260多k个字节，如果按照23帧每秒，十秒钟大概要60兆，一分钟大概3.6G，在想想这个尺寸这么小，我们看的一般都是720P的，这样算的话硬盘根本不够用，一个东京热以后就再也不能一本道了，所以必须要压缩一下子，怎么压缩呢，这里介绍下JPEG的一个大致思路，就是矩阵分解： <span class="math display">\[
A=u_1\sigma_1 v_1^T+\dots+u_r\sigma_r v_r^T\\
A=\sigma_1 u_1 v_1^T+\dots+\sigma_r u_r v_r^T\\
A=\sigma_1 S_1+\dots+\sigma_r S_r\\
where:\,S_i=u_i v_i^T
\]</span> 这样就是按照singular的大小，给所有singular vector排序，奇异值越大证明这个奇异值向量对原始数据影响越大，所以这两奇异向量组成的这个基就越重要，当然要提前加进去，所以我们如果选择一部分奇异值和奇异向量，比如<span class="math inline">\(N=100\)</span> 也就是200个奇异向量和100个奇异值，那么一共是<span class="math inline">\(200\times 512+100=102500\)</span> 比原来的260k减少了一半，如果用的更少那么减少的更多，当然图像质量也就有所损失了，写了个python程序，可以观察一下： 使用多组S来还原原始数据，使用S越多还原度越高，但需要的存储空间就越大，S就是上面<span class="math inline">\(u_iv_i\)</span>的结果，可以看做图像的一个切片，视频中第一幅为原图，第二幅为若干张S相加得到的结果，最后一张是他们之间的差，我们可以暂时理解为压缩误差。</p><p>视频演示地址<a href="http://player.youku.com/embed/XMzE5NTIyNjQ4MA==">http://player.youku.com/embed/XMzE5NTIyNjQ4MA==</a></p><p>代码</p><div class=sourceCode id=cb1><pre class="sourceCode python"><code class="sourceCode python"><span id=cb1-1><a href=#cb1-1></a><span class=im>import</span> cv2</span>
<span id=cb1-2><a href=#cb1-2></a><span class=im>import</span> numpy <span class=im>as</span> np</span>
<span id=cb1-3><a href=#cb1-3></a><span class=im>from</span> numpy <span class=im>import</span> linalg</span>
<span id=cb1-4><a href=#cb1-4></a><span class=im>import</span> copy</span>
<span id=cb1-5><a href=#cb1-5></a>N<span class=op>=</span><span class=dv>500</span></span>
<span id=cb1-6><a href=#cb1-6></a>image<span class=op>=</span>cv2.imread(<span class=st>&#39;lena.jpg&#39;</span>,<span class=dv>0</span>)</span>
<span id=cb1-7><a href=#cb1-7></a>cv2.imshow(<span class=st>&#39;src&#39;</span>,image)</span>
<span id=cb1-8><a href=#cb1-8></a>U,Sigma,V<span class=op>=</span>linalg.svd(image)</span>
<span id=cb1-9><a href=#cb1-9></a>sigma_size<span class=op>=</span><span class=bu>len</span>(Sigma)</span>
<span id=cb1-10><a href=#cb1-10></a>image_s<span class=op>=</span>[]</span>
<span id=cb1-11><a href=#cb1-11></a>waittime<span class=op>=</span><span class=dv>0</span></span>
<span id=cb1-12><a href=#cb1-12></a><span class=cf>for</span> i <span class=kw>in</span> <span class=bu>range</span>(N):</span>
<span id=cb1-13><a href=#cb1-13></a></span>
<span id=cb1-14><a href=#cb1-14></a>    <span class=cf>if</span> i<span class=op>&gt;</span>sigma_size:</span>
<span id=cb1-15><a href=#cb1-15></a>        <span class=cf>break</span></span>
<span id=cb1-16><a href=#cb1-16></a>    image_s.append(U[:,i].reshape(<span class=bu>len</span>(U[:,i]),<span class=dv>1</span>)<span class=op>*</span>V[i]<span class=op>*</span>Sigma[i])</span>
<span id=cb1-17><a href=#cb1-17></a>    <span class=cf>if</span> i<span class=op>&gt;</span><span class=dv>0</span>:</span>
<span id=cb1-18><a href=#cb1-18></a>        image_s[i]<span class=op>+=</span>image_s[i<span class=dv>-1</span>]</span>
<span id=cb1-19><a href=#cb1-19></a>    <span class=bu>print</span> <span class=st>&#39;total: &#39;</span>,i,<span class=st>&#39; Singular Value&#39;</span></span>
<span id=cb1-20><a href=#cb1-20></a>    show_image<span class=op>=</span>copy.deepcopy(np.uint8(image_s[i]))</span>
<span id=cb1-21><a href=#cb1-21></a>    font <span class=op>=</span> cv2.FONT_HERSHEY_TRIPLEX</span>
<span id=cb1-22><a href=#cb1-22></a>    cv2.putText(show_image, `i`, (<span class=dv>10</span>, <span class=dv>500</span>), font, <span class=dv>4</span>, (<span class=dv>255</span>, <span class=dv>255</span>, <span class=dv>0</span>), <span class=dv>1</span>, <span class=va>False</span>)</span>
<span id=cb1-23><a href=#cb1-23></a>    cv2.imshow(<span class=st>&#39;Compression&#39;</span>,show_image)</span>
<span id=cb1-24><a href=#cb1-24></a>    cv2.imshow(<span class=st>&#39;Different&#39;</span>,np.uint8(image<span class=op>-</span>image_s[i]))</span>
<span id=cb1-25><a href=#cb1-25></a>    cv2.waitKey(waittime)</span>
<span id=cb1-26><a href=#cb1-26></a>    waittime<span class=op>=</span><span class=dv>100</span></span></code></pre></div><p>上面公式中每个S都是一个rank=1的矩阵，如果两个这样的矩阵相加，rank就变成了2，n个相加就是rank=n，所以SVD分解后再组合有点像切片，每一片都是有规律的，同样的道理，SVD换成小波，那就有了JPEG2000，小波的基矩阵也是满足一些特殊性质的，后面可能会将，但是不确定，所以数据压缩可以理解为最关键的一步就是两个向量相乘，能够得到一个矩阵，这个矩阵组成了基础的片，然后多个片加权求和就得到了还原数据。 SVD的应用应该非常多这里就写了一个，比较重要直观的，下面还是要继续研究原理。</p><h2 id=基和svdthe-bases-and-the-svd>基和SVD(The Bases and the SVD)<a hidden class=anchor aria-hidden=true href=#基和svdthe-bases-and-the-svd>#</a></h2><p>书中并没有给出严格的证明和推到过程，老爷子还是走的启发式套路，我们来从基开始看，假设一个矩阵A是个2x2的矩阵，这样比较好计算，并且A是非奇异矩阵，也就是A可逆，rank=2，span成整个 <span class="math inline">\(\Re^2\)</span> 空间，那么我们应该可以找到两个向量正交的单位向量 <span class="math inline">\(v_1,v_2\)</span> 满足 <span class="math inline">\(u_1=\frac{Av_1}{|Av_1|},u_2=\frac{Av_2}{|Av_2|}\)</span> 使得<span class="math inline">\(u_2\)</span>和<span class="math inline">\(u_1\)</span> 正交，并且是单位向量，那么就有： <span class="math display">\[
A
\begin{bmatrix}v_1&v_2\end{bmatrix}=
\begin{bmatrix}Av_1&Av_2\end{bmatrix}=
\begin{bmatrix}|Av_1|\cdot u_1&|Av_2|\cdot u_2\end{bmatrix}=
\begin{bmatrix}u_1&u_2\end{bmatrix}
\begin{bmatrix}|Av_1|&\\&|Av_2|\end{bmatrix}\\
AV=U\Sigma
\]</span> 总结下，这个构造基的过程是瞄准了目标去的，也就是说目标就是类似于构造 <span class="math inline">\(Ax=\alpha y\)</span> 形状的一种形式，但是x和y要满足不同的关系，如果相等那么就是特征值，如果不相等就可以构造出多组相互正交形成奇异值，并且通过上面的构造过程，我们可以得知奇异值等于缩放u到单位向量的缩放比例 <span class="math inline">\(|Av|\)</span> 其实可以用一种更直观的方法解答SVD的存在： 假设对任意矩阵A存在分解 <span class="math inline">\(A=U\Sigma V^T\)</span> 并且其中<span class="math inline">\(U^TU=I\)</span> <span class="math inline">\(V^TV=I\)</span> 那么我们要证明U和V的存在即可： <span class="math display">\[
A^TA=(U\Sigma V^T)^T(U\Sigma V^T)\\
A^TA=V\Sigma (U^TU) \Sigma V^T\\
A^TA=V\Sigma^2 V^T
\]</span> 虽然A不是对称的，但因为<span class="math inline">\(A^TA\)</span> 是对称矩阵，存在正交矩阵Q使得 <span class="math inline">\(A^TA=Q\Lambda Q^T\)</span>，那么这时候的V就是 <span class="math inline">\(A^TA\)</span> 的Q这个是没问题的，至于 <span class="math inline">\(\lambda_i=\sigma_i^2 \geq 0\)</span> 成立的原因是 <span class="math inline">\(A^TA\)</span> 是个正定矩阵（A中各列线性无关），或者半正定矩阵（A中各列线性相关），所以其特征值 <span class="math inline">\(\lambda\)</span> 必然非负数，所以根号后能得到奇异值，根据<span class="math inline">\(Av_i=\sigma u_i\)</span> 可以求出剩下的 <span class="math inline">\(u_i\)</span> ,当然这是理论上的方法，实际上的数值计算过程中可以避免 <span class="math inline">\(A^TA\)</span> 这种大规模矩阵乘法。 回忆一下正定矩阵关于椭圆的那个<a href=http://face2ai.com/Math-Linear-Algebra-Chapter-6-5/>例子</a> 一个2x2正定矩阵对应二维空间一个椭圆（或者圆），其正交特征矩阵Q矩阵是对椭圆轴的旋转，特征值矩阵 <span class="math inline">\(\Lambda\)</span> 是对轴的拉伸，那么我们的SVD有同样的功效，而且有过之无不及，思考： 作为<span class="math inline">\(A^TA\)</span> 的正交特征矩阵<span class="math inline">\(V\)</span>也是一个旋转矩阵，旋转的是圆的轴，<span class="math inline">\(V^T\)</span> 当然就是反方向旋转，<span class="math inline">\(\Sigma\)</span> 是对图形的拉伸，圆的拉成长的，接着 <span class="math inline">\(AA^T\)</span> 的正交特征矩阵也是旋转，整个过程如下图：</p><p><img src=./rotation_reflection.png></p><p>所以一个2x2的可逆矩阵，对一个圆的操作就是先拉伸，然后旋转。图中来自<a href="http://www.norsemathology.org/wiki/index.php?title=The_Singular_Value_Decomposition">Cliff Long and Tom Herm</a></p><p>上面讲解了如何求V，同样的道理也可以求U， <span class="math display">\[
AA^T=(U\Sigma V^T)(U\Sigma V^T)^T\\
AA^T=(U\Sigma V^T)(V\Sigma U)\\
AA^T=U\Sigma^2 U^T
\]</span> U是<span class="math inline">\(AA^T\)</span> 的正交特征矩阵，也就是说同一个 <span class="math inline">\(\Sigma^2\)</span> 既是 <span class="math inline">\(AA^T\)</span> 又是 <span class="math inline">\(A^TA\)</span> 的特征值，所以上面那个疑问也得到了证明。 我们重新梳理一下这个证明过程，我们首先假设结论成立，来找到使结论成立的条件，也就是V和U矩阵，结果很理想的，我们找到了，所以原来结论成立，成立的条件就是我们刚找到的这两个矩阵（如果算上奇异值，可以说是三个矩阵），思考过程可以通过2x2构造那里来推出简单情况下，来验证我们的结论，然后再推广到任意矩阵。 至此SVD的基本来路我们已经算是摸着门了，所以可以深入开发下V和U矩阵，这两个矩阵是方阵，那么组成这些矩阵的向量门也是很有来历的，总结个表格，前面在第一部分有说过，就是reduced SVD那部分，这里在啰嗦一边：</p><table><thead><tr class=header><th style=text-align:center>Number</th><th style=text-align:center>in which Matrix(column)</th><th style=text-align:center>in which Subspace</th></tr></thead><tbody><tr class=odd><td style=text-align:center>r</td><td style=text-align:center>V</td><td style=text-align:center>rowspcae of A</td></tr><tr class=even><td style=text-align:center>n-r</td><td style=text-align:center>V</td><td style=text-align:center>nullspace of A</td></tr><tr class=odd><td style=text-align:center>r</td><td style=text-align:center>U</td><td style=text-align:center>column space of A</td></tr><tr class=even><td style=text-align:center>m-r</td><td style=text-align:center>U</td><td style=text-align:center>nullspace of <span class="math inline">\(A^T\)</span></td></tr></tbody></table><p>前r列对应的是 <span class="math inline">\(A^TA\)</span> 和 <span class="math inline">\(AA^T\)</span> 的特征向量，因为其间的正交关系，所以SVD是没有什么问题的。 严格的证明： <span class="math display">\[
A^TAv_i=\sigma_i^2v_i
\]</span> 这个是可以作为条件使用的，其成立的必然原因是 <span class="math inline">\(A^TA\)</span> 是正定或半正定矩阵，所以必然存在n个大于等于0的实数特征值，这样可以得到<span class="math inline">\(v_i,\sigma_i\)</span> ，在这个条件下我们目标是证明：存在<span class="math inline">\(u_i\)</span> 使得<span class="math inline">\(Av_i=\sigma_i u_i\)</span> 成立</p><p>对条件两边同时乘上 <span class="math inline">\(v_i^T\)</span> 后： <span class="math display">\[
v_i^TA^TAv_i=\sigma_i^2v_i^Tv_i\\
||Av_i||^2=\sigma_i^2
so:\\
||Av_i||=\sigma_i\\
\]</span> 对条件两边同时乘A得到： <span class="math display">\[
AA^TAv_i=\sigma_i^2Av_i\\
set\,unit\,vector:\;u_i=\frac{Av_i}{\sigma}\\
AA^Tu_i=\sigma_i^2 u_i\\
\]</span> 上面两个过程及其精妙，尤其是下面这个，用<span class="math inline">\(u_i=\frac{Av_i}{\sigma}\)</span> 进行置换后，得到<span class="math inline">\(u_i\)</span> 是<span class="math inline">\(AA^T\)</span> 的特征向量，然后得出结论，存在 <span class="math inline">\(u_i=\frac{Av_i}{\sigma}\)</span> 使得命题成立，而且u就是<span class="math inline">\(AA^T\)</span>的特征向量，并且相互正交： 下面证明u相互正交: <span class="math display">\[
u_i^Tu_j=(Av_i)^T(Av_j)=v_i^T(A^TAv_j)=v_i^T(\sigma^2v_j)=0
\]</span> QED</p><p>然后老爷子在书上发话了，这是这本书最高潮的部分了，也是基础理论的最后一步了，因为他用到了所有的前面的理论： 1. 四个子空间的维度（我们上面那个表） 2. 正交 3. 正交基来对角化矩阵A 4. 最后得到SVD <span class="math inline">\(A=U\Sigma V^T\)</span></p><p>于是我写了一天这篇博客，比之前看书收获了更多，也可能有写纰漏，难免的因为水平有限，而且这个是比较精髓的部分，自然难度也比较大</p><h2 id=搜索网络searching-the-web>搜索网络(Searching the Web)<a hidden class=anchor aria-hidden=true href=#搜索网络searching-the-web>#</a></h2><p>这里讲了个应用，但是我实在不想写了，已经精疲力尽了，所以我打算后面选几个应用写，这个作为候选，这里略 ## Conclusion 线性代数的高潮算是来了，但是后面还有一个比较有意思的主题，也是很常用的，叫做线性变换，也可以作为切入线性代数的一个切入点，我们这个系列的博客是从线性方程组开始的，当然也可以通过线性变换引出矩阵，我们下一篇来讲解这个，待续。。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/jpeg/>JPEG</a></li><li><a href=https://go.face2ai.com/tags/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/>奇异值分解</a></li><li><a href=https://go.face2ai.com/tags/%E7%89%B9%E5%BE%81%E5%80%BC/>特征值</a></li><li><a href=https://go.face2ai.com/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/>特征向量</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) on twitter" href="https://twitter.com/intent/tweet/?text=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-7%3aSVD%e5%88%86%e8%a7%a3%28Singular%20Value%20Decomposition-SVD%29&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-7.zh%2f&hashtags=JPEG%2c%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3%2c%e7%89%b9%e5%be%81%e5%80%bc%2c%e7%89%b9%e5%be%81%e5%90%91%e9%87%8f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-7.zh%2f&title=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-7%3aSVD%e5%88%86%e8%a7%a3%28Singular%20Value%20Decomposition-SVD%29&summary=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-7%3aSVD%e5%88%86%e8%a7%a3%28Singular%20Value%20Decomposition-SVD%29&source=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-7.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-7.zh%2f&title=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-7%3aSVD%e5%88%86%e8%a7%a3%28Singular%20Value%20Decomposition-SVD%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-7.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) on whatsapp" href="https://api.whatsapp.com/send?text=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-7%3aSVD%e5%88%86%e8%a7%a3%28Singular%20Value%20Decomposition-SVD%29%20-%20https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-7.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD) on telegram" href="https://telegram.me/share/url?text=%e3%80%90%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0%e3%80%916-7%3aSVD%e5%88%86%e8%a7%a3%28Singular%20Value%20Decomposition-SVD%29&url=https%3a%2f%2fgo.face2ai.com%2fmath%2fmath-linear-algebra-chapter-6-7.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>