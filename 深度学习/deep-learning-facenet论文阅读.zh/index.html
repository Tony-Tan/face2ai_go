<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>FaceNet论文阅读 | 谭升的博客</title><meta name=keywords content="Face Recognition,FaceNet,GoogleNet"><meta name=description content="Abstract: In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity
提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度
Keywords: 人脸识别,FaceNet,GoogleNet"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-facenet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.zh/><link crossorigin=anonymous href=../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="FaceNet论文阅读"><meta property="og:description" content="Abstract: In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity
提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度
Keywords: 人脸识别,FaceNet,GoogleNet"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-facenet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.zh/"><meta property="article:section" content="深度学习"><meta property="article:published_time" content="2017-10-13T09:17:36+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="FaceNet论文阅读"><meta name=twitter:description content="Abstract: In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity
提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度
Keywords: 人脸识别,FaceNet,GoogleNet"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"FaceNet论文阅读","item":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-facenet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"FaceNet论文阅读","name":"FaceNet论文阅读","description":"Abstract: In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity\n提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度\nKeywords: 人脸识别,FaceNet,GoogleNet\n","keywords":["Face Recognition","FaceNet","GoogleNet"],"articleBody":"Abstract: In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity\n提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度\nKeywords: 人脸识别,FaceNet,GoogleNet\nFaceNet论文阅读 从老的wordpress转移过来，之前一年一直在做人脸识别算法研究，现在回头看看too young too simple，但是这些经历也算对自己的有很多帮助，认清自己的水平，知道自己什么方面比较差，这样也算是有所帮助\n Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.\n 深度卷积网络直接优化embedding，而不是以往深度学习的连接层\n To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method\n 在训练过程中，我们使用粗略对齐的匹配和非匹配人脸区块的triplets，这些triplets是通过一个出色的在线triplet挖掘方法得到的\n 128-bytes per face\n 每个人脸使用128维向量表征\nIntroduction  The network is trained such that the squared L2 distances in the embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances.\n 网络输出embedding ，在embedding空间直接描述人脸的相似程度，同一个人有较近的距离，不同的人有较远的距离\n Previous face recognition approaches based on deep networks use a classification layer [15, 17] trained over a set of known face identities and then take an intermediate bottleneck layer as a representation used to generalize recognition beyond the set of identities used in training\n 之前人脸识别给予深度网络的使用一个分类器层，独立地使用一组已知的人脸来训练，然后使用一个中间瓶颈层作为一个人脸的表征，来用于识别不同于训练样本类别的人脸\n The downsides of this approach are its indirectness and its inefficiency: one has to hope that the bottleneck representation generalizes well to new faces; and by using a bottleneck layer the representation size per face is usually very large (1000s of dimensions)\n 以前识别算法的劣势在于不直接和低效：其必须希望瓶颈层的表征对于新的人脸有好的范化能力；使用瓶颈层的表征的维数非常大，1000或更多。\n Some recent work [15] has reduced this dimensionality using PCA, but this is a linear transformation that can be easily learnt in one layer of the network\n 一些最近的工作为了减少维数使用pca降维，但是其是一个线性变换，可以在任意一层网络层轻松学到。\n FaceNet directly trains its output to be a compact 128-D embedding using a triplet-based loss function based on LMNN [19].\n FaceNet直接训练他的输出使其达到紧凑的128维embedding，使用基于LMNN 【19】的triplet 损失函数\n Our triplets consist of two matching face thumbnails and a non-matching face thumbnail and the loss aims to separate the positive pair from the negative by a distance margin\n triplet包含两张匹配的人脸，和一张非匹配的人脸，目的是用一对正例，和一对负例，并保证ap和an之间差一个margin（空隙，空间）\n The thumbnails are tight crops of the face area, no 2D or 3D alignment, other than scale and translation is performed\n 样本内容为人脸紧凑的切割，不需要任何2D，3D的对齐，尺度变换，和其他变换\n Choosing which triplets to use turns out to be very important for achieving good performance and,\n 选择triplets很重要\n inspired by curriculum learning [1], we present a novel online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains.\n 受到【1】的启发，我们提出了一个新奇的在线负样本挖掘策略，来确保在网络训练过程中持续增加triplets的难度\nTo improve clustering accuracy, we also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person.\n   为了提高聚类正确性，使用hard正样本挖掘技术，来确保单个人脸的EMbedding约束在球形内\n 可以处理以前觉得很难的样本，如下：\nRelatedWork  Similarly to other recent works which employ deep networks [15, 17], our approach is a purely data driven method which learns its representation directly from the pixels of the face. Rather than using engineered features, we use a large dataset of labelled faces to attain the appropriate invariances to pose, illumination, and other variational conditions.\n 类似于15，17，我们的应用是纯粹的数据驱动的方法，使用人脸图片的像素作为输入，而不是使用人工构建的特征，使用大量的标记后的人脸数据使网络对于pose，illumination和其他conditions稳定。\n本文讨论了两种网络架构：\n 1:The first architecture is based on the Zeiler\u0026Fergus [22] model which consists of multiple inter-leaved layers of convolutions, non-linear activations, local response normalizations, and max pooling layers\n  2:The second architecture is based on the Inception model of Szegedy et al. which was recently used as the winning approach for ImageNet 2014 [16]\n  We have found that these models can reduce the number of parameters by up to 20 times and have the potential to reduce the number of FLOPS required for comparable performance\n 以上两种网络结构能减少20倍以上的参数，并且能潜在的减少FLOPS的数量（与同类的网络）\n近期人脸识别工作一大堆，挑几个代表性的说：\n The works of [15, 17, 23] all employ a complex system of multiple stages, that combines the output of a deep con-volutional network with PCA for dimensionality reduction and an SVM for classification：\n 深度学习+PCA+SVM\n Zhenyao et al. [23] employ a deep network to “warp” faces into a canonical frontal view and then learn CNN that classifies each face as belonging to a known identity. For face verification, PCA on the network output in conjunction with an ensemble of SVMs is used.\n 使用深度网络把脸弄成正面视角的，然后训练网络，verification的时候PCA网络输出，然后用svm分类\n Taigman et al. [17] propose a multi-stage approach that aligns faces to a general 3D shape model. Amulti-class network is trained to perform the face recognition task on over four thousand identities.\n 多阶段应用来align人脸，得到3d的形状，然后训练多类网络来进行识别\n Sun et al. [14, 15] propose a compact and therefore relatively cheap to compute network. They use an ensemble of 25 of these network, each operating on a different face patch.\n 小型紧凑的网络模型，易于计算，共使用25个这样的网络，每个网络对应于人脸的不同区块。\n The verification loss is similar to the triplet loss we employ [12, 19], in that it minimizes the L2-distance between faces of the same identity and enforces a margin between the distance of faces of different identities.\n Verification loss 类似于triplet loss 我们引用自12，19的方法，最小化L2距离，在同一个人之间和不同人之间的margin。\nMethod two different core architectures:\n The Zeiler\u0026Fergus [22] style networks   the recent Inception [16] type networks\n Given the model details, and treating it as a black box (see Figure 2), the most important part of our approach lies in the end-to-end learning of the whole system.\n网络模型当做黑盒如下图，我们主要关心更重要的。端对端的系统学习\nTo this end we employ the triplet loss that directly reflects what we want to achieve in face verification, recognition and clustering.\n在一端，我们使用triplet loss 直接反应我们想要在人脸分辨，识别，聚类中达成的目标\nNamely, we strive for an embedding f(x), from an image x into a feature space Rd,\n我们使用embedding ， f（x）直接将图像从x域映射到特征空间R（d维），\nThe triplet loss, however, tries to enforce a margin between each pair of faces from one person to all other faces. This allows the faces for one identity to live on a manifold, while still enforcing the distance and thus discriminability to other identities\nTriplet loss 试图去注意在正样本对和负样本对之间的margin，这使得同一类的人脸存在于多种情况，同时也关注其间的距离，因此对于其他类可分辨。\nTriplet Loss Here we want to ensure that an image 我们要确保对于一张图：\n1.of a specific person is closer to all other images 同一人的不同图片距离$x_i^p(positive)$\n2.of the same person than it is to any image 母样本，或者称为锚点$x_i^a(anchor)$\n3.of any other person\n对于其他样本（负样本）$x_i^n(negative)$\n我们希望确保同一人的ap相近，ab距离远，如下图 因此，我们希望：\n$$ ||x_i^a-x_i^p||_2^2+\\alpha 最小化的Loss：\n$$\\sum_i^N[||f(x_i^a)-f(x_i^p)||^2_2-||f(x_i^a)-f(x_i^n)||^2_2+\\alpha]_{+}$$\n Generating all possible triplets would result in many triplets that are easily satisfied (i.e. fulfill the constraint in Eq. (1)). These triplets would not contribute to the training and result in slower convergence, as they would still be passed through the network. It is crucial to select hard triplets, that are active and can therefore contribute to improving the model.\n 计算所有可能的triplet容易导致其中很多对于网络是无用的（网络可以轻易使其满足公式1），这些triplet对于训练和结果收敛速度没有贡献，选择难以分类的是重要的，这些是有吸引力的，而且可以帮助提高模型\nTriplet Selection In order to ensure fast convergence it is crucial to select triplets that violate the triplet constraint in Eq. (1)\n为了快速收敛，选择违反公式1的triplet具有决定性作用。\n选择hard-positive和hard-nagetive：\n$$ argmax_{x_i^p}||f(x_i^a)-f(x_i^p)||^2_2 $$\n$$ argmax_{x_i^p}||f(x_i^a)-f(x_i^n)||^2_2 $$\nIt is infeasible to compute the argmin and argmax across the whole training set.\n在完整数据集上计算argmin和argmax是不可能的 Additionally, it might lead to poor training, as mislabelled and poorly imaged faces would dominate the hard positives and negatives. There are two obvious choices that avoid this issue:\n尤其是，其可能导致不好的结果，因为有一些训练数据未准确标记，模糊的图片可能会决定hard positives或者hard negative，有两种明确的选择可以避免这两个问题：\n Generate triplets offline every n steps, using the most recent network checkpoint and computing the argmin and argmax on a subset of the data.\n 每N步后线下寻找triplet，使用最近生成的网络，计算argmin和argmax在数据的一个子集上\n Generate triplets online. This can be done by selecting the hard positive/negative exemplars from within a mini-batch.\n 在线获取triplet，通过从一个小的batch中选取hard positive或者hard negative样本。\n Here, we focus on the online generation and use large mini-batches in the order of a few thousand exemplars and only compute the argmin and argmax within a mini-batch.\n 这里我们专注于在线获取，使用大的mini-batches包含几千个样本，而且只计算argmin和argmax在这个mini-batche上\n To have a meaningful representation of the anchor-positive distances, it needs to be ensured that a minimal number of exemplars of any one identity is present in each mini-batch. In our experiments we sample the training data such that around 40 faces are selected per identity per mini-batch. Additionally, randomly sampled negative faces are added to each mini-batch.\n 为了获得一个有意义的AP距离的表达，其需要确保在每一个mini-batch任一类必须有少量的样本被选出，我们的经验是我们在训练样本中采样大约40个人脸图像每个人每个mini-batch。此外，随机采样负样本人脸，加入到每一个mini-batch\n Instead of picking the hardest positive, we use all anchor-positive pairs in a mini-batch while still selecting the hard negatives.\n 我们选取mini-batch中所有的ap对而不是选择最难的ap对。但是我们依然选择最难的an对\n We don’t have a side-by-side comparison of hard anchor-positive pairs versus all anchor-positive pairs within a mini-batch, but we found in practice that the all anchor- positive method was more stable and converged slightly faster at the beginning of training.\n 我们没有对mini-batch中的所有ap对进行side-by-side的hard程度比较，但是我们的经验是所有的ap对都使用能够增强收敛稳定性和速度，在训练初始阶段。\n We also explored the offline generation of triplets in conjunction with the online generation and it may allow the use of smaller batch sizes, but the experiments were inconclusive.\n offline和online结合，这种方法允许使用更小的batch大小，但是结果是没啥结果。。\n Selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically it can result in a collapsed model (i.e. f(x) = 0). In order to mitigate this, it helps to select xni such that\n 选择最难的AN对可以在训练早起引导向局部最小值，特别的，他能勾引起模型塌陷，为了避免这种情况，我们选取n的时候要满足以下条件：\n$$||f(x_i^a)-f(x_i^p)||^2_2We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchor-positive distance. Those negatives lie inside the margin α.\n 我们称这些negative为semi-hard，因为他们距离anchor比positive远，但是还难以区分，因为距离还是比较接近ap距离，这些negative在margin alpha内！\n As mentioned before, correct triplet selection is crucial for fast convergence.\n 选好triplet能加速收敛\n On the one hand we would like to use small mini-batches as these tend to improve convergence during Stochastic Gradient Descent (SGD) [20].\n 一方面我们使用小的mini-batch加速收敛，使用sgd\n On the other hand, implementation details make batches of tens to hundreds of exemplars more efficient.\n 另一方面，执行细节使得样本的几十到几千的batch更有效\n The main constraint with regards to the batch size, however, is the way we select hard relevant triplets from within the mini-batches.\n Batch size 的主要的约束是我们从mini-batches选择hard triplet的方式\n In most experiments we use a batch size of around 1,800 exemplars\n Batch size大约1800个样例\nDeep Convolutional Networks  In all our experiments we train the CNN using Stochastic Gradient Descent (SGD) with standard backprop [8, 11] and AdaGrad [5]. In most experiments we start with a learning rate of 0.05 which we lower to finalize the model. The models are initialized from random, similar to [16], and trained on a CPU cluster for 1,000 to 2,000 hours. The decrease in the loss (and increase in accuracy) slows down drastically after 500h of training, but additional training can still significantly improve performance. The margin α is set to 0.2.\n 使用BP算法和SGD优化来训练CNN，AdaGrad用来调整步长，初始化步长为0.05，模型随机初始化，类似于16，使用cpu集群训练了1000-2000小时，500小时后训练效果变化变慢，但还是继续训练还是有效果的，alpha选择0.2\n The first category, shown in Table 1, adds 1×1×d convolutional layers, as suggested in [9], between the standard convolutional layers of the Zeiler\u0026Fergus [22] architecture and results in a model 22 layers deep. It has a total of 140 million parameters and requires around 1.6 billion FLOPS per image. The second category we use is based on GoogLeNet style Inception models [16]. These models have 20× fewer parameters (around 6.6M-7.5M) and up to 5×fewer FLOPS(between 500M-1.6B).\n ","wordCount":"1617","inLanguage":"en","datePublished":"2017-10-13T09:17:36Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-facenet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a></div><h1 class=post-title>FaceNet论文阅读</h1><div class=post-meta><span title="2017-10-13 09:17:36 +0000 UTC">October 13, 2017</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#facenet%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb aria-label=FaceNet论文阅读>FaceNet论文阅读</a><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#relatedwork aria-label=RelatedWork>RelatedWork</a></li><li><a href=#method aria-label=Method>Method</a></li><li><a href=#triplet-loss aria-label="Triplet Loss">Triplet Loss</a></li><li><a href=#triplet-selection aria-label="Triplet Selection">Triplet Selection</a></li><li><a href=#deep-convolutional-networks aria-label="Deep Convolutional Networks">Deep Convolutional Networks</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity</p><p>提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度</p><p><strong>Keywords:</strong> 人脸识别,FaceNet,GoogleNet</p><h1 id=facenet论文阅读>FaceNet论文阅读<a hidden class=anchor aria-hidden=true href=#facenet论文阅读>#</a></h1><p>从老的wordpress转移过来，之前一年一直在做人脸识别算法研究，现在回头看看too young too simple，但是这些经历也算对自己的有很多帮助，认清自己的水平，知道自己什么方面比较差，这样也算是有所帮助</p><blockquote><p>Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.</p></blockquote><p>深度卷积网络直接优化embedding，而不是以往深度学习的连接层</p><blockquote><p>To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method</p></blockquote><p>在训练过程中，我们使用粗略对齐的匹配和非匹配人脸区块的triplets，这些triplets是通过一个出色的在线triplet挖掘方法得到的</p><blockquote><p>128-bytes per face</p></blockquote><p>每个人脸使用128维向量表征</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><blockquote><p>The network is trained such that the squared L2 distances in the embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances.</p></blockquote><p>网络输出embedding ，在embedding空间直接描述人脸的相似程度，同一个人有较近的距离，不同的人有较远的距离</p><blockquote><p>Previous face recognition approaches based on deep networks use a classification layer [15, 17] trained over a set of known face identities and then take an intermediate bottleneck layer as a representation used to generalize recognition beyond the set of identities used in training</p></blockquote><p>之前人脸识别给予深度网络的使用一个分类器层，独立地使用一组已知的人脸来训练，然后使用一个中间瓶颈层作为一个人脸的表征，来用于识别不同于训练样本类别的人脸</p><blockquote><p>The downsides of this approach are its indirectness and its inefficiency: one has to hope that the bottleneck representation generalizes well to new faces; and by using a bottleneck layer the representation size per face is usually very large (1000s of dimensions)</p></blockquote><p>以前识别算法的劣势在于不直接和低效：其必须希望瓶颈层的表征对于新的人脸有好的范化能力；使用瓶颈层的表征的维数非常大，1000或更多。</p><blockquote><p>Some recent work [15] has reduced this dimensionality using PCA, but this is a linear transformation that can be easily learnt in one layer of the network</p></blockquote><p>一些最近的工作为了减少维数使用pca降维，但是其是一个线性变换，可以在任意一层网络层轻松学到。</p><blockquote><p>FaceNet directly trains its output to be a compact 128-D embedding using a triplet-based loss function based on LMNN [19].</p></blockquote><p>FaceNet直接训练他的输出使其达到紧凑的128维embedding，使用基于LMNN 【19】的triplet 损失函数</p><blockquote><p>Our triplets consist of two matching face thumbnails and a non-matching face thumbnail and the loss aims to separate the positive pair from the negative by a distance margin</p></blockquote><p>triplet包含两张匹配的人脸，和一张非匹配的人脸，目的是用一对正例，和一对负例，并保证ap和an之间差一个margin（空隙，空间）</p><blockquote><p>The thumbnails are tight crops of the face area, no 2D or 3D alignment, other than scale and translation is performed</p></blockquote><p>样本内容为人脸紧凑的切割，不需要任何2D，3D的对齐，尺度变换，和其他变换</p><blockquote><p>Choosing which triplets to use turns out to be very important for achieving good performance and,</p></blockquote><p>选择triplets很重要</p><blockquote><p>inspired by curriculum learning [1], we present a novel online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains.</p></blockquote><p>受到【1】的启发，我们提出了一个新奇的在线负样本挖掘策略，来确保在网络训练过程中持续增加triplets的难度</p><p>To improve clustering accuracy, we also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person.</p><blockquote></blockquote><blockquote><p>为了提高聚类正确性，使用hard正样本挖掘技术，来确保单个人脸的EMbedding约束在球形内</p></blockquote><p>可以处理以前觉得很难的样本，如下：</p><p><img loading=lazy src=./12.png alt></p><h2 id=relatedwork>RelatedWork<a hidden class=anchor aria-hidden=true href=#relatedwork>#</a></h2><blockquote><p>Similarly to other recent works which employ deep networks [15, 17], our approach is a purely data driven method which learns its representation directly from the pixels of the face. Rather than using engineered features, we use a large dataset of labelled faces to attain the appropriate invariances to pose, illumination, and other variational conditions.</p></blockquote><p>类似于15，17，我们的应用是纯粹的数据驱动的方法，使用人脸图片的像素作为输入，而不是使用人工构建的特征，使用大量的标记后的人脸数据使网络对于pose，illumination和其他conditions稳定。</p><p>本文讨论了两种网络架构：</p><blockquote><p>1:The first architecture is based on the Zeiler&Fergus [22] model which consists of multiple inter-leaved layers of convolutions, non-linear activations, local response normalizations, and max pooling layers</p></blockquote><blockquote><p>2:The second architecture is based on the Inception model of Szegedy et al. which was recently used as the winning approach for ImageNet 2014 [16]</p></blockquote><blockquote><p>We have found that these models can reduce the number of parameters by up to 20 times and have the potential to reduce the number of FLOPS required for comparable performance</p></blockquote><p>以上两种网络结构能减少20倍以上的参数，并且能潜在的减少FLOPS的数量（与同类的网络）</p><p>近期人脸识别工作一大堆，挑几个代表性的说：</p><blockquote><p>The works of [15, 17, 23] all employ a complex system of multiple stages, that combines the output of a deep con-volutional network with PCA for dimensionality reduction and an SVM for classification：</p></blockquote><p>深度学习+PCA+SVM</p><blockquote><p>Zhenyao et al. [23] employ a deep network to “warp” faces into a canonical frontal view and then learn CNN that classifies each face as belonging to a known identity. For face verification, PCA on the network output in conjunction with an ensemble of SVMs is used.</p></blockquote><p>使用深度网络把脸弄成正面视角的，然后训练网络，verification的时候PCA网络输出，然后用svm分类</p><blockquote><p>Taigman et al. [17] propose a multi-stage approach that aligns faces to a general 3D shape model. Amulti-class network is trained to perform the face recognition task on over four thousand identities.</p></blockquote><p>多阶段应用来align人脸，得到3d的形状，然后训练多类网络来进行识别</p><blockquote><p>Sun et al. [14, 15] propose a compact and therefore relatively cheap to compute network. They use an ensemble of 25 of these network, each operating on a different face patch.</p></blockquote><p>小型紧凑的网络模型，易于计算，共使用25个这样的网络，每个网络对应于人脸的不同区块。</p><blockquote><p>The verification loss is similar to the triplet loss we employ [12, 19], in that it minimizes the L2-distance between faces of the same identity and enforces a margin between the distance of faces of different identities.</p></blockquote><p>Verification loss 类似于triplet loss 我们引用自12，19的方法，最小化L2距离，在同一个人之间和不同人之间的margin。</p><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><p>two different core architectures:</p><ol><li>The Zeiler&Fergus [22] style networks</li></ol><blockquote><p>the recent Inception [16] type networks</p></blockquote><p>Given the model details, and treating it as a black box (see Figure 2), the most important part of our approach lies in the end-to-end learning of the whole system.</p><p>网络模型当做黑盒如下图，我们主要关心更重要的。端对端的系统学习</p><p><img loading=lazy src=./11.png alt></p><p>To this end we employ the triplet loss that directly reflects what we want to achieve in face verification, recognition and clustering.</p><p>在一端，我们使用triplet loss 直接反应我们想要在人脸分辨，识别，聚类中达成的目标</p><p>Namely, we strive for an embedding f(x), from an image x into a feature space Rd,</p><p>我们使用embedding ， f（x）直接将图像从x域映射到特征空间R（d维），</p><p>The triplet loss, however, tries to enforce a margin between each pair of faces from one person to all other faces. This allows the faces for one identity to live on a manifold, while still enforcing the distance and thus discriminability to other identities</p><p>Triplet loss 试图去注意在正样本对和负样本对之间的margin，这使得同一类的人脸存在于多种情况，同时也关注其间的距离，因此对于其他类可分辨。</p><h2 id=triplet-loss>Triplet Loss<a hidden class=anchor aria-hidden=true href=#triplet-loss>#</a></h2><p>Here we want to ensure that an image
我们要确保对于一张图：</p><p>1.of a specific person is closer to all other images
同一人的不同图片距离$x_i^p(positive)$</p><p>2.of the same person than it is to any image
母样本，或者称为锚点$x_i^a(anchor)$</p><p>3.of any other person</p><p>对于其他样本（负样本）$x_i^n(negative)$</p><p>我们希望确保同一人的ap相近，ab距离远，如下图
<img loading=lazy src=./7.png alt></p><p>因此，我们希望：</p><p>$$
||x_i^a-x_i^p||_2^2+\alpha &lt; ||x_i^a-x_i^n||_2^2 ,\forall(x_i^a,x_i^p,x_i^n)\in\tau
$$
alpha表示margin在正例对和负例对之间的距离，T是一组可能的triplets在训练数据集中的。基数为N。</p><p>最小化的Loss：</p><p>$$\sum_i^N[||f(x_i^a)-f(x_i^p)||^2_2-||f(x_i^a)-f(x_i^n)||^2_2+\alpha]_{+}$$</p><blockquote><p>Generating all possible triplets would result in many triplets that are easily satisfied (i.e. fulfill the constraint in Eq. (1)). These triplets would not contribute to the training and result in slower convergence, as they would still be passed through the network. It is crucial to select hard triplets, that are active and can therefore contribute to improving the model.</p></blockquote><p>计算所有可能的triplet容易导致其中很多对于网络是无用的（网络可以轻易使其满足公式1），这些triplet对于训练和结果收敛速度没有贡献，选择难以分类的是重要的，这些是有吸引力的，而且可以帮助提高模型</p><h2 id=triplet-selection>Triplet Selection<a hidden class=anchor aria-hidden=true href=#triplet-selection>#</a></h2><p>In order to ensure fast convergence it is crucial to select triplets that violate the triplet constraint in Eq. (1)</p><p>为了快速收敛，选择违反公式1的triplet具有决定性作用。</p><p>选择hard-positive和hard-nagetive：</p><p>$$
argmax_{x_i^p}||f(x_i^a)-f(x_i^p)||^2_2
$$</p><p>$$
argmax_{x_i^p}||f(x_i^a)-f(x_i^n)||^2_2
$$</p><p>It is infeasible to compute the argmin and argmax across the whole training set.</p><p>在完整数据集上计算argmin和argmax是不可能的
Additionally, it might lead to poor training, as mislabelled and poorly imaged faces would dominate the hard positives and negatives. There are two obvious choices that avoid this issue:</p><p>尤其是，其可能导致不好的结果，因为有一些训练数据未准确标记，模糊的图片可能会决定hard positives或者hard negative，有两种明确的选择可以避免这两个问题：</p><blockquote><p>Generate triplets offline every n steps, using the most recent network checkpoint and computing the argmin and argmax on a subset of the data.</p></blockquote><p>每N步后线下寻找triplet，使用最近生成的网络，计算argmin和argmax在数据的一个子集上</p><blockquote><p>Generate triplets online. This can be done by selecting the hard positive/negative exemplars from within a mini-batch.</p></blockquote><p>在线获取triplet，通过从一个小的batch中选取hard positive或者hard negative样本。</p><blockquote><p>Here, we focus on the online generation and use large mini-batches in the order of a few thousand exemplars and only compute the argmin and argmax within a mini-batch.</p></blockquote><p>这里我们专注于在线获取，使用大的mini-batches包含几千个样本，而且只计算argmin和argmax在这个mini-batche上</p><blockquote><p>To have a meaningful representation of the anchor-positive distances, it needs to be ensured that a minimal number of exemplars of any one identity is present in each mini-batch. In our experiments we sample the training data such that around 40 faces are selected per identity per mini-batch. Additionally, randomly sampled negative faces are added to each mini-batch.</p></blockquote><p>为了获得一个有意义的AP距离的表达，其需要确保在每一个mini-batch任一类必须有少量的样本被选出，我们的经验是我们在训练样本中采样大约40个人脸图像每个人每个mini-batch。此外，随机采样负样本人脸，加入到每一个mini-batch</p><blockquote><p>Instead of picking the hardest positive, we use all anchor-positive pairs in a mini-batch while still selecting the hard negatives.</p></blockquote><p>我们选取mini-batch中所有的ap对而不是选择最难的ap对。但是我们依然选择最难的an对</p><blockquote><p>We don’t have a side-by-side comparison of hard anchor-positive pairs versus all anchor-positive pairs within a mini-batch, but we found in practice that the all anchor- positive method was more stable and converged slightly faster at the beginning of training.</p></blockquote><p>我们没有对mini-batch中的所有ap对进行side-by-side的hard程度比较，但是我们的经验是所有的ap对都使用能够增强收敛稳定性和速度，在训练初始阶段。</p><blockquote><p>We also explored the offline generation of triplets in conjunction with the online generation and it may allow the use of smaller batch sizes, but the experiments were inconclusive.</p></blockquote><p>offline和online结合，这种方法允许使用更小的batch大小，但是结果是没啥结果。。</p><blockquote><p>Selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically it can result in a collapsed model (i.e. f(x) = 0). In order to mitigate this, it helps to select xni such that</p></blockquote><p>选择最难的AN对可以在训练早起引导向局部最小值，特别的，他能勾引起模型塌陷，为了避免这种情况，我们选取n的时候要满足以下条件：</p><p>$$||f(x_i^a)-f(x_i^p)||^2_2&lt;||f(x_i^a)-f(x_i^n)||^2_2$$</p><blockquote><p>We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchor-positive distance. Those negatives lie inside the margin α.</p></blockquote><p>我们称这些negative为semi-hard，因为他们距离anchor比positive远，但是还难以区分，因为距离还是比较接近ap距离，这些negative在margin alpha内！</p><blockquote><p>As mentioned before, correct triplet selection is crucial for fast convergence.</p></blockquote><p>选好triplet能加速收敛</p><blockquote><p>On the one hand we would like to use small mini-batches as these tend to improve convergence during Stochastic Gradient Descent (SGD) [20].</p></blockquote><p>一方面我们使用小的mini-batch加速收敛，使用sgd</p><blockquote><p>On the other hand, implementation details make batches of tens to hundreds of exemplars more efficient.</p></blockquote><p>另一方面，执行细节使得样本的几十到几千的batch更有效</p><blockquote><p>The main constraint with regards to the batch size, however, is the way we select hard relevant triplets from within the mini-batches.</p></blockquote><p>Batch size 的主要的约束是我们从mini-batches选择hard triplet的方式</p><blockquote><p>In most experiments we use a batch size of around 1,800 exemplars</p></blockquote><p>Batch size大约1800个样例</p><h2 id=deep-convolutional-networks>Deep Convolutional Networks<a hidden class=anchor aria-hidden=true href=#deep-convolutional-networks>#</a></h2><blockquote><p>In all our experiments we train the CNN using Stochastic Gradient Descent (SGD) with standard backprop [8, 11] and AdaGrad [5]. In most experiments we start with a learning rate of 0.05 which we lower to finalize the model. The models are initialized from random, similar to [16], and trained on a CPU cluster for 1,000 to 2,000 hours. The decrease in the loss (and increase in accuracy) slows down drastically after 500h of training, but additional training can still significantly improve performance. The margin α is set to 0.2.</p></blockquote><p>使用BP算法和SGD优化来训练CNN，AdaGrad用来调整步长，初始化步长为0.05，模型随机初始化，类似于16，使用cpu集群训练了1000-2000小时，500小时后训练效果变化变慢，但还是继续训练还是有效果的，alpha选择0.2</p><blockquote><p>The first category, shown in Table 1, adds 1×1×d convolutional layers, as suggested in [9], between the standard convolutional layers of the Zeiler&Fergus [22] architecture and results in a model 22 layers deep. It has a total of 140 million parameters and requires around 1.6 billion FLOPS per image.
The second category we use is based on GoogLeNet style Inception models [16]. These models have 20× fewer parameters (around 6.6M-7.5M) and up to 5×fewer FLOPS(between 500M-1.6B).</p></blockquote><p><img loading=lazy src=./1.png alt></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/face-recognition/>Face Recognition</a></li><li><a href=https://go.face2ai.com/tags/facenet/>FaceNet</a></li><li><a href=https://go.face2ai.com/tags/googlenet/>GoogleNet</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share FaceNet论文阅读 on twitter" href="https://twitter.com/intent/tweet/?text=FaceNet%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb&url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-facenet%25E8%25AE%25BA%25E6%2596%2587%25E9%2598%2585%25E8%25AF%25BB.zh%2f&hashtags=FaceRecognition%2cFaceNet%2cGoogleNet"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share FaceNet论文阅读 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-facenet%25E8%25AE%25BA%25E6%2596%2587%25E9%2598%2585%25E8%25AF%25BB.zh%2f&title=FaceNet%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb&summary=FaceNet%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb&source=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-facenet%25E8%25AE%25BA%25E6%2596%2587%25E9%2598%2585%25E8%25AF%25BB.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share FaceNet论文阅读 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-facenet%25E8%25AE%25BA%25E6%2596%2587%25E9%2598%2585%25E8%25AF%25BB.zh%2f&title=FaceNet%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share FaceNet论文阅读 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-facenet%25E8%25AE%25BA%25E6%2596%2587%25E9%2598%2585%25E8%25AF%25BB.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share FaceNet论文阅读 on whatsapp" href="https://api.whatsapp.com/send?text=FaceNet%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb%20-%20https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-facenet%25E8%25AE%25BA%25E6%2596%2587%25E9%2598%2585%25E8%25AF%25BB.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share FaceNet论文阅读 on telegram" href="https://telegram.me/share/url?text=FaceNet%e8%ae%ba%e6%96%87%e9%98%85%e8%af%bb&url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-facenet%25E8%25AE%25BA%25E6%2596%2587%25E9%2598%2585%25E8%25AF%25BB.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>