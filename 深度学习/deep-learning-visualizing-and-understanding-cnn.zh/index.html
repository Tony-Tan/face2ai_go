<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Visualizing and Understanding CNN | 谭升的博客</title><meta name=keywords content="CNN Visualizing,CNN"><meta name=description content="Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下
Keywords: CNN Visualizing"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-visualizing-and-understanding-cnn.zh/><link crossorigin=anonymous href=../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="Visualizing and Understanding CNN"><meta property="og:description" content="Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下
Keywords: CNN Visualizing"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-visualizing-and-understanding-cnn.zh/"><meta property="article:section" content="深度学习"><meta property="article:published_time" content="2017-09-13T16:46:09+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Visualizing and Understanding CNN"><meta name=twitter:description content="Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下
Keywords: CNN Visualizing"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"Visualizing and Understanding CNN","item":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-visualizing-and-understanding-cnn.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Visualizing and Understanding CNN","name":"Visualizing and Understanding CNN","description":"Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下 Keywords: CNN Visualizing\n","keywords":["CNN Visualizing","CNN"],"articleBody":"Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下 Keywords: CNN Visualizing\n卷积神经网络的可视化 前言 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下，写成博客，代码后续给出，不足之处还请大家指出。 大型卷积神经网络在图片分类上很成功，然而我们不知道他为什么能表现的如此不错，或者如何提高。\nAbstract：  In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of inter-mediate feature layers and the operation of the classifier 我们研究一个优秀的可视化技术，能够给出函数内部特征层以及分类层的信息 Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. 可视化使我们找到比Kri在ImageNet分类更好的网络架构。 We also perform an ablation study to discover the performance contribution from different model layers. 我们通过切块研究发现不同层对分类的作用。 We show our ImageNet model generalizes well to other datasets: whenthe softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. 我们展示了我们的 ImageNet 模型在其他数据集上获得优秀的表现：当我们重新训练SoftMax分类器。其结果信服的打败了当前SOTA结果，在Caltech-101和Caltech-256数据集上\n 评论：作者要解决的是可视化深度学习模型，来给出内部的结构，工作原理，以及内在结构的相关性等。并且在这个基础上反向选择优化不同的深度架构（模型）来得到更好的模型，并给出了监督学习的Pre-training方法，在不同测试数据集上表现不俗。\nIntroduction 卷积神经网络很牛，在各种分类比赛上获得state-of-the-art的结果。\n 卷积神经网络在各大测试集上获得好结果的原因： Several factors are responsible for this renewed interest in convnet models:\n  (i) the availability of much larger training sets, with 大量的训练数据\n  (ii) powerful GPU implementations, making the training of very large models practical GPU的高效计算\n  (iii) better model regularization strategies, such as Dropout (Hinton et al., 2012). 更优秀的网络结构（例如Dropout）\n  Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. 如果不知道内部原因，我们的新模型只能停留在实验，观察的基础上\n  In this paper we introduce a visualization technique that reveals theinput stimuli that excite individual feature maps at any layer in themodel. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. 本文提出了一种可视化技术，其能够揭示输入是如何激活那些独立的特征映射在模型中的任一层。这项技术也允许我们来观察特征在训练过程中的进化过程来判断模型潜在的问题。\n  The visualization technique we propose uses a multi-layeredDeconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. 我们提出了使用多层逆卷积网络，（Zeiler et al 2014年）提出的，将特征反向映射会到输入层观察结果\n  We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification 通过遮挡输入图片的部分，对分类器进行分析，来揭示哪些部分对分类结果产生相对重的影响\n  Using these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones thatoutperform their results on ImageNet. 使用这些工具，我们开始使用此架构探索不同的架构，认识在ImageNet上表现出色的结构\n  We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. 我们随后探索架构对于其他数据集的范化能力，在只重新训练softmax分类器的基础上。\n  As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton etal., 2006) and others (Bengio et al., 2007; Vincent et al., 2008) 监督学习的Pre-training来对比无监督的Pre-training方法（Hinton et al., 2006 Bengio et al., 2007; Vincent et al., 2008）\n 评论：主要就是说，以前都是不知道为啥深度学习会工作，不知道如何优化，只是考实验观察，现在我们能牛x的知道为啥能工作了，虽然没有数学证明，但我们知道怎么调了，知道工作原理，知道怎么Pre-training。。。\nRelated Work  Our approach, by contrast, provides a non-parametric view of invariance, showing which patterns from the training set activate the feature map. 我们的工作提出了一种无参数的不变性观点，来展示训练数据的哪些部分激活了特征映射\n 评论：没有评论\nApproach  We use standard fully supervised convnet models throughout the paper, as defined by (LeCun et al., 1989) and (Krizhevsky et al., 2012). 我们在整篇文章使用标准完全监督卷积网络模型，在 (LeCun et al., 1989) and (Krizhevsky et al., 2012)定义的。 (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters;\n  (ii) passing the responses through a rectified linear function (relu(x) = max(x, 0));\n  (iii) max pooling over local neighborhoodsand\n  (iv) a local contrast operation that normalizes the responses across feature maps.\n  The top few layers of the network are conventional fully-connected\n  networks and the final layer is a softmax classifie\n Visualization with a Deconvnet  We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps. 我们提出了一个高级的方法来映射激活反向到输入像素空间，来展示在特征空间哪一部分输入引起了这个给定的激活 In (Zeiler et al., 2011), deconvnets were proposed as a way ofperforming unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convent. 在(Zeiler et al., 2011)，Deconvnets 被提出作为一种表现非监督学习的方法。这里他们不再用于任何其学习能力，只是用于研究已经训练好的Convnet To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. 为了测试一个给定的神经元激活，我们设置所有其他的同层神经元激活值为零，传导特征映射作为输入来激活deconvnet层 Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space isreached. 在激活选定特征的神经元后Unpool，rectify，filter来重建本层的激活。\n Unpooling：  In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In thedeconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure 最大池化不可逆，我们通过记录位置来进行近似，记录被称为一组switch值，在deconvnet中，逆池化使用这些switch值来定位重建上一层，保留激活分布。在Fig1中说明\n Rectification：  The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive),we pass the reconstructed signal through a relu non-linearity. Convnet使用ReLu非线性函数，保证激活值非负；为保证每层特征可重建，我们让所有重建信号经过ReLu层。\n Filtering：  The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, notthe output of the layer beneath. Convnet使用学习到的Filters从前一层来获取特征映射。相反，deconvnet使用同一filter的转置，但是操作的对象是整流结果(Rectification)，而不是之前的层。\n 总结：  Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative 由于模型是训练成有区别的，因此他们理所应当的展示输入图片的那些部分是有区别的。 Note that these projections are not samples from the model, since there is no generative process involved 注意这些映射不是从模型中采样，因为没有范化处理涉及。\n 评论：此段描述了具体如何反向将特征映射到像素空间。\nTraining Details  The architecture, shown in Fig. 3, is similar to that used by (Krizhevsky et al., 2012) for ImageNet classification 结构在Fig 3中（本文第一张图）。。。\n 训练方法：\n The model was trained on the ImageNet 2012 training set (1.3 million images, spread over 1000 different classes). Each RGB image was preprocessed by resizing the smallest dimension to 256, cropping the center 256x256 region, subtracting the per-pixel mean (across all images) and then using 10 different sub-crops of size 224x224 (corners+ center with(out) horizontal flips). Stochastic gradient descent with a mini-batch size of 128 was used to update the parameters, starting with a learning rate of 10−2, in conjunction with a momentum term of 0.9. We anneal the learning rate throughout training manually when the validation error plateaus. Dropout (Hinton et al., 2012) is used in the fully connected layers (6 and 7) with a rate of 0.5. 此处翻译略过，描述了卷积神经网络的训练方法。 Visualization of the first layer filters during training reveals that a few of them dominate, as shown in Fig. 6(a). To combat this, werenormalize each filter in the convolutional layers whose RMS value exceeds a fixed radius of 10−1 to this fixed radius 第一层 Filter的可视化在训练过程揭示，其中一部分起支配作用，如Fig 6 a 所示，为了对抗这种情况，我们重新归一化RMS值超过fixed-radius的0.1倍的每一个在卷基层的Filter\n 评论：详细的训练过程\nConvnet Visualization 关于特征：\n Feature Visualization: Fig. 2 shows feature visualizations from our model once training is complete. However, instead of showing the single strongest activation for a given feature map, we show the top 9 activations. 特征可视化：图2显示的特征可视化是当模型训练完成时就确定的，然而不显示对于给定特征映射的单一强刺激而显示top9\n  Alongside these visualizations we show the corresponding image patches. These have greater variation than visualizations as the latter solely focus on the discriminant structure within each patch. 沿着这个可视化我们可以观察到相当的当前图像区域。这有相当大的可视化成都相对于单独把注意力放到每一个path。\n  The projections from each layer show the hierarchical nature of the features in the network. 不同层的映射表现出网络中不同自然层级的特征\n  Feature Evolution during Training: Fig. 4 visualizes the progression during training of the strongest activation (across all training examples) within a given feature map projected back to pixel space. 特征在训练过程中的进化：图4，训练较强反应的神经元（在所有训练样本中）在给定特征映射逆向投影到像素空间的过程中的可视化。\n  Sudden jumps in appearance result from a change in the image from which the strongest activation originates. 表面上突然的跳跃来自图像最强的激活区域（此区域能够激发网络中的部分神经元产生大的特征变化） The lower layers of the model can be seen to converge within a fewepochs. However, the upper layers only develop after a considerable number of epochs (40-50), demonstrating the need to let the models train until fully converged. 模型的较低层可以在一定周期内观察到。然而，高层的网络只在相当大的周期后才能被建立起来，表明模型需要继续训练到完全收敛 Feature Invariance: Fig. 5 shows 5 sample images being translated,rotated and scaled by varying degrees while looking at the changes in the feature vectors from the top and bottom layers of the model,relative to the untransformed feature. 特征独立性：图5，显示五个样本图经过变换，旋转，缩放多种随机模型，然后从底层到高层观察特征向量与未变换的特征向量进行对比\n 小的变换对于模型的第一层有显著影响，但是对于顶层特征影响不大，对于变换和缩放大致呈线性\n The network output is stable to translations and scalings. 网络输出对于变换和尺度缩放稳定。 In general, the output is not invariant to rotation, except forobject with rotational symmetry (e.g. entertainment center). 然而，输出对于旋转变换不稳定，除非是对齐式的旋转。\n 评论：训练过程中的特征是怎么来的。\nArchitecture Selection  While visualization of a trained model gives insight into its operation, it can also assist with selecting good architectures in the first place. 当可视化一个训练好的模型给出了其内部的操作，也能帮助我们选取更好的架构。 The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies. 第一层filter是混合了极其高频和极其低频的信息，只有少量的中频信息。 Additionally, the 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. 第二层可视化展示了混淆的手工结果由在第一层中较大的步长（4）引起的 To remedy these problems, we 解决办法： (i) reduced the 1st layer filter size from 11x11 to 7x7 (ii) made the stride of the convolution 2, rather than 4.\n  This new architecture retains much more information inthe 1st and 2nd layer fea- tures, as shown in Fig. 6(c) \u0026 (e). More importantly, it also improves the classification performance as shown in Section 5.1.\n 评论：本段讲如何选取架构，说明步长在其中的影响\nOcclusion Sensitivity  With image classification approaches, a natural question is if themodel is truly identifying the location of the object in the image, or just using the surrounding context. 对于图像分类的应用，一个自然的问题是模型是否只利用图片中的物体，还是使用周围的上下文信息 Fig. 7 attempts to answer this question by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier. 图7试图回答这个问题，通过系统的遮挡输入图片不同的位置，使用一个灰色方框，然后监视分类器的输出\n  When the occluder covers the image region that appears in the visualization, we see a strong drop in activity in the feature map. 当遮挡覆盖到可视化中出现的区域，我们发现在特征映射层有一个强烈的drop Fig. 4 and Fig. 2. 图4和图2\n 评论：遮挡不同区域的影响，不同区域敏感度不同。\nCorrespondence Analysis 一致性分析\n Deep models differ from many existing recognition approaches in that there is no explicit mechanism for establishing correspondence betweenspecific object parts in different images (e.g. faces have a particular spatial configuration of the eyes and nose) 深度学习模型与现存其他识别机制不同在于：其不存在对于在不同图片之间某些物体的特殊部分之间的准确的区别关系（例如：脸部存在一个鼻子和脸的特别空间关系） 不同的特征向量计算公式：\n  We then measure the consistency of this difference vector delta between all related image pairs (i, j):\n  我们然后计算所有图片对之间的不同。 where H is Hamming distance. H为汉明距离 A lower value indicates greater consistency in the change resulting from the masking operation, hence tighter correspondence between the same object parts in different images (i.e. blocking the left eye) 在遮挡操作的变换结果中，一个较低的值表示部件之间较大的相关性fig8：\n Table 1 评论：不同特征的独立性验证，如果你有鼻子眼睛嘴的脸部特征，遮住鼻子对最后的特征向量影响不大，说明他们之间的相关性比较强，类似于一张图如果有鼻子，基本也有眼睛，所以你遮住眼睛也会得到差不多的特征向量。 总结：简单的学习了一下这篇文章，后面第五部分讲的是经验，关于如何训练高质量的网络，会在下一篇推出，欢迎收看。\n总结 又是一片翻译文，哈哈哈哈，有需要的可以读读，营养还是有的，只是我现在不太喜欢这种文章\n","wordCount":"1578","inLanguage":"en","datePublished":"2017-09-13T16:46:09Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-visualizing-and-understanding-cnn.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a></div><h1 class=post-title>Visualizing and Understanding CNN</h1><div class=post-meta><span title="2017-09-13 16:46:09 +0000 UTC">September 13, 2017</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e5%8f%af%e8%a7%86%e5%8c%96 aria-label=卷积神经网络的可视化>卷积神经网络的可视化</a><ul><li><a href=#%e5%89%8d%e8%a8%80 aria-label=前言>前言</a></li><li><a href=#abstract aria-label=Abstract：>Abstract：</a></li><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#related-work aria-label="Related Work">Related Work</a></li><li><a href=#approach aria-label=Approach>Approach</a><ul><li><a href=#visualization-with-a-deconvnet aria-label="Visualization with a Deconvnet">Visualization with a Deconvnet</a><ul><li><a href=#unpooling aria-label=Unpooling：>Unpooling：</a></li><li><a href=#rectification aria-label=Rectification：>Rectification：</a></li><li><a href=#filtering aria-label=Filtering：>Filtering：</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结：>总结：</a></li></ul></li></ul></li><li><a href=#training-details aria-label="Training Details">Training Details</a></li><li><a href=#convnet-visualization aria-label="Convnet Visualization">Convnet Visualization</a><ul><li><a href=#architecture-selection aria-label="Architecture Selection">Architecture Selection</a></li><li><a href=#occlusion-sensitivity aria-label="Occlusion Sensitivity">Occlusion Sensitivity</a></li><li><a href=#correspondence-analysis aria-label="Correspondence Analysis">Correspondence Analysis</a></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93-1 aria-label=总结>总结</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下
<strong>Keywords:</strong> CNN Visualizing</p><h1 id=卷积神经网络的可视化>卷积神经网络的可视化<a hidden class=anchor aria-hidden=true href=#卷积神经网络的可视化>#</a></h1><h2 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h2><p>研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下，写成博客，代码后续给出，不足之处还请大家指出。
大型卷积神经网络在图片分类上很成功，然而我们不知道他为什么能表现的如此不错，或者如何提高。</p><h2 id=abstract>Abstract：<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><blockquote><p>In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of inter-mediate feature layers and the operation of the classifier
我们研究一个优秀的可视化技术，能够给出函数内部特征层以及分类层的信息
Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark.
可视化使我们找到比Kri在ImageNet分类更好的网络架构。
We also perform an ablation study to discover the performance contribution from different model layers.
我们通过切块研究发现不同层对分类的作用。
We show our ImageNet model generalizes well to other datasets: when>the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.
我们展示了我们的 ImageNet 模型在其他数据集上获得优秀的表现：当我们重新训练SoftMax分类器。其结果信服的打败了当前SOTA结果，在Caltech-101和Caltech-256数据集上</p></blockquote><p>评论：作者要解决的是可视化深度学习模型，来给出内部的结构，工作原理，以及内在结构的相关性等。并且在这个基础上反向选择优化不同的深度架构（模型）来得到更好的模型，并给出了监督学习的Pre-training方法，在不同测试数据集上表现不俗。</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>卷积神经网络很牛，在各种分类比赛上获得state-of-the-art的结果。</p><blockquote><p>卷积神经网络在各大测试集上获得好结果的原因：
Several factors are responsible for this renewed interest in convnet
models:</p></blockquote><blockquote><p>(i) the availability of much larger training sets, with
大量的训练数据</p></blockquote><blockquote><p>(ii) powerful GPU implementations, making the training of very large models practical
GPU的高效计算</p></blockquote><blockquote><p>(iii) better model regularization strategies, such as Dropout (Hinton et al., 2012).
更优秀的网络结构（例如Dropout）</p></blockquote><blockquote><p>Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error.
如果不知道内部原因，我们的新模型只能停留在实验，观察的基础上</p></blockquote><blockquote><p>In this paper we introduce a visualization technique that reveals the>input stimuli that excite individual feature maps at any layer in the>model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model.
本文提出了一种可视化技术，其能够揭示输入是如何激活那些独立的特征映射在模型中的任一层。这项技术也允许我们来观察特征在训练过程中的进化过程来判断模型潜在的问题。</p></blockquote><blockquote><p>The visualization technique we propose uses a multi-layered>Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space.
我们提出了使用多层逆卷积网络，（Zeiler et al 2014年）提出的，将特征反向映射会到输入层观察结果</p></blockquote><blockquote><p>We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification
通过遮挡输入图片的部分，对分类器进行分析，来揭示哪些部分对分类结果产生相对重的影响</p></blockquote><blockquote><p>Using these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that>outperform their results on ImageNet.
使用这些工具，我们开始使用此架构探索不同的架构，认识在ImageNet上表现出色的结构</p></blockquote><blockquote><p>We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top.
我们随后探索架构对于其他数据集的范化能力，在只重新训练softmax分类器的基础上。</p></blockquote><blockquote><p>As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et>al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008)
监督学习的Pre-training来对比无监督的Pre-training方法（Hinton et al., 2006 Bengio et al., 2007; Vincent et al., 2008）</p></blockquote><p>评论：主要就是说，以前都是不知道为啥深度学习会工作，不知道如何优化，只是考实验观察，现在我们能牛x的知道为啥能工作了，虽然没有数学证明，但我们知道怎么调了，知道工作原理，知道怎么Pre-training。。。</p><h2 id=related-work>Related Work<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h2><blockquote><p>Our approach, by contrast, provides a non-parametric view of invariance, showing which patterns from the training set activate the feature map.
我们的工作提出了一种无参数的不变性观点，来展示训练数据的哪些部分激活了特征映射</p></blockquote><p>评论：没有评论</p><h2 id=approach>Approach<a hidden class=anchor aria-hidden=true href=#approach>#</a></h2><blockquote><p>We use standard fully supervised convnet models throughout the paper, as defined by (LeCun et al., 1989) and (Krizhevsky et al., 2012).
我们在整篇文章使用标准完全监督卷积网络模型，在 (LeCun et al., 1989) and (Krizhevsky et al., 2012)定义的。
(i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters;</p></blockquote><blockquote><p>(ii) passing the responses through a rectified linear function (relu(x) = max(x, 0));</p></blockquote><blockquote><p>(iii) max pooling over local neighborhoodsand</p></blockquote><blockquote><p>(iv) a local contrast operation that normalizes the responses across feature maps.</p></blockquote><blockquote><p>The top few layers of the network are conventional fully-connected</p></blockquote><blockquote><p>networks and the final layer is a softmax classifie</p></blockquote><p><img loading=lazy src=./20160913190123281.png alt></p><h3 id=visualization-with-a-deconvnet>Visualization with a Deconvnet<a hidden class=anchor aria-hidden=true href=#visualization-with-a-deconvnet>#</a></h3><blockquote><p>We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps.
我们提出了一个高级的方法来映射激活反向到输入像素空间，来展示在特征空间哪一部分输入引起了这个给定的激活
In (Zeiler et al., 2011), deconvnets were proposed as a way of>performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convent.
在(Zeiler et al., 2011)，Deconvnets 被提出作为一种表现非监督学习的方法。这里他们不再用于任何其学习能力，只是用于研究已经训练好的Convnet
To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer.
为了测试一个给定的神经元激活，我们设置所有其他的同层神经元激活值为零，传导特征映射作为输入来激活deconvnet层
Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is>reached.
在激活选定特征的神经元后Unpool，rectify，filter来重建本层的激活。</p></blockquote><h4 id=unpooling>Unpooling：<a hidden class=anchor aria-hidden=true href=#unpooling>#</a></h4><blockquote><p>In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the>deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure
最大池化不可逆，我们通过记录位置来进行近似，记录被称为一组switch值，在deconvnet中，逆池化使用这些switch值来定位重建上一层，保留激活分布。在Fig1中说明</p></blockquote><p><img loading=lazy src=./20160913191649067.png alt></p><h4 id=rectification>Rectification：<a hidden class=anchor aria-hidden=true href=#rectification>#</a></h4><blockquote><p>The convnet uses relu non-linearities, which rectify the feature maps
thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive),we pass the reconstructed signal through a relu non-linearity.
Convnet使用ReLu非线性函数，保证激活值非负；为保证每层特征可重建，我们让所有重建信号经过ReLu层。</p></blockquote><h4 id=filtering>Filtering：<a hidden class=anchor aria-hidden=true href=#filtering>#</a></h4><blockquote><p>The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not>the output of the layer beneath.
Convnet使用学习到的Filters从前一层来获取特征映射。相反，deconvnet使用同一filter的转置，但是操作的对象是整流结果(Rectification)，而不是之前的层。</p></blockquote><h4 id=总结>总结：<a hidden class=anchor aria-hidden=true href=#总结>#</a></h4><blockquote><p>Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative
由于模型是训练成有区别的，因此他们理所应当的展示输入图片的那些部分是有区别的。
Note that these projections are not samples from the model, since there is no generative process involved
注意这些映射不是从模型中采样，因为没有范化处理涉及。</p></blockquote><p>评论：此段描述了具体如何反向将特征映射到像素空间。</p><h2 id=training-details>Training Details<a hidden class=anchor aria-hidden=true href=#training-details>#</a></h2><blockquote><p>The architecture, shown in Fig. 3, is similar to that used by (Krizhevsky et al., 2012) for ImageNet classification
结构在Fig 3中（本文第一张图）。。。</p></blockquote><p>训练方法：</p><blockquote><p>The model was trained on the ImageNet 2012 training set (1.3 million images, spread over 1000 different classes). Each RGB image was preprocessed by resizing the smallest dimension to 256, cropping the center 256x256 region, subtracting the per-pixel mean (across all images) and then using 10 different sub-crops of size 224x224 (corners+ center with(out) horizontal flips). Stochastic gradient descent with a mini-batch size of 128 was used to update the parameters, starting
with a learning rate of 10−2, in conjunction with a momentum term of 0.9. We anneal the learning rate throughout training manually when the validation error plateaus. Dropout (Hinton et al., 2012) is used in the fully connected layers (6 and 7) with a rate of 0.5.
此处翻译略过，描述了卷积神经网络的训练方法。
Visualization of the first layer filters during training reveals that a few of them dominate, as shown in Fig. 6(a). To combat this, we>renormalize each filter in the convolutional layers whose RMS value exceeds a fixed radius of 10−1 to this fixed radius
第一层 Filter的可视化在训练过程揭示，其中一部分起支配作用，如Fig 6 a 所示，为了对抗这种情况，我们重新归一化RMS值超过fixed-radius的0.1倍的每一个在卷基层的Filter</p></blockquote><p><img loading=lazy src=./20160913221329283.png alt>
评论：详细的训练过程</p><h2 id=convnet-visualization>Convnet Visualization<a hidden class=anchor aria-hidden=true href=#convnet-visualization>#</a></h2><p>关于特征：</p><blockquote><p>Feature Visualization: Fig. 2 shows feature visualizations from our model once training is complete. However, instead of showing the single strongest activation for a given feature map, we show the top 9 activations.
特征可视化：图2显示的特征可视化是当模型训练完成时就确定的，然而不显示对于给定特征映射的单一强刺激而显示top9</p></blockquote><p><img loading=lazy src=./20160913214421030.png alt></p><blockquote><p>Alongside these visualizations we show the corresponding image patches. These have greater variation than visualizations as the latter solely focus on the discriminant structure within each patch.
沿着这个可视化我们可以观察到相当的当前图像区域。这有相当大的可视化成都相对于单独把注意力放到每一个path。</p></blockquote><blockquote><p>The projections from each layer show the hierarchical nature of the features in the network.
不同层的映射表现出网络中不同自然层级的特征</p></blockquote><blockquote><p>Feature Evolution during Training: Fig. 4 visualizes the progression during training of the strongest activation (across all training examples) within a given feature map projected back to pixel space.
特征在训练过程中的进化：图4，训练较强反应的神经元（在所有训练样本中）在给定特征映射逆向投影到像素空间的过程中的可视化。</p></blockquote><p><img loading=lazy src=./20160913215022775.png alt></p><blockquote><p>Sudden jumps in appearance result from a change in the image from which the strongest activation originates.
表面上突然的跳跃来自图像最强的激活区域（此区域能够激发网络中的部分神经元产生大的特征变化）
The lower layers of the model can be seen to converge within a few>epochs. However, the upper layers only develop after a considerable number of epochs (40-50), demonstrating the need to let the models train until fully converged.
模型的较低层可以在一定周期内观察到。然而，高层的网络只在相当大的周期后才能被建立起来，表明模型需要继续训练到完全收敛
Feature Invariance: Fig. 5 shows 5 sample images being translated,rotated and scaled by varying degrees while looking at the changes in the feature vectors from the top and bottom layers of the model,relative to the untransformed feature.
特征独立性：图5，显示五个样本图经过变换，旋转，缩放多种随机模型，然后从底层到高层观察特征向量与未变换的特征向量进行对比</p></blockquote><p><img loading=lazy src=./20160913214923055.png alt>
小的变换对于模型的第一层有显著影响，但是对于顶层特征影响不大，对于变换和缩放大致呈线性</p><blockquote><p>The network output is stable to translations and scalings.
网络输出对于变换和尺度缩放稳定。
In general, the output is not invariant to rotation, except for>object with rotational symmetry (e.g. entertainment center).
然而，输出对于旋转变换不稳定，除非是对齐式的旋转。</p></blockquote><p>评论：训练过程中的特征是怎么来的。</p><h3 id=architecture-selection>Architecture Selection<a hidden class=anchor aria-hidden=true href=#architecture-selection>#</a></h3><blockquote><p>While visualization of a trained model gives insight into its operation, it can also assist with selecting good architectures in the first place.
当可视化一个训练好的模型给出了其内部的操作，也能帮助我们选取更好的架构。
The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies.
第一层filter是混合了极其高频和极其低频的信息，只有少量的中频信息。
Additionally, the 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions.
第二层可视化展示了混淆的手工结果由在第一层中较大的步长（4）引起的
To remedy these problems, we
解决办法：
(i) reduced the 1st layer filter size from 11x11 to 7x7
(ii) made the stride of the convolution 2, rather than 4.</p></blockquote><blockquote><p>This new architecture retains much more information in>the 1st and 2nd layer fea- tures, as shown in Fig. 6(c) & (e). More importantly, it also improves the classification performance as shown in Section 5.1.</p></blockquote><p>评论：本段讲如何选取架构，说明步长在其中的影响</p><h3 id=occlusion-sensitivity>Occlusion Sensitivity<a hidden class=anchor aria-hidden=true href=#occlusion-sensitivity>#</a></h3><blockquote><p>With image classification approaches, a natural question is if themodel is truly identifying the location of the object in the image, or just using the surrounding context.
对于图像分类的应用，一个自然的问题是模型是否只利用图片中的物体，还是使用周围的上下文信息
Fig. 7 attempts to answer this question by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier.
图7试图回答这个问题，通过系统的遮挡输入图片不同的位置，使用一个灰色方框，然后监视分类器的输出</p></blockquote><p><img loading=lazy src=./20160913215441245.png alt></p><blockquote><p>When the occluder covers the image region that appears in the visualization, we see a strong drop in activity in the feature map.
当遮挡覆盖到可视化中出现的区域，我们发现在特征映射层有一个强烈的drop
Fig. 4 and Fig. 2. 图4和图2</p></blockquote><p>评论：遮挡不同区域的影响，不同区域敏感度不同。</p><h3 id=correspondence-analysis>Correspondence Analysis<a hidden class=anchor aria-hidden=true href=#correspondence-analysis>#</a></h3><p>一致性分析</p><blockquote><p>Deep models differ from many existing recognition approaches in that there is no explicit mechanism for establishing correspondence between>specific object parts in different images (e.g. faces have a particular spatial configuration of the eyes and nose)
深度学习模型与现存其他识别机制不同在于：其不存在对于在不同图片之间某些物体的特殊部分之间的准确的区别关系（例如：脸部存在一个鼻子和脸的特别空间关系）
不同的特征向量计算公式：</p></blockquote><p><img loading=lazy src=./20160913220428886.png alt></p><blockquote><p>We then measure the consistency of this difference vector delta between all related image pairs (i, j):</p></blockquote><p><img loading=lazy src=./20160913220143576.png alt></p><blockquote><p>我们然后计算所有图片对之间的不同。
where H is Hamming distance.
H为汉明距离
A lower value indicates greater consistency in the change resulting
from the masking operation, hence tighter correspondence between the
same object parts in different images (i.e. blocking the left eye)
在遮挡操作的变换结果中，一个较低的值表示部件之间较大的相关性fig8：</p></blockquote><p><img loading=lazy src=./20160913215615949.png alt>
Table 1
<img loading=lazy src=./20160913220319983.png alt>
评论：不同特征的独立性验证，如果你有鼻子眼睛嘴的脸部特征，遮住鼻子对最后的特征向量影响不大，说明他们之间的相关性比较强，类似于一张图如果有鼻子，基本也有眼睛，所以你遮住眼睛也会得到差不多的特征向量。
总结：简单的学习了一下这篇文章，后面第五部分讲的是经验，关于如何训练高质量的网络，会在下一篇推出，欢迎收看。</p><h2 id=总结-1>总结<a hidden class=anchor aria-hidden=true href=#总结-1>#</a></h2><p>又是一片翻译文，哈哈哈哈，有需要的可以读读，营养还是有的，只是我现在不太喜欢这种文章</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/cnn-visualizing/>CNN Visualizing</a></li><li><a href=https://go.face2ai.com/tags/cnn/>CNN</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing and Understanding CNN on twitter" href="https://twitter.com/intent/tweet/?text=Visualizing%20and%20Understanding%20CNN&url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-visualizing-and-understanding-cnn.zh%2f&hashtags=CNNVisualizing%2cCNN"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing and Understanding CNN on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-visualizing-and-understanding-cnn.zh%2f&title=Visualizing%20and%20Understanding%20CNN&summary=Visualizing%20and%20Understanding%20CNN&source=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-visualizing-and-understanding-cnn.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing and Understanding CNN on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-visualizing-and-understanding-cnn.zh%2f&title=Visualizing%20and%20Understanding%20CNN"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing and Understanding CNN on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-visualizing-and-understanding-cnn.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing and Understanding CNN on whatsapp" href="https://api.whatsapp.com/send?text=Visualizing%20and%20Understanding%20CNN%20-%20https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-visualizing-and-understanding-cnn.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Visualizing and Understanding CNN on telegram" href="https://telegram.me/share/url?text=Visualizing%20and%20Understanding%20CNN&url=https%3a%2f%2fgo.face2ai.com%2f%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%2fdeep-learning-visualizing-and-understanding-cnn.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>