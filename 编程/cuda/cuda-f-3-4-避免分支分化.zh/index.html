<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【CUDA 基础】3.4 避免分支分化 | 谭升的博客</title><meta name=keywords content="规约问题,分支分化"><meta name=description content="Abstract: 介绍规约问题中的分支分化问题
Keywords: 规约问题，分支分化 此篇有些结果和参考书中结果相反，需要更深入的技术才能解决"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96.zh/><link crossorigin=anonymous href=../../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="【CUDA 基础】3.4 避免分支分化"><meta property="og:description" content="Abstract: 介绍规约问题中的分支分化问题
Keywords: 规约问题，分支分化 此篇有些结果和参考书中结果相反，需要更深入的技术才能解决"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96.zh/"><meta property="article:section" content="编程"><meta property="article:published_time" content="2018-04-17T23:32:55+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【CUDA 基础】3.4 避免分支分化"><meta name=twitter:description content="Abstract: 介绍规约问题中的分支分化问题
Keywords: 规约问题，分支分化 此篇有些结果和参考书中结果相反，需要更深入的技术才能解决"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"【CUDA 基础】3.4 避免分支分化","item":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【CUDA 基础】3.4 避免分支分化","name":"【CUDA 基础】3.4 避免分支分化","description":"Abstract: 介绍规约问题中的分支分化问题 Keywords: 规约问题，分支分化 此篇有些结果和参考书中结果相反，需要更深入的技术才能解决\n","keywords":["规约问题","分支分化"],"articleBody":"Abstract: 介绍规约问题中的分支分化问题 Keywords: 规约问题，分支分化 此篇有些结果和参考书中结果相反，需要更深入的技术才能解决\n避免分支分化 我坚持写博客是因为我上次最困惑最难过的那段时间通过写博客改变了我的非常不好的情况，所以我认为写些东西梳理自己的思路能够改变我的生活，所以我会一直坚持，学习的内容是没有止境的，所以博客也可以写很多。 写博客为了收入我之前也想过，最后放弃了，因为如果你的目的就是挣钱，有写博客这大把时间还不如出去跑个滴滴或者送个外卖来得快，所以我把之前有的捐赠部分都取消掉了，我并不否定那些博客写的质量非常高的人因此有收入，做得好，帮助到人了就可以获得收入，但是为了收入去帮助人，那叫服务。所以我后面可能会挂一个小广告，但是绝对不会因为广告搞得博客非常凌乱，而且收入应该只用作服务器和域名费用，仅此而已，我个人对于做事非常看重目的，我的目的是分享知识，并不是收入，收入只是附加的。 本文介绍一个并行计算中最常见的典型情况，并行分化(线程束分化的等价问题)，以及规约问题，以及其初步优化。\n并行规约问题 在串行编程中，我们最最最常见的一个问题就是一组特别多数字通过计算变成一个数字，比如加法，也就是求这一组数据的和，或者乘法，这种计算当有如下特点的时候，我们可以用并行归约的方法处理他们：\n 结合性 交换性  对应的加法或者乘法就是交换律和结合律，在我们的数学分析系列已经详细的介绍了加法和乘法的结合律和交换律的证明。所以对于所有有这两个性质的计算，都可以使用归约式计算。 为什么叫归约，归约是一种常见的计算方式（串并行都可以），一开始我听到这个名字的时候应该是在两年前了，感觉很迷惑，后来发现，归约的归有递归的意思，约就是减少，这样就很明显了，每次迭代计算方式都是相同的（归），从一组多个数据最后得到一个数（约）。 归约的方式基本包括如下几个步骤：\n 将输入向量划分到更小的数据块中 用一个线程计算一个数据块的部分和 对每个数据块的部分和再求和得到最终的结果。  数据分块保证我们可以用一个线程块来处理一个数据块。 一个线程处理更小的块，所以一个线程块可以处理一个较大的块，然后多个块完成整个数据集的处理。 最后将所有线程块得到的结果相加，就是结果，这一步一般在cpu上完成。\n归约问题最常见的加法计算是把向量的数据分成对，然后用不同线程计算每一对元素，得到的结果作为输入继续分成对，迭代的进行，直到最后一个元素。 成对的划分常见的方法有以下两种：\n 相邻配对：元素与他们相邻的元素配对  交错配对：元素与一定距离的元素配对   图中将两种方式表现的很清楚了，我们可以用代码实现以下。 首先是cpu版本实现交错配对归约计算的代码：\nint recursiveReduce(int *data, int const size) { // terminate check \tif (size == 1) return data[0]; // renew the stride \tint const stride = size / 2; if (size % 2 == 1) { for (int i = 0; i  stride; i++) { data[i] += data[i + stride]; } data[0] += data[size - 1]; } else { for (int i = 0; i  stride; i++) { data[i] += data[i + stride]; } } // call \treturn recursiveReduce(data, stride); } 和书上的代码有些不同，因为书上的代码没有考虑数组长度非2的整数幂次的结果。所以我加了一个处理奇数数组最后一个无人配对的元素的处理。 这个加法运算可以改成任何满足结合律和交换律的计算。比如乘法，求最大值等。 下面我们就来通过不同的配对方式，不同的数据组织来看CUDA的执行效率。\n并行规约中的分化 线程束分化已经明确说明了，有判断条件的地方就会产生分支，比如if 和 for这类关键词。 如下图所表示的那样，我们对相邻元素配对进行内核实现的流程描述：\n根据上一小节介绍： 第一步：是把这个一个数组分块，每一块只包含部分数据，如上图那样（图中数据较少，但是我们假设一块上只有这么多。），我们假定这是线程块的全部数据 第二步：就是每个线程要做的事，橙色圆圈就是每个线程做的操作，可见线程threadIdx.x=0 的线程进行了三次计算，奇数线程一致在陪跑，没做过任何计算，但是根据3.2中介绍，这些线程虽然什么都不干，但是不可以执行别的指令，4号线程做了两步计算，2号和6号只做了一次计算。 第三步：将所有块得到的结果相加，就是最终结果 这个计算划分就是最简单的并行规约算法，完全符合上面我们提到的三步走的套路 值得注意的是，我们每次进行一轮计算（黄色框，这些操作同时并行）的时候，部分全局内存要进行一次修改，但只有部分被替换，而不被替换的，也不会在后面被使用到，如蓝色框里标注的内存，就被读了一次，后面就完全没有人管了。 我们现在把我们的内核代码贴出来\n__global__ void reduceNeighbored(int * g_idata,int * g_odata,unsigned int n) { //set thread ID \tunsigned int tid = threadIdx.x; //boundary check \tif (tid = n) return; //convert global data pointer to the \tint *idata = g_idata + blockIdx.x*blockDim.x; //in-place reduction in global memory \tfor (int stride = 1; stride  blockDim.x; stride *= 2) { if ((tid % (2 * stride)) == 0) { idata[tid] += idata[tid + stride]; } //synchronize within block \t__syncthreads(); } //write result for this block to global mem \tif (tid == 0) g_odata[blockIdx.x] = idata[0]; } 这里面唯一要注意的地方就是同步指令\n__syncthreads(); 原因还是能从图上找到，我们的每一轮操作都是并行的，但是不保证所有线程能同时执行完毕，所以需要等待，执行的快的等待慢的，这样就能避免块内的线程竞争内存了。 被操作的两个对象之间的距离叫做跨度，也就是变量stride， 完整的执行逻辑如下， 注意主机端和设备端的分界，注意设备端的数据分块。 完整的可执行代码Github:https://github.com/Tony-Tan/CUDA_Freshman\n这里把主函数贴出来，但注意里面包含后面的核函数执行部分，所以想要运行还是去github上拉一下吧，顺便点个star\nint main(int argc,char** argv) { ......... int size = 1  24; ......... dim3 block(blocksize, 1); dim3 grid((size - 1) / block.x + 1, 1); ......... //cpu reduction \tint cpu_sum = 0; iStart = cpuSecond(); for (int i = 0; i  size; i++) cpu_sum += tmp[i]; printf(\"cpu sum:%d \\n\", cpu_sum); iElaps = cpuSecond() - iStart; printf(\"cpu reduce elapsed %lf ms cpu_sum: %d\\n\", iElaps, cpu_sum); //kernel 1:reduceNeighbored  CHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); warmup grid, block (idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i  grid.x; i++) gpu_sum += odata_host[i]; printf(\"gpu warmup elapsed %lf ms gpu_sum: %d\\n\", iElaps, gpu_sum, grid.x, block.x); //kernel 1:reduceNeighbored  CHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); reduceNeighbored  grid, block  (idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i  grid.x; i++) gpu_sum += odata_host[i]; printf(\"gpu reduceNeighbored elapsed %lf ms gpu_sum: %d\\n\", iElaps, gpu_sum, grid.x, block.x); //kernel 2:reduceNeighboredLess  CHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); reduceNeighboredLess grid, block(idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i  grid.x; i++) gpu_sum += odata_host[i]; printf(\"gpu reduceNeighboredLess elapsed %lf ms gpu_sum: %d\\n\", iElaps, gpu_sum, grid.x, block.x); //kernel 3:reduceInterleaved \tCHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); reduceInterleaved  grid, block  (idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i  grid.x; i++) gpu_sum += odata_host[i]; printf(\"gpu reduceInterleaved elapsed %lf ms gpu_sum: %d\\n\", iElaps, gpu_sum, grid.x, block.x); // free host memory  ..... } 代码太长不美观，删减一下，只留下了内核执行部分，可见，主函数只有最后一个循环求和的过程是要注意别忘了的，其他都是常规操作 还有一点，需要注意实际任务中数组不可能每次都是2的整数幂，如果不是2的整数幂需要确定数组边界。\n上图就是执行结果，为啥有那么多，因为我把下面两个经过优化的也装进去了，黄色框框里是我们上面这段代码执行结果和时间，warmup 是为了启动gpu防止首次启动计算时gpu的启动过程耽误时间，影响效率测试，warmup的代码就是reducneighbored的代码，可见还是有微弱的差别的。\n改善并行规约的分化 上面归约显然是最原始的，未经过优化的东西是不能拿出去使用的，或者说一个真理是，不可能一下子就写出来满意的代码。\nif ((tid % (2 * stride)) == 0) 这个条件判断给内核造成了极大的分支，如图所示： 第一轮 有 $\\frac{1}{2}$ 的线程没用 第二轮 有 $\\frac{3}{4}$ 的线程没用 第三轮 有 $\\frac{7}{8}$ 的线程没用\n可见线程利用率是非常低的，因为这些线程在一个线程束，所以，只能等待，不能执行别的指令。 对于上面的低利用率，我们想到了下面这个方案来解决： 注意橙色圆形内的标号是线程符号，这样的计算线程的利用率是高于原始版本的，核函数如下：\n__global__ void reduceNeighboredLess(int * g_idata,int *g_odata,unsigned int n) { unsigned int tid = threadIdx.x; unsigned idx = blockIdx.x*blockDim.x + threadIdx.x; // convert global data pointer to the local point of this block \tint *idata = g_idata + blockIdx.x*blockDim.x; if (idx  n) return; //in-place reduction in global memory \tfor (int stride = 1; stride  blockDim.x; stride *= 2) { //convert tid into local array index \tint index = 2 * stride *tid; if (index  blockDim.x) { idata[index] += idata[index + stride]; } __syncthreads(); } //write result for this block to global men \tif (tid == 0) g_odata[blockIdx.x] = idata[0]; } 最关键的一步就是\nint index = 2 * stride *tid; 这一步保证index能够向后移动到有数据要处理的内存位置，而不是简单的用tid对应内存地址，导致大量线程空闲。 那么这样做的效率高在哪？ 首先我们保证在一个块中前几个执行的线程束是在接近满跑的，而后半部分线程束基本是不需要执行的，当一个线程束内存在分支，而分支都不需要执行的时候，硬件会停止他们调用别人，这样就节省了资源，所以高效体现在这，如果还是所有分支不满足的也要执行，即便整个线程束都不需要执行的时候，那这种方案就无效了，还好现在的硬件比较只能，于是，我们执行后得到如下结果 这个效率提升惊人，有木有，直接降了一位！大约差了一半。 我们前面一直在介绍一个叫做nvprof的工具，那么我们现在就来看看，每个线程束上执行指令的平均数量，同样我会给出四个内核的，但我们之关心黄框内的 使用如下指令\nnvprof --metrics inst_per_warp ./reduceInteger 指标结果是原始内核444.9 比新内核 150.5 可见原始内核中有很多分支指令被执行，而这些分支指令是没有意义的。 分化程度越高，inst_per_warp这个指标会相对越高。这个大家要记一下，以后我们测试效率的时候会经常使用。 接着我们看一下内存加载吞吐：\nnvprof --metrics gld_throughput ./reduceInteger 依旧只看黄色框框，我们的新内核，内存效率要高很多，也接近一倍了，原因还是我们上面分析的，一个线程块，前面的几个线程束都在干活，而后面几个根本不干活，不干活的不会被执行，而干活的内存请求肯定很集中，最大效率的利用带宽，而最naive的内核，不干活的线程也在线程束内跟着跑，又不请求内存，所以内存访问被打碎，理论上是只有一半的内存效率，测试来看非常接近。\n交错配对的规约 上面的套路是修改线程处理的数据，使部分线程束最大程度利用数据，接下来采用同样的思想，但是方法不同，接下来我们使用的方法是调整跨度，也就是我们每个线程还是处理对应的内存的位置，但内存对不是相邻的了，而是隔了一定距离的： 示意图是最好的描述方法： 我们依然把上图当做一个完整的线程块，那么前半部分的线程束依然是最大负载在跑，后半部分的线程束也是啥活不干\n__global__ void reduceInterleaved(int * g_idata, int *g_odata, unsigned int n) { unsigned int tid = threadIdx.x; unsigned idx = blockIdx.x*blockDim.x + threadIdx.x; // convert global data pointer to the local point of this block \tint *idata = g_idata + blockIdx.x*blockDim.x; if (idx = n) return; //in-place reduction in global memory \tfor (int stride = blockDim.x/2; stride 0; stride =1) { if (tid stride) { idata[tid] += idata[tid + stride]; } __syncthreads(); } //write result for this block to global men \tif (tid == 0) g_odata[blockIdx.x] = idata[0]; } 执行结果 如果单从优化原理的角度，这个内核和前面的内核应该是相同效率的，但是测试结果是，这个新内核比前面的内核速度快了不少，所以我们还是考察一下指标吧：\nnvprof --metrics inst_per_warp ./reduceInteger nvprof --metrics gld_throughput ./reduceInteger reduceInterleaved内存效率居然是最低的，但是线程束内分化却是最小的。 而书中说reduceInterleaved 的优势在内存读取，而非线程束分化，我们实际操作却得出了完全不同结论，到底是内存的无情，还是编译器的绝望，请看我们下个系列，到时候我们会直接研究机器码，来确定到底是什么影响了看似类似，却有结果悬殊的两个内核\n此处需要查看机器码，确定两个内核的实际不同\n总结 本文比较尴尬最后得出来一个跟书中相反的结论，但是整篇的思路还是很流畅的，所以我还是把它发出来了，按照我的性格这种有疑问没解决的（无法确定是否是已有技术无法解决的）一般不会发出来，但是考虑到可能是编译器进化导致的结果，所以还是先发出来，后面如果研究明白了，在CUDA进阶系列作为案例讨论。\n","wordCount":"794","inLanguage":"en","datePublished":"2018-04-17T23:32:55Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a></div><h1 class=post-title>【CUDA 基础】3.4 避免分支分化</h1><div class=post-meta><span title="2018-04-17 23:32:55 +0000 UTC">April 17, 2018</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e9%81%bf%e5%85%8d%e5%88%86%e6%94%af%e5%88%86%e5%8c%96 aria-label=避免分支分化>避免分支分化</a><ul><li><a href=#%e5%b9%b6%e8%a1%8c%e8%a7%84%e7%ba%a6%e9%97%ae%e9%a2%98 aria-label=并行规约问题>并行规约问题</a></li><li><a href=#%e5%b9%b6%e8%a1%8c%e8%a7%84%e7%ba%a6%e4%b8%ad%e7%9a%84%e5%88%86%e5%8c%96 aria-label=并行规约中的分化>并行规约中的分化</a></li><li><a href=#%e6%94%b9%e5%96%84%e5%b9%b6%e8%a1%8c%e8%a7%84%e7%ba%a6%e7%9a%84%e5%88%86%e5%8c%96 aria-label=改善并行规约的分化>改善并行规约的分化</a></li><li><a href=#%e4%ba%a4%e9%94%99%e9%85%8d%e5%af%b9%e7%9a%84%e8%a7%84%e7%ba%a6 aria-label=交错配对的规约>交错配对的规约</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 介绍规约问题中的分支分化问题
<strong>Keywords:</strong> 规约问题，分支分化 <font color=ff0000>此篇有些结果和参考书中结果相反，需要更深入的技术才能解决</font></p><h1 id=避免分支分化>避免分支分化<a hidden class=anchor aria-hidden=true href=#避免分支分化>#</a></h1><p>我坚持写博客是因为我上次最困惑最难过的那段时间通过写博客改变了我的非常不好的情况，所以我认为写些东西梳理自己的思路能够改变我的生活，所以我会一直坚持，学习的内容是没有止境的，所以博客也可以写很多。
写博客为了收入我之前也想过，最后放弃了，因为如果你的目的就是挣钱，有写博客这大把时间还不如出去跑个滴滴或者送个外卖来得快，所以我把之前有的捐赠部分都取消掉了，我并不否定那些博客写的质量非常高的人因此有收入，做得好，帮助到人了就可以获得收入，但是为了收入去帮助人，那叫服务。所以我后面可能会挂一个小广告，但是绝对不会因为广告搞得博客非常凌乱，而且收入应该只用作服务器和域名费用，仅此而已，我个人对于做事非常看重目的，我的目的是分享知识，并不是收入，收入只是附加的。
本文介绍一个并行计算中最常见的典型情况，并行分化(<a href=https://face2ai.com/CUDA-F-3-2-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8-P1/>线程束分化</a>的等价问题)，以及规约问题，以及其初步优化。</p><h2 id=并行规约问题>并行规约问题<a hidden class=anchor aria-hidden=true href=#并行规约问题>#</a></h2><p>在串行编程中，我们最最最常见的一个问题就是一组特别多数字通过计算变成一个数字，比如加法，也就是求这一组数据的和，或者乘法，这种计算当有如下特点的时候，我们可以用并行归约的方法处理他们：</p><ol><li>结合性</li><li>交换性</li></ol><p>对应的加法或者乘法就是交换律和结合律，在我们的数学分析系列已经详细的介绍了加法和乘法的结合律和交换律的证明。所以对于所有有这两个性质的计算，都可以使用归约式计算。
为什么叫归约，归约是一种常见的计算方式（串并行都可以），一开始我听到这个名字的时候应该是在两年前了，感觉很迷惑，后来发现，归约的归有递归的意思，约就是减少，这样就很明显了，每次迭代计算方式都是相同的（归），从一组多个数据最后得到一个数（约）。
归约的方式基本包括如下几个步骤：</p><ol><li>将输入向量划分到更小的数据块中</li><li>用一个线程计算一个数据块的部分和</li><li>对每个数据块的部分和再求和得到最终的结果。</li></ol><p>数据分块保证我们可以用一个线程块来处理一个数据块。
一个线程处理更小的块，所以一个线程块可以处理一个较大的块，然后多个块完成整个数据集的处理。
最后将所有线程块得到的结果相加，就是结果，这一步一般在cpu上完成。</p><p>归约问题最常见的加法计算是把向量的数据分成对，然后用不同线程计算每一对元素，得到的结果作为输入继续分成对，迭代的进行，直到最后一个元素。
成对的划分常见的方法有以下两种：</p><ol><li>相邻配对：元素与他们相邻的元素配对
<img loading=lazy src=./xianglin.png alt></li><li>交错配对：元素与一定距离的元素配对
<img loading=lazy src=./jiaocuo.png alt></li></ol><p>图中将两种方式表现的很清楚了，我们可以用代码实现以下。
首先是cpu版本实现交错配对归约计算的代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=nf>recursiveReduce</span><span class=p>(</span><span class=kt>int</span> <span class=o>*</span><span class=n>data</span><span class=p>,</span> <span class=kt>int</span> <span class=k>const</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=c1>// terminate check
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>if</span> <span class=p>(</span><span class=n>size</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=c1>// renew the stride
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=k>const</span> <span class=n>stride</span> <span class=o>=</span> <span class=n>size</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=p>(</span><span class=n>size</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>stride</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>{</span>
</span></span><span class=line><span class=cl>			<span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>data</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=n>stride</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>		<span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+=</span> <span class=n>data</span><span class=p>[</span><span class=n>size</span> <span class=o>-</span> <span class=mi>1</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=k>else</span>
</span></span><span class=line><span class=cl>	<span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>stride</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>{</span>
</span></span><span class=line><span class=cl>			<span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>data</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=n>stride</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=c1>// call
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>return</span> <span class=n>recursiveReduce</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>stride</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>和书上的代码有些不同，因为书上的代码没有考虑数组长度非2的整数幂次的结果。所以我加了一个处理奇数数组最后一个无人配对的元素的处理。
这个加法运算可以改成任何满足结合律和交换律的计算。比如乘法，求最大值等。
下面我们就来通过不同的配对方式，不同的数据组织来看CUDA的执行效率。</p><h2 id=并行规约中的分化>并行规约中的分化<a hidden class=anchor aria-hidden=true href=#并行规约中的分化>#</a></h2><p><a href=https://face2ai.com/CUDA-F-3-2-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8-P1/>线程束分化</a>已经明确说明了，有判断条件的地方就会产生分支，比如if 和 for这类关键词。
如下图所表示的那样，我们对相邻元素配对进行内核实现的流程描述：</p><p><img loading=lazy src=https://tony4ai-1251394096.cos.ap-hongkong.myqcloud.com/blog_images/3_21.png alt></p><p>根据上一小节介绍：
第一步：是把这个一个数组分块，每一块只包含部分数据，如上图那样（图中数据较少，但是我们假设一块上只有这么多。），我们假定这是线程块的全部数据
第二步：就是每个线程要做的事，橙色圆圈就是每个线程做的操作，可见线程threadIdx.x=0 的线程进行了三次计算，奇数线程一致在陪跑，没做过任何计算，但是根据3.2中介绍，这些线程虽然什么都不干，但是不可以执行别的指令，4号线程做了两步计算，2号和6号只做了一次计算。
第三步：将所有块得到的结果相加，就是最终结果
这个计算划分就是最简单的并行规约算法，完全符合上面我们提到的三步走的套路
值得注意的是，我们每次进行一轮计算（黄色框，这些操作同时并行）的时候，部分全局内存要进行一次修改，但只有部分被替换，而不被替换的，也不会在后面被使用到，如蓝色框里标注的内存，就被读了一次，后面就完全没有人管了。
我们现在把我们的内核代码贴出来</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>reduceNeighbored</span><span class=p>(</span><span class=kt>int</span> <span class=o>*</span> <span class=n>g_idata</span><span class=p>,</span><span class=kt>int</span> <span class=o>*</span> <span class=n>g_odata</span><span class=p>,</span><span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=c1>//set thread ID
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>//boundary check
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>&gt;=</span> <span class=n>n</span><span class=p>)</span> <span class=k>return</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>//convert global data pointer to the
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=o>*</span><span class=n>idata</span> <span class=o>=</span> <span class=n>g_idata</span> <span class=o>+</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=o>*</span><span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>//in-place reduction in global memory
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>stride</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>stride</span> <span class=o>&lt;</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=n>stride</span> <span class=o>*=</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=p>((</span><span class=n>tid</span> <span class=o>%</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>stride</span><span class=p>))</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>{</span>
</span></span><span class=line><span class=cl>			<span class=n>idata</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>idata</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=n>stride</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>		<span class=c1>//synchronize within block
</span></span></span><span class=line><span class=cl><span class=c1></span>		<span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=c1>//write result for this block to global mem
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>g_odata</span><span class=p>[</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>idata</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>这里面唯一要注意的地方就是同步指令</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><p>原因还是能从图上找到，我们的每一轮操作都是并行的，但是不保证所有线程能同时执行完毕，所以需要等待，执行的快的等待慢的，这样就能避免块内的线程竞争内存了。
被操作的两个对象之间的距离叫做跨度，也就是变量stride，
完整的执行逻辑如下，
<img loading=lazy src=./3_22.png alt>
注意主机端和设备端的分界，注意设备端的数据分块。
完整的可执行代码Github:<a href=https://github.com/Tony-Tan/CUDA_Freshman>https://github.com/Tony-Tan/CUDA_Freshman</a></p><p>这里把主函数贴出来，但注意里面包含后面的核函数执行部分，所以想要运行还是去github上拉一下吧，顺便点个star</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span><span class=kt>char</span><span class=o>**</span> <span class=n>argv</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=p>.........</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>size</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>&lt;&lt;</span> <span class=mi>24</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=p>.........</span>
</span></span><span class=line><span class=cl>	<span class=n>dim3</span> <span class=n>block</span><span class=p>(</span><span class=n>blocksize</span><span class=p>,</span> <span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>dim3</span> <span class=n>grid</span><span class=p>((</span><span class=n>size</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>block</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=p>.........</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1>//cpu reduction
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=n>cpu_sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=n>iStart</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>size</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>cpu_sum</span> <span class=o>+=</span> <span class=n>tmp</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=n>printf</span><span class=p>(</span><span class=s>&#34;cpu sum:%d </span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>cpu_sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>iElaps</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>()</span> <span class=o>-</span> <span class=n>iStart</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=n>printf</span><span class=p>(</span><span class=s>&#34;cpu reduce                 elapsed %lf ms cpu_sum: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>iElaps</span><span class=p>,</span> <span class=n>cpu_sum</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=c1>//kernel 1:reduceNeighbored
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>idata_host</span><span class=p>,</span> <span class=n>bytes</span><span class=p>,</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>));</span>
</span></span><span class=line><span class=cl>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaDeviceSynchronize</span><span class=p>());</span>
</span></span><span class=line><span class=cl>	<span class=n>iStart</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>warmup</span> <span class=o>&lt;&lt;&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span> <span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaDeviceSynchronize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>iElaps</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>()</span> <span class=o>-</span> <span class=n>iStart</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>odata_host</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>gpu_sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>gpu_sum</span> <span class=o>+=</span> <span class=n>odata_host</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=n>printf</span><span class=p>(</span><span class=s>&#34;gpu warmup                 elapsed %lf ms gpu_sum: %d&lt;&lt;&lt;grid %d block %d&gt;&gt;&gt;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>iElaps</span><span class=p>,</span> <span class=n>gpu_sum</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>,</span> <span class=n>block</span><span class=p>.</span><span class=n>x</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1>//kernel 1:reduceNeighbored
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>idata_host</span><span class=p>,</span> <span class=n>bytes</span><span class=p>,</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>));</span>
</span></span><span class=line><span class=cl>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaDeviceSynchronize</span><span class=p>());</span>
</span></span><span class=line><span class=cl>	<span class=n>iStart</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>reduceNeighbored</span> <span class=o>&lt;&lt;</span> <span class=o>&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span> <span class=o>&gt;&gt;</span> <span class=o>&gt;</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaDeviceSynchronize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>iElaps</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>()</span> <span class=o>-</span> <span class=n>iStart</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>odata_host</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>gpu_sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>gpu_sum</span> <span class=o>+=</span> <span class=n>odata_host</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=n>printf</span><span class=p>(</span><span class=s>&#34;gpu reduceNeighbored       elapsed %lf ms gpu_sum: %d&lt;&lt;&lt;grid %d block %d&gt;&gt;&gt;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>iElaps</span><span class=p>,</span> <span class=n>gpu_sum</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>,</span> <span class=n>block</span><span class=p>.</span><span class=n>x</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1>//kernel 2:reduceNeighboredLess
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>idata_host</span><span class=p>,</span> <span class=n>bytes</span><span class=p>,</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>));</span>
</span></span><span class=line><span class=cl>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaDeviceSynchronize</span><span class=p>());</span>
</span></span><span class=line><span class=cl>	<span class=n>iStart</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>reduceNeighboredLess</span> <span class=o>&lt;&lt;&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaDeviceSynchronize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>iElaps</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>()</span> <span class=o>-</span> <span class=n>iStart</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>odata_host</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>gpu_sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>gpu_sum</span> <span class=o>+=</span> <span class=n>odata_host</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=n>printf</span><span class=p>(</span><span class=s>&#34;gpu reduceNeighboredLess   elapsed %lf ms gpu_sum: %d&lt;&lt;&lt;grid %d block %d&gt;&gt;&gt;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>iElaps</span><span class=p>,</span> <span class=n>gpu_sum</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>,</span> <span class=n>block</span><span class=p>.</span><span class=n>x</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1>//kernel 3:reduceInterleaved
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>idata_host</span><span class=p>,</span> <span class=n>bytes</span><span class=p>,</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>));</span>
</span></span><span class=line><span class=cl>	<span class=n>CHECK</span><span class=p>(</span><span class=n>cudaDeviceSynchronize</span><span class=p>());</span>
</span></span><span class=line><span class=cl>	<span class=n>iStart</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>reduceInterleaved</span> <span class=o>&lt;&lt;</span> <span class=o>&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span> <span class=o>&gt;&gt;</span> <span class=o>&gt;</span><span class=p>(</span><span class=n>idata_dev</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaDeviceSynchronize</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=n>iElaps</span> <span class=o>=</span> <span class=n>cpuSecond</span><span class=p>()</span> <span class=o>-</span> <span class=n>iStart</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>odata_host</span><span class=p>,</span> <span class=n>odata_dev</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>int</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=n>gpu_sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>gpu_sum</span> <span class=o>+=</span> <span class=n>odata_host</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=n>printf</span><span class=p>(</span><span class=s>&#34;gpu reduceInterleaved      elapsed %lf ms gpu_sum: %d&lt;&lt;&lt;grid %d block %d&gt;&gt;&gt;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>		<span class=n>iElaps</span><span class=p>,</span> <span class=n>gpu_sum</span><span class=p>,</span> <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>,</span> <span class=n>block</span><span class=p>.</span><span class=n>x</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=c1>// free host memory
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=p>.....</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>代码太长不美观，删减一下，只留下了内核执行部分，可见，主函数只有最后一个循环求和的过程是要注意别忘了的，其他都是常规操作
还有一点，需要注意实际任务中数组不可能每次都是2的整数幂，如果不是2的整数幂需要确定数组边界。</p><p><img loading=lazy src=./result1.png alt></p><p>上图就是执行结果，为啥有那么多，因为我把下面两个经过优化的也装进去了，黄色框框里是我们上面这段代码执行结果和时间，warmup 是为了启动gpu防止首次启动计算时gpu的启动过程耽误时间，影响效率测试，warmup的代码就是reducneighbored的代码，可见还是有微弱的差别的。</p><h2 id=改善并行规约的分化>改善并行规约的分化<a hidden class=anchor aria-hidden=true href=#改善并行规约的分化>#</a></h2><p>上面归约显然是最原始的，未经过优化的东西是不能拿出去使用的，或者说一个真理是，不可能一下子就写出来满意的代码。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>if</span> <span class=p>((</span><span class=n>tid</span> <span class=o>%</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>stride</span><span class=p>))</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></div><p>这个条件判断给内核造成了极大的分支，如图所示：
<img loading=lazy src=./3_21.png alt></p><p>第一轮 有 $\frac{1}{2}$ 的线程没用
第二轮 有 $\frac{3}{4}$ 的线程没用
第三轮 有 $\frac{7}{8}$ 的线程没用</p><p>可见线程利用率是非常低的，因为这些线程在一个线程束，所以，只能等待，不能执行别的指令。
对于上面的低利用率，我们想到了下面这个方案来解决：
<img loading=lazy src=./3_23.png alt>
注意橙色圆形内的标号是线程符号，这样的计算线程的利用率是高于原始版本的，核函数如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>reduceNeighboredLess</span><span class=p>(</span><span class=kt>int</span> <span class=o>*</span> <span class=n>g_idata</span><span class=p>,</span><span class=kt>int</span> <span class=o>*</span><span class=n>g_odata</span><span class=p>,</span><span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=o>*</span><span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>// convert global data pointer to the local point of this block
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=o>*</span><span class=n>idata</span> <span class=o>=</span> <span class=n>g_idata</span> <span class=o>+</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=o>*</span><span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=p>(</span><span class=n>idx</span> <span class=o>&gt;</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>//in-place reduction in global memory
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>stride</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>stride</span> <span class=o>&lt;</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=n>stride</span> <span class=o>*=</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=c1>//convert tid into local array index
</span></span></span><span class=line><span class=cl><span class=c1></span>		<span class=kt>int</span> <span class=n>index</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>stride</span> <span class=o>*</span><span class=n>tid</span><span class=p>;</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=p>(</span><span class=n>index</span> <span class=o>&lt;</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>{</span>
</span></span><span class=line><span class=cl>			<span class=n>idata</span><span class=p>[</span><span class=n>index</span><span class=p>]</span> <span class=o>+=</span> <span class=n>idata</span><span class=p>[</span><span class=n>index</span> <span class=o>+</span> <span class=n>stride</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>		<span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=c1>//write result for this block to global men
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>g_odata</span><span class=p>[</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>idata</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>最关键的一步就是</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=n>index</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>stride</span> <span class=o>*</span><span class=n>tid</span><span class=p>;</span>
</span></span></code></pre></div><p>这一步保证index能够向后移动到有数据要处理的内存位置，而不是简单的用tid对应内存地址，导致大量线程空闲。
那么这样做的效率高在哪？
首先我们保证在一个块中前几个执行的线程束是在接近满跑的，而后半部分线程束基本是不需要执行的，当一个线程束内存在分支，而分支都不需要执行的时候，硬件会停止他们调用别人，这样就节省了资源，所以高效体现在这，如果还是所有分支不满足的也要执行，即便整个线程束都不需要执行的时候，那这种方案就无效了，还好现在的硬件比较只能，于是，我们执行后得到如下结果
<img loading=lazy src=./result2.png alt>
这个效率提升惊人，有木有，直接降了一位！大约差了一半。
我们前面一直在介绍一个叫做nvprof的工具，那么我们现在就来看看，每个线程束上执行指令的平均数量，同样我会给出四个内核的，但我们之关心黄框内的
使用如下指令</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvprof --metrics inst_per_warp ./reduceInteger
</span></span></code></pre></div><p><img loading=lazy src=./nvprof.png alt></p><p>指标结果是原始内核444.9 比新内核 150.5 可见原始内核中有很多分支指令被执行，而这些分支指令是没有意义的。
分化程度越高，inst_per_warp这个指标会相对越高。这个大家要记一下，以后我们测试效率的时候会经常使用。
接着我们看一下内存加载吞吐：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvprof --metrics gld_throughput ./reduceInteger
</span></span></code></pre></div><p><img loading=lazy src=./gld_throughput.png alt></p><p>依旧只看黄色框框，我们的新内核，内存效率要高很多，也接近一倍了，原因还是我们上面分析的，一个线程块，前面的几个线程束都在干活，而后面几个根本不干活，不干活的不会被执行，而干活的内存请求肯定很集中，最大效率的利用带宽，而最naive的内核，不干活的线程也在线程束内跟着跑，又不请求内存，所以内存访问被打碎，理论上是只有一半的内存效率，测试来看非常接近。</p><h2 id=交错配对的规约>交错配对的规约<a hidden class=anchor aria-hidden=true href=#交错配对的规约>#</a></h2><p>上面的套路是修改线程处理的数据，使部分线程束最大程度利用数据，接下来采用同样的思想，但是方法不同，接下来我们使用的方法是调整跨度，也就是我们每个线程还是处理对应的内存的位置，但内存对不是相邻的了，而是隔了一定距离的：
示意图是最好的描述方法：
<img loading=lazy src=./3_24.png alt>
我们依然把上图当做一个完整的线程块，那么前半部分的线程束依然是最大负载在跑，后半部分的线程束也是啥活不干</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>reduceInterleaved</span><span class=p>(</span><span class=kt>int</span> <span class=o>*</span> <span class=n>g_idata</span><span class=p>,</span> <span class=kt>int</span> <span class=o>*</span><span class=n>g_odata</span><span class=p>,</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=kt>unsigned</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=o>*</span><span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>// convert global data pointer to the local point of this block
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=o>*</span><span class=n>idata</span> <span class=o>=</span> <span class=n>g_idata</span> <span class=o>+</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=o>*</span><span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=p>(</span><span class=n>idx</span> <span class=o>&gt;=</span> <span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=c1>//in-place reduction in global memory
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>stride</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=o>/</span><span class=mi>2</span><span class=p>;</span> <span class=n>stride</span> <span class=o>&gt;</span><span class=mi>0</span><span class=p>;</span> <span class=n>stride</span> <span class=o>&gt;&gt;=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>&lt;</span><span class=n>stride</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=p>{</span>
</span></span><span class=line><span class=cl>			<span class=n>idata</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>idata</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=n>stride</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>		<span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=c1>//write result for this block to global men
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>g_odata</span><span class=p>[</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>idata</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>执行结果
<img loading=lazy src=./result3.png alt></p><p>如果单从优化原理的角度，这个内核和前面的内核应该是相同效率的，但是测试结果是，这个新内核比前面的内核速度快了不少，所以我们还是考察一下指标吧：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvprof --metrics inst_per_warp ./reduceInteger
</span></span></code></pre></div><p><img loading=lazy src=./nvprof2.png alt></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nvprof --metrics gld_throughput ./reduceInteger
</span></span></code></pre></div><p><img loading=lazy src=./gld_throughput2.png alt></p><p>reduceInterleaved内存效率居然是最低的，但是线程束内分化却是最小的。
而书中说reduceInterleaved 的优势在内存读取，而非线程束分化，我们实际操作却得出了完全不同结论，到底是内存的无情，还是编译器的绝望，请看我们下个系列，到时候我们会直接研究机器码，来确定到底是什么影响了看似类似，却有结果悬殊的两个内核</p><p><font color=ff0000>此处需要查看机器码，确定两个内核的实际不同</font></p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>本文比较尴尬最后得出来一个跟书中相反的结论，但是整篇的思路还是很流畅的，所以我还是把它发出来了，按照我的性格这种有疑问没解决的（无法确定是否是已有技术无法解决的）一般不会发出来，但是考虑到可能是编译器进化导致的结果，所以还是先发出来，后面如果研究明白了，在CUDA进阶系列作为案例讨论。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/%E8%A7%84%E7%BA%A6%E9%97%AE%E9%A2%98/>规约问题</a></li><li><a href=https://go.face2ai.com/tags/%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96/>分支分化</a></li></ul><nav class=paginav><a class=prev href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-5-%E5%B1%95%E5%BC%80%E5%BE%AA%E7%8E%AF.zh/><span class=title>« Prev</span><br><span>【CUDA 基础】3.5 展开循环</span></a>
<a class=next href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0.zh/><span class=title>Next »</span><br><span>【CUDA 基础】3.3 并行性表现</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.4 避免分支分化 on twitter" href="https://twitter.com/intent/tweet/?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.4%20%e9%81%bf%e5%85%8d%e5%88%86%e6%94%af%e5%88%86%e5%8c%96&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-4-%25E9%2581%25BF%25E5%2585%258D%25E5%2588%2586%25E6%2594%25AF%25E5%2588%2586%25E5%258C%2596.zh%2f&hashtags=%e8%a7%84%e7%ba%a6%e9%97%ae%e9%a2%98%2c%e5%88%86%e6%94%af%e5%88%86%e5%8c%96"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.4 避免分支分化 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-4-%25E9%2581%25BF%25E5%2585%258D%25E5%2588%2586%25E6%2594%25AF%25E5%2588%2586%25E5%258C%2596.zh%2f&title=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.4%20%e9%81%bf%e5%85%8d%e5%88%86%e6%94%af%e5%88%86%e5%8c%96&summary=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.4%20%e9%81%bf%e5%85%8d%e5%88%86%e6%94%af%e5%88%86%e5%8c%96&source=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-4-%25E9%2581%25BF%25E5%2585%258D%25E5%2588%2586%25E6%2594%25AF%25E5%2588%2586%25E5%258C%2596.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.4 避免分支分化 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-4-%25E9%2581%25BF%25E5%2585%258D%25E5%2588%2586%25E6%2594%25AF%25E5%2588%2586%25E5%258C%2596.zh%2f&title=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.4%20%e9%81%bf%e5%85%8d%e5%88%86%e6%94%af%e5%88%86%e5%8c%96"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.4 避免分支分化 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-4-%25E9%2581%25BF%25E5%2585%258D%25E5%2588%2586%25E6%2594%25AF%25E5%2588%2586%25E5%258C%2596.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.4 避免分支分化 on whatsapp" href="https://api.whatsapp.com/send?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.4%20%e9%81%bf%e5%85%8d%e5%88%86%e6%94%af%e5%88%86%e5%8c%96%20-%20https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-4-%25E9%2581%25BF%25E5%2585%258D%25E5%2588%2586%25E6%2594%25AF%25E5%2588%2586%25E5%258C%2596.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.4 避免分支分化 on telegram" href="https://telegram.me/share/url?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.4%20%e9%81%bf%e5%85%8d%e5%88%86%e6%94%af%e5%88%86%e5%8c%96&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-4-%25E9%2581%25BF%25E5%2585%258D%25E5%2588%2586%25E6%2594%25AF%25E5%2588%2586%25E5%258C%2596.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>