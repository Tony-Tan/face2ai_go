<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【CUDA 基础】6.1 流和事件概述 | 谭升的博客</title><meta name=keywords content="流,事件"><meta name=description content="Abstract: 本文介绍CUDA中流和事件的理论描述。
Keywords: 流，事件"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-1-%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E6%A6%82%E8%BF%B0.zh/><link crossorigin=anonymous href=../../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="【CUDA 基础】6.1 流和事件概述"><meta property="og:description" content="Abstract: 本文介绍CUDA中流和事件的理论描述。
Keywords: 流，事件"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-1-%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E6%A6%82%E8%BF%B0.zh/"><meta property="article:section" content="编程"><meta property="article:published_time" content="2018-06-10T21:45:15+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【CUDA 基础】6.1 流和事件概述"><meta name=twitter:description content="Abstract: 本文介绍CUDA中流和事件的理论描述。
Keywords: 流，事件"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"【CUDA 基础】6.1 流和事件概述","item":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-1-%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E6%A6%82%E8%BF%B0.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【CUDA 基础】6.1 流和事件概述","name":"【CUDA 基础】6.1 流和事件概述","description":"Abstract: 本文介绍CUDA中流和事件的理论描述。 Keywords: 流，事件\n","keywords":["流","事件"],"articleBody":"Abstract: 本文介绍CUDA中流和事件的理论描述。 Keywords: 流，事件\n流和事件概述 前面几章我们一直围绕GPU设备展开，我们的代码除了在核函数的配置的部分研究过主机端执行的代码，其他部分基本都是在设备代码上进行的，这一章我们就从主机端来讲讲如何优化CUDA应用。 CUDA流：一系列异步CUDA操作，比如我们常见的套路，在主机端分配设备主存（cudaMalloc），主机向设备传输数据（cudaMemcpy），核函数启动，复制数据回主机（Memcpy）这些操作中有些是异步的，执行顺序也是按照主机代码中的顺序执行的（但是异步操作的结束不一定是按照代码中的顺序的）。 流能封装这些异步操作，并保持操作顺序，允许操作在流中排队。保证其在前面所有操作启动之后启动，有了流，我们就能查询排队状态了。 我们上面举得一般情况下的操作基本可以分为以下三种：\n 主机与设备间的数据传输 核函数启动 其他的由主机发出的设备执行的命令  流中的操作相对于主机来说总是异步的，CUDA运行时决定何时可以在设备上执行操作。我们要做的就是控制这些操作在其结果出来之前，不启动需要调用这个结果的操作。 一个流中的不同操作有着严格的顺序。但是不同流之间是没有任何限制的。多个流同时启动多个内核，就形成了网格级别的并行。 CUDA流中排队的操作和主机都是异步的，所以排队的过程中并不耽误主机运行其他指令，所以这就隐藏了执行这些操作的开销。 CUDA编程的一个典型模式是，也就是我们上面讲到的一般套路：\n 将输入数据从主机复制到设备上 在设备上执行一个内核 将结果从设备移回主机  一般的生产情况下，内核执行的时间要长于数据传输，所以我们前面的例子大多是数据传输更耗时，这是不实际的。当重叠核函数执行和数据传输操作，可以屏蔽数据移动造成的时间消耗，当然正在执行的内核的数据需要提前复制到设备上的，这里说的数据传输和内核执行是同时操作的是指当前传输的数据是接下来流中的内核需要的。这样总的执行时间就被缩减了。 流在CUDA的API调用可以实现流水线和双缓冲技术。 CUDA的API也分为同步和异步的两种：\n 同步行为的函数会阻塞主机端线程直到其完成 异步行为的函数在调用后会立刻把控制权返还给主机。  异步行为和流式构建网格级并行的支柱。 虽然我们从软件模型上提出了流，网格级并行的概念，但是说来说去我们能用的就那么一个设备，如果设备空闲当然可以同时执行多个核，但是如果设备已经跑满了，那么我们认为并行的指令也必须排队等待——PCIe总线和SM数量是有限的，当他们被完全占用，流是没办法做什么的，除了等待\n我们接下来就要研究多种计算能力的设备上的流是如何运行的。\nCUDA流 我们的所有CUDA操作都是在流中进行的，虽然我们可能没发现，但是有我们前面的例子中的指令，内核启动，都是在CUDA流中进行的，只是这种操作是隐式的，所以肯定还有显式的，所以，流分为：\n 隐式声明的流，我们叫做空流 显式声明的流，我们叫做非空流  如果我们没有特别声明一个流，那么我们的所有操作是在默认的空流中完成的，我们前面的所有例子都是在默认的空流中进行的。 空流是没办法管理的，因为他连个名字都没有，似乎也没有默认名，所以当我们想控制流，非空流是非常必要的。 基于流的异步内核启动和数据传输支持以下类型的粗粒度并发\n 重叠主机和设备计算 重叠主机计算和主机设备数据传输 重叠主机设备数据传输和设备计算 并发设备计算（多个设备）  CUDA编程和普通的C++不同的就是，我们有两个“可运算的设备”也就是CPU和GPU这两个东西，这种情况下，他们之间的同步并不是每一步指令都互相通信执行进度的，设备不知道主机在干啥，主机也不是完全知道设备在干啥。但是数据传输是同步的，也就是主机要等设备接收完数据才干别的，也就是说你爸给你寄了一袋大米，然后老人家啥也不做，拨通电话跟你保持通话不停的问你收到了么？直到你回答收到了，这就是同步的。内核启动就是异步的，你爸爸又要给你钱花，去银行给你汇了五百块钱，银行说第二天到账，他就可以回家该干嘛干嘛了，而不需要在银行等一晚，第二天你收到了，打个电话说一声就行了，这就是异步的。异步操作，可以重叠主机计算和设备计算。 前面用的cudaMemcpy就是个同步操作，我们还提到过隐式同步——从设备复制结果数据回主机，要等设备执行完。当然数据传输有异步版本：\ncudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t count,cudaMemcpyKind kind, cudaStream_t stream = 0); 值得注意的就是最后一个参数，stream表示流，一般情况设置为默认流，这个函数和主机是异步的，执行后控制权立刻归还主机，当然我们需要声明一个非空流：\ncudaError_t cudaStreamCreate(cudaStream_t* pStream); 这样我们就有一个可以被管理的流了，这段代码是创建了一个流，有C++经验的人能看出来，这个是为一个流分配必要资源的函数，给流命名声明流的操作应该是：\ncudaStream_t a; 定义了一个叫a的流，但是这个流没法用，相当于只有了名字，资源还是要用cudaStreamCreate分配的。 接下来必须要特别注意： 执行异步数据传输时，主机端的内存必须是固定的，非分页的！！ 执行异步数据传输时，主机端的内存必须是固定的，非分页的！！ 执行异步数据传输时，主机端的内存必须是固定的，非分页的！！\n讲内存模型的时候我们说到过，分配方式：\ncudaError_t cudaMallocHost(void **ptr, size_t size); cudaError_t cudaHostAlloc(void **pHost, size_t size, unsigned int flags); 主机虚拟内存中分配的数据在物理内存中是随时可能被移动的，我们必须确保其在整个生存周期中位置不变，这样在异步操作中才能准确的转移数据，否则如果操作系统移动了数据的物理地址，那么我们的设备可能还是回到之前的物理地址取数据，这就会出现未定义的错误。\n在非空流中执行内核需要在启动核函数的时候加入一个附加的启动配置：\nkernel_namegrid, block, sharedMemSize, stream(argument list); pStream参数就是附加的参数，使用目标流的名字作为参数，比如想把核函数加入到a流中，那么这个stream就变成a。 前面我们为一个流分配资源，当然后面就要回收资源，回收方式：\ncudaError_t cudaStreamDestroy(cudaStream_t stream); 这个回收函数很有意思，由于流和主机端是异步的，你在使用上面指令回收流的资源的时候，很有可能流还在执行，这时候，这条指令会正常执行，但是不会立刻停止流，而是等待流执行完成后，立刻回收该流中的资源。这样做是合理的也是安全的。 当然，我们可以查询流执行的怎么样了，下面两个函数就是帮我们查查我们的流到哪了：\ncudaError_t cudaStreamSynchronize(cudaStream_t stream); cudaError_t cudaStreamQuery(cudaStream_t stream); 这两条执行的行为非常不同，cudaStreamSynchronize会阻塞主机，直到流完成。cudaStreamQuery则是立即返回，如果查询的流执行完了，那么返回cudaSuccess否则返回cudaErrorNotReady。 下面这段示例代码就是典型多个流中调度CUDA操作的常见模式：\nfor (int i = 0; i  nStreams; i++) { int offset = i * bytesPerStream; cudaMemcpyAsync(\u0026d_a[offset], \u0026a[offset], bytePerStream, streams[i]); kernelgrid, block, 0, streams[i](\u0026d_a[offset]); cudaMemcpyAsync(\u0026a[offset], \u0026d_a[offset], bytesPerStream, streams[i]); } for (int i = 0; i  nStreams; i++) { cudaStreamSynchronize(streams[i]); } 第一个for中循环执行了nStreams个流，每个流中都是“复制数据，执行核函数，最后将结果复制回主机”这一系列操作。 下面的图就是一个简单的时间轴示意图，假设nStreams=3，所有传输和核启动都是并发的：\nH2D是主机到设备的内存传输，D2H是设备到主机的内存传输。显然这些操作没有并发执行，而是错开的，原因是PCIe总线是共享的，当第一个流占据了主线，后来的就一定要等待，等待主线空闲。编程模型和硬件的实际执行时有差距了。 上面同时从主机到设备涉及硬件竞争要等待，如果是从主机到设备和从设备到主机同时发生，这时候不会产生等待，而是同时进行。 内核并发最大数量也是有极限的，不同计算能力的设备不同，Fermi设备支持16路并发，Kepler支持32路并发。设备上的所有资源都是限制并发数量的原因，比如共享内存，寄存器，本地内存，这些资源都会限制最大并发数。\n流调度 从编程模型看，所有流可以同时执行，但是硬件毕竟有限，不可能像理想情况下的所有流都有硬件可以使用，所以硬件上如何调度这些流是我们理解流并发的关键\n虚假的依赖关系 在Fermi架构上16路流并发执行但是所有流最终都是在单一硬件上执行的，Fermi只有一个硬件工作队列，所以他们虽然在编程模型上式并行的，但是在硬件执行过程中是在一个队列中（像串行一样）。当要执行某个网格的时候CUDA会检测任务依赖关系，如果其依赖于其他结果，那么要等结果出来后才能继续执行。单一流水线可能会导致虚假依赖关系：\n这个图就是虚假依赖的最准确的描述，我们有三个流，流中的操作相互依赖，比如B要等待A的结果，Z要等待Y的结果，当我们把三个流塞到一个队列中，那么我们就会得到紫色箭头的样子，这个硬件队列中的任务可以并行执行，但是要考虑依赖关系，所以，我们按照顺序会这样执行：\n 执行A，同时检查B是否有依赖关系，当然此时B依赖于A而A没执行完，所以整个队列阻塞 A执行完成后执行B，同时检查C，发现依赖，等待 B执行完后，执行C同时检查，发现P没有依赖，如果此时硬件有多于资源P开始执行 P执行时检查Q，发现Q依赖P，所以等待  这种一个队列的模式，会产生一种，虽然P依赖B的感觉，虽然不依赖，但是B不执行完，P没办法执行，而所谓并行，只有一个依赖链的头和尾有可能并行，也就是红圈中任务可能并行，而我们的编程模型中设想的并不是这样的。\nHyper-Q技术 解决上面虚假依赖的最好办法就是多个工作队列，这样就从根本上解决了虚假依赖关系，Hyper-Q就是这种技术，32个硬件工作队列同时执行多个流，这就可以实现所有流的并发，最小化虚假依赖：\n流的优先级 3.5以上的设备可以给流优先级，也就是优先级高的（数字上更小的，类似于C++运算符优先级） 优先级只影响核函数，不影响数据传输，高优先级的流可以占用低优先级的工作。 下面函数创建一个有指定优先级的流\ncudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags,int priority); 不同的设备有不同的优先级等级，下面函数可以查询当前设备的优先级分布情况：\ncudaError_t cudaDeviceGetStreamPriorityRange(int *leastPriority, int *greatestPriority); leastPriority表示最低优先级（整数，远离0） greatestPriority表示最高优先级（整数，数字较接近0） 如果设备不支持优先级返回0\nCUDA事件 CUDA事件不同于我们前面介绍的内存事务，不要搞混，事件也是软件层面上的概念。事件的本质就是一个标记，它与其所在的流内的特定点相关联。可以使用时间来执行以下两个基本任务：\n 同步流执行 监控设备的进展 流中的任意点都可以通过API插入事件以及查询事件完成的函数，只有事件所在流中其之前的操作都完成后才能触发事件完成。默认流中设置事件，那么其前面的所有操作都完成时，事件才出发完成。 事件就像一个个路标，其本身不执行什么功能，就像我们最原始测试c语言程序的时候插入的无数多个printf一样。  创建和销毁 事件的声明如下：\ncudaEvent_t event; 同样声明完后要分配资源：\ncudaError_t cudaEventCreate(cudaEvent_t* event); 回收事件的资源\ncudaError_t cudaEventDestroy(cudaEvent_t event); 如果回收指令执行的时候事件还没有完成，那么回收指令立即完成，当事件完成后，资源马上被回收。\n记录事件和计算运行时间 事件的一个主要用途就是记录事件之间的时间间隔。 事件通过下面指令添加到CUDA流：\ncudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0); 在流中的事件主要左右就是等待前面的操作完成，或者测试指定流中操作完成情况，下面和流类似的事件测试指令（是否出发完成）会阻塞主机线程知道事件被完成。\ncudaError_t cudaEventSynchronize(cudaEvent_t event); 同样，也有异步版本：\ncudaError_t cudaEventQuery(cudaEvent_t event); 这个不会阻塞主机线程，而是直接返回结果和stream版本的类似。 另一个函数用在事件上的是记录两个事件之间的时间间隔：\ncudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop); 这个函数记录两个事件start和stop之间的时间间隔，单位毫秒，两个事件不一定是同一个流中。这个时间间隔可能会比实际大一些，因为cudaEventRecord这个函数是异步的，所以加入时间完全不可控，不能保证两个事件之间的间隔刚好是两个事件之间的。 一段简单的记录事件时间间隔的代码\n// create two events cudaEvent_t start, stop; cudaEventCreate(\u0026start); cudaEventCreate(\u0026stop); // record start event on the default stream cudaEventRecord(start); // execute kernel kernelgrid, block(arguments); // record stop event on the default stream cudaEventRecord(stop); // wait until the stop event completes cudaEventSynchronize(stop); // calculate the elapsed time between two events float time; cudaEventElapsedTime(\u0026time, start, stop); // clean up the two events cudaEventDestroy(start); cudaEventDestroy(stop); 这段代码显示，我们的事件被插入到空流中，设置两个事件作为标记，然后记录他们之间的时间间隔。 cudaEventRecord是异步的，所以间隔不准，这是特别要注意的。\n流同步 在研究线程并行的时候我们就发现并行这种一旦开始就万马奔腾的模式，想要控制就要让大家到一个固定的位置停下来，就是同步，同步好处是保证代码有可能存在内存竞争的地方降低风险，第二就是相互协调通信，当然坏处就是效率会降低，原因很简单，就是当部分线程等待的时候，设备有一些资源是空闲的，所以这会带来性能损耗。 同样，在流中也有同步，下面我们就研究一下流同步。 流分成阻塞流和非阻塞流，在非空流中所有操作都是非阻塞的，所以流启动以后，主机还要完成自己的任务，有时候就可能需要同步主机和流之间的进度，或者同步流和流之间的进度。 从主机的角度，CUDA操作可以分为两类：\n 内存相关操作 内核启动  内核启动总是异步的，虽然某些内存是同步的，但是他们也有异步版本。 前面我们提到了流的两种类型：\n 异步流（非空流） 同步流（空流/默认流）  没有显式声明的流式默认同步流，程序员声明的流都是异步流，异步流通常不会阻塞主机，同步流中部分操作会造成阻塞，主机等待，什么都不做，直到某操作完成。 非空流并不都是非阻塞的，其也可以分为两种类型：\n 阻塞流 非阻塞流  虽然正常来讲，非空流都是异步操作，不存在阻塞主机的情况，但是有时候可能被空流中的操作阻塞。如果一个非空流被声明为非阻塞的，那么没人能阻塞他，如果声明为阻塞流，则会被空流阻塞。 有点晕，就是非空流有时候可能需要在运行到一半和主机通信，这时候我们更希望他能被阻塞，而不是不受控制，这样我们就可以自己设定这个流到底受不受控制，也就是是否能被阻塞，下面我们研究如何使用这两种流。\n阻塞流和非阻塞流 cudaStreamCreate创建的是阻塞流，意味着里面有些操作会被阻塞，直到空流中默写操作完成。 空流不需要显式声明，而是隐式的，他是阻塞的，跟所有阻塞流同步。 下面这个过程很重要： 当操作A发布到空流中，A执行之前，CUDA会等待A之前的全部操作都发布到阻塞流中，所有发布到阻塞流中的操作都会挂起，等待，直到在此操作指令之前的操作都完成，才开始执行。 有点复杂，因为这涉及到代码编写的过程和执行的过程，两个过程混在一起说，肯定有点乱，我们来个例子压压惊就好了：\nkernel_11, 1, 0, stream_1(); kernel_21, 1(); kernel_31, 1, 0, stream_2(); 上面这段代码，有三个流，两个有名字的，一个空流，我们认为stream_1和stream_2是阻塞流，空流是阻塞的，这三个核函数都在阻塞流上执行，具体过程是，kernel_1被启动，控制权返回主机，然后启动kernel_2，但是此时kernel_2 不会并不会马山执行，他会等到kernel_1执行完毕，同理启动完kernel_2 控制权立刻返回给主机，主机继续启动kernel_3,这时候kernel_3 也要等待，直到kernel_2执行完，但是从主机的角度，这三个核都是异步的，启动后控制权马上还给主机。 然后我们就想创建一个非阻塞流，因为我们默认创建的是阻塞版本：\ncudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags); 第二个参数就是选择阻塞还是非阻塞版本：\ncudaStreamDefault;// 默认阻塞流 cudaStreamNonBlocking: //非阻塞流，对空流的阻塞行为失效。 如果前面的stream_1和stream_2声明为非阻塞的，那么上面的调用方法的结果是三个核函数同时执行。\n隐式同步 前面几章核函数计时的时候，我们说过要同步，并且提到过cudaMemcpy 可以隐式同步，也介绍了\ncudaDeviceSynchronize; cudaStreamSynchronize; cudaEventSynchronize; 这几个也是同步指令，可以用来同步不同的对象，这些是显式的调用的；与上面的隐式不同。 隐式同步的指令其最原始的函数功能并不是同步，所以同步效果是隐式的，这个我们需要非常注意，忽略隐式同步会造成性能下降。所谓同步就是阻塞的意思，被忽视的隐式同步就是被忽略的阻塞，隐式操作常出现在内存操作上，比如：\n 锁页主机内存分布 设备内存分配 设备内存初始化 同一设备两地址之间的内存复制 一级缓存，共享内存配置修改  这些操作都要时刻小心，因为他们带来的阻塞非常不容易察觉\n显式同步 显式同步相比就更加光明磊落了，因为一条指令就一个作用，没啥副作用，常见的同步有：\n 同步设备 同步流 同步流中的事件 使用事件跨流同步  下面的函数就可以阻塞主机线程，直到设备完成所有操作：\ncudaError_t cudaDeviceSynchronize(void); 这个函数我们前面常用，但是尽量少用，这个会拖慢效率。 然后是流版本的，我们可以同步流，使用下面两个函数：\ncudaError_t cudaStreamSynchronize(cudaStream_t stream); cudaError_t cudaStreamQuery(cudaStream_t stream); 这两个函数，第一个是同步流的，阻塞主机直到完成，第二个可以完成非阻塞流测试。也就是测试一下这个流是否完成。 我们提到事件，事件的作用就是在流中设定一些标记用来同步，和检查是否执行到关键点位（事件位置），也是用类似的函数\ncudaError_t cudaEventSynchronize(cudaEvent_t event); cudaError_t cudaEventQuery(cudaEvent_t event); 这两个函数的性质和上面的非常类似。 事件提供了一个流之间同步的方法：\ncudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event); 这条命令的含义是，指定的流要等待指定的事件，事件完成后流才能继续，这个事件可以在这个流中，也可以不在，当在不同的流的时候，这个就是实现了跨流同步。 如下图\n可配置事件 CDUA提供了一种控制事件行为和性能的函数：\ncudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags); 其中参数是：\ncudaEventDefault cudaEventBlockingSync cudaEventDisableTiming cudaEventInterprocess 其中cudaEventBlockingSync指定使用cudaEventSynchronize同步会造成阻塞调用线程。cudaEventSynchronize默认是使用cpu周期不断重复查询事件状态，而当指定了事件是cudaEventBlockingSync的时候，会将查询放在另一个线程中，而原始线程继续执行，直到事件满足条件，才会通知原始线程，这样可以减少CPU的浪费，但是由于通讯的时间，会造成一定的延迟。 cudaEventDisableTiming表示事件不用于计时，可以减少系统不必要的开支也能提升cudaStreamWaitEvent和cudaEventQuery的效率 cudaEventInterprocess表明可能被用于进程之间的事件\n总结 这一篇理论多，验证少，如果看不懂，不妨先看后面的例子，再回来研究理论。\n","wordCount":"406","inLanguage":"en","datePublished":"2018-06-10T21:45:15Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-1-%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E6%A6%82%E8%BF%B0.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a></div><h1 class=post-title>【CUDA 基础】6.1 流和事件概述</h1><div class=post-meta><span title="2018-06-10 21:45:15 +0000 UTC">June 10, 2018</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%b5%81%e5%92%8c%e4%ba%8b%e4%bb%b6%e6%a6%82%e8%bf%b0 aria-label=流和事件概述>流和事件概述</a><ul><li><a href=#cuda%e6%b5%81 aria-label=CUDA流>CUDA流</a></li><li><a href=#%e6%b5%81%e8%b0%83%e5%ba%a6 aria-label=流调度>流调度</a><ul><li><a href=#%e8%99%9a%e5%81%87%e7%9a%84%e4%be%9d%e8%b5%96%e5%85%b3%e7%b3%bb aria-label=虚假的依赖关系>虚假的依赖关系</a></li><li><a href=#hyper-q%e6%8a%80%e6%9c%af aria-label=Hyper-Q技术>Hyper-Q技术</a></li></ul></li><li><a href=#%e6%b5%81%e7%9a%84%e4%bc%98%e5%85%88%e7%ba%a7 aria-label=流的优先级>流的优先级</a></li><li><a href=#cuda%e4%ba%8b%e4%bb%b6 aria-label=CUDA事件>CUDA事件</a><ul><li><a href=#%e5%88%9b%e5%bb%ba%e5%92%8c%e9%94%80%e6%af%81 aria-label=创建和销毁>创建和销毁</a></li><li><a href=#%e8%ae%b0%e5%bd%95%e4%ba%8b%e4%bb%b6%e5%92%8c%e8%ae%a1%e7%ae%97%e8%bf%90%e8%a1%8c%e6%97%b6%e9%97%b4 aria-label=记录事件和计算运行时间>记录事件和计算运行时间</a></li></ul></li><li><a href=#%e6%b5%81%e5%90%8c%e6%ad%a5 aria-label=流同步>流同步</a><ul><li><a href=#%e9%98%bb%e5%a1%9e%e6%b5%81%e5%92%8c%e9%9d%9e%e9%98%bb%e5%a1%9e%e6%b5%81 aria-label=阻塞流和非阻塞流>阻塞流和非阻塞流</a></li><li><a href=#%e9%9a%90%e5%bc%8f%e5%90%8c%e6%ad%a5 aria-label=隐式同步>隐式同步</a></li><li><a href=#%e6%98%be%e5%bc%8f%e5%90%8c%e6%ad%a5 aria-label=显式同步>显式同步</a></li><li><a href=#%e5%8f%af%e9%85%8d%e7%bd%ae%e4%ba%8b%e4%bb%b6 aria-label=可配置事件>可配置事件</a></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 本文介绍CUDA中流和事件的理论描述。
<strong>Keywords:</strong> 流，事件</p><h1 id=流和事件概述>流和事件概述<a hidden class=anchor aria-hidden=true href=#流和事件概述>#</a></h1><p>前面几章我们一直围绕GPU设备展开，我们的代码除了在核函数的配置的部分研究过主机端执行的代码，其他部分基本都是在设备代码上进行的，这一章我们就从主机端来讲讲如何优化CUDA应用。
CUDA流：一系列异步CUDA操作，比如我们常见的套路，在主机端分配设备主存（cudaMalloc），主机向设备传输数据（cudaMemcpy），核函数启动，复制数据回主机（Memcpy）这些操作中有些是异步的，执行顺序也是按照主机代码中的顺序执行的（但是异步操作的结束不一定是按照代码中的顺序的）。
流能封装这些异步操作，并保持操作顺序，允许操作在流中排队。保证其在前面所有操作启动之后启动，有了流，我们就能查询排队状态了。
我们上面举得一般情况下的操作基本可以分为以下三种：</p><ul><li>主机与设备间的数据传输</li><li>核函数启动</li><li>其他的由主机发出的设备执行的命令</li></ul><p>流中的操作相对于主机来说总是异步的，CUDA运行时决定何时可以在设备上执行操作。我们要做的就是控制这些操作在其结果出来之前，不启动需要调用这个结果的操作。
一个流中的不同操作有着严格的顺序。但是不同流之间是没有任何限制的。多个流同时启动多个内核，就形成了网格级别的并行。
CUDA流中排队的操作和主机都是异步的，所以排队的过程中并不耽误主机运行其他指令，所以这就隐藏了执行这些操作的开销。
CUDA编程的一个典型模式是，也就是我们上面讲到的一般套路：</p><ol><li>将输入数据从主机复制到设备上</li><li>在设备上执行一个内核</li><li>将结果从设备移回主机</li></ol><p>一般的生产情况下，内核执行的时间要长于数据传输，所以我们前面的例子大多是数据传输更耗时，这是不实际的。当重叠核函数执行和数据传输操作，可以屏蔽数据移动造成的时间消耗，当然正在执行的内核的数据需要提前复制到设备上的，这里说的数据传输和内核执行是同时操作的是指当前传输的数据是接下来流中的内核需要的。这样总的执行时间就被缩减了。
流在CUDA的API调用可以实现流水线和双缓冲技术。
CUDA的API也分为同步和异步的两种：</p><ul><li>同步行为的函数会阻塞主机端线程直到其完成</li><li>异步行为的函数在调用后会立刻把控制权返还给主机。</li></ul><p>异步行为和流式构建网格级并行的支柱。
虽然我们从软件模型上提出了流，网格级并行的概念，但是说来说去我们能用的就那么一个设备，如果设备空闲当然可以同时执行多个核，但是如果设备已经跑满了，那么我们认为并行的指令也必须排队等待——PCIe总线和SM数量是有限的，当他们被完全占用，流是没办法做什么的，除了等待</p><p>我们接下来就要研究多种计算能力的设备上的流是如何运行的。</p><h2 id=cuda流>CUDA流<a hidden class=anchor aria-hidden=true href=#cuda流>#</a></h2><p>我们的所有CUDA操作都是在流中进行的，虽然我们可能没发现，但是有我们前面的例子中的指令，内核启动，都是在CUDA流中进行的，只是这种操作是隐式的，所以肯定还有显式的，所以，流分为：</p><ul><li>隐式声明的流，我们叫做空流</li><li>显式声明的流，我们叫做非空流</li></ul><p>如果我们没有特别声明一个流，那么我们的所有操作是在默认的空流中完成的，我们前面的所有例子都是在默认的空流中进行的。
空流是没办法管理的，因为他连个名字都没有，似乎也没有默认名，所以当我们想控制流，非空流是非常必要的。
基于流的异步内核启动和数据传输支持以下类型的粗粒度并发</p><ul><li>重叠主机和设备计算</li><li>重叠主机计算和主机设备数据传输</li><li>重叠主机设备数据传输和设备计算</li><li>并发设备计算（多个设备）</li></ul><p>CUDA编程和普通的C++不同的就是，我们有两个“可运算的设备”也就是CPU和GPU这两个东西，这种情况下，他们之间的同步并不是每一步指令都互相通信执行进度的，设备不知道主机在干啥，主机也不是完全知道设备在干啥。但是数据传输是同步的，也就是主机要等设备接收完数据才干别的，也就是说你爸给你寄了一袋大米，然后老人家啥也不做，拨通电话跟你保持通话不停的问你收到了么？直到你回答收到了，这就是同步的。内核启动就是异步的，你爸爸又要给你钱花，去银行给你汇了五百块钱，银行说第二天到账，他就可以回家该干嘛干嘛了，而不需要在银行等一晚，第二天你收到了，打个电话说一声就行了，这就是异步的。异步操作，可以重叠主机计算和设备计算。
前面用的cudaMemcpy就是个同步操作，我们还提到过隐式同步——从设备复制结果数据回主机，要等设备执行完。当然数据传输有异步版本：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaMemcpyAsync</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=k>const</span> <span class=kt>void</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=n>size_t</span> <span class=n>count</span><span class=p>,</span><span class=n>cudaMemcpyKind</span> <span class=n>kind</span><span class=p>,</span> <span class=n>cudaStream_t</span> <span class=n>stream</span> <span class=o>=</span> <span class=mi>0</span><span class=p>);</span>
</span></span></code></pre></div><p>值得注意的就是最后一个参数，stream表示流，一般情况设置为默认流，这个函数和主机是异步的，执行后控制权立刻归还主机，当然我们需要声明一个非空流：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamCreate</span><span class=p>(</span><span class=n>cudaStream_t</span><span class=o>*</span> <span class=n>pStream</span><span class=p>);</span>
</span></span></code></pre></div><p>这样我们就有一个可以被管理的流了，这段代码是创建了一个流，有C++经验的人能看出来，这个是为一个流分配必要资源的函数，给流命名声明流的操作应该是：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaStream_t</span> <span class=n>a</span><span class=p>;</span>
</span></span></code></pre></div><p>定义了一个叫a的流，但是这个流没法用，相当于只有了名字，资源还是要用cudaStreamCreate分配的。
接下来必须要特别注意：
执行异步数据传输时，主机端的内存必须是固定的，非分页的！！
执行异步数据传输时，主机端的内存必须是固定的，非分页的！！
执行异步数据传输时，主机端的内存必须是固定的，非分页的！！</p><p>讲内存模型的时候我们说到过，分配方式：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaMallocHost</span><span class=p>(</span><span class=kt>void</span> <span class=o>**</span><span class=n>ptr</span><span class=p>,</span> <span class=n>size_t</span> <span class=n>size</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaHostAlloc</span><span class=p>(</span><span class=kt>void</span> <span class=o>**</span><span class=n>pHost</span><span class=p>,</span> <span class=n>size_t</span> <span class=n>size</span><span class=p>,</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>flags</span><span class=p>);</span>
</span></span></code></pre></div><p>主机虚拟内存中分配的数据在物理内存中是随时可能被移动的，我们必须确保其在整个生存周期中位置不变，这样在异步操作中才能准确的转移数据，否则如果操作系统移动了数据的物理地址，那么我们的设备可能还是回到之前的物理地址取数据，这就会出现未定义的错误。</p><p>在非空流中执行内核需要在启动核函数的时候加入一个附加的启动配置：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>kernel_name</span><span class=o>&lt;&lt;&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span><span class=p>,</span> <span class=n>sharedMemSize</span><span class=p>,</span> <span class=n>stream</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>argument</span> <span class=n>list</span><span class=p>);</span>
</span></span></code></pre></div><p>pStream参数就是附加的参数，使用目标流的名字作为参数，比如想把核函数加入到a流中，那么这个stream就变成a。
前面我们为一个流分配资源，当然后面就要回收资源，回收方式：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamDestroy</span><span class=p>(</span><span class=n>cudaStream_t</span> <span class=n>stream</span><span class=p>);</span>
</span></span></code></pre></div><p>这个回收函数很有意思，由于流和主机端是异步的，你在使用上面指令回收流的资源的时候，很有可能流还在执行，这时候，这条指令会正常执行，但是不会立刻停止流，而是等待流执行完成后，立刻回收该流中的资源。这样做是合理的也是安全的。
当然，我们可以查询流执行的怎么样了，下面两个函数就是帮我们查查我们的流到哪了：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamSynchronize</span><span class=p>(</span><span class=n>cudaStream_t</span> <span class=n>stream</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamQuery</span><span class=p>(</span><span class=n>cudaStream_t</span> <span class=n>stream</span><span class=p>);</span>
</span></span></code></pre></div><p>这两条执行的行为非常不同，cudaStreamSynchronize会阻塞主机，直到流完成。cudaStreamQuery则是立即返回，如果查询的流执行完了，那么返回cudaSuccess否则返回cudaErrorNotReady。
下面这段示例代码就是典型多个流中调度CUDA操作的常见模式：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>nStreams</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>offset</span> <span class=o>=</span> <span class=n>i</span> <span class=o>*</span> <span class=n>bytesPerStream</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMemcpyAsync</span><span class=p>(</span><span class=o>&amp;</span><span class=n>d_a</span><span class=p>[</span><span class=n>offset</span><span class=p>],</span> <span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>offset</span><span class=p>],</span> <span class=n>bytePerStream</span><span class=p>,</span> <span class=n>streams</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>kernel</span><span class=o>&lt;&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>streams</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>&gt;&gt;</span><span class=p>(</span><span class=o>&amp;</span><span class=n>d_a</span><span class=p>[</span><span class=n>offset</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaMemcpyAsync</span><span class=p>(</span><span class=o>&amp;</span><span class=n>a</span><span class=p>[</span><span class=n>offset</span><span class=p>],</span> <span class=o>&amp;</span><span class=n>d_a</span><span class=p>[</span><span class=n>offset</span><span class=p>],</span> <span class=n>bytesPerStream</span><span class=p>,</span> <span class=n>streams</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>nStreams</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>cudaStreamSynchronize</span><span class=p>(</span><span class=n>streams</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>第一个for中循环执行了nStreams个流，每个流中都是“复制数据，执行核函数，最后将结果复制回主机”这一系列操作。
下面的图就是一个简单的时间轴示意图，假设nStreams=3，所有传输和核启动都是并发的：</p><p><img loading=lazy src=./6-1.png alt=6-1></p><p>H2D是主机到设备的内存传输，D2H是设备到主机的内存传输。显然这些操作没有并发执行，而是错开的，原因是PCIe总线是共享的，当第一个流占据了主线，后来的就一定要等待，等待主线空闲。编程模型和硬件的实际执行时有差距了。
上面同时从主机到设备涉及硬件竞争要等待，如果是从主机到设备和从设备到主机同时发生，这时候不会产生等待，而是同时进行。
内核并发最大数量也是有极限的，不同计算能力的设备不同，Fermi设备支持16路并发，Kepler支持32路并发。设备上的所有资源都是限制并发数量的原因，比如共享内存，寄存器，本地内存，这些资源都会限制最大并发数。</p><h2 id=流调度>流调度<a hidden class=anchor aria-hidden=true href=#流调度>#</a></h2><p>从编程模型看，所有流可以同时执行，但是硬件毕竟有限，不可能像理想情况下的所有流都有硬件可以使用，所以硬件上如何调度这些流是我们理解流并发的关键</p><h3 id=虚假的依赖关系>虚假的依赖关系<a hidden class=anchor aria-hidden=true href=#虚假的依赖关系>#</a></h3><p>在Fermi架构上16路流并发执行但是所有流最终都是在单一硬件上执行的，Fermi只有一个硬件工作队列，所以他们虽然在编程模型上式并行的，但是在硬件执行过程中是在一个队列中（像串行一样）。当要执行某个网格的时候CUDA会检测任务依赖关系，如果其依赖于其他结果，那么要等结果出来后才能继续执行。单一流水线可能会导致虚假依赖关系：</p><p><img loading=lazy src=./6-2.png alt=6-2></p><p>这个图就是虚假依赖的最准确的描述，我们有三个流，流中的操作相互依赖，比如B要等待A的结果，Z要等待Y的结果，当我们把三个流塞到一个队列中，那么我们就会得到紫色箭头的样子，这个硬件队列中的任务可以并行执行，但是要考虑依赖关系，所以，我们按照顺序会这样执行：</p><ol><li>执行A，同时检查B是否有依赖关系，当然此时B依赖于A而A没执行完，所以整个队列阻塞</li><li>A执行完成后执行B，同时检查C，发现依赖，等待</li><li>B执行完后，执行C同时检查，发现P没有依赖，如果此时硬件有多于资源P开始执行</li><li>P执行时检查Q，发现Q依赖P，所以等待</li></ol><p>这种一个队列的模式，会产生一种，虽然P依赖B的感觉，虽然不依赖，但是B不执行完，P没办法执行，而所谓并行，只有一个依赖链的头和尾有可能并行，也就是红圈中任务可能并行，而我们的编程模型中设想的并不是这样的。</p><h3 id=hyper-q技术>Hyper-Q技术<a hidden class=anchor aria-hidden=true href=#hyper-q技术>#</a></h3><p>解决上面虚假依赖的最好办法就是多个工作队列，这样就从根本上解决了虚假依赖关系，Hyper-Q就是这种技术，32个硬件工作队列同时执行多个流，这就可以实现所有流的并发，最小化虚假依赖：</p><p><img loading=lazy src=./6-3.png alt=6-3></p><h2 id=流的优先级>流的优先级<a hidden class=anchor aria-hidden=true href=#流的优先级>#</a></h2><p>3.5以上的设备可以给流优先级，也就是优先级高的（数字上更小的，类似于C++运算符优先级）
优先级只影响核函数，不影响数据传输，高优先级的流可以占用低优先级的工作。
下面函数创建一个有指定优先级的流</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamCreateWithPriority</span><span class=p>(</span><span class=n>cudaStream_t</span><span class=o>*</span> <span class=n>pStream</span><span class=p>,</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>flags</span><span class=p>,</span><span class=kt>int</span> <span class=n>priority</span><span class=p>);</span>
</span></span></code></pre></div><p>不同的设备有不同的优先级等级，下面函数可以查询当前设备的优先级分布情况：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaDeviceGetStreamPriorityRange</span><span class=p>(</span><span class=kt>int</span> <span class=o>*</span><span class=n>leastPriority</span><span class=p>,</span> <span class=kt>int</span> <span class=o>*</span><span class=n>greatestPriority</span><span class=p>);</span>
</span></span></code></pre></div><p>leastPriority表示最低优先级（整数，远离0）
greatestPriority表示最高优先级（整数，数字较接近0）
如果设备不支持优先级返回0</p><h2 id=cuda事件>CUDA事件<a hidden class=anchor aria-hidden=true href=#cuda事件>#</a></h2><p>CUDA事件不同于我们前面介绍的内存事务，不要搞混，事件也是软件层面上的概念。事件的本质就是一个标记，它与其所在的流内的特定点相关联。可以使用时间来执行以下两个基本任务：</p><ul><li>同步流执行</li><li>监控设备的进展
流中的任意点都可以通过API插入事件以及查询事件完成的函数，只有事件所在流中其之前的操作都完成后才能触发事件完成。默认流中设置事件，那么其前面的所有操作都完成时，事件才出发完成。
事件就像一个个路标，其本身不执行什么功能，就像我们最原始测试c语言程序的时候插入的无数多个printf一样。</li></ul><h3 id=创建和销毁>创建和销毁<a hidden class=anchor aria-hidden=true href=#创建和销毁>#</a></h3><p>事件的声明如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>;</span>
</span></span></code></pre></div><p>同样声明完后要分配资源：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventCreate</span><span class=p>(</span><span class=n>cudaEvent_t</span><span class=o>*</span> <span class=n>event</span><span class=p>);</span>
</span></span></code></pre></div><p>回收事件的资源</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventDestroy</span><span class=p>(</span><span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>);</span>
</span></span></code></pre></div><p>如果回收指令执行的时候事件还没有完成，那么回收指令立即完成，当事件完成后，资源马上被回收。</p><h3 id=记录事件和计算运行时间>记录事件和计算运行时间<a hidden class=anchor aria-hidden=true href=#记录事件和计算运行时间>#</a></h3><p>事件的一个主要用途就是记录事件之间的时间间隔。
事件通过下面指令添加到CUDA流：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventRecord</span><span class=p>(</span><span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>,</span> <span class=n>cudaStream_t</span> <span class=n>stream</span> <span class=o>=</span> <span class=mi>0</span><span class=p>);</span>
</span></span></code></pre></div><p>在流中的事件主要左右就是等待前面的操作完成，或者测试指定流中操作完成情况，下面和流类似的事件测试指令（是否出发完成）会阻塞主机线程知道事件被完成。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventSynchronize</span><span class=p>(</span><span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>);</span>
</span></span></code></pre></div><p>同样，也有异步版本：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventQuery</span><span class=p>(</span><span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>);</span>
</span></span></code></pre></div><p>这个不会阻塞主机线程，而是直接返回结果和stream版本的类似。
另一个函数用在事件上的是记录两个事件之间的时间间隔：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventElapsedTime</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>ms</span><span class=p>,</span> <span class=n>cudaEvent_t</span> <span class=n>start</span><span class=p>,</span> <span class=n>cudaEvent_t</span> <span class=n>stop</span><span class=p>);</span>
</span></span></code></pre></div><p>这个函数记录两个事件start和stop之间的时间间隔，单位毫秒，两个事件不一定是同一个流中。这个时间间隔可能会比实际大一些，因为cudaEventRecord这个函数是异步的，所以加入时间完全不可控，不能保证两个事件之间的间隔刚好是两个事件之间的。
一段简单的记录事件时间间隔的代码</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=c1>// create two events
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>cudaEvent_t</span> <span class=n>start</span><span class=p>,</span> <span class=n>stop</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventCreate</span><span class=p>(</span><span class=o>&amp;</span><span class=n>start</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventCreate</span><span class=p>(</span><span class=o>&amp;</span><span class=n>stop</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// record start event on the default stream
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>cudaEventRecord</span><span class=p>(</span><span class=n>start</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// execute kernel
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>kernel</span><span class=o>&lt;&lt;&lt;</span><span class=n>grid</span><span class=p>,</span> <span class=n>block</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>arguments</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// record stop event on the default stream
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>cudaEventRecord</span><span class=p>(</span><span class=n>stop</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// wait until the stop event completes
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>cudaEventSynchronize</span><span class=p>(</span><span class=n>stop</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// calculate the elapsed time between two events
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>float</span> <span class=n>time</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventElapsedTime</span><span class=p>(</span><span class=o>&amp;</span><span class=n>time</span><span class=p>,</span> <span class=n>start</span><span class=p>,</span> <span class=n>stop</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=c1>// clean up the two events
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>cudaEventDestroy</span><span class=p>(</span><span class=n>start</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventDestroy</span><span class=p>(</span><span class=n>stop</span><span class=p>);</span>
</span></span></code></pre></div><p>这段代码显示，我们的事件被插入到空流中，设置两个事件作为标记，然后记录他们之间的时间间隔。
cudaEventRecord是异步的，所以间隔不准，这是特别要注意的。</p><h2 id=流同步>流同步<a hidden class=anchor aria-hidden=true href=#流同步>#</a></h2><p>在研究线程并行的时候我们就发现并行这种一旦开始就万马奔腾的模式，想要控制就要让大家到一个固定的位置停下来，就是同步，同步好处是保证代码有可能存在内存竞争的地方降低风险，第二就是相互协调通信，当然坏处就是效率会降低，原因很简单，就是当部分线程等待的时候，设备有一些资源是空闲的，所以这会带来性能损耗。
同样，在流中也有同步，下面我们就研究一下流同步。
流分成阻塞流和非阻塞流，在非空流中所有操作都是非阻塞的，所以流启动以后，主机还要完成自己的任务，有时候就可能需要同步主机和流之间的进度，或者同步流和流之间的进度。
从主机的角度，CUDA操作可以分为两类：</p><ul><li>内存相关操作</li><li>内核启动</li></ul><p>内核启动总是异步的，虽然某些内存是同步的，但是他们也有异步版本。
前面我们提到了流的两种类型：</p><ul><li>异步流（非空流）</li><li>同步流（空流/默认流）</li></ul><p>没有显式声明的流式默认同步流，程序员声明的流都是异步流，异步流通常不会阻塞主机，同步流中部分操作会造成阻塞，主机等待，什么都不做，直到某操作完成。
非空流并不都是非阻塞的，其也可以分为两种类型：</p><ul><li>阻塞流</li><li>非阻塞流</li></ul><p>虽然正常来讲，非空流都是异步操作，不存在阻塞主机的情况，但是有时候可能被空流中的操作阻塞。如果一个非空流被声明为非阻塞的，那么没人能阻塞他，如果声明为阻塞流，则会被空流阻塞。
有点晕，就是非空流有时候可能需要在运行到一半和主机通信，这时候我们更希望他能被阻塞，而不是不受控制，这样我们就可以自己设定这个流到底受不受控制，也就是是否能被阻塞，下面我们研究如何使用这两种流。</p><h3 id=阻塞流和非阻塞流>阻塞流和非阻塞流<a hidden class=anchor aria-hidden=true href=#阻塞流和非阻塞流>#</a></h3><p>cudaStreamCreate创建的是阻塞流，意味着里面有些操作会被阻塞，直到空流中默写操作完成。
空流不需要显式声明，而是隐式的，他是阻塞的，跟所有阻塞流同步。
下面这个过程很重要：
当操作A发布到空流中，A执行之前，CUDA会等待A之前的全部操作都发布到阻塞流中，所有发布到阻塞流中的操作都会挂起，等待，直到在此操作指令之前的操作都完成，才开始执行。
有点复杂，因为这涉及到代码编写的过程和执行的过程，两个过程混在一起说，肯定有点乱，我们来个例子压压惊就好了：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>kernel_1</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>stream_1</span><span class=o>&gt;&gt;&gt;</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=n>kernel_2</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=o>&gt;&gt;&gt;</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=n>kernel_3</span><span class=o>&lt;&lt;&lt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>stream_2</span><span class=o>&gt;&gt;&gt;</span><span class=p>();</span>
</span></span></code></pre></div><p>上面这段代码，有三个流，两个有名字的，一个空流，我们认为stream_1和stream_2是阻塞流，空流是阻塞的，这三个核函数都在阻塞流上执行，具体过程是，kernel_1被启动，控制权返回主机，然后启动kernel_2，但是此时kernel_2 不会并不会马山执行，他会等到kernel_1执行完毕，同理启动完kernel_2 控制权立刻返回给主机，主机继续启动kernel_3,这时候kernel_3 也要等待，直到kernel_2执行完，但是从主机的角度，这三个核都是异步的，启动后控制权马上还给主机。
然后我们就想创建一个非阻塞流，因为我们默认创建的是阻塞版本：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamCreateWithFlags</span><span class=p>(</span><span class=n>cudaStream_t</span><span class=o>*</span> <span class=n>pStream</span><span class=p>,</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>flags</span><span class=p>);</span>
</span></span></code></pre></div><p>第二个参数就是选择阻塞还是非阻塞版本：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaStreamDefault</span><span class=p>;</span><span class=c1>// 默认阻塞流
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=nl>cudaStreamNonBlocking</span><span class=p>:</span> <span class=c1>//非阻塞流，对空流的阻塞行为失效。
</span></span></span></code></pre></div><p>如果前面的stream_1和stream_2声明为非阻塞的，那么上面的调用方法的结果是三个核函数同时执行。</p><h3 id=隐式同步>隐式同步<a hidden class=anchor aria-hidden=true href=#隐式同步>#</a></h3><p>前面几章核函数计时的时候，我们说过要同步，并且提到过cudaMemcpy 可以隐式同步，也介绍了</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaDeviceSynchronize</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>cudaStreamSynchronize</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventSynchronize</span><span class=p>;</span>
</span></span></code></pre></div><p>这几个也是同步指令，可以用来同步不同的对象，这些是显式的调用的；与上面的隐式不同。
隐式同步的指令其最原始的函数功能并不是同步，所以同步效果是隐式的，这个我们需要非常注意，忽略隐式同步会造成性能下降。所谓同步就是阻塞的意思，被忽视的隐式同步就是被忽略的阻塞，隐式操作常出现在内存操作上，比如：</p><ul><li>锁页主机内存分布</li><li>设备内存分配</li><li>设备内存初始化</li><li>同一设备两地址之间的内存复制</li><li>一级缓存，共享内存配置修改</li></ul><p>这些操作都要时刻小心，因为他们带来的阻塞非常不容易察觉</p><h3 id=显式同步>显式同步<a hidden class=anchor aria-hidden=true href=#显式同步>#</a></h3><p>显式同步相比就更加光明磊落了，因为一条指令就一个作用，没啥副作用，常见的同步有：</p><ul><li>同步设备</li><li>同步流</li><li>同步流中的事件</li><li>使用事件跨流同步</li></ul><p>下面的函数就可以阻塞主机线程，直到设备完成所有操作：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaDeviceSynchronize</span><span class=p>(</span><span class=kt>void</span><span class=p>);</span>
</span></span></code></pre></div><p>这个函数我们前面常用，但是尽量少用，这个会拖慢效率。
然后是流版本的，我们可以同步流，使用下面两个函数：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamSynchronize</span><span class=p>(</span><span class=n>cudaStream_t</span> <span class=n>stream</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamQuery</span><span class=p>(</span><span class=n>cudaStream_t</span> <span class=n>stream</span><span class=p>);</span>
</span></span></code></pre></div><p>这两个函数，第一个是同步流的，阻塞主机直到完成，第二个可以完成非阻塞流测试。也就是测试一下这个流是否完成。
我们提到事件，事件的作用就是在流中设定一些标记用来同步，和检查是否执行到关键点位（事件位置），也是用类似的函数</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventSynchronize</span><span class=p>(</span><span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventQuery</span><span class=p>(</span><span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>);</span>
</span></span></code></pre></div><p>这两个函数的性质和上面的非常类似。
事件提供了一个流之间同步的方法：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaStreamWaitEvent</span><span class=p>(</span><span class=n>cudaStream_t</span> <span class=n>stream</span><span class=p>,</span> <span class=n>cudaEvent_t</span> <span class=n>event</span><span class=p>);</span>
</span></span></code></pre></div><p>这条命令的含义是，指定的流要等待指定的事件，事件完成后流才能继续，这个事件可以在这个流中，也可以不在，当在不同的流的时候，这个就是实现了跨流同步。
如下图</p><p><img loading=lazy src=./6-4.png alt=6-4></p><h3 id=可配置事件>可配置事件<a hidden class=anchor aria-hidden=true href=#可配置事件>#</a></h3><p>CDUA提供了一种控制事件行为和性能的函数：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaError_t</span> <span class=nf>cudaEventCreateWithFlags</span><span class=p>(</span><span class=n>cudaEvent_t</span><span class=o>*</span> <span class=n>event</span><span class=p>,</span> <span class=kt>unsigned</span> <span class=kt>int</span> <span class=n>flags</span><span class=p>);</span>
</span></span></code></pre></div><p>其中参数是：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>cudaEventDefault</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventBlockingSync</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventDisableTiming</span>
</span></span><span class=line><span class=cl><span class=n>cudaEventInterprocess</span>
</span></span></code></pre></div><p>其中cudaEventBlockingSync指定使用cudaEventSynchronize同步会造成阻塞调用线程。cudaEventSynchronize默认是使用cpu周期不断重复查询事件状态，而当指定了事件是cudaEventBlockingSync的时候，会将查询放在另一个线程中，而原始线程继续执行，直到事件满足条件，才会通知原始线程，这样可以减少CPU的浪费，但是由于通讯的时间，会造成一定的延迟。
cudaEventDisableTiming表示事件不用于计时，可以减少系统不必要的开支也能提升cudaStreamWaitEvent和cudaEventQuery的效率
cudaEventInterprocess表明可能被用于进程之间的事件</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>这一篇理论多，验证少，如果看不懂，不妨先看后面的例子，再回来研究理论。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/%E6%B5%81/>流</a></li><li><a href=https://go.face2ai.com/tags/%E4%BA%8B%E4%BB%B6/>事件</a></li></ul><nav class=paginav><a class=prev href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-2-%E5%B9%B6%E5%8F%91%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C.zh/><span class=title>« Prev</span><br><span>【CUDA 基础】6.2 并发内核执行</span></a>
<a class=next href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-0-%E6%B5%81%E5%92%8C%E5%B9%B6%E5%8F%91.zh/><span class=title>Next »</span><br><span>【CUDA 基础】6.0 流和并发</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】6.1 流和事件概述 on twitter" href="https://twitter.com/intent/tweet/?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%916.1%20%e6%b5%81%e5%92%8c%e4%ba%8b%e4%bb%b6%e6%a6%82%e8%bf%b0&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-6-1-%25E6%25B5%2581%25E5%2592%258C%25E4%25BA%258B%25E4%25BB%25B6%25E6%25A6%2582%25E8%25BF%25B0.zh%2f&hashtags=%e6%b5%81%2c%e4%ba%8b%e4%bb%b6"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】6.1 流和事件概述 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-6-1-%25E6%25B5%2581%25E5%2592%258C%25E4%25BA%258B%25E4%25BB%25B6%25E6%25A6%2582%25E8%25BF%25B0.zh%2f&title=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%916.1%20%e6%b5%81%e5%92%8c%e4%ba%8b%e4%bb%b6%e6%a6%82%e8%bf%b0&summary=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%916.1%20%e6%b5%81%e5%92%8c%e4%ba%8b%e4%bb%b6%e6%a6%82%e8%bf%b0&source=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-6-1-%25E6%25B5%2581%25E5%2592%258C%25E4%25BA%258B%25E4%25BB%25B6%25E6%25A6%2582%25E8%25BF%25B0.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】6.1 流和事件概述 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-6-1-%25E6%25B5%2581%25E5%2592%258C%25E4%25BA%258B%25E4%25BB%25B6%25E6%25A6%2582%25E8%25BF%25B0.zh%2f&title=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%916.1%20%e6%b5%81%e5%92%8c%e4%ba%8b%e4%bb%b6%e6%a6%82%e8%bf%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】6.1 流和事件概述 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-6-1-%25E6%25B5%2581%25E5%2592%258C%25E4%25BA%258B%25E4%25BB%25B6%25E6%25A6%2582%25E8%25BF%25B0.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】6.1 流和事件概述 on whatsapp" href="https://api.whatsapp.com/send?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%916.1%20%e6%b5%81%e5%92%8c%e4%ba%8b%e4%bb%b6%e6%a6%82%e8%bf%b0%20-%20https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-6-1-%25E6%25B5%2581%25E5%2592%258C%25E4%25BA%258B%25E4%25BB%25B6%25E6%25A6%2582%25E8%25BF%25B0.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】6.1 流和事件概述 on telegram" href="https://telegram.me/share/url?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%916.1%20%e6%b5%81%e5%92%8c%e4%ba%8b%e4%bb%b6%e6%a6%82%e8%bf%b0&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-6-1-%25E6%25B5%2581%25E5%2592%258C%25E4%25BA%258B%25E4%25BB%25B6%25E6%25A6%2582%25E8%25BF%25B0.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>