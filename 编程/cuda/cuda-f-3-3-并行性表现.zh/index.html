<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【CUDA 基础】3.3 并行性表现 | 谭升的博客</title><meta name=keywords content="nvprof"><meta name=description content="Abstract: 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率）
Keywords: nvprof"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0.zh/><link crossorigin=anonymous href=../../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="【CUDA 基础】3.3 并行性表现"><meta property="og:description" content="Abstract: 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率）
Keywords: nvprof"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0.zh/"><meta property="article:section" content="编程"><meta property="article:published_time" content="2018-04-15T21:17:52+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【CUDA 基础】3.3 并行性表现"><meta name=twitter:description content="Abstract: 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率）
Keywords: nvprof"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"【CUDA 基础】3.3 并行性表现","item":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【CUDA 基础】3.3 并行性表现","name":"【CUDA 基础】3.3 并行性表现","description":"Abstract: 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率） Keywords: nvprof\n","keywords":["nvprof"],"articleBody":"Abstract: 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率） Keywords: nvprof\n并行性表现 继续更新CUDA，前面为了加速概率论的学习停了一段CUDA，从今天开始继续CUDA和数学分析的更新，每一篇都写一点废话就相当于自己的日记了，之前很佩服那些写日记的人，因为根本不知道日记可以写些什么，但是现在看看，如果写一些文字记录自己，首先可以反思当下，其次是过一段时间以后可以看看自己到底有没有进步，这些都是有用的，所以大家可以略过我的废话，直接看正文。\n本文的主要内容就是进一步理解线程束在硬件上执行的本质过程，结合上几篇关于执行模型的学习，本文相对简单，通过修改核函数的配置，来观察核函数的执行速度，以及分析硬件利用数据，分析性能，调整核函数配置是CUDA开发人员必须掌握的技能，本篇只研究对核函数的配置是如何影响效率的（也就是通过网格，块的配置来获得不同的执行效率。） 本文全文只用到下面的核函数\n__global__ void sumMatrix(float * MatA,float * MatB,float * MatC,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx=ix+iy*ny; if (ixnx \u0026\u0026 iyny) { MatC[idx]=MatA[idx]+MatB[idx]; } } 没有任何优化的最简单的二维矩阵加法。 全部代码：\nint main(int argc,char** argv) { //printf(\"strating...\\n\");  //initDevice(0);  int nx=113; int ny=113; int nxy=nx*ny; int nBytes=nxy*sizeof(float); //Malloc  float* A_host=(float*)malloc(nBytes); float* B_host=(float*)malloc(nBytes); float* C_host=(float*)malloc(nBytes); float* C_from_gpu=(float*)malloc(nBytes); initialData(A_host,nxy); initialData(B_host,nxy); //cudaMalloc  float *A_dev=NULL; float *B_dev=NULL; float *C_dev=NULL; CHECK(cudaMalloc((void**)\u0026A_dev,nBytes)); CHECK(cudaMalloc((void**)\u0026B_dev,nBytes)); CHECK(cudaMalloc((void**)\u0026C_dev,nBytes)); CHECK(cudaMemcpy(A_dev,A_host,nBytes,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(B_dev,B_host,nBytes,cudaMemcpyHostToDevice)); int dimx=argc2?atoi(argv[1]):32; int dimy=argc2?atoi(argv[2]):32; double iStart,iElaps; // 2d block and 2d grid  dim3 block(dimx,dimy); dim3 grid((nx-1)/block.x+1,(ny-1)/block.y+1); iStart=cpuSecond(); sumMatrixgrid,block(A_dev,B_dev,C_dev,nx,ny); CHECK(cudaDeviceSynchronize()); iElaps=cpuSecond()-iStart; printf(\"GPU Execution configuration\\n\", grid.x,grid.y,block.x,block.y,iElaps); CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost)); cudaFree(A_dev); cudaFree(B_dev); cudaFree(C_dev); free(A_host); free(B_host); free(C_host); free(C_from_gpu); cudaDeviceReset(); return 0; } 可见我们用两个 $8192\\times 8192$ 的矩阵相加来测试我们效率。 注意一下这里的GPU内存，一个矩阵是 $2^{13}\\times 2^{13}\\times 2^2=2^{28}$ 字节 也就是 256M，三个矩阵就是 768M 因为我们的GPU内存就是 2G 的，所以我们没办法进行更大的矩阵计算了（无法使用原文使用的是 $2^{14}$ 的方矩阵）。\n用 nvprof 检测活跃的线程束 对比性能要控制变量，上面的代码只用两个变量，也就是块的x和y的大小，所以，调整x和y的大小来产生不同的效率，我们先来看看结果： 图片看不清，数据结果如下\n   gridDim blockDim time(s)     256,256 32,32 0.008304   256,512 32,16 0.008332   512,256 16,32 0.008341   512,512 16,16 0.008347   512,1024 16,8 0.008351   1024，512 8,16 0.008401    当块大小超过硬件的极限，并没有报错，而是返回了错误值，这个值得大家注意 另外，每个机器执行此代码效果可能定不一样，所以大家要根据自己的硬件分析数据。 书上给出的 M2070 就和我们的结果不同，2070的 (32,16) 效率最高，而我们的 (32,32) 效率最高，毕竟架构不同，而且CUDA版本不同导致了优化后的机器码差异很大，所以我们还是来看看活跃线程束的情况，使用\nnvprof --metrics achieved_occupancy ./simple_sum_matrix 得出结果    gridDim blockDim time(s) Achieved Occupancy     256,256 32,32 0.008304 0.813609   256,512 32,16 0.008332 0.841264   512,256 16,32 0.008341 0.855385   512,512 16,16 0.008347 0.876081   512,1024 16,8 0.008351 0.875807   1024，512 8,16 0.008401 0.857242    可见活跃线程束比例高的未必执行速度快，但实际上从原理出发，应该是利用率越高效率越高，但是还受到其他因素制约。 活跃线程束比例的定义是：每个周期活跃的线程束的平均值与一个sm支持的线程束最大值的比。\n用 nvprof 检测内存操作 下面我们继续用nvprof来看看内存利用率如何。 首先使用:\nnvprof --metrics gld_throughput ./simple_sum_matrix 来看一下内核的内存读取效率：    gridDim blockDim time(s) Achieved Occupancy GLD Throughput (GB/s)     256,256 32,32 0.008304 0.813609 60.270   256,512 32,16 0.008332 0.841264 60.042   512,256 16,32 0.008341 0.855385 59.996   512,512 16,16 0.008347 0.876081 59.967   512,1024 16,8 0.008351 0.875807 59.976   1024，512 8,16 0.008401 0.857242 59.440    可以看出虽然第一种配置的线程束活跃比例不高，但是吞吐量最大所以可见吞吐量和线程束活跃比例一起都对最终的效率有影响。 接着我们看看全局加载效率，全局效率的定义是：被请求的全局加载吞吐量占所需的全局加载吞吐量的比值（全局加载吞吐量），也就是说应用程序的加载操作利用了设备内存带宽的程度；注意区别吞吐量和全局加载效率的区别，这个在前面我们已经解释过吞吐量了，忘了的同学回去看看。\nnvprof --metrics gld_efficiency ./simple_sum_matrix 获得如下运行结果 很遗憾，在当前机器上进行测试所有的利用率都是 100% ，可见CUDA对核函数进行了优化，在 M2070上 使用以前的CUDA版本，并没有如此高的加载效率，有效加载效率是指在全部的内存请求中（当前在总线上传递的数据）有多少是我们要用于计算的。\n书上说如果线程块中内层的维度（blockDim.x）过小，小于线程束会影响加载效率，但是目前来看，不存在这个问题了。 随着硬件的升级，以前的一些问题，可能就不是问题了，当然对付老的设备，这些技巧还是很有用的。\n增大并行性 上面说 “线程块中内层的维度（blockDim.x）过小” 是否对现在的设备还有影响，我们来看一下下面的试验： 用表格列举一下数据\n   gridDim blockDim time(s)     (128,4096) (64,2) 0.008391   (128,2048) (64,4) 0.008411   (128,1024) (64,8) 0.008405   (64,4096) (128,2) 0.008454   (64,2048) (128,4) 0.008430   (64,1024) (128,8) 0.008418   (32,4096) (256,2) 0.008468   (32,2048) (256,4) 0.008439   (32,1024) (256,8) fail    通过这个表我们发现，最快的还是第一个，块最小的反而获得最高的效率，这里与书上的结果又不同了，我再想书上的数据量大可能会影响结果，当数据量大的时候有可能决定时间的因素会发生变化，但是一些结果是可以观察到\n 尽管（64，4） 和 （128，2） 有同样大小的块，但是执行效率不同，说明内层线程块尺寸影响效率。 最后的块参数无效 第一种方案速度最快  我们调整块的尺寸，还是为了增加并行性，或者说增加活跃的线程束，我们来看看线程束的活跃比例： 得到如下数据\n   gridDim blockDim time(s) Achieved Occupancy     (128,4096) (64,2) 0.008391 0.888596   (128,2048) (64,4) 0.008411 0.866298   (128,1024) (64,8) 0.008405 0.831536   (64,4096) (128,2) 0.008454 0.893161   (64,2048) (128,4) 0.008430 0.862629   (64,1024) (128,8) 0.008418 0.833540   (32,4096) (256,2) 0.008468 0.859110   (32,2048) (256,4) 0.008439 0.825036   (32,1024) (256,8) fail Nan    可见最高的利用率没有最高的效率。 没有任何一个因素可以直接左右最后的效率，一定是大家一起作用得到最终的结果，多因一效的典型例子，于是在优化的时候，我们应该首先保证测试时间的准确性，客观性，以及稳定性，说实话，我们上面的时间测试方法并不那么稳定，更稳定方法应该是测几次的平均时间，来降低人为误差。\n总结 指标与性能\n 大部分情况，单一指标不能优化出最优性能 总体性能直接相关的是内核的代码本质（内核才是关键） 指标与性能之间选择平衡点 从不同的角度寻求指标平衡，最大化效率 网格和块的尺寸为调节性能提供了一个不错的起点  从这个起点开始，我们后面逐渐的深入到各项指标，总之，用CUDA就是为了高效，而研究这些指标是提高效率最快的途径（当然内核算法提升空间更大），再强调一下，本文的所有数据只针对我使用的设备，对于任何其他的设备这些数据会完全不同，大家主要学习这几个测试指标和其间的相互关系。\n","wordCount":"356","inLanguage":"en","datePublished":"2018-04-15T21:17:52Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a></div><h1 class=post-title>【CUDA 基础】3.3 并行性表现</h1><div class=post-meta><span title="2018-04-15 21:17:52 +0000 UTC">April 15, 2018</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%b9%b6%e8%a1%8c%e6%80%a7%e8%a1%a8%e7%8e%b0 aria-label=并行性表现>并行性表现</a><ul><li><a href=#%e7%94%a8-nvprof-%e6%a3%80%e6%b5%8b%e6%b4%bb%e8%b7%83%e7%9a%84%e7%ba%bf%e7%a8%8b%e6%9d%9f aria-label="用 nvprof 检测活跃的线程束">用 nvprof 检测活跃的线程束</a></li><li><a href=#%e7%94%a8-nvprof-%e6%a3%80%e6%b5%8b%e5%86%85%e5%ad%98%e6%93%8d%e4%bd%9c aria-label="用 nvprof 检测内存操作">用 nvprof 检测内存操作</a></li><li><a href=#%e5%a2%9e%e5%a4%a7%e5%b9%b6%e8%a1%8c%e6%80%a7 aria-label=增大并行性>增大并行性</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率）
<strong>Keywords:</strong> nvprof</p><h1 id=并行性表现>并行性表现<a hidden class=anchor aria-hidden=true href=#并行性表现>#</a></h1><p>继续更新CUDA，前面为了加速概率论的学习停了一段CUDA，从今天开始继续CUDA和数学分析的更新，每一篇都写一点废话就相当于自己的日记了，之前很佩服那些写日记的人，因为根本不知道日记可以写些什么，但是现在看看，如果写一些文字记录自己，首先可以反思当下，其次是过一段时间以后可以看看自己到底有没有进步，这些都是有用的，所以大家可以略过我的废话，直接看正文。</p><p>本文的主要内容就是进一步理解线程束在硬件上执行的本质过程，结合上几篇关于执行模型的学习，本文相对简单，通过修改核函数的配置，来观察核函数的执行速度，以及分析硬件利用数据，分析性能，调整核函数配置是CUDA开发人员必须掌握的技能，本篇只研究对核函数的配置是如何影响效率的（也就是通过网格，块的配置来获得不同的执行效率。）
本文全文只用到下面的核函数</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>sumMatrix</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span> <span class=n>MatA</span><span class=p>,</span><span class=kt>float</span> <span class=o>*</span> <span class=n>MatB</span><span class=p>,</span><span class=kt>float</span> <span class=o>*</span> <span class=n>MatC</span><span class=p>,</span><span class=kt>int</span> <span class=n>nx</span><span class=p>,</span><span class=kt>int</span> <span class=n>ny</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>ix</span><span class=o>=</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=o>+</span><span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=o>*</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>iy</span><span class=o>=</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=o>+</span><span class=n>blockDim</span><span class=p>.</span><span class=n>y</span><span class=o>*</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>idx</span><span class=o>=</span><span class=n>ix</span><span class=o>+</span><span class=n>iy</span><span class=o>*</span><span class=n>ny</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>ix</span><span class=o>&lt;</span><span class=n>nx</span> <span class=o>&amp;&amp;</span> <span class=n>iy</span><span class=o>&lt;</span><span class=n>ny</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>MatC</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span><span class=o>=</span><span class=n>MatA</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span><span class=o>+</span><span class=n>MatB</span><span class=p>[</span><span class=n>idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>没有任何优化的最简单的二维矩阵加法。
全部代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>(</span><span class=kt>int</span> <span class=n>argc</span><span class=p>,</span><span class=kt>char</span><span class=o>**</span> <span class=n>argv</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=c1>//printf(&#34;strating...\n&#34;);
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>//initDevice(0);
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>int</span> <span class=n>nx</span><span class=o>=</span><span class=mi>1</span><span class=o>&lt;&lt;</span><span class=mi>13</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>ny</span><span class=o>=</span><span class=mi>1</span><span class=o>&lt;&lt;</span><span class=mi>13</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>nxy</span><span class=o>=</span><span class=n>nx</span><span class=o>*</span><span class=n>ny</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>nBytes</span><span class=o>=</span><span class=n>nxy</span><span class=o>*</span><span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>//Malloc
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>float</span><span class=o>*</span> <span class=n>A_host</span><span class=o>=</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)</span><span class=n>malloc</span><span class=p>(</span><span class=n>nBytes</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=kt>float</span><span class=o>*</span> <span class=n>B_host</span><span class=o>=</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)</span><span class=n>malloc</span><span class=p>(</span><span class=n>nBytes</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=kt>float</span><span class=o>*</span> <span class=n>C_host</span><span class=o>=</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)</span><span class=n>malloc</span><span class=p>(</span><span class=n>nBytes</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=kt>float</span><span class=o>*</span> <span class=n>C_from_gpu</span><span class=o>=</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)</span><span class=n>malloc</span><span class=p>(</span><span class=n>nBytes</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>initialData</span><span class=p>(</span><span class=n>A_host</span><span class=p>,</span><span class=n>nxy</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>initialData</span><span class=p>(</span><span class=n>B_host</span><span class=p>,</span><span class=n>nxy</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>//cudaMalloc
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>float</span> <span class=o>*</span><span class=n>A_dev</span><span class=o>=</span><span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>float</span> <span class=o>*</span><span class=n>B_dev</span><span class=o>=</span><span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>float</span> <span class=o>*</span><span class=n>C_dev</span><span class=o>=</span><span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMalloc</span><span class=p>((</span><span class=kt>void</span><span class=o>**</span><span class=p>)</span><span class=o>&amp;</span><span class=n>A_dev</span><span class=p>,</span><span class=n>nBytes</span><span class=p>));</span>
</span></span><span class=line><span class=cl>  <span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMalloc</span><span class=p>((</span><span class=kt>void</span><span class=o>**</span><span class=p>)</span><span class=o>&amp;</span><span class=n>B_dev</span><span class=p>,</span><span class=n>nBytes</span><span class=p>));</span>
</span></span><span class=line><span class=cl>  <span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMalloc</span><span class=p>((</span><span class=kt>void</span><span class=o>**</span><span class=p>)</span><span class=o>&amp;</span><span class=n>C_dev</span><span class=p>,</span><span class=n>nBytes</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>A_dev</span><span class=p>,</span><span class=n>A_host</span><span class=p>,</span><span class=n>nBytes</span><span class=p>,</span><span class=n>cudaMemcpyHostToDevice</span><span class=p>));</span>
</span></span><span class=line><span class=cl>  <span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>B_dev</span><span class=p>,</span><span class=n>B_host</span><span class=p>,</span><span class=n>nBytes</span><span class=p>,</span><span class=n>cudaMemcpyHostToDevice</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>dimx</span><span class=o>=</span><span class=n>argc</span><span class=o>&gt;</span><span class=mi>2</span><span class=o>?</span><span class=n>atoi</span><span class=p>(</span><span class=n>argv</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span><span class=o>:</span><span class=mi>32</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>dimy</span><span class=o>=</span><span class=n>argc</span><span class=o>&gt;</span><span class=mi>2</span><span class=o>?</span><span class=n>atoi</span><span class=p>(</span><span class=n>argv</span><span class=p>[</span><span class=mi>2</span><span class=p>])</span><span class=o>:</span><span class=mi>32</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=kt>double</span> <span class=n>iStart</span><span class=p>,</span><span class=n>iElaps</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// 2d block and 2d grid
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>dim3</span> <span class=n>block</span><span class=p>(</span><span class=n>dimx</span><span class=p>,</span><span class=n>dimy</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>dim3</span> <span class=n>grid</span><span class=p>((</span><span class=n>nx</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>/</span><span class=n>block</span><span class=p>.</span><span class=n>x</span><span class=o>+</span><span class=mi>1</span><span class=p>,(</span><span class=n>ny</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>/</span><span class=n>block</span><span class=p>.</span><span class=n>y</span><span class=o>+</span><span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>iStart</span><span class=o>=</span><span class=n>cpuSecond</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=n>sumMatrix</span><span class=o>&lt;&lt;&lt;</span><span class=n>grid</span><span class=p>,</span><span class=n>block</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>A_dev</span><span class=p>,</span><span class=n>B_dev</span><span class=p>,</span><span class=n>C_dev</span><span class=p>,</span><span class=n>nx</span><span class=p>,</span><span class=n>ny</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>CHECK</span><span class=p>(</span><span class=n>cudaDeviceSynchronize</span><span class=p>());</span>
</span></span><span class=line><span class=cl>  <span class=n>iElaps</span><span class=o>=</span><span class=n>cpuSecond</span><span class=p>()</span><span class=o>-</span><span class=n>iStart</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>printf</span><span class=p>(</span><span class=s>&#34;GPU Execution configuration&lt;&lt;&lt;(%d,%d),(%d,%d)|%f sec</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>grid</span><span class=p>.</span><span class=n>x</span><span class=p>,</span><span class=n>grid</span><span class=p>.</span><span class=n>y</span><span class=p>,</span><span class=n>block</span><span class=p>.</span><span class=n>x</span><span class=p>,</span><span class=n>block</span><span class=p>.</span><span class=n>y</span><span class=p>,</span><span class=n>iElaps</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>CHECK</span><span class=p>(</span><span class=n>cudaMemcpy</span><span class=p>(</span><span class=n>C_from_gpu</span><span class=p>,</span><span class=n>C_dev</span><span class=p>,</span><span class=n>nBytes</span><span class=p>,</span><span class=n>cudaMemcpyDeviceToHost</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>cudaFree</span><span class=p>(</span><span class=n>A_dev</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>cudaFree</span><span class=p>(</span><span class=n>B_dev</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>cudaFree</span><span class=p>(</span><span class=n>C_dev</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>free</span><span class=p>(</span><span class=n>A_host</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>free</span><span class=p>(</span><span class=n>B_host</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>free</span><span class=p>(</span><span class=n>C_host</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>free</span><span class=p>(</span><span class=n>C_from_gpu</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>cudaDeviceReset</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>可见我们用两个 $8192\times 8192$ 的矩阵相加来测试我们效率。
注意一下这里的GPU内存，一个矩阵是 $2^{13}\times 2^{13}\times 2^2=2^{28}$ 字节 也就是 256M，三个矩阵就是 768M 因为我们的GPU内存就是 2G 的，所以我们没办法进行更大的矩阵计算了（无法使用原文使用的是 $2^{14}$ 的方矩阵）。</p><h2 id=用-nvprof-检测活跃的线程束>用 nvprof 检测活跃的线程束<a hidden class=anchor aria-hidden=true href=#用-nvprof-检测活跃的线程束>#</a></h2><p>对比性能要控制变量，上面的代码只用两个变量，也就是块的x和y的大小，所以，调整x和y的大小来产生不同的效率，我们先来看看结果：
<img loading=lazy src=./1_1.png alt></p><p>图片看不清，数据结果如下</p><table><thead><tr><th style=text-align:center>gridDim</th><th style=text-align:center>blockDim</th><th style=text-align:center>time(s)</th></tr></thead><tbody><tr><td style=text-align:center>256,256</td><td style=text-align:center>32,32</td><td style=text-align:center>0.008304</td></tr><tr><td style=text-align:center>256,512</td><td style=text-align:center>32,16</td><td style=text-align:center>0.008332</td></tr><tr><td style=text-align:center>512,256</td><td style=text-align:center>16,32</td><td style=text-align:center>0.008341</td></tr><tr><td style=text-align:center>512,512</td><td style=text-align:center>16,16</td><td style=text-align:center>0.008347</td></tr><tr><td style=text-align:center>512,1024</td><td style=text-align:center>16,8</td><td style=text-align:center>0.008351</td></tr><tr><td style=text-align:center>1024，512</td><td style=text-align:center>8,16</td><td style=text-align:center>0.008401</td></tr></tbody></table><p>当块大小超过硬件的极限，并没有报错，而是返回了错误值，这个值得大家注意
另外，每个机器执行此代码效果可能定不一样，所以大家要根据自己的硬件分析数据。
书上给出的 M2070 就和我们的结果不同，2070的 (32,16) 效率最高，而我们的 (32,32) 效率最高，毕竟架构不同，而且CUDA版本不同导致了优化后的机器码差异很大，所以我们还是来看看活跃线程束的情况，使用</p><pre tabindex=0><code>nvprof --metrics achieved_occupancy ./simple_sum_matrix
</code></pre><p>得出结果
<img loading=lazy src=./1_2.png alt></p><table><thead><tr><th style=text-align:center>gridDim</th><th style=text-align:center>blockDim</th><th style=text-align:center>time(s)</th><th style=text-align:center>Achieved Occupancy</th></tr></thead><tbody><tr><td style=text-align:center>256,256</td><td style=text-align:center>32,32</td><td style=text-align:center>0.008304</td><td style=text-align:center>0.813609</td></tr><tr><td style=text-align:center>256,512</td><td style=text-align:center>32,16</td><td style=text-align:center>0.008332</td><td style=text-align:center>0.841264</td></tr><tr><td style=text-align:center>512,256</td><td style=text-align:center>16,32</td><td style=text-align:center>0.008341</td><td style=text-align:center>0.855385</td></tr><tr><td style=text-align:center>512,512</td><td style=text-align:center>16,16</td><td style=text-align:center>0.008347</td><td style=text-align:center>0.876081</td></tr><tr><td style=text-align:center>512,1024</td><td style=text-align:center>16,8</td><td style=text-align:center>0.008351</td><td style=text-align:center>0.875807</td></tr><tr><td style=text-align:center>1024，512</td><td style=text-align:center>8,16</td><td style=text-align:center>0.008401</td><td style=text-align:center>0.857242</td></tr></tbody></table><p>可见活跃线程束比例高的未必执行速度快，但实际上从原理出发，应该是利用率越高效率越高，但是还受到其他因素制约。
活跃线程束比例的定义是：每个周期活跃的线程束的平均值与一个sm支持的线程束最大值的比。</p><h2 id=用-nvprof-检测内存操作>用 nvprof 检测内存操作<a hidden class=anchor aria-hidden=true href=#用-nvprof-检测内存操作>#</a></h2><p>下面我们继续用nvprof来看看内存利用率如何。
首先使用:</p><pre tabindex=0><code>nvprof --metrics gld_throughput ./simple_sum_matrix
</code></pre><p>来看一下内核的内存读取效率：
<img loading=lazy src=./1_3.png alt></p><table><thead><tr><th style=text-align:center>gridDim</th><th style=text-align:center>blockDim</th><th style=text-align:center>time(s)</th><th style=text-align:center>Achieved Occupancy</th><th style=text-align:center>GLD Throughput (GB/s)</th></tr></thead><tbody><tr><td style=text-align:center>256,256</td><td style=text-align:center>32,32</td><td style=text-align:center>0.008304</td><td style=text-align:center>0.813609</td><td style=text-align:center>60.270</td></tr><tr><td style=text-align:center>256,512</td><td style=text-align:center>32,16</td><td style=text-align:center>0.008332</td><td style=text-align:center>0.841264</td><td style=text-align:center>60.042</td></tr><tr><td style=text-align:center>512,256</td><td style=text-align:center>16,32</td><td style=text-align:center>0.008341</td><td style=text-align:center>0.855385</td><td style=text-align:center>59.996</td></tr><tr><td style=text-align:center>512,512</td><td style=text-align:center>16,16</td><td style=text-align:center>0.008347</td><td style=text-align:center>0.876081</td><td style=text-align:center>59.967</td></tr><tr><td style=text-align:center>512,1024</td><td style=text-align:center>16,8</td><td style=text-align:center>0.008351</td><td style=text-align:center>0.875807</td><td style=text-align:center>59.976</td></tr><tr><td style=text-align:center>1024，512</td><td style=text-align:center>8,16</td><td style=text-align:center>0.008401</td><td style=text-align:center>0.857242</td><td style=text-align:center>59.440</td></tr></tbody></table><p>可以看出虽然第一种配置的线程束活跃比例不高，但是吞吐量最大所以可见吞吐量和线程束活跃比例一起都对最终的效率有影响。
接着我们看看全局加载效率，全局效率的定义是：被请求的全局加载吞吐量占所需的全局加载吞吐量的比值（全局加载吞吐量），也就是说应用程序的加载操作利用了设备内存带宽的程度；注意区别吞吐量和全局加载效率的区别，这个在前面我们已经解释过吞吐量了，忘了的同学回去看看。</p><pre tabindex=0><code>nvprof --metrics gld_efficiency ./simple_sum_matrix
</code></pre><p>获得如下运行结果
<img loading=lazy src=./1_4.png alt></p><p>很遗憾，在当前机器上进行测试所有的利用率都是 100% ，可见CUDA对核函数进行了优化，在 M2070上 使用以前的CUDA版本，并没有如此高的加载效率，有效加载效率是指在全部的内存请求中（当前在总线上传递的数据）有多少是我们要用于计算的。</p><p>书上说如果线程块中内层的维度（blockDim.x）过小，小于线程束会影响加载效率，但是目前来看，不存在这个问题了。
随着硬件的升级，以前的一些问题，可能就不是问题了，当然对付老的设备，这些技巧还是很有用的。</p><h2 id=增大并行性>增大并行性<a hidden class=anchor aria-hidden=true href=#增大并行性>#</a></h2><p>上面说 &ldquo;线程块中内层的维度（blockDim.x）过小&rdquo; 是否对现在的设备还有影响，我们来看一下下面的试验：
<img loading=lazy src=./1_5.png alt></p><p>用表格列举一下数据</p><table><thead><tr><th style=text-align:center>gridDim</th><th style=text-align:center>blockDim</th><th style=text-align:center>time(s)</th></tr></thead><tbody><tr><td style=text-align:center>(128,4096)</td><td style=text-align:center>(64,2)</td><td style=text-align:center>0.008391</td></tr><tr><td style=text-align:center>(128,2048)</td><td style=text-align:center>(64,4)</td><td style=text-align:center>0.008411</td></tr><tr><td style=text-align:center>(128,1024)</td><td style=text-align:center>(64,8)</td><td style=text-align:center>0.008405</td></tr><tr><td style=text-align:center>(64,4096)</td><td style=text-align:center>(128,2)</td><td style=text-align:center>0.008454</td></tr><tr><td style=text-align:center>(64,2048)</td><td style=text-align:center>(128,4)</td><td style=text-align:center>0.008430</td></tr><tr><td style=text-align:center>(64,1024)</td><td style=text-align:center>(128,8)</td><td style=text-align:center>0.008418</td></tr><tr><td style=text-align:center>(32,4096)</td><td style=text-align:center>(256,2)</td><td style=text-align:center>0.008468</td></tr><tr><td style=text-align:center>(32,2048)</td><td style=text-align:center>(256,4)</td><td style=text-align:center>0.008439</td></tr><tr><td style=text-align:center>(32,1024)</td><td style=text-align:center>(256,8)</td><td style=text-align:center>fail</td></tr></tbody></table><p>通过这个表我们发现，最快的还是第一个，块最小的反而获得最高的效率，这里与书上的结果又不同了，我再想书上的数据量大可能会影响结果，当数据量大的时候有可能决定时间的因素会发生变化，但是一些结果是可以观察到</p><ul><li>尽管（64，4） 和 （128，2） 有同样大小的块，但是执行效率不同，说明内层线程块尺寸影响效率。</li><li>最后的块参数无效</li><li>第一种方案速度最快</li></ul><p>我们调整块的尺寸，还是为了增加并行性，或者说增加活跃的线程束，我们来看看线程束的活跃比例：
<img loading=lazy src=./1_6.png alt></p><p>得到如下数据</p><table><thead><tr><th style=text-align:center>gridDim</th><th style=text-align:center>blockDim</th><th style=text-align:center>time(s)</th><th style=text-align:center>Achieved Occupancy</th></tr></thead><tbody><tr><td style=text-align:center>(128,4096)</td><td style=text-align:center>(64,2)</td><td style=text-align:center>0.008391</td><td style=text-align:center>0.888596</td></tr><tr><td style=text-align:center>(128,2048)</td><td style=text-align:center>(64,4)</td><td style=text-align:center>0.008411</td><td style=text-align:center>0.866298</td></tr><tr><td style=text-align:center>(128,1024)</td><td style=text-align:center>(64,8)</td><td style=text-align:center>0.008405</td><td style=text-align:center>0.831536</td></tr><tr><td style=text-align:center>(64,4096)</td><td style=text-align:center>(128,2)</td><td style=text-align:center>0.008454</td><td style=text-align:center>0.893161</td></tr><tr><td style=text-align:center>(64,2048)</td><td style=text-align:center>(128,4)</td><td style=text-align:center>0.008430</td><td style=text-align:center>0.862629</td></tr><tr><td style=text-align:center>(64,1024)</td><td style=text-align:center>(128,8)</td><td style=text-align:center>0.008418</td><td style=text-align:center>0.833540</td></tr><tr><td style=text-align:center>(32,4096)</td><td style=text-align:center>(256,2)</td><td style=text-align:center>0.008468</td><td style=text-align:center>0.859110</td></tr><tr><td style=text-align:center>(32,2048)</td><td style=text-align:center>(256,4)</td><td style=text-align:center>0.008439</td><td style=text-align:center>0.825036</td></tr><tr><td style=text-align:center>(32,1024)</td><td style=text-align:center>(256,8)</td><td style=text-align:center>fail</td><td style=text-align:center>Nan</td></tr></tbody></table><p>可见最高的利用率没有最高的效率。
没有任何一个因素可以直接左右最后的效率，一定是大家一起作用得到最终的结果，多因一效的典型例子，于是在优化的时候，我们应该首先保证测试时间的准确性，客观性，以及稳定性，说实话，我们上面的时间测试方法并不那么稳定，更稳定方法应该是测几次的平均时间，来降低人为误差。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>指标与性能</p><ul><li>大部分情况，单一指标不能优化出最优性能</li><li>总体性能直接相关的是内核的代码本质（内核才是关键）</li><li>指标与性能之间选择平衡点</li><li>从不同的角度寻求指标平衡，最大化效率</li><li>网格和块的尺寸为调节性能提供了一个不错的起点</li></ul><p>从这个起点开始，我们后面逐渐的深入到各项指标，总之，用CUDA就是为了高效，而研究这些指标是提高效率最快的途径（当然内核算法提升空间更大），再强调一下，本文的所有数据只针对我使用的设备，对于任何其他的设备这些数据会完全不同，大家主要学习这几个测试指标和其间的相互关系。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/nvprof/>nvprof</a></li></ul><nav class=paginav><a class=prev href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96.zh/><span class=title>« Prev</span><br><span>【CUDA 基础】3.4 避免分支分化</span></a>
<a class=next href=https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-2-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8-p2.zh/><span class=title>Next »</span><br><span>【CUDA 基础】3.2 理解线程束执行的本质(Part II)</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.3 并行性表现 on twitter" href="https://twitter.com/intent/tweet/?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.3%20%e5%b9%b6%e8%a1%8c%e6%80%a7%e8%a1%a8%e7%8e%b0&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-3-%25E5%25B9%25B6%25E8%25A1%258C%25E6%2580%25A7%25E8%25A1%25A8%25E7%258E%25B0.zh%2f&hashtags=nvprof"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.3 并行性表现 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-3-%25E5%25B9%25B6%25E8%25A1%258C%25E6%2580%25A7%25E8%25A1%25A8%25E7%258E%25B0.zh%2f&title=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.3%20%e5%b9%b6%e8%a1%8c%e6%80%a7%e8%a1%a8%e7%8e%b0&summary=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.3%20%e5%b9%b6%e8%a1%8c%e6%80%a7%e8%a1%a8%e7%8e%b0&source=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-3-%25E5%25B9%25B6%25E8%25A1%258C%25E6%2580%25A7%25E8%25A1%25A8%25E7%258E%25B0.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.3 并行性表现 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-3-%25E5%25B9%25B6%25E8%25A1%258C%25E6%2580%25A7%25E8%25A1%25A8%25E7%258E%25B0.zh%2f&title=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.3%20%e5%b9%b6%e8%a1%8c%e6%80%a7%e8%a1%a8%e7%8e%b0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.3 并行性表现 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-3-%25E5%25B9%25B6%25E8%25A1%258C%25E6%2580%25A7%25E8%25A1%25A8%25E7%258E%25B0.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.3 并行性表现 on whatsapp" href="https://api.whatsapp.com/send?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.3%20%e5%b9%b6%e8%a1%8c%e6%80%a7%e8%a1%a8%e7%8e%b0%20-%20https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-3-%25E5%25B9%25B6%25E8%25A1%258C%25E6%2580%25A7%25E8%25A1%25A8%25E7%258E%25B0.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【CUDA 基础】3.3 并行性表现 on telegram" href="https://telegram.me/share/url?text=%e3%80%90CUDA%20%e5%9f%ba%e7%a1%80%e3%80%913.3%20%e5%b9%b6%e8%a1%8c%e6%80%a7%e8%a1%a8%e7%8e%b0&url=https%3a%2f%2fgo.face2ai.com%2f%25E7%25BC%2596%25E7%25A8%258B%2fcuda%2fcuda-f-3-3-%25E5%25B9%25B6%25E8%25A1%258C%25E6%2580%25A7%25E8%25A1%25A8%25E7%258E%25B0.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>