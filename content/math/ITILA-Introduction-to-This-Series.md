---
title: ã€ä¿¡æ¯è®ºã€æ¨ç†ä¸å­¦ä¹ ç®—æ³•ã€‘æœ¬ç³»åˆ—åšå®¢ä»‹ç»
categories:
  - Information Theory
tags:
  - Information Theory
  - ä¿¡æ¯è®º
  - Inference
  - æ¨ç†
  - Learning Algorithms
  - ç®—æ³•
toc: true
date: 2018-09-21 11:53:59
---

**Abstract:** æœ¬æ–‡æ˜¯æœ¬ç³»åˆ—çš„ç¬¬ä¸€ç¯‡ï¼Œä»‹ç»æœ¬ç³»åˆ—çš„ä¸»è¦å†…å®¹
**Keywords:** Information Theoryï¼Œä¿¡æ¯è®ºï¼ŒInferenceï¼Œæ¨ç†ï¼ŒLearning Algorithmsï¼Œå­¦ä¹ ç®—æ³•

<!--more-->
# ä¿¡æ¯è®ºã€æ¨ç†ä¸å­¦ä¹ ç®—æ³•ä»‹ç»
è¿™ä¸ªç³»åˆ—æ˜¯ä¿¡æ¯è®ºç›¸å…³å†…å®¹çš„ä»‹ç»ï¼Œä¿¡æ¯è®ºæ˜¯ä»€ä¹ˆå¯èƒ½æœ‰äº›åšæœºå™¨å­¦ä¹ æˆ–è€…AIçš„åŒå­¦ä»¬ä¸å¤ªäº†è§£ï¼Œè€Œåšé€šä¿¡çš„åŒå­¦åº”è¯¥æ˜¯éå¸¸æ¸…æ¥šçš„ï¼Œå¦‚ä½•å‡†ç¡®çš„å®šä¹‰ä¿¡æ¯è®ºæ˜¯ä»€ä¹ˆï¼Œä¸åœ¨æˆ‘çš„èƒ½åŠ›èŒƒå›´å†…ï¼Œä½†æ˜¯æˆ‘ä»¬å¹³æ—¶æ¥è§¦åˆ°çš„å›¾åƒï¼Œæˆ–è€…ç®€å•ç‚¹è¯´ç°åº¦å›¾åƒï¼Œä¸€ä¸ª8bitçš„åƒç´ ç‚¹èƒ½æœ‰å¤šå°‘é˜¶ç°åº¦ï¼Œä¸ºä»€ä¹ˆæœ‰256ï¼Œè€Œä¸æ˜¯258æˆ–è€…å…¶ä»–çš„ï¼Œè¿™ä¸ªå…¶å®å°±å±äºä¿¡æ¯è®ºçš„çŸ¥è¯†èŒƒå›´ï¼Œè€Œä¿¡æ¯è®ºå’Œæœºå™¨å­¦ä¹ æœ‰å…³ç³»ä¹ˆï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œå‡¡æ˜¯å¤„ç†ä¿¡æ¯ï¼Œä¼ é€’ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œéƒ½å¤šå¤šå°‘å°‘è·Ÿä¿¡æ¯è®ºæœ‰é‚£ä¹ˆç‚¹å…³ç³»ã€‚
æœ¬ç³»åˆ—ä¸»è¦é¢å¯¹çš„è¯»è€…æ˜¯ï¼š **ä»äº‹æœºå™¨å­¦ä¹ ï¼Œäººå·¥æ™ºèƒ½ç±»å†…å®¹ç ”ç©¶çš„åŒå­¦ï¼Œå·¥ç¨‹å¸ˆï¼Œæˆ–è€…çˆ±å¥½è€…**
éœ€è¦çš„èƒŒæ™¯çŸ¥è¯†ï¼šå·¥ç¨‹ä¸“ä¸šï¼Œç§‘å­¦ç±»ä¸“ä¸šï¼Œæˆ–è€…æ•°å­¦ç±»æœ¬ç§‘1ï¼Œ2å¹´çº§çš„æ•°å­¦çŸ¥è¯†ï¼ŒåŒ…æ‹¬ï¼Œå¾®ç§¯åˆ†ï¼Œæ¦‚ç‡è®ºï¼Œçº¿æ€§ä»£æ•°çš„åŸºæœ¬çŸ¥è¯†ï¼ˆæœ¬ç«™å·²ç»å®Œæˆè¿™äº›åŸºç¡€çŸ¥è¯†çš„å…¨éƒ¨åšå®¢ï¼Œå¯ä»¥éšæ—¶æŸ¥é˜…ï¼‰ã€‚
æœ¬ä¹¦å°é¢ï¼š
![](./cover.jpg)
## æœºå™¨å­¦ä¹ ï¼Œä¿¡æ¯è®º
ä¼ ç»Ÿçš„ä¿¡æ¯è®ºè¯¾ç¨‹ä¸ä»…åŒ…æ‹¬Shannonçš„ä¿¡æ¯åŒ–æ€æƒ³ï¼Œä¹Ÿæœ‰å®é™…è§£å†³é—®é¢˜çš„ç°å®åº”ç”¨ï¼Œæˆ‘ä»¬è¿™ä¸ªç³»åˆ—æ›´åŠ è¿›ä¸€æ­¥çš„åŒ…æ‹¬äº†ï¼š
- Bayesian Data Modelling
- Monte Carlo Methods
- Variational Methods
- Clustering ALgorithms
- Neural Networks

ä¸ºä»€ä¹ˆè¦æŠŠä¿¡æ¯è®ºå’Œæœºå™¨å­¦ä¹ å¼„åˆ°ä¸€èµ·ï¼Ÿ
ä¿¡æ¯è®ºå’Œæœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªç¡¬å¸çš„ä¸¤é¢ï¼
60å¹´ä»£ä¸€ä¸ªé¢†åŸŸ â€”â€” æ§åˆ¶ç†è®ºï¼ˆcyberneticsï¼‰åœ¨ä¿¡æ¯è®ºï¼Œè®¡ç®—æœºç§‘å­¦ï¼Œå’Œç¥ç»ç§‘å­¦ç­‰å­¦ç§‘ä¸­éå¸¸ç«çˆ†ï¼Œè¿™äº›ç§‘å­¦å®¶ä»¬éƒ½åœ¨ç ”ç©¶ä¸€ä¸ªç›¸åŒçš„é—®é¢˜ï¼Œé‚£æ—¶å€™ä¿¡æ¯è®ºå’Œæœºå™¨å­¦ä¹ è¿˜æ˜¯å±äºåŒä¸€ç±»ã€‚å¤§è„‘æ˜¯ä¸€ä¸ªå‹ç¼©ä¿¡æ¯ï¼Œè¿›è¡Œæ²Ÿé€šçš„ç³»ç»Ÿï¼Œè€Œåœ¨æ•°æ®å‹ç¼©ï¼ˆdata conpressionï¼‰å’Œçº é”™ç ä¸Šï¼ˆerror-correcting codeï¼‰è¡¨ç°æœ€å¥½çš„ï¼ˆstate-of-the-artï¼‰çš„ç®—æ³•ä¸Šä½¿ç”¨çš„å·¥å…·ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ä¹Ÿä¼šä½¿ç”¨ã€‚
è¿™äº›ç§ç§è¿¹è±¡éƒ½è¡¨æ˜ï¼Œæœºå™¨å­¦ä¹ å’Œä¿¡æ¯è®ºæœ‰ç€å¯†åˆ‡çš„å…³è”ï¼Œè€Œæˆ‘ä»¬æœ¬ç³»åˆ—æ›´å…³æ³¨çš„å°±æ˜¯ä¿¡æ¯è®ºåœ¨æœºå™¨å­¦ä¹ æ–¹é¢çš„åº”ç”¨ï¼Œæˆ–è€…å¸®åŠ©æˆ‘ä»¬ç†è§£ä¸€äº›ç®—æ³•çš„ç‰¹ç‚¹å’Œå±€é™ã€‚
## å­¦ä¹ åœ°å›¾
æœ¬ä¹¦çš„ç›®å½•å¦‚ä¸‹,å½“ç„¶è¿™äº›è¯¾ä¸æ˜¯æˆ‘ä»¬æ‰€æœ‰è¦å­¦çš„ï¼Œæˆ‘ç”»äº†ä¸ªåœ°å›¾ï¼Œå¤§æ¦‚åº”è¯¥æ˜¯æŒ‰ç…§è¿™ä¸ªåœ°å›¾æ¥å®Œæˆæˆ‘ä»¬çš„åšå®¢çš„ï¼š
### Preface
1 Introduction to Information Theory
2 Probability, Entropy, and Inference
3 More about Inference

### I Data Compression
4 The Source Coding Theorem
5 Symbol Codes
6 Stream Codes
7 Codes for Integers

### II Noisy-Channel Coding
8 Dependent Random Variables
9 Communication over a Noisy Channel
10 The Noisy-Channel Coding Theorem
11 Error-Correcting Codes and Real Channels

### III Further Topics in Information Theory
12 Hash Codes: Codes for Ecient Information Retrieval
13 Binary Codes
14 Very Good Linear Codes Exist
15 Further Exercises on Information Theory
16 Message Passing
17 Communication over Constrained Noiseless Channels
18 Crosswords and Codebreaking
19 Why have Sex? Information Acquisition and Evolution

### IV Probabilities and Inference
20 An Example Inference Task: Clustering
21 Exact Inference by Complete Enumeration
22 Maximum Likelihood and Clustering
23 Useful Probability Distributions
24 Exact Marginalization
25 Exact Marginalization in Trellises
26 Exact Marginalization in Graphs
27 Laplace's Method
28 Model Comparison and Occam's Razor
29 Monte Carlo Methods
30 Ecient Monte Carlo Methods
31 Ising Models
32 Exact Monte Carlo Sampling
33 Variational Methods
34 Independent Component Analysis and Latent Variable Modelling
35 Random Inference Topics
36 Decision Theory
37 Bayesian Inference and Sampling Theory

### V Neural networks
38 Introduction to Neural Networks
39 The Single Neuron as a Classier
40 Capacity of a Single Neuron
41 Learning as Inference
42 Hopeld Networks
43 Boltzmann Machines
44 Supervised Learning in Multilayer Networks
45 Gaussian Processes
46 Deconvolution

### VI Sparse Graph Codes
47 Low-Density Parity-Check Codes
48 Convolutional Codes and Turbo Codes
49 Repeat{Accumulate Codes
50 Digital Fountain Codes

### åœ°å›¾

æ ¹æ®æˆ‘ä»¬çš„ç›®æ ‡å’Œä¹¦ä¸Šç»™å‡ºçš„å»ºè®®ï¼Œæˆ‘ä»¬è¦å­¦ä¹ ä¸‹é¢è¿™äº›ç« èŠ‚ï¼Œç®­å¤´ä¹‹é—´è¡¨ç¤ºå…ˆåå…³ç³»ï¼Œç®­å¤´æŒ‡å‘çš„è¯¾ç¨‹éœ€è¦åœ¨å‰é¢çš„è¯¾ç¨‹å®Œæˆåæ‰èƒ½è¿›è¡Œï¼š
![](https://raw.githubusercontent.com/Tony-Tan/MachineLearningMath/master/information.png)

github: [https://github.com/Tony-Tan/MachineLearningMath](https://github.com/Tony-Tan/MachineLearningMath) ä¸Šæœ‰é«˜æ¸…å¤§å›¾

## æ€»ç»“
æœ¬æ–‡æ˜¯ä¿¡æ¯è®ºçš„ç¬¬ä¸€è¯¾ï¼Œåç»­å°±å›´ç»•ä¸Šå›¾å±•å¼€ï¼Œå¯¹äºåŸºç¡€ä¸æ˜¯äº†è§£çš„åŒå­¦å¯ä»¥å»çœ‹å‰é¢çš„å…¶ä»–åšå®¢ï¼Œè°¢è°¢æ”¯æŒã€‚




