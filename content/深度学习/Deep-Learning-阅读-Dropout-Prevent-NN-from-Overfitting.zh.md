---
title: Droupout 的论文解读
toc: true
categories:
  - Deep Learning
date: 2017-09-13 15:49:22
tags:
  - Dropout
  - Overfit

---
**Abstract:** 使用dropout处理过拟合，原论文的理解总结
**Keywords:** Dropout，Overfit
<!--more-->
# Droupout 的论文
本文从原来的博客迁移过来，主要是学习dropout的论文，一些翻译和自己的理解

## Abstract

深度神经网络因为拥有大量的参数，所有具有非常强大的学习功能，而大量参数带来的问题就是过拟合（overfit），过拟合严重影响模型的范化性能，而大型深度神经网络计算速度又较慢，很难在实际应用中通过联合多个模型来给出结果。

Dropout就是来解决这种问题的方法，其中心思想是在训练的过程中随机的裁剪神经网络中的部分单元来获得完整模型的子模型，对子模型进行训练。这使得所有单元对于训练数据的过度适配。子模型的数量对于原模型是指数级的（子集与全集的关系）。

在测试过程中，对所有子网络预测结果的联合结果也有比较简单的近似方法，通过减小整个网络的连接权重来近似平均。

值得注意的是，这种方法可以有效减轻过拟合，相比于其他正则化方法，dropout表现更好，在监督学习如视觉识别，语义识别，文档分类等给出了state-of-the-art的表现。

## Introduction

深度神经网络包含多个非线性隐含层，这使得它能够学习非常复杂的模型，在有限的训练集上一些噪声的采样会造成某些复杂的输入输出关系，所以他们只存在于训练样本中，而在实际的测试阶段不存在这类样本，即使其拥有与实际样本相同的概率分布。这就造成了过拟合，为此我们提出了很多方法来减轻其影响：

**1. 当出现过拟合迹象时马上停止训练**

**2. 权重惩罚，包含L1，L2正则化等**

**3. 软权重共享（soft weight sharing）**

如果有无限的计算资源，最好的正则化的方法是对于一个固定大小的模型，对于所有可能的参数集给出的预测进行加权平均，每个模型的权重由其对训练样本后验概率给出。这种方法对于简单的小模型比较有用，但是我们更想要使用更小的计算量来完成贝叶斯黄金准则（Bayesian gold standard）。为完成上述过程，我们对共享参数的模型（一个完整的大模型）的指数级个数的子模型（大模型dropout的子模型）的预测进行等权重几何平均。

模型联合几乎总是可以提升机器学习方法的表现，但对于大型模型，这个将会非常难达成。当每个模型之间完全独立时，联合多个模型时能有测试表现有较好的提升，而对于获得完全独立的模型，要么有不同的架构（architecture）要么使用不同的训练数据。训练不同架构的模型看起来有些吃力，因为每个模型都拥有很多的超参数，优化一个模型将耗费大量的时间，并且一个大的模型通常需要大量的数据，所以训练多个大型模型使用多个不同的大训练集，这非常困难。即使有人能够满足上面所有的条件，无限的计算量，超大的数据集，但是在测试的时候需要快速响应，这也是个计算瓶颈。

dropout就是来解决上述两个关键点的：

**1. 大量独立的模型**

**2. 不是那么大量的训练集**

其基本描述是阻止过拟合，提供一种方法有效的联合指数级的不同的神经网络的近似形式。dropout的意思是丢掉网络中的一些单元（可见的或隐藏的神经元），通过暂时性的移除一些单元，包括其输入输出链接全部移除，移除的单元是随机挑选的，如图：

![](./屏幕快照-2016-10-23-下午12.15.52-1024x469.png)

对于一个简单的例子，每个单元按照一个固定概率P决定取舍，所有单元之间相互独立，P可以设置为一个集合（每个单元拥有自己的P）也可以简单的把所有单元都设置成0.5。设置成0.5就接近于找到模型所有子集的集合。对于输入单元，p一般选择大于0.5小于1之间的数（使输入尽量完整些）。

对于一个子模型，是dropout后剩下的所有单元的子网络，如图1b，一个有n个单元的网络拥有 $ 2^n $个可能的子集。所有这些网络共享参数，所以参数还是 $O(n^2)$ 或者更少。然后按照正常方法训练子网络。所以训练一个dropout的神经网络可以被看做训练一个$2^n$个子网络的集合，所有子网络共享权值，有些子网络很少，甚至没有被训练过。

在测试阶段，明确的平均地平均指数级子网络的输出结果是不可能的，然而在实际中有一种非常简单的近似方法，这种思想被用于神经网络的测试阶段，而不需要把网络dropout后再测试。网络中的权重被缩小（与训练时的子网络比较），如果子网络选取方法是按照概率p来dropout，测试时所有权重乘以p来，
如图2：

![](./屏幕快照-2016-10-23-下午1.44.22-1024x383.png)
来确保任何隐含层测试时的输出与激活输出（训练时dropout的情况下的激活输出）一致，通过scale-down（缩小）2^n个共享权值的网络组合成一个网络在test过程中使用。我们发现使用这种训练时dropout，测试时加权合并近似的网络，在大量分类任务中
与其他正则化方方法相比，dropout的泛化误差相当的小。

dropout的思想不止用于feed-forward神经网络，其可以被更广泛的用于以图（graph）为基础的模型上，例如玻尔兹曼机，在这篇文章中，我们将介绍dropout版受限玻尔兹曼机，并与标准的受限玻尔兹曼机（RBM）相比较，作者经验，dropout RBM比较好。

本文结构：

**Sec2：此方法的灵感来源**

**Sec3：类似工作**

**Sec4：正式描述dropout模型**

**Sec5：训练dropout模型的算法**

**Sec6：给出作者们实验的数据**

**Sec7：分析dropout在不同性质的神经网络的上表现，并描述为何dropout能工作**

**Sec8：dropout RBMs**

**Sec9：marginalizing dropout的思想**

**Appendix A：实际训练网络的知道，包括实际情况下详细的分析，包括在训练dropout时如何选择超参数**

## Motivation
dropout的动机是进化论里面的有性生殖法则，有性生殖是通过获得双亲各一半的基因加上一小部分突变来构成一个子代。无性生殖是直接复制上一代完整基因序列完成。表面看起来无性生殖应该是更好的方法优化个体适应性，因为一套已经工作很好的基因序列可以完整的直接传递给下一代。另一方面，有性生殖似乎打破了这套调整有限（不太变化）的基因集合，特别是这个集合非常庞大，表面上看，这使得这套已经进化的非常复杂的基因对于有机体的适应性降低。然而，有性生殖是生物进化的最优方式。

一个可能的解释有性生殖优势的可能是，长时间以来，自然选择准则可能不是个体适应性能够应对的，而需要基因组合后的综合能力，通过基因随机组合使个体变得强大，由于基因不能表现出所有祖先的，但是能够学习到一些有用的特点，或者和一些其他的小数其他基因配合产生个体强有力的适应性（基因突变）。 根据这理论，有性生殖不只是保留并传递有用的基因，同时也减少个体基因对个体过度的封闭性（这将不利于个体获得新的特性）。类似的，神经网络的每一层隐藏单元使用dropout训练必须随机选取网络中的单元。这使得隐含层单元更robust，并驱使子网络学习有用的特征，依靠单元本身而不是其他层的单元来修正自身错误。而且，同一层隐含单元各自学习不同的特征。可能有人认为通过复制每个隐含层单元来使得网络比dropout训练出来的更加robust，但实际上这个烂主意。
一个对于dropout相近但又稍微不同的动机，来自如何搞破坏，如果十个阴谋每个阴谋需要五个人来执行，和五十个人完成一个大阴谋相比，前者的破坏性更强，因为对于五十个人要紧密合作产生的破坏性，五个人更容易完成一些小阴谋，所以当大环境变了，五十个人的训练可能与实际环境不一样，阴谋没办法产生效果，五个人的阴谋成功概率会大一些，所以产生的破坏性更大一些。

## Related Work
Dropout可以当做通过在隐含层加噪声来正则化神经网络。给神经元加噪声这种方法被用于DAEs，在那里，噪声加到自动解码的输入单元，神经网络被训练来重建没有噪声的输入。本文的工作延展了这个思路，通过展现Dropout能够有效地利用在隐含层，也可以解释为模型的平均。我们也发现，加入噪声不知能用于非监督特征学习，也可以用来延展监督学习问题。实际上，我们的方法可以被用于其他以神经元（unit）结构，例如：玻尔兹曼机。Dropout 裁剪掉20%的输入单元，50%的隐含层单元经常是最优的。

由于dropout可以被看做随机正则化技巧，自然的被看做近似边缘化噪声的同类问题。本文中，我们可以用简单的例子说明，Dropout可以边缘化来得到确定性的正则方法。最近也有人用dropout类似的方法做实验，但仅限于dropout输入，而未对隐含层进行。

在dropout过程中，我们随机优化包含噪声分布的损失函数，这能够被看成是最小化一个损失函数期望。

## 原理
对于一个L层隐含层的神经网络 $l\in \{1,.....,L\}$，$z^{(l)}$ ,第l层的输入，$y^{(l)}$ 定义为第l层的输出 $y^{(0)}=x$ 是输入，$W^{(l)}$ 和 $b^{(l)}$为weight和biases

feed-forward：

$$z_i^{(l+1)}=w_i^{(l+1)}+b_i^{(l+1)}\\
y_i^{(l+1)}=f(z_i^{(l+1)})$$
f是激活函数：
$$f(x)=1/(1+e^{(-x)})$$

加入dropout后:（敲黑板，同志们，注意啦，下面是整个文章的核心啦）
$$
r_j^{(l)}\sim Bernoulli(p)\\
\tilde{y}^{(l)}=r^{(l)}*y^{(l)} \\
z_i^{(l+1)}=w_i^{(l+1)}\tilde{y}^l+b_i^{(l+1)}\\
y_i^{(l+1)}=f(z_i^{(l+1)})
$$

![](./屏幕快照-2016-11-18-下午6.39.37-1024x516.png)

上图为正常神经网络和dropout后的网络结构比较
对于layer l $r^{(l)}$ 是一个独立的伯努利随机分布的向量，其中元素为1的可能性为p。这个向量的目的是dropout，随机的选取一些输出，也就是说随机的去掉一些连接，这样的结果就是从 $y^{(l)}$ 来得到dropout后的裁剪过的 $\tilde{y}^{(l)}$ ，这个裁剪过的输出作为下一个单元的输入，这种操作应用于所有layer，这样的整体结果，网络被随机裁剪成一个子网络。
在训练阶段，loss函数的导数反向传导只经过子网络，相当于其他被dropout的节点将不被更新。
在测试阶段，所有weight被缩放 $W_{test}^{(l)}=pW^{(l)}$ ，此后不再使用dropout
如图：
![](./屏幕快照-2016-11-18-下午7.13.59-1024x410.png)

## 总结
现在来看，这篇文章主要是翻译工作，没有什么自己的理解和感受，因为阅读原文你会发现，你能想到的，文章都有，你想不到的，文章也有一部分，可能这就是大牛的论文吧





