---
title: 【强化学习】1-2 强化学习举例
categories:
    - Reinforcement Learning
    - RL-An Introduction
tags:
    - 强化学习
keywords:
    - 强化学习
    - 强化学习举例
    - Agent
    - Environment
    - 环境
    - Reaction
    - 反应
toc: true
date: 2018-08-30 23:27:59
---

**Abstract:** 本文介绍几个对应于强化学习的生活中的例子，来具体化前面提到的名词和几个重要理论在自然界中的表现。
**Keywords:** 强化学习，强化学习举例，Agent，Environment，环境，Reaction，反应

<!--more-->
# 强化学习举例
## 强化学习例子(Examples)
这几个例子都是实际自然界或者生活的例子，并不是RL的具体应用，所以不要理解错了，我们通过观察自然，观察生物智能的形成和遗传，是我们了解智能的有效方法，个人愚见，没准这也是唯一突破当前理解障碍的方法，生物通过数万年的演化，遗传，庞大的种群保证了其有大量的样本，来完成筛选和淘汰，每个个体的基因，神经系统，数量大到可能无法想象。所以如果连这些都没考虑过，没深入研究过，应该是对问题没有深刻理解的。
陶哲轩说过，如果你对问题的来源内容背景都不是很了解的话就想去解决问题，那么这个非常困难的。
我们来看几个例子：
1. 一个专业棋手下棋，当他每下一步的时候，他考虑的都是在计算预测，当他走了某一步以后，可能的结果以及对方会进行的反制措施，或者有时候，凭借直觉立刻来决定这步棋怎么走。
2. 一个自主的控制者，实时调节参数来控制石油精炼加工的工艺，这个控制者可以自主的取平衡 产出-消耗-质量 之间的平衡关系，而不需要完全按照工程师给出的精确结果。
3. 小羚羊，小牛，在刚出生的几分钟就能挣扎的站起来，半小时左右就能以20mile/hour 的速度奔跑
4. 移动的机器人，能决定是否需要进入一个新房间找垃圾还是马上找到路线去充电，他的决定取决于当前的电量，以及找到路线需要花费的能量
5. Phil准备他的早餐，虽然在我们看来，这个谁都可以，非常平常，但是整个过程，非常严苛，准备早饭的着一系列动作隐藏了一个巨大的复杂的条件网络，目标和子目标网络，比如，我们分析一下：走到厨房，打开柜子，选择原料，拿到原料，打开原料包装，然后把剩余的放回去；接着下一套动作是取碗，勺子，拿牛奶；这些的所有过程包括了眼睛的动作，寻找，定位，协调手完成动作；迅速决定用什么动作，把这些东西以什么样的轨迹放到哪里，并且不要碰撒旁边的其他容器。一个简单的早饭仔细分析竟然如此复杂，每一步都有明确的goal，比如取原料，是为了吃里面的东西，而不是为了打开包装，在吃饭的时候，用勺子吃了第一勺食物，是为了吃下一勺，以及最后从中获得能量。无论Phil是否享受吃饭的过程，如果当前身体的station告诉他很需要能量，需要大概吃多少，以及想吃什么，他都会按照这个指令去做的。



## 强化学习的特征(Features of Examples and RL)

上面5个都是我们生活中自然界的例子，所有例子经过分析都可以得出以下结论：
- 所有例子都包含interaction（作用，反应）
- 这些interaction都是在agent和environment之间产生的
- agent要做出决定，做什么
- environment是agent所处的环境，agent在其中搜寻，并达到自己的目标
- environment不管是否已知，agent都要去搜索

Agent的Actions会影响未来Environment的State，以及Agent后面的选择空间，例如：
- 下棋的这一步决定，直接影响棋手下一步的走法
- 机器人走的下一步，会影响他的电量，和他找到充电站的消耗能量

所有这些action的结果都会在若干步后体现，而不是马上反映出来，所以目前agent能做的就是预测和计划（prediction and planning）


## 有效性(Effects of Actions)
上面这些例子，所有action的结果全部无法完全预测，所以agent只能自己随时注意environment的变化，随时做出反应。
比如在Phile做饭的过程中，他要仔细盯着要拿出来多少材料，加多少牛奶，而且不能溢出来。
所有例子中的目标在某种意义上说都是非常明确地，agent通过直观的感受来判断是否向着目标前进。比如：
- 棋手知道什么样算是赢了
- 石油提炼工知道生产了多少油
- 机器人知道自己有多少电，还有多久能到充电站
- Phil知道自己吃没吃饱


## Agent的经验(Experience of Agent)
所有例子，agent都能根据经验提高他们的表现：
- 棋手反复训练能提高技艺
- 羚羊通过反复的尝试知道怎么能站稳，能奔跑
- Phil天天做早餐，所以知道什么样的工序最优

那么agent后面的技能是根据前面的经验，那么刚开始的agent从哪来的经验呢？（Agent初始知识来源）
- 来自类似的任务
- 通过设计，人工完成
- 生物进化

所有这些都是agent初始化的内容，但是agent最终表现，都是要靠和environment之间的interaction完成的，这个过程逐渐修正agent的行为，执行在当前环境特异化的操作。

## Conclusion
前面的高谈阔论，不过就是做早饭的一个过程，可见，我们获得了如此行动能力，和智慧是多么的复杂和令人惊叹的


### References
1. Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.

原文来自：[https://face2ai.com/RL-RSAB-1-2-Examples/](https://face2ai.com/RL-RSAB-1-2-Examples/)转载标明出处
