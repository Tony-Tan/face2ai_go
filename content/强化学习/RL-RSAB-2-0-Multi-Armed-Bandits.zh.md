---
title: 【强化学习】2.0 多臂赌博机
categories:
    - Reinforcement Learning
    - RL-An Introduction
tags:
    - 强化学习
    - 多臂赌博机
date: 2018-10-07 17:03:13
---

**Abstract:** 本文是第二章“多臂赌博机”的绪论，介绍本章主要内容
**Keywords:** 强化学习，多臂赌博机
<!--more-->
# 多臂赌博机
强化学习与其他学习算法最大的不同在于训练信息，我们熟知的监督学习，无论从简单的线性回归，到复杂的深度学习，所有这些监督学习用到的训练信息都是Instructing（指导，讲授）的，也就是说训练信息中包含明确的行为指导，比如对于一张输入图片判断是否有人脸，标记好的训练数据会明确的对结果进行校正——是否有人脸，如果有人脸在哪，训练模型偏差会被准确计算，同时通过优化算法逐步减少这个偏差，直到我们设定的阈值后完成训练。
而强化学习的训练信息则不同，强化学习的每一步没有指导信息，而是只有一个“评价”（evaluate），评价这个行为(action)的得分，得分也就是好坏，但是没有正确错误的说法，也没有最好或者最坏的说法。
这种评价机制导致了强化学习需要在学习的过程中加入探索(exploration)，来用trial-and-error的搜索方式得到好的模型。
## “指导”型反馈和“评价”型反馈
两种不同的训练信息产生两种不同的反馈模型：
- Purely Evaluative Feedback
    - 简单的评价型反馈，只是反馈一个值，这个值评价行为的好坏，注意Purely这个修饰，也就是朴素的，简单的评价反馈是只返回一个值，而复杂的评价反馈可能结合别的信息。
- Purely Instructive Feedback
    - 与评价反馈不同，指导型反馈，直接返回正确的做法，而且是当action完成的一瞬间就能反馈这个信息，当然这个也是purely的版本，不包含复杂的附加信息。

**指导型反馈是监督学习的基础**,以上两种反馈的区别为：
1. 评价型反馈完全取决于行为（action）
2. 指导型反馈独立于行为（action）

当然这两个反馈也不是水火不容，只要你愿意，他们还是可以结合在一起使用的；1中评价性反馈与行为相关可能很好理解，2中的指导型反馈独立于行为可能不太好理解，我们可以这么理解，如果我们输入的信息是N个类别的数据，那么反馈信息就是当前这条数据的正确分类，而这个分类就是独立于算法做出行为的独立反馈。

## 本章重点
本章我们主要研究评价型在简化的强化学习上的应用，简化到什么程度？只有一个situation，已经有很多人研究过使用评价型反馈解决这些问题，这种简化可以避免让我们一开始就陷入复杂关系的问题中，而无法看到强化学习的细节，而且这种简化的模型可以让我们清楚的看到evaluative feedback和instruct feedback的不同，以及帮助我们发现如何将他们联合起来的方法。
这种特殊的，无关联的评价性反馈问题，可以有很多具体例子，在本章中，我们用简化的 **多臂赌博机(k-armed bandit)** 作为研究对象。通过这个问题介绍一些简单的方法。这些方法在后续章节中将会被扩展为能解决 **完整强化学习问题** 的方法。
本章最后我们会简单的了解一下**完整的强化学习问题** 以及多臂赌博机之间相互影响的时候的问题 —— 也就是多situation的情况。
![](./octopus.jpg)
## 总结
可能你对单situation和多situation还不太能区分，或者你可能连多臂赌博机是什么都不知道，但是没关系，我们后面会用一章的时间研究这个赌博机。
感谢您的阅读，请多提宝贵意见


## References
1. Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.




--------------------------
原文地址:[ https://face2ai.com/RL-RSAB-2-0-Multi-Armed-Bandits](https://face2ai.com/RL-RSAB-2-0-Multi-Armed-Bandits)
