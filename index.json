[{"content":"Abstract: 一封来自读者的信 Keywords: 计算机视觉，就业，人工智能，深度学习\n收到一位读者的来信，字里行间都看到了当年的自己，中间有一些问题觉得很多人可能都有，所以在他同意的情况下，我把邮件部分贴出来，希望能帮助更多的人。 原文：\n 同学，您好： 很高兴收到的微信咨询，本来想在微信上回复你，但是我觉得还是好好思考一下你的问题然后把这些问题大概总结一下，回复给你可能更准确也更有帮助，另外我觉得有你这些问题的人应该不少，所以，如果你同意我会在博客上公开这封回信和我们的部分聊天记录，当然我会隐藏掉所有您的个人信息。 看我的博客的很多人都是在校的学生，包括一些本科的同学以及硕士博士研究生，其中我感觉硕士居多，很不幸，我个人是没有读过硕士的，所以对大部分硕士课程以及硕士工作没有什么了解，当然我更不能深入了解每个课题组的深入程度。 我收到的询问一般都是一些入门级的同学发给我的，这很正常，入门之前会恐慌迷茫不知所措，所以需要询问；一旦入门了，需要的工作就是调整，深入了，即使是讨论，也不会找我这种江湖野怪，所以我无法了解到那些人，所以本文不适合那些已经入门的人。 下面来回答你的问题：\n  你首先介绍了一下你目前的情况，985，研一，老师是做机器视觉，机械部分，视觉机器人，困惑是自己不知道选什么方向 实验室别人是做什么的，有人做深度学习，有人做后端，有人做视觉检测 你的问题是你要不要跟着师兄们的脚步还是跟着老师的脚步做机械部分 就业形势，深度学习以后的发展，自己非科班，转机器视觉是因为机械行业不景气。 下面是我个人的对你目前状况的理解，和一些个人的建议，仅供你参考。对于任何问题每个人都有不同看法，有人务实可能选择走保守道路，有人理想，选择走一些理想化的路线，我肯定也会有自己的态度，而对你来说，我的态度对你来说就是一种噪音，或者更准确说，就是误导，所以，你可以多听几个人的建议，综合来看。你听邻居大婶的意见，还不如自己做决定。 985，这个数字是在校学生表示自己身份的象征，在学校其间，能证明我们能力的只有学校好坏，而到了社会能证明我们身份的就是变成，车，房子，衣着等，其实我不认为985就有多好，我也不会看不起三本的学生，因为有些985的学生最后干的事，如果从社会的角度还不如我们村种菜的大爷对社会的贡献大。我们的社会文化导致了我们从出生开始就要被比较那些能看到的东西，身高，长相，小学考第几，初中考第几，上的什么高中什么大学，这些都是挂在表面的东西，而这个现象的结果是导致我们自己也开始只追求外在的东西，所以对于985，我给你的建议是，如果你内心足够强大，可以完全不顾及外在的东西，深入修炼自己的内心（包括对事物的理解，基础知识，比如数学，语言，编程这些基础的知识技术，过程很苦，看不到什么提高，但是我认为是非常有用的）。老师做的自己不喜欢，或者就业形势不好：我知道很多人不知道自己喜欢什么，或者自己喜欢的东西不能被用来谋生（唱歌跳舞什么的），首先我觉得这个问题我之前也遇到过，最求喜欢的还是追求活下来，如果你的爱好不能够支持你的生活，我建议把80%的精力放在你的谋生技能上，也就是说你的谋生技能要在行业平均水平之上，至于做什么，你可以自己找一个自己相关的，又不太讨厌的做，而你真正的爱好，只有你自己知道的那个，你可以用业余的时间来继续，提高水平同时能够体会到快乐。如果你既不知道自己喜欢什么，也不知道做什么事会开心，我觉得你应该继续找到这件事，如果找不到，这会是个悲剧。自己选方向：工作方向，与目前已经掌握的技能相关，且自己能够掌控的，行业我会在后面说。 实验室里的人都在做别的，自己要不要学：不能别人干什么你就觉得那个好，从众可以用来保护自己，但是不能作为自己奋斗目标，每个人的性格和技能基础都不同，所以他们做的事可能不一定适合你，像1中说的，你可以80%的精力用于你以后的事业，20%用于爱好。IT行业的细分很多，也很细，所以找一个自己不太讨厌的其实不难，不要担心这个行业怎么样，关键是你的目标是学一点然后找个公司混日子，还是说把这个干好，如果你能把某项技能掌握的很好的话，找工作谋生一般没问题，当然技术最好是主流或者有一定前景的，传呼机维修这种技术就是一个反面典型（来自郭德纲的段子）。 这和2中的回答一样，不要只看试验是的几个人是做什么的，多在网上看看最新的东西，如果可以，多去Youtube上逛逛是好的，你们村的集市的视野是不如世博会的，找几个自己喜欢的，好好调查一下。导师和师兄在很多人看来就是权威了，当然，实验室环境是，但是这些权威其实没有你看得那么权威，我们从小就喜欢跟随权威，比如有人也把我当成权威，说实话，这些权威包括我就是普通人，而权威的见解就是他们的个人见解，有些很片面，有些甚至就是不对的，所以你不需要100%的跟随他们。 就业形势不好，因为经济不好，宏观经济以及行业环境都不太好，所以我觉得你要做的更应该是提高自己的技能，求值就是个求期望的过程，要为了得到大的期望，你能做的就是提高自己的概率，也就是让自己在专业方面变得更强大一些，而不是找一个行业招人多的，后者想法会让人一直堕落下去。非科班的问题是基础不好，所以你要做得的是补充自己的基础，深度学习（调参）以后没什么发展，这是我说的，我不是权威，但是研究其工作原理会有前途，但是我们目前的应用主要工作是调参，没人会花钱请你去研究模型的数学原理（Google这些公司除外，我说的是中国公司）人工智能未来会是个非常强的方向，但是不一定是深度学习。计算机视觉，机械这些行业都是强应用行业，机械我不懂，但是计算机视觉我觉得应该是好方向，但是目前国内的水平一般，因为计算机视觉的一个主要特征是替代人类，完成重复工作，而我国目前最不缺的就是人，所以计算机视觉的最近几年的招聘情况我不知道，如果计算机视觉的需求量大的话，这是个非常不错的行业，我理解机械也是个非常有前景的行业，只是热度目前不如人工智能类的这些工作。 希望能帮助你，注意，我的这些都是给你提供信息，而不是强烈建议或者指导，没有一个人的思想能指导别人的行为，只是提供信息，希望你多思考，多浏览， Tony Nov,24,2018  收到的回信原文：\n 谭老师，您好 多谢老师抽出时间为我指点迷津，正像老师所说的，我应该首先对自己有个清晰的认识，然后再选择自己的发展方向。 现在的社会是资产财富论能力，然而大学入学时大家都差不多，不同的专业进入社会开始拉开差距，生化环材也正是因为这个原因被称为劝退专业，机械则是屌丝专业，个人觉得这些专业的学生并不比CS EE 差，但是赚不到钱的就得在这个社会的下游，作为机械专业的学生，是不甘心吧，当然也是当初自找的，生化环材机友们和所有人一样爱财，也希望自己和家人以后能过得好点，我想我会转互联网/IT吧，如老师所说，大多数人不知道自己喜欢什么，我也是，就专业方面来说，机器人和应用开发 人工智能都挺有好感，但谈不上热衷，大概不是真的喜欢吧，作为生活的一部分，我想我会转互联网，作为爱好追求，我还应该多去了解自己，谢谢老师提点，我以为选择了一个方向就会是生活的全部了，所以害怕决定。 把这个问题公开我觉得挺好的，希望所有有想法的人勇敢一点。\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-a-mail.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 一封来自读者的信\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 计算机视觉，就业，人工智能，深度学习\u003c/p\u003e","title":"一封来自读者的信"},{"content":"Abstract: 本文介绍第一种强化学习方法——行为评价方法(Action-value Methods)，非常简单但是可以通过这个简单的算法来感受下强化学习的难点和问题解决的思路 Keywords: 强化学习, k臂赌博机, 多臂赌博机, 利用, 探索, 行为评价方法，样本均值方法, $\\varepsilon$-greedy方法\n行为评价方法(Action-value Methods) 本文介绍解决k-臂赌博机的第一种简单的naive的方法，注意区分两个重要的概念，评价方法(value function)产生的值(value)和奖励信号(reward signal)之间的区别。\n评价方法和奖励信号的回顾 注意我们前面介绍的两个概念，相似但是完全不同，就是评价方法(value function)产生的对行为的评价，以及行为执行后的奖励信号(reward signal)。\n这里再举个小栗子🌰吧，加入你是个超级富豪，你去街边游戏厅玩一块钱一次的赌博机，当然游戏厅有一排赌博机，假设有 $k$ 个，你随身都带着助理，你的助理是哈佛耶鲁MIT三大名校的统计学博士，你俩去玩赌博机，玩之前，你的助理会告诉你你下一局应该把你的一块钱下注到哪台机器，你肯定要问他他是怎么算出来的，于是，他拿出笔记本电脑，连接到了超级服务器上，用一个超级复杂的公式，评估出了所有赌博机的输赢概率，最后得出，第二台赌博机，赢的概率最大，为90(这个数是博士设计出来的公式的结果)比别的赌博机的对应值都高，所以你心悦诚服的去把你的一块钱压到了二号赌博机。 这个例子，你的博士助理就是一个value function或者说他的那套算法是value function也可以，那个90就是\u0008某个行为(选择的某台机器的value值)。 于是你就去赌博了，堵你的一块钱，经过赌博机的一些列运行，果不其然，你赢了两块钱，你很高兴的买了一个冰棍，\u0008你和你的博士朋友一起吃了起来 —— 这里面的两块钱就是reward signal。 可见value是和reward signal \u0008完全不同但又息息相关的概念\n上面我们应该大概能区分action的value和reward signal了。 我们继续回顾我们上文（点击查看详情）讲到的这个公式： $$ q_{\\ast}(a)\\doteq\\mathbb{E}[R_t|A_t=a] $$\n公式中 $q_{\\ast}(a)$ 是一个函数，表示reward signal的期望，公式中包含了 $R_t$ 为对应的第 $t$ 个过程中的 Reward Signal ，而接着就用 $$ Q_t(a)\\approx q_\\ast(a) $$ 来定义了个value function：换句话说，我们刚弄了个每一步的reward signal的期望，就被人顺水推舟做了value function，因为value function 总是要和reward signal相关的value变大的时候reward signal一定也要对应的变大或者变小也可以，只要他们之间的变化时按照固定规律进行的，我们就能通过value\u0008 function来最大化reward signal。 还有一个区别value 和reward signal\u0008的办法是value时选择action之前用的，reward是\u0008action之后(甚至是多个action\u0008s之后)得到的，\u0008来自environment的反馈。\n一定要注意的就是value function和reward signal之间的关联和相互利用，不要糊涂了，我们的最终目的是最大化reward signal\n样本均值(sample-average method)方法 上面已经用reward signal的期望来做value function了，那么我们第一种方法也就这么来了，我们计算 $\\text{action}_a$ 在过去所有步骤中出现的时候得到的reward signal平均值，作为$\\text{action}_a$ 的value，这就产生了下面这个value function:\n$$ Q_t(a)\\doteq\\frac{\\text{a出现的时候reward signal的和}}{\\text{a出现的次数}}=\\frac{\\sum^{t-1}{i=1}R_i\\cdot1{A_i=a}}{\\sum^{t-1}{i=1}1{A_i=a}} $$ 这里面唯一不好理解的就是符号 $1_{A_i=a}$ 这里的1不是一个数值，你可以把它理解为一个布尔变量，他不是值，他是个指示变量，当第 $t$ 步骤中 $A_t$ 采用的是a的话，那么这个值$1_{A_t=a}=1$ 否则 $1_{A_t=a}=0$ \u0008 $R_i$ 就是对应的第 $i$ \u0008步获得的reward signal。所以这个奖励信号就和一个指示函数相乘\u0008了，最后求出来当a被使用的时候的reward\u0008 signal的和了，分母是同样的原理，是一个计数器。 如果你学过数字信号或者模拟信号，你会了解有一个叫做使能信号的东西，$1_{A_t=a}$ 与之类似。 但是这里有除法就要注意如果我们之前\u0008一直没有采用过$\\text{action}a$ 那么分母就是0了，所以我们可以硬性规定一下，如果分母为0，那么 $Q_t(a)=0$ 根据大数定理，当我们的action使用的足够多的时候，样本的期望就是随机变量的期望，对大数定理不太了解的同学可以参考:大数定理也就是说，当我们步骤足够多的话，action也都被大量使用时，$Q_t(a)$ 收敛于 $q\\ast(a)$ 。 以上只是一种非常\u0008简单的value\u0008 function的设计，有效与否要等到后面的试验，但是从理论上来说，他是可以满足我们的需求的。 那么上面我们就完成value\u0008 function的设计，接着我们要考虑的就是如何选择 action 了，是 exploitation 和 exploration 呢？\n$\\varepsilon$-greedy方法 第一种，最传统的办法，选择期望最高的，也就是贪心的选择，对应的 action 就属于 exploitation，使用数学表达这个过程就是 在$t$ 步选择$A^{\\ast}{t}$使得 $Q{t}(A^{\\ast}_{t})=\\text{max}_aQ_t(a)$ 贪心选择的整体被写作：\n$$ A_t\\doteq \\mathop{\\arg\\max}{a} Q_t(a) $$ 上面这个符号 $\\mathop{\\arg\\max}{a}$ 会在你的人工智能历程中一直跟你纠缠不休，用中文解释就是在所有可行的 $a$ 中，找到一个能使 $Q_t(a)$ 最大的，并返回这个 $a$ ，也就是$A_t$ 每一步都是使用时 $Q_t(a)$ 最大的 $a$。\n贪心方法最大的问题就是，他每次都使用前面所有行为中看起来最大收益的行为，但是他无法保证这个行为确实是最好的，比如，有一种行为更好，但是前面的几次都发挥失常，没有得到较好的reward signal，这种情况是完全有可能的，而贪心算法解决不了这个问题。\n一种改进就是以一定概率 $\\varepsilon$ (相对较小)执行随机选择的行为，选择范围是所有可行的行为，且他们被选择的概率相等,这种随机选择的action就是我们前面提到的exploration，这种方法叫做$\\varepsilon$-greedy方法。 这种方法的优点是随着学习的不断增长，所有的行为的被执行次数都是趋近于无穷的，那么这时候可以保证 $Q_t(a)$ 收敛于 $q_\\ast(a)$。因为我们执行exploitation的概率是 $1-\\varepsilon$ 其数值是接近1的，而exploration的概率$\\varepsilon$ 是接近0的，当执行了非常多的过程后$Q_t(a)$ 收敛于 $q_\\ast(a)$，此时我们有$1-\\varepsilon$ 的比例是执行的贪心过程，那么我们的所有步骤中就有不小于 $1-\\varepsilon$ 的比例是选择的最大收益的行为，这种结果将会是相当客观的。 上面这套理论从字面上看起来近乎完美，但是这个收敛是个渐进的保证，换句话说学习次数在现实中不可能趋近于无限，我们的训练次数也是有限的，我们目前还不能根据一个理论上的极限情况来断定在实际执行过程中也获得很好的表现。\n总结 本文介绍了value function的设计，以及两种方法来选择action，我们从此篇开始，正式进入强化学习的大门。\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.   原文地址: https://face2ai.com/RL-RSAB-2-2-Action-value-Methods\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-2-2-action-value-methods.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍第一种强化学习方法——行为评价方法(Action-value Methods)，非常简单但是可以通过这个简单的算法来感受下强化学习的难点和问题解决的思路\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 强化学习, k臂赌博机, 多臂赌博机, 利用, 探索, 行为评价方法，样本均值方法, $\\varepsilon$-greedy方法\u003c/p\u003e","title":"【强化学习】2.2 行为评价方法(Action-value Methods)"},{"content":"Abstract: 本文介绍Valine评论系统的自定义 Keywords: Hexo，Next，Valine评论系统，Valine邮件\nHexo下next主题valine强化版本的改造 使用Hexo下Next主题会遇到评论设置上的麻烦，好用的被墙了，剩下的都不太好用。但是Next集成了一个valine评论很有改造空间。 我们这里只提供一个改造思路，具体的执行细节我会给出参考网址。\n使用valine 按照next或者官方说明，安装是不需要安装什么的，只是要设置下leancloud上的数据段，如果你的文章浏览数据是用leancloud存储的，那么这个过程你应该很了解了，具体的过程可以参考： \u00081. 官网：https://valine.js.org 2. hexo-theme-next上的issues:https://github.com/iissnan/hexo-theme-next/pull/1983\n第一个Bug 这是基本的，如果你的网页做过大型优化，比如参考过： https://reuixiy.github.io/优化过next主题的同学会在topx中出现Bug,页面会变成只有title和阅读次数的状态，解决办法是修改themes/next/layout/_third-party/comments/valine.swig中的第一行为:\nif theme.valine.enable and theme.valine.appid and theme.valine.appkey and page.title !=== \u0026#39;阅读排行\u0026#39; 阅读排行是你topx的page的title根据你的命名适当修改。\n增强Valine Valine的评论系统轻量级，所以功能就那么完善，比如邮件通知，你都找不到评论在哪篇文章，所以我找到了一个增强版的Valine：\n 赵俊同学的杰作http://www.zhaojun.im/hexo-valine-modify/  赵同学给出了一个后台的强力解决方案，让我们的邮件通知不那么简陋，也能找到评论再哪篇文章了，为了稳妥起见，一下内容为赵俊同学的博客原文摘录：\n Hexo 优化 --- 支持邮件通知的评论 Valine 增强版 发表于 2018-01-11 |  更新于 2018-10-06 |  分类于 Hexo简介此项目是一个对 Valine 评论系统的拓展应用，可增强 Valine 的邮件通知功能。基于 Leancloud 的云引擎与云函数。可以提供邮件 通知站长 和 @ 通知 的功能，而且还支持自定义邮件通知模板。\n点击查看演示\n注：本项目修改于 panjunwen 的项目 : Valine-Admin，原作者博客: Valine Admin 配置手册, (部分逻辑于功能不同，还请读者不要搞混配置项.)\n快速开始首先需要确保 Valine 的基础功能是正常的，参考 Valine Docs。\n然后进入 Leancloud 对应的 Valine 应用中。\n点击 云引擎 -\u0026gt; 设置 填写代码库并保存：https://github.com/zhaojun1998/Valine-Admin\n\n切换到部署标签页，分支使用 master，点击部署即可：\n\n\n配置项此外，你需要设置云引擎的环境变量以提供必要的信息，点击云引擎的设置页，设置如下信息：\n\n必选参数\nSITE_NAME : 网站名称。SITE_URL : 网站地址, 最后不要加 / 。SMTP_USER : SMTP 服务用户名，一般为邮箱地址。SMTP_PASS : SMTP 密码，一般为授权码，而不是邮箱的登陆密码，请自行查询对应邮件服务商的获取方式SMTP_SERVICE : 邮件服务提供商，支持 QQ、163、126、Gmail、\"Yahoo\"、...... ，全部支持请参考 : Nodemailer Supported services。SENDER_NAME : 寄件人名称。高级配置自定义邮件模板\n自定义收件邮箱\n自定义邮件服务器\nWeb 评论管理\nLeancloud 休眠策略(必看)\n更新历史7.7 兼容 valine v1.2.0-beta 版本对 at 的更改 点击查看。7.1 修复 Web 后台登录安全 bug6.14 添加自定义邮件服务器功能. 点击查看升级 FAQ部署最新代码 :\n\n重启容器:\n\n注: 更新新版本与更改环境变量均需要重启容器后生效。\nLeanCloud 休眠策略免费版的 LeanCloud 容器，是有强制性休眠策略的，不能 24 小时运行：\n每天必须休眠 6 个小时30 分钟内没有外部请求，则休眠。休眠后如果有新的外部请求实例则马上启动（但激活时此次发送邮件会失败）。分析了一下上方的策略，如果不想付费的话，最佳使用方案就设置定时器，每天 7 - 23 点每 20 分钟访问一次，这样可以保持每天的绝大多数时间邮件服务是正常的。\n附 Linux crontab 定时器代码：\n复制1\n*/20 7-23 * * * curl https://你配置的域名前缀.leanapp.cn\n注 : 此 crontab 不是LeanCloud 后台的定时任务，如果你没有 Linux 机器来配置此定时器，那么可以在此 issues 中回复我，我帮你加上。\n如对本项目有意见或建议，欢迎去 Github 提 issues。\n# Hexo优化 # Valine # 评论系统 怎样才算一个好的技术博客？Hexo 优化 --- lazyload 图片懒加载   希望赵同学的博客不会关闭github也不会删库，这样的结果就是我们的邮件通知会美观很多，而且还附加了评论所在地址，非常方便。 如果使用上述后台设置那么在主题下的配置文件，valine选项中的邮件通知要关掉，不然会收到两份通知：\nvaline: enable: true appid: xxxxxx appkey: xxxxxxx notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: true # Verification code placeholder: 无需注册，填写正确的邮箱，评论被回复就有邮件通知了~ # comment box placeholder avatar: retro # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size visitor: false  原文地址: https://face2ai.com/other-Hexo-next-valine-leancloud\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-hexo-next-valine-leancloud.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍Valine评论系统的自定义\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Hexo，Next，Valine评论系统，Valine邮件\u003c/p\u003e","title":"【Hexo】Hexo下next主题valine强化版本的改造"},{"content":"Abstract: 在强化学习中，平衡Exploitation和Exploration将会是一个从始至终的问题，我们本章用简单的k臂赌博机问题来从具体的每一步来分析和研究这个问题，本节先介绍下问题的描述和大概的解决思路，为本章后面的问题解决做好铺垫 Keywords: 强化学习，k臂赌博机，多臂赌博机，利用，探索，Exploitation，Exploration\nk臂赌博机（k-armed bandits）问题 赌博机，说到赌博，大家都觉得这是一个非常不好的活动，但是说回来，赌博是催生数理统计和概率的主要动力，具体可以看未完成的系列：数理统计学简史 作为不赌博的好孩子，大部分人对k臂赌博机可能不是很了解，首先我们来介绍一下这种赌博机： 这就是为什么是k臂赌博机，那个臂就是旁边的控制器，通过拉动控制器就能出发机器开关，当出现指定的图案就会有相应的奖励，这是1臂赌博机，如果有多个这种机器放在一排，那么这就是k臂赌博机。\nk臂赌博机问题描述 下面我们来从学术的角度描述一下这个问题，描述如下： 我们面对的选择包含 $k$ 个选项，或者 $k$ 种可选的行为，每一个选择或者行为都对应了一个奖励信号(rewarding signal，忘记了回到前面看看) 每种选择对应的奖励信号是随机的，但是都来自固定的分布，当然来自不同选择的奖励信号服从的分布都不同，但是不会随时间改变，比如，$k$ 种可选的行为中 $f_1$ 表示第一种行为的奖励信号的随机分布，其可以不同于 $f_2$ 也就是第二种行为的奖励信号的随机分布，但是为了简化问题，我们目前研究的问题中 $f_1,f_2,\\dots,f_k$ 都不随时间变化。 我们的目的是通过\u0008每一步观察，选择，执行不同行为来最大化我们的\u0008奖励信号，当然这个过程需要长时间，或者是多步以后来观察结果，一次或者两次的\u0008观察是没有研究意义的。\nk臂赌博机 经过上面的描述，我们就可以把描述和赌博机联系在一起了，首先我们对赌博机进行如下假设：\n 赌博机出现的结果都是满足某种随机分布的，当然这个可能是赌场老板设定的，也可以是自然产生的 赌场里面有很多赌博机可以供我们选择，如果赌场就一个赌博机，我们就要换一家研究k-\u0008armed bandits problem了 赌博机的\u0008内的设置不会随时间改变，也就是1中的分布不随时间改变，没有幕后黑手操控比赛  有上面三点假设，我们就可以解释为什么我们本章研究的问题的是k臂赌博机了，我们面对 $k$ 个赌博机，我们的目的是最大化我们的收益，所以我们的做法是选择一个赌博机，然后下注（假定从始至终都不变）启动所有机器，获得结果，观察其他机器的行为，决定下一局是否换别的机器下注，对应上面的问题：\n 奖励信号 对应 单次赌博收益 可选行为 对应 本次使用哪台机器 每个行为对应的奖励信号的随机分布 对应 每台赌博机出现不同结果的随机分布  所以这就是我们上面描述的问题的生活中的例子，或者说我们可以通过生活中这个例子来得到问题。\n上面的例子是赌博机的例子，下面还有一个类似的类比，就是医生\u0008看病的例子，医生每天要面对一些列的病人，每个病人用什么样的治疗方案就是一个选择的过程，而每种选择都对应着不同效果，而治疗效果就是奖励信号，当医生面对络绎不绝的病人时，医生的目标就是把奖励信号最大化，也就是最大程度的让更多人康复，这个类比也符合上面我们的问题描述\n 奖励信号 对应 病人康复程度 可选行为 对应 可选的治疗方案 每个行为对应的奖励信号的随机分布 对应 每种治疗方案对当前患者的效果的随机分布  数学描述 把上面的语言描述转换成数学描述就是如下了： 当前为第 $t$ 次选择(对应赌博中的第 $t$ 局，医生的第 $t$ 个患者), 有 $k$ 中选择，我们在此次选择的行为是\u0008： $A_t$ 对应获得的奖励信号是 $R_t$ ，那么对于这一轮选择，假设我们选择了 $a$ 我们获得奖励信号的期望 $q_*$ 就是： $$ q_{\\ast}(a)\\doteq\\mathbb{E}[R_t|A_t=a] $$\n如果你对 $A_t$ 和 $a$ 搞不清楚，我可以大概说一下，$A_t$ 是一个总称，本轮的所有选择的总称， 而 $a$ 是特定的一个行为，所以期望的公式就可以解释的清楚了，因为不同行为对应不同的分布，而我们希望使用期望来衡量这个行为的奖励信号。 如果我们明确知道每一步(局)每个行为(\u0008机器)将会出什么结果，那么我们就不需要选择\u0008了，直接选最大的那个就好了，所以我们这里假定我们不知道，也许你大概知道期望，但是对结果还是无法确定的（你可以一直观察某个赌博机的结果，利用大数定理通过采样结果来估计原始分布的结果） 这里我们对\u0008 $t$ 步的特定行为 $a$ 的评价(前面说的value function中value，和rewarding signal直接相关)的期望进行定义： $$ Q_t(a)\\approx q_*(a) $$\n这样就可以利用我们上面对问题的分析，以及使用前面提到的value function来解决这个问题了\n强化学习解决k臂赌博机问题 上面我们应该已经能从整体上掌握k臂赌博机的问题过程了，那么我们接下来就要用我们前面提到的一些概念来解决这个问题了。 如果从我们自身出发，我们希望每一步都能最大化我们的收益（或者叫做奖励信号），我们自身会对所有赌博机都有一个评估，无论是感性的还是理性的，我们都会认为某个或者某几个赌博机获得高回报的可能性大一些，那么我们就有很多种玩法了：\n 贪婪的我 —— 每次都玩那个我认为回报高的赌博机 任性的我 —— 每次随便玩，就是不选我认为回报高的 会强化学习的我 —— 每贪婪若干次后任性一次（玩自认为回报高的机器几次后，随机玩一次别的机器，看看是否会改变自己前面的观点）  前面我们反复说过两个单词（第一次考托福的时候我还用这两个单词写过作文😜）\u0026ldquo;exploitation\u0026rdquo; or \u0026ldquo;exploration\u0026rdquo; ，上面1中的贪婪也被称为 \u0026ldquo;greedy actions\u0026rdquo; 当你选择这种action的时候，你的action对应的就是\u0026quot;exploitation\u0026quot;；相反，如果我们就是不选我们认为回报高的，也就是2的这种行为，我们称为 \u0026ldquo;exploration\u0026rdquo;，如果我们还是想赢点钱，这种行为也不是完全傻瓜的，因为我们可以通过这种行为来纠正我们对每台机器回报高低的期望（有可能你对机器回报高低的判断是错误的，实际上也是这样的），换句话说，每台机器回报高低我们根本就是乱猜，所以1中的贪婪也有可能执迷不悟，而通过偶尔的\u0026quot;2\u0026quot;一下，没准会得到更多的收获，也就是3中给出的做法，会有更多收获。 Exploitation的做法肯定是正确的，但是从长期来看Exploration可能会产生更高的收益(短期来看exploration的收益大概率不如exploitation)，因为exploration很有可能找到比当前执行的action收益更高的其他action。但是具体怎么安排或者说怎么平衡Exploitation和exploration就是我们今后要一直研究的问题了。我们经常会用 \u0026ldquo;conflict\u0026rdquo; 来形容Exploitation和Exploration之间的选择。 是Exploitation还是Exploration这个问题理论上是没有通用解的，每一个环境，每一个问题，每一步都是不一样的，也无法确定，对于k臂赌博机问题，目前有一些专用的数学公式可以比较好的平衡Exploitation和Exploration之间的关系，但是这些公式或多或少都对问题的某些方面进行了限制和假设，这就使得这些方法在相似问题的不同环境下可能会失效。在后面章节我们研究的全面的强化学习(Fall Reinforcement Learning Problem, k臂赌博机 问题是简化后的问题)的时候，对应的环境会变得不同，一些假设条件也会不成了，这时候这些方法也会失效。这时候(某些假设不成立的时候)算法对应的收敛性和收敛边界都会失效。 我们目前的研究不关心是否以一种高效，漂亮的（sophisticated）方法来平衡Exploitation和Exploration，而是只要能平衡就行，不管漂不漂亮，高不高效，本章我们就列举几个简单的方法来平衡他们来获得比只 exploitation更好的结果。\n总结 在强化学习中，平衡Exploitation和Exploration将会是一个从始至终的问题，我们本章用简单的k臂赌博机问题来从具体的每一步来分析和研究这个问题，从而获得更直观，更详细的理解。\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.   原文地址:https://face2ai.com/RL-RSAB-2-1-A-k-armed-Bandit-Problem\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-2-1-a-k-armed-bandit-problem.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 在强化学习中，平衡Exploitation和Exploration将会是一个从始至终的问题，我们本章用简单的k臂赌博机问题来从具体的每一步来分析和研究这个问题，本节先介绍下问题的描述和大概的解决思路，为本章后面的问题解决做好铺垫\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 强化学习，k臂赌博机，多臂赌博机，利用，探索，Exploitation，Exploration\u003c/p\u003e","title":"【强化学习】2.1 k臂赌博机(k-armed bandits)问题"},{"content":"Abstract: 本文是第二章“多臂赌博机”的绪论，介绍本章主要内容 Keywords: 强化学习，多臂赌博机\n多臂赌博机 强化学习与其他\u0008学习算法最大的不同在于训练信息，我们熟知的监督学习，无论从简单的线性回归，到复杂的深度学习，所有这些监督学习用到的训练信息都是Instructing（指导，讲授）的，也就是说训练信息中包含明确的行为指导，比如对于一张输入图片判断是否有人脸，标记好的训练数据会明确的对结果进行校正——是否有人脸，如果有人脸在哪，训练模型偏差会被准确计算，同时通过优化算法逐步减少这个偏差，直到我们设定的阈值后完成训练。 而强化学习的训练信息则不同，强化学习的每一步没有指导信息，而是只有一个“评价”（evaluate），评价这个行为(action)的得分，得分也就是好坏，但是没有正确错误的说法，也没有最好或者最坏的说法。 这种评价\u0008机制导致了强化学习需要在学习的过程中加入探索(exploration)，来用\u0008trial-and-error的搜索方式得到好的模型。\n“指导”型反馈和“评价”型反馈 两种不同的训练信息产生两种不同的反馈模型：\n Purely Evaluative Feedback  简单的评价型反馈，只是反馈一个值，这个值评价行为的好坏，注意Purely这个修饰，也就是朴素的，简单的\u0008评价反馈是只返回一个值，而复杂的评价反馈可能结合别的信息。   Purely Instructive Feedback  \u0008与评价反馈不同，指导型反馈，直接返回正确的做法，而且是当action\u0008完成的一瞬间就能反馈这个信息，当然这个也是purely的版本，不包含复杂的附加信息。    指导型反馈是监督学习的基础,以上两种反馈的区别为：\n 评价型反馈完全取决于行为（action） 指导型反馈独立于行为（action）  当然这两个反馈也不是水火不容，只要你愿意，他们还是可以结合在一起使用的；1中评价性反馈与行为相关可能很好理解，2中的指导型反馈独立于行为可能不太好理解，我们可以这么理解，如果我们输入的信息是N个类别的数据，那么反馈信息就是当前这条数据的正确分类，而这个分类就是独立于算法做出行为的独立反馈。\n本章重点 本章我们主要研究评价型在简化的强化学习上的应用，简化到什么程度？只有一个situation，\u0008已经有很多人研究过使用评价型反馈解决这些问题，这种简化可以避免让我们一开始就陷入复杂关系的问题中，而无法看到强化学习的细节，而且这种简化的模型可以让我们清楚的看到evaluative feedback和instruct feedback的不同，以及帮助我们发现如何将他们联合起来的方法。 这种特殊的，无关联的评价性反馈问题，可以有很多具体例子，在本章中，我们用简化的 多臂赌博机(k-armed bandit) 作为研究对象。通过这个问题介绍一些简单的方法。这些方法在后续章节中将会被扩展为能解决 完整强化学习问题 的方法。 本章最后我们会简单的了解一下完整的强化学习问题 以及多臂赌博机之间相互影响的时候的问题 —— 也就是多situation的情况。 总结 可能你对单situation和多situation还不太\u0008能区分，或者你可能连多臂赌博机是什么都不知道，但是没关系，我们后面会用一章的时间研究这个赌博机。 感谢您的阅读，请多提宝贵意见\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.   原文地址:https://face2ai.com/RL-RSAB-2-0-Multi-Armed-Bandits\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-2-0-multi-armed-bandits.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是第二章“多臂赌博机”的绪论，介绍本章主要内容\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 强化学习，多臂赌博机\u003c/p\u003e","title":"【强化学习】2.0 多臂赌博机"},{"content":"Abstract: 强化学习第一章小结 Keywords: 强化学习历史，强化学习总结\n本章总结、强化学习历史简述 总结 强化学习就是一种通过计算方式来理解和进行目标导向学习的方法。其直接表现是通过agent和环境的交互进行学习，而不需要对环境做任何限制或者需要复杂的环境模型，从本书作者来看，强化学习是第一个研究agent在和环境交互的时候产生的问题计算化的领域，通过研究和环境的交互，达到长期的目标。 强化学习有一个非常明显的框架，就是agent和环境之间的action、state和reward之间的相互关系。这个框架尝试着从一种简单的方式来反应人工智能问题的基本特点，而这些特点包括：“诱因”（cause） 和 “结果”（effect），“不确定”（uncertainty）和 “非决定论”（nondeterminism） 以及 “清晰目标的存在性”（existence of explicit goal）。\n强化学习历史 强化学习的历史不是很久远，但是由于研究的方向很多，所以没办法把每条只限都列举出来，这里我们主要分成三个方向：\n 研究 “trial” 和 “error”  起源于早期对动物学习的研究 早期人工智能的主要方向 1980s强化学习复苏的主要动力   优化控制  使用 value function 求解 使用 dynamic programming 求解   1和2的混合  1和2看起来相互独立，而且独立程度很高，但是我们前面说到的井字棋中使用到了“时序差分方法”（temporal-difference method）    相关论文见引用1中的1.7节\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.   原文地址:https://face2ai.com/RL-RSAB-1-6-Summary-History\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-6-summary-history.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 强化学习第一章小结\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 强化学习历史，强化学习总结\u003c/p\u003e","title":"【强化学习】1.6 本章总结、强化学习历史简述"},{"content":"Abstract: 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。 Keywords: 强化学习，强化学习举例，Tic-Tac-Toe\n强化学习的一个扩展举例 今天我们来讲一个很有趣的例子，英文名字叫\u0026quot;Tic-Tac-Toe\u0026quot; 中文名字有翻译成“井字棋”或者什么的，我们这里为了方便就称之为“井字棋”，叫井字棋的原因是因为其棋盘是个“井”字形的，玩法简单，但是这个玩的过程可以使用强化学习的方法来学习，这个简单的棋可以让我们从各个细节审视强化学习的特点，有点，缺点，以及一些小技巧。\n\u0026ldquo;Tic-Tac-Toe\u0026quot;简介 规则描述 Tic-Tac-Toe的规则描述如下：\n 使用 $3\\times 3$ 的正方形棋盘，双方各使用 \u0026lsquo;x\u0026rsquo;和\u0026rsquo;o\u0026rsquo; 作为棋子，轮流下棋， 谁的棋子最先连成一行或者一列，或者对角线，那么获胜，棋局结束 对方完成这个过程，则失败 如果在最终双方都没能连成一行或者一列，或者对角线的情况下，棋局结束，则为平局。  下图来自Wikipedia:\u0026ldquo;井字棋\u0026rdquo;\n上面的棋局，使用 \u0026lsquo;x\u0026rsquo; 的一方率先完成连成一行的准则，故执\u0026rsquo;x\u0026rsquo;一方获胜。\n简单规则下的问题 下面这个视频是我在Google上找到的一个小程序录的一段视频。\nyour browser does not support the video tag  可见，在高级的情况下，双方（我和AI）基本都没法获胜，也就是平局会大量出现，原因是，我们对这种棋的技巧掌握的都很熟练，无法在现行环境（规则）下战胜对方，通过这个观察我们也能发现，在规则相对简单的游戏（“博弈”）中，平局会大量出现。 那么问题就要来了，我们 - 也就是人，在这种简单的规则下，多以平局收场，那么这怎么训练agent呢？如果每局训练总是平局，agent就不知道往什么方向走了。这里我们就要做出一些修改，我们让与我们agent下棋的人或者另一个agent不那么高级，换句话说，我们在简单规则下，降低规则执行者的能力，进而模拟出更高级的博弈（所谓更高级的博弈，无非是我们能力不足才会觉得当前环境，或者规则很困难）。 在后面的训练里，agent会将平局和失败都当做失败来处理，agent的目标明确，就是胜利。\n非强化学习型的解决方法 这个棋局太简单，但是在如此简单的规则下，传统方法（非学习方法）都有诸多问题：\n 使用传统的“极大极小化”（minimax）方法，这个方法会假定对方按照某个既定方案下棋，而事实是对方可能无既定方案，或者既定方案我们永远无法得知。所以这个传统的博弈思想在此不适用。而且“极大极小化”（minimax）方法有个非常大的缺点：如果其认定当前状态必然失败的情况下，即使后面对手出现重大失误（可以逆转取胜的机会），其也不会采取任何致胜招数，而是按照既定失败的套路继续下去。 对于这种连续决策型问题的传统的优化方法，例如动态规划(dynamic programming)，可以计算出在对阵任何对手时候的最优解，但是前提是：对手的全部信息要作为输入提前提交给算法，其中包括在某特定情况（棋局）下，对手下一步棋下在哪的概率。如果我们提前不知道这些信息（大部分情况下，这个信息无法获得） 对于2中的动态规划方法，有另一种改进版就是在训练时和对手多次交手，从而记录学习出对手的信息，或者叫做给对手建模（model）然后再使用动态规划来寻找答案。  上面3中方法中，1和2对解决问题基本没有帮助，3有帮助。3和我们后面会介绍的很多强化学习方法有着非常相似的思路。\n进化方法(Evolutionary Method)学习 \u0026ldquo;Tic-Tac-Toe\u0026rdquo; 进化方法 中讲解了进化方法的缺点，就是没有使用agent和环境之间的交互信息来改变agent。而这里如果把进化方法直接使用到“井字棋”中，其表策略（policy）的特点是：直接search全部可能的位置，找到获胜概率最大的位置，然后下棋。也就是，策略要考虑到当前盘面（ $3\\times 3$ 的棋盘上 x和o的分布）并给出获胜概率最大的下棋位置。而这个概率的获得就需要和对手多次下棋，记录，学习出来的。有了这些信息，agent就知道接下来一步或者接下来很多步应该怎么走。 一个典型的进化方法就是在“策略空间”的 hill-climb ，这种方法的策略是逐渐找出能提高表现的策略（并不是一下找到最优的方法，而是像爬山一样，每一步都找到一个能提高agent表现的方案，一步一步的向上爬）。 遗传方法就更加直接暴力了，直接直接评估大量的策略(policies)，去除不合格的，留下好的，然后产生新一代的，以此迭代，直到问题解决。 解决“井字棋”问题，理论上存在很多种不同的优化方法。\n评价函数(Value Function)学习 \u0026ldquo;Tic-Tac-Toe\u0026rdquo; 上面我们说了进化方法在井字棋中使用，下面我们就要看看另一个方向的方法 —— 评价函数(value Function)的方法了。\n设计评价函数 我们列一个表，这个表中的每个格子对应一种状态（state），整张表就对应井字棋的全部情况，这个表的每一项都对应一个概率值，这个概率值表示当前状态下最后获胜的期望，注意两点，一个是当前的状态，第二是最终获胜的期望。这个期望，我们就把其当做评价函数的结果，value —— value值。这个表就是我们的评价函数(Value Function)了. 在井字棋中，这个表就包含下一步棋应该怎么走的评估。通过当前状态，我们可以缩小下一步可能出现的状态范围，然后比较所有可能的状态，如果A状态比B状态有更高的获胜期望，那么我们认为A状态比B状态好，所以我们倾向于把棋子走到A状态显示的位置。 对于这个状态表，假设我们执x，那么如果某状态中包含x在一行或者一列或者对角线，那么这个状态的获胜期望是1，相反，如果o在一行或者一列或者对角线，那么这个状态的获胜期望是0；同样，如果对应状态是棋盘下满，而没有获胜方，这时候期望同样是0 。除了上述这些情况，其他状态的初始化值都是0.5，即有一半的可能性会获胜。\n执行(exploitation)和探索(exploration) 当我们有了这张表（上面的评价函数）我们就有了制胜法宝，但是具体执行也是有不同方案的，我们可以查找最大value的状态，然后执行贪心(greedily)选择，这是使得获胜期望最大的策略，也就是完全执行表（value function）中的指示，这个被称为exploitation。 但是我们有时候（偶尔，occasionally）在某些步骤中不选择最大value的状态，而是选择随机状态，因为这种方式可能带我们到一个前所未见的state下。\n上面两段描述的评价函数，以及状态表在井字棋中可以表现为下图：\n学习评价函数（value function） 在下棋的过程中，我们不断修改已有value function对于井字棋，也就是上面我们提到的那张表，我们的目标是试图使其更加准确，为了达到这个目的，我们增加了一步“返回”（back up） 过程（上图中的红色箭头），这个过程在每一步贪心选择后执行，这一步执行的实质是通过当前状态值（value）来适当修改上一步状态的value，使它更接近现在状态的值（评价函数的结果，value），比如图中的红箭头就是让e的值（评价函数的结果，value）更接近g的值（评价函数的结果，value）。 上面的数学表达为，让 $s$ 为贪心选择之前的状态的值（评价函数的结果，value）， $s\u0026rsquo;$ 为贪心选择后的值（评价函数的结果，value），然后我们的back up更新 $s$ 值，方式是： $$ V(s)\\leftarrow V(s)+\\alpha[V(s\u0026rsquo;)-V(s)] $$\n其中 $\\alpha$ 是一个小的正小数，叫做“步长”（step-size）参数，这个参数影响学习速率（the rate of learning）注意这里rate是速率的意思，而不是比率的意思，所以学习率这种翻译，我觉得欠妥。 这种基于 $[V(s\u0026rsquo;)-V(s)]$ 的变化方式（update rule）被称为“时序差分学习”（temporal-difference learning），字面解释：基于两个不同时间点的值（评价函数的结果，value）的差值的方法，这个解释基本能反应这类方法的特点。 这类方法是我们后面要重点学习。\n上述方法对于这个任务可以非常出色的得出不错的结果，例如，在步长参数（step-size）被精确递减后，这个算法对于固定的对手是收敛的，每一步都能给出胜算最高的走法。也就是说，这个方法对于一个固定的对手给出了最优策略。这里的一个关键就是 步长参数（step-size） 的调整，如果这个参数不调整，那么最后的结果也会收敛，但是速度会慢很多。\n“进化方法”与“评价函数”的区别 上面这些细节也佐证了我们前面提到的：“进化方法”和“评价函数学习法”的区别：\n 进化方法的为了学习一个策略，其根本原则是策略不变（不是永久不变，是在目前的短时间内），而去和对手进行多场游戏（就是和环境的交互，interaction），或者使用一个对手的模型，来模拟进行多场游戏。在进行多次游戏后，胜利的次数给出当前策略胜率的无偏估计，然后这个无偏估计被用来优化策略（根据一定的规则从多个策略中淘汰某些，或者其他办法生成新的策略）。 但是问题是每次策略进化都需要多场游戏来计算概率，而每场游戏内的信息被忽略掉了（因为计算概率只关心结果的 —— 胜利出现的次数）而且当一个player（也可以成为agent）获得了胜利，他本场比赛的所有行为都会被认为是正确的，且每一步给予相同的得分，但实际上并不是这样，首先并不是每一步的重要性都一样，其次是并不是每一步都是正确的选择。 对比之下“评价函数学习法”就有不同的表现了，每一步棋和环境的交互信息都会被利用来学习。  总之，进化方法和“评价函数学习法”都是在搜索policy的空间，但是“评价函数学习法”搜索起来更高效，因为他利用了更多的有效信息。\n\u0026ldquo;Tic-Tac-Toe\u0026rdquo; 中的强化学习 这个小小的游戏基本展现了强化学习的所有特性：\n 首先强调和环境的交互，这里就是和对手下棋。 \u0008目标明确，正确的行为需要 Planning 或者 Foresight 来处理延迟出现的结果 另一个显著的特征是，RL形成有效的 Planing 以及 lookahead \u0008，而且这个过程不需要使用对手模型（model of opponent），也不会对后面全部的可选操作序列进行搜索（减少policy的搜索空间）  虽然RL的这几个有点很是吸引人，但是这不能掩盖其某些缺点：\n 训练的时的对手，不可能是人，而还是程序，所以这个对手不是Nature的 学习过程也是打碎成不同的步骤（对于井字棋每一局都是一步），而不是连续的进行，只有下完了才会产生reward信号，而不是连续的。 同样对于某些连续的任务，我们也要把它拆成离散形式。  井字棋的搜索范围很小，现在alpha go所面对搜索空间比井字棋大到不知道哪去了~，1992年的时候就有学者研究了\u0008比井字棋更复杂的\u0008游戏，空间大概规模是 $10^{20}$ 他们使用了神经网络作为模型，结合上面的方法得出了很好的结果，具体我们在后面16章学习。 强化学习面对如此巨大的policy空间的行为，主要取决于他对之前学习的信息的理解程度，理解的越好，其行为就更加可靠，反之则不可靠。\n先验知识(Prior Knowledge)与模型(Model) 在井字棋游戏中，RL的agent只知道游戏规则毫无游戏经验或者先验知识，先验知识是锦上添花的元素，也就是，有或者没有这个要靠缘分，就算没有我们也要解决问题，而如果有，那么我们可以利用先验知识节省大量时间，或者大幅度提高结果。RL处理先验知识有多种方式，并且对学习结果。 我们目前处理的情况都是当前环境对agent有明确的反馈，且state明确，在有些问题中state可能是隐藏的或者有大量重复的state这种情况过于复杂，不在我们初级范围内。 Model同样是强化学习中的一个组成要素：当我们的RL（agent）\u0008学习应该怎么样应对不同的状况的时候，他需要思考的就是环境会怎么对他的action进行反应，有些问题确实如此，环境对action的反应就是一个模型，但是有的问题可能比这要更复杂一些，他们有的时候什么模型都没有，不知道环境会做出什么样的反应，面对这样的问题RL也有解决方案。同样的，有模型可以帮助RL更快的学习。 但是我们的井字棋就没有这个model，原因是对手怎么下棋是不可能有模型的，这就是个 Model-Free system。存在精确模型的系统由于模型被精确的使用，所以\u0008做起来相对简单，但是有的时候建立模型的过程会成为这个问题的瓶颈，本书中我们会讨论一些Model-Free的问题，同时组合这些问题成为更复杂的系统。\n总结 本文通过研究Tic-Tac-Toe这个小游戏，从实际应用的角度分析了RL的各个方面，基本涵盖了大部分主要内容，后面我们进入第二章，开始分析具体算法，欢迎大家关注。\nReferences Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.\n 原文地址:https://face2ai.com/RL-RSAB-1-5-An-Extended-Example\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-5-an-extended-example.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 强化学习，强化学习举例，Tic-Tac-Toe\u003c/p\u003e","title":"【强化学习】 1.5 强化学习的一个扩展举例"},{"content":"**Abstract:**本文介绍强化学习和优化方法之间的关系，他们之间一些共同误区以及强化学习的工程性质 **Keywords:**强化学习，优化方法，强化学习工程化\n强化学习与优化方法 优化方法是机器学习的一个关键组成部分，大部分监督学习和部分非监督学习都要依靠优化方法找到模型的参数，强化学习中也会使用优化方法中的一些算法来使agent获得更多的reward signal，而强化学习或者优化方法在进行过程中有些说法会让人感到迷惑。\n“强化学习的目标”与“优化问题的目标” 前面我们提到过，强化学习的目标或者叫做战略目标就是让agent获得更多的reward signal，注意“更多”这个说法，从数学的角度讲，更多是个增长的过程，而这个过程有没有极限，也就是会不会有“最多”这种情况，这也成了强化学习的一个极限。\n同样的问题也出现在优化问题中，优化问题的解是否还可以进一步优化，或者，这个解已经是全局最优的了，这个判断其实是非常困难的。\n所以，我们在描述强化学习或者优化问题的时候，提到“最大化xx的鼓励信号(reward signal)\u0026hellip;.”或者 “最优化xx函数” 这些都不是强调一定要找到最大的唯一解，事实上，这个解可能本身就不存在，或者即使存在，由于种种原因，我们也没办法找到这个解。 强化学习要做的就是不断的让agent去尝试找到比目前更好的 策略(policies) 来得到更多的鼓励信号(reward signal) 如果用一句话概括就是：\n \u0026ldquo;Optimization is not the same as optimality\u0026rdquo;（优化不等于最优）\n 如果某个优化方法在别的系统中表现非常优秀，但是当他移植到其他系统中仍然需要仔细评估，和测试，因为系统的 “不可预测性” 使得算法的工作 “环境非常复杂”，所以不能依靠算法之前在别的系统的表现来随意的评估其在当前系统中的表现。agent通常能够发现预料之外的 策略(policies) 来获得意想不到的好结果。如果把这个过程对比自然界中的生物的行为的话，那么就应该是我们所说的“创造力”了。这种创造力表现在，“ 变化（variation） ”和“ 选择(selection) ”上，这是进化能解决物种所面对的生存挑战的基础，也是强化学习解决人工智能所面对的挑战的基础。但是创造是有风险的，因为有的时候我们并不知道“变化（variation）”是朝向好的方向还是不好的方向或者根本就是无意识无目标的变化。\n优化，变化等以上观点一直以来都被思考，就是哪个才是最好的，如果改变agent目前的策略效果会变好还是变坏是不确定的，所谓的继续优化是 “优化” 还是 “恶化”？一个很有意思的比喻是；\n \u0026ldquo;Be careful what you wish for because you just might get it\u0026rdquo;(小心你想得到的，有可能你已经得到了)\n 强化学习和工程 为了解决上面这个“世纪难题”（最优性，不可预测性，变化，等问题），有很多策略（注意这是抽象的宏观的方案，而不是具体实施的方案）被使用，比如：\n 限制agent的可变化方向 使目标函数(objective function)对风险敏感  但是这些方案都是比较片面的解决方法，不能从根本解决问题，标准工程化的要求是，在系统正式上线工作之前，要经过严格的测试，因为系统一旦上线，产品一旦销售，我们的技术必须对使用者负责，如果系统存在不可预测的风险，且风险对于使用者是高危的，不可抗拒的，那么我们必须在测试阶段发现，并且解决，这和其他工程师一样的。由于强化学习等机器学习算法都存在不可预知的行为，所以在上线之前更应该严格评估。\n优化结果的随机性不可预测性只是强化学习工程化中的风险点之一，其他工程中存在风险，在强化学习中同样也大大小小的存在，所以在评估时，不能只关注强化学习的风险点，比如数据安全，网络安全，等等在其他工程中的风险，在强化学习都一并存在。 讨论详细的问题在我们这里很不现实，我们只要记住，严格的测试系统，强化学习系统，机器学习系统，提前发现风险，解决风险，降低风险这些事情无论何时都是重中之重。\n总结 本文介绍了强化学习和优化方法之间的一些概念混乱，优化不一定找到最优解。以及强化学习的工程化问题，降低风险，解决风险是所有机器学习工程应用中的重点。\nReferences Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.\n原文地址：https://face2ai.com/RL-RSAB-1-4-1-Connection-to-Optimization-Method\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-4-1-connection-to-optimization-method.zh/","summary":"\u003cp\u003e**Abstract:**本文介绍强化学习和优化方法之间的关系，他们之间一些共同误区以及强化学习的工程性质\n**Keywords:**强化学习，优化方法，强化学习工程化\u003c/p\u003e","title":"【强化学习】 1.4.1 强化学习与优化方法"},{"content":"Abstract: 本文是本系列的第一篇，介绍本系列的主要内容 Keywords: Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法\n信息论、推理与学习算法介绍 这个系列是信息论相关内容的介绍，信息论是什么可能有些做机器学习或者AI的同学们不太了解，而做通信的同学应该是非常清楚的，如何准确的定义信息论是什么，不在我的能力范围内，但是我们平时接触到的图像，或者简单点说灰度图像，一个8bit的像素点能有多少阶灰度，为什么有256，而不是258或者其他的，这个其实就属于信息论的知识范围，而信息论和机器学习有关系么？答案是肯定的，凡是处理信息，传递信息的过程，都多多少少跟信息论有那么点关系。 本系列主要面对的读者是： 从事机器学习，人工智能类内容研究的同学，工程师，或者爱好者 需要的背景知识：工程专业，科学类专业，或者数学类本科1，2年级的数学知识，包括，微积分，概率论，线性代数的基本知识（本站已经完成这些基础知识的全部博客，可以随时查阅）。 本书封面： 机器学习，信息论 传统的信息论课程不仅包括Shannon的信息化思想，也有实际解决问题的现实应用，我们这个系列更加进一步的包括了：\n Bayesian Data Modelling Monte Carlo Methods Variational Methods Clustering ALgorithms Neural Networks  为什么要把信息论和机器学习弄到一起？ 信息论和机器学习是一个硬币的两面！ 60年代一个领域 —— 控制理论（cybernetics）在信息论，计算机科学，和神经科学等学科中非常火爆，这些科学家们都在研究一个相同的问题，那时候信息论和机器学习还是属于同一类。大脑是一个压缩信息，进行沟通的系统，而在数据压缩（data conpression）和纠错码上（error-correcting code）表现最好的（state-of-the-art）的算法上使用的工具，在机器学习中也会使用。 这些种种迹象都表明，机器学习和信息论有着密切的关联，而我们本系列更关注的就是信息论在机器学习方面的应用，或者帮助我们理解一些算法的特点和局限。\n学习地图 本书的目录如下,当然这些课不是我们所有要学的，我画了个地图，大概应该是按照这个地图来完成我们的博客的：\nPreface 1 Introduction to Information Theory 2 Probability, Entropy, and Inference 3 More about Inference\nI Data Compression 4 The Source Coding Theorem 5 Symbol Codes 6 Stream Codes 7 Codes for Integers\nII Noisy-Channel Coding 8 Dependent Random Variables 9 Communication over a Noisy Channel 10 The Noisy-Channel Coding Theorem 11 Error-Correcting Codes and Real Channels\nIII Further Topics in Information Theory 12 Hash Codes: Codes for E\u000ecient Information Retrieval 13 Binary Codes 14 Very Good Linear Codes Exist 15 Further Exercises on Information Theory 16 Message Passing 17 Communication over Constrained Noiseless Channels 18 Crosswords and Codebreaking 19 Why have Sex? Information Acquisition and Evolution\nIV Probabilities and Inference 20 An Example Inference Task: Clustering 21 Exact Inference by Complete Enumeration 22 Maximum Likelihood and Clustering 23 Useful Probability Distributions 24 Exact Marginalization 25 Exact Marginalization in Trellises 26 Exact Marginalization in Graphs 27 Laplace\u0026rsquo;s Method 28 Model Comparison and Occam\u0026rsquo;s Razor 29 Monte Carlo Methods 30 E\u000ecient Monte Carlo Methods 31 Ising Models 32 Exact Monte Carlo Sampling 33 Variational Methods 34 Independent Component Analysis and Latent Variable Modelling 35 Random Inference Topics 36 Decision Theory 37 Bayesian Inference and Sampling Theory\nV Neural networks 38 Introduction to Neural Networks 39 The Single Neuron as a Classi\u000cer 40 Capacity of a Single Neuron 41 Learning as Inference 42 Hop\u000celd Networks 43 Boltzmann Machines 44 Supervised Learning in Multilayer Networks 45 Gaussian Processes 46 Deconvolution\nVI Sparse Graph Codes 47 Low-Density Parity-Check Codes 48 Convolutional Codes and Turbo Codes 49 Repeat{Accumulate Codes 50 Digital Fountain Codes\n地图 根据我们的目标和书上给出的建议，我们要学习下面这些章节，箭头之间表示先后关系，箭头指向的课程需要在前面的课程完成后才能进行： github: https://github.com/Tony-Tan/MachineLearningMath 上有高清大图\n总结 本文是信息论的第一课，后续就围绕上图展开，对于基础不是了解的同学可以去看前面的其他博客，谢谢支持。\n","permalink":"https://go.face2ai.com/math/itila-introduction-to-this-series/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是本系列的第一篇，介绍本系列的主要内容\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Information Theory，信息论，Inference，推理，Learning Algorithms，学习算法\u003c/p\u003e","title":"【信息论、推理与学习算法】本系列博客介绍"},{"content":"Abstract: 本文介绍强化学习中的一些局限（limitation）和机遇（scope），介绍进化方法和决策梯度方法的区别和优劣 Keywords: Evolutionary Method，进化方法，Policy Gradient Methods，决策梯度方法\n“进化方法” 和 “决策梯度方法” 概论 进化方法是我在学习“强化学习”这本书之前认为的在人工智能中必然要有的一个部分，但是本书给了我一盆冷水，本书作者认为进化算法对强化学习的作用不太明显，或者说缺点更多，不适合用作强化学习的方法。 但是我认为AI如果能达成，一定是模拟人或者动物的智慧形成过程的，即使进化方法不是学习技能（learn skills in individual lifetime）的主要方法，但是其对智慧的长期形成一定有非常重要影响，不能因为进化方法不适合强化学习的某些任务就彻底否定他，相反我们要注意他们的结合。 本书在讲述强化学习的过程中主要是围绕 Estimating Value Function展开的，但是Estimating Value Function在强化学习中不是必须的，Estimating Value Function前一篇介绍过https://face2ai.com/RL-RSAB-1-3-Elements-of-RL/。\n进化方法(Evolution Method) 在wiki上有进化的详细解释，如果有对进化算法不了解的同学可以大概看看或者自己google 。如果用概括性的语言描述一下，大概是这个样子的：\n 进化方法，或者遗传算法是模拟生物繁殖时候基因交换，基因变异等过程来对某个Agent进行优化的方法\n 举个例子：我们有一个任务M，我们模拟用10000个随机生成的Agent去完成任务，这些agnet是第一代，其中只有100个完成了；那么我们重新组合这100个的“基因”（按照各种设计的方法）产生10000个新的Agent，这些Agent就是第二代，去完成任务M，有200个成功了，继续进行第三代，第四代，直到找到一组或者多组稳定的“基因”能成功完成任务，这个过程就是进化方法，其主要过程在于组合基因和淘汰不满足对象。 类似的进化过程，进化算法，以及模拟退火等优化方法都是类似的套路。\n这类方法是没有 Estimating Value Function 的\n这就是本文的一个关键结论。\n通过我们上面例子的描述可以发现，我们的进化算法只有“生”和“死”，比如产生新的agent是生，淘汰agent则是死，agent的全部能力全部来自遗传，这从人类进化的角度来看是不对，我们并不是靠生孩子进化到如此的，每个agent在有生之年不多研究进去，我们的人类才进化到现在的文明，作者也是这个观点，而且其对agent的有生之年的学习过程（agent自我完善policy的过程，人和动物完善自己的skill的过程）更加关注。 但是进化方法也是有些作用的，在某些强化学习问题中，进化方法也是可以使用的，一般这种问题有以下特点：\n Policy的搜索空间很小 Policy的搜索空间是结构化的，比较易于搜索 时间充足，比如这个训练程序可以跑一个世纪  还有一种特殊的情况，就是在agent不能准确感知环境(Environment)的状态(State)的时候，进化方法很有用处. 这个也可以跟生物进化联系起来，在最初的地球，没有生物，有机物出现，第一个有机体出现，他们有任何感知能力，只能通过遗传，变异这种方式获得适应环境的生物，在变异出器官组织后，才能对环境进行感知互动，进化自己的policy\n本书（本系列博客）主要重点在于agent在和环境interact的时候的“学习”，而进化方法就没这个过程，并且在interaction的过程中的细节，是agent进化的一个重要信息来源，这些细节信息使用起来会使得policy 的搜索更加高效， 所以我们可以总结一下进化方法在强化学习过程中的几个缺点：\n 忽视太多强化学习问题背后的结构信息 舍弃了搜索policy过程中其实是一个函数，来对应state和action（进化算法没有这个） 忽略了agent的lifetime的policy选择和他所处的环境和状态等信息。  虽然agent当前的state可能是错误的，就是agent可能是判断错误了（偶尔会出现这种问题），但是这并不影响这些信息（state，action等）对学习过程的帮助。\n“进化方法”和“学习过程”（主要指agent自我修正调整policy的过程）在自然过程中有很多相同的特征，但是其目前看来确实不适合强化学习问题，所以本书（本系列博客）中的强化学习方法一般不包括进化方法\n决策梯度方法(Policy Gradient Method) 这里有一个类似进化方法的学习方法，我们会在后面经常用到，这也是一个非常简单的方法，可以解决一些小的问题，叫做 “决策梯度方法”（Policy Gradient Method） 这种方法的特点是agent中的参数是有限的，换句话说，这个空间是离散且有限的，注意离散和不连续是两回事，有限的离散的空间参数个数是固定的，这样的话搜索空间会被缩小到很小，并且存在理论上的最优解，这种方法在有限的参数空间调整自己的policy来获得更好的reward，这个调整过程有点像进化算法的组合那一步，但是不同的是，这里的调整要根据agent和state之间的action来调整，而进化方法的调整方法是不关心这些信息的，所以他们有类似，但是又大不同。 决策梯度方法和其他的强化学习方法没有什么严格的定义区别，所以没有必要过于计较区分算法的归类。\nConclusion 本文介绍了进化方法和强化学习之间的一些关系，以及决策梯度方法之间的一些知识，注意进化方法是没有Estimating Value Function 的，这是问题的关键。\nReferences Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.\n原文地址：https://www.face2ai.com/RL-RSAB-1-4-0-Limitations-and-Scope转载请标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-4-0-limitations-and-scope.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e  本文介绍强化学习中的一些局限（limitation）和机遇（scope），介绍进化方法和决策梯度方法的区别和优劣\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Evolutionary Method，进化方法，Policy Gradient Methods，决策梯度方法\u003c/p\u003e","title":"【强化学习】 1.4.0 “进化方法”和 “决策梯度方法”  概论"},{"content":"Abstract: 本文介绍除了agent和environment以外的，对于强化学习最重要的最基础的四个元素。 Keywords: Policy，策略，Reward Signal，奖励，Value Function，评价函数，Model of Environment，环境模型\n强化学习的基础元素 前面我们说到了几个生活中的例子，和几个RL中基本模型，本文我们继续深入，探索目前强化学习中最基本的组成元素。\n策略(Policy) 策略是我的翻译，我不知道“正确”的翻译是什么，但是我们还是老原则，用英文。书中的定义是\n A policy defines the learning agent\u0026rsquo;s way of behaving at a given time.\n 换成中文，我觉得更接地气一点的就是Policy就是agent的在任意时刻的产生行为的依据，也就是说agent，或者人或者动物或者robot，在某一时刻，有明确的最终目的，但是此时此刻，面对这个时间点的环境的时候，他要产生某种规则的方式，就是policy。如果用数学的的形式就是 $$ f(环境)\\to 行为 $$ 这里面的 $f$ 是个映射，是不是函数不一定，这个映射，综合当前的所有环境信息，产生一个action。最简单的例子，这个policy可以是个映射表，当出现什么环境的时候，就做出什么样的决定，如果你穷举了下棋的所有棋局，那么每种盘面都对应了最佳的走法，我们把这个对应走法记录在一章表上，那么这个表（中的内容）就是policy\n心理学中把这个叫做刺激反应法则（stimulus-response rule）或者叫做 association（翻译不出来） Policy可以是非常简单的规则，比如自动驾驶看到红灯要停车，也可以及其复杂，比如自动驾驶要把在菜市场里的车开出来。 Policy是agent面对环境做出action的核心，但是这个核心有时候也可以是随机的，换句话说，上面那张“表”里面的信息可能是随机——有时候随机并不代表不可靠\n奖励信号(Reward Signal) 激励信号，或者奖励信号，书中的定义：\n A reward signal defines the goal in a reinforcement leaning problem\n 激励信号，就是agent的得分，目前我们的研究的agent在每一步都有来自环境的反馈，由于我们没有所谓的有teacher，所以我们会通过一个我们设计的[reward函数]，来计算出每一步我们得到的评分，而agent存在的目的就是在他的生命周期内最大化这个得分的总和。 reward signal是agent来自每一个action之后环境给与的反馈，得分定义这个action的好坏，以及好坏的程度；类似于生物中，对于刺激的反应，是表示舒服还是难受。 这个goal是立刻的明确的，不能有延迟以及模棱两可的评分，或者说是客观公正的，不能被agent改变的，换句话说，这个goal是公正的，agent不能又当运动员又当裁判。agent只能通过根据环境改变自己的行为来最大化自己的goal，评判有专门的[裁判]（后面会说到）。 agent不断学习新的知识来提高自己的得分，其工作方法是改变自己的policy，所以reward signal 就成了调整policy的主要依据了。 同样这个裁判也有可能是随机，没错，一个疯了的裁判，至于为什么会有这种情况，我也不知道后面会不会涉及。 第三章我们会介绍这个reward 函数对于agent不变是有生物学原理的，原理来自我们的大脑。\n值函数(Value Function) 注意区分，Reward Function 和Value Function是不同的两个函数，作用不同，性质不同。书中的定义：\n Whereas the reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run.\n 用中文说就是，reward是每次的结果，value function是来预测这个agent到最后能得到的总分。value function有一定的预测行为在里面，包括后面可能出现的情况，而这些情况是否发生目前是不确定的，一个类似的例子就是，你在准备高考，参加了两次模拟考试，reward是你这两次模拟考试的好坏，而value函数是要预测你最终高考结果的。 很有可能出现这种情况，前面几个step的reward 都很低，但是value function却依然很好。反之亦然会出现。\n在RL中reward是最主要的，value function次之。如果没了reward那么agent就是无头苍蝇，而且没有reward就没了value的概念。value更像是辅助agent 来得到更多reward的参考书。 然而，agent做决定的时候有时候更多的考虑value function，这个原因是我们的RL最终目标是得到更高的reward总和，而不是某一步的reward，所以这会给我们一个错觉，就是value function比reward要更重要。 作为得到更多reward的参考，value的获得更加复杂，比获得reward要复杂的多，reward依据当前环境给出，而value是依据后面的环境给出，一个是静态的，一个是动态的（环境在变，agent的policy也在变） 在一般的RL算法的重要组成就是要找到有效value预测值，这个工作会是未来几十年的重点研究方向\n环境模型(Model of Environment) 最后一个就是环境模型，环境是个很大的很抽象的概念，环境模型和环境并不是一会儿事，但是关系密切。\n某个state下，agent的某个action可能会获得什么样的反馈，环境模型可以预测这个反馈，而实际的反馈是环境根据state和action给出的，所以环境模型越接近环境，就会越准确，环境模型更多的用来做Planning，就是agent在做action之前“思考，计划” 使用环境模型和planning进行的RL，我们称之为“基于模型的方法”，相反，就是“不基于模型的方法” 不基于环境模型的方法是更直接的trail-and-error 方式，经常被看做是planning方式的对立方法。但是第八章，会介绍同时使用 trail-and-error 和 模型以及planning 的方法。 基于模型的方法会跨越我们RL中各种层级：low-level, trail-and-error, high level以及deliborating planning。\nConclusion 本文介绍了强化学习的四种基本组成元素，不但基础而且非常重要，基本就是后面所有的研究对象了。需要好好研究。\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.  原文来自：https://face2ai.com/RL-RSAB-1-3-Elements-of-RL转载标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-3-elements-of-rl.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍除了agent和environment以外的，对于强化学习最重要的最基础的四个元素。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Policy，策略，Reward Signal，奖励，Value Function，评价函数，Model of Environment，环境模型\u003c/p\u003e","title":"【强化学习】1.3 强化学习的基础元素"},{"content":"Abstract: 本文介绍几个对应于强化学习的生活中的例子，来具体化前面提到的名词和几个重要理论在自然界中的表现。 Keywords: 强化学习，强化学习举例，Agent，Environment，环境，Reaction，反应\n强化学习举例 强化学习例子(Examples) 这几个例子都是实际自然界或者生活的例子，并不是RL的具体应用，所以不要理解错了，我们通过观察自然，观察生物智能的形成和遗传，是我们了解智能的有效方法，个人愚见，没准这也是唯一突破当前理解障碍的方法，生物通过数万年的演化，遗传，庞大的种群保证了其有大量的样本，来完成筛选和淘汰，每个个体的基因，神经系统，数量大到可能无法想象。所以如果连这些都没考虑过，没深入研究过，应该是对问题没有深刻理解的。 陶哲轩说过，如果你对问题的来源内容背景都不是很了解的话就想去解决问题，那么这个非常困难的。 我们来看几个例子：\n 一个专业棋手下棋，当他每下一步的时候，他考虑的都是在计算预测，当他走了某一步以后，可能的结果以及对方会进行的反制措施，或者有时候，凭借直觉立刻来决定这步棋怎么走。 一个自主的控制者，实时调节参数来控制石油精炼加工的工艺，这个控制者可以自主的取平衡 产出-消耗-质量 之间的平衡关系，而不需要完全按照工程师给出的精确结果。 小羚羊，小牛，在刚出生的几分钟就能挣扎的站起来，半小时左右就能以20mile/hour 的速度奔跑 移动的机器人，能决定是否需要进入一个新房间找垃圾还是马上找到路线去充电，他的决定取决于当前的电量，以及找到路线需要花费的能量 Phil准备他的早餐，虽然在我们看来，这个谁都可以，非常平常，但是整个过程，非常严苛，准备早饭的着一系列动作隐藏了一个巨大的复杂的条件网络，目标和子目标网络，比如，我们分析一下：走到厨房，打开柜子，选择原料，拿到原料，打开原料包装，然后把剩余的放回去；接着下一套动作是取碗，勺子，拿牛奶；这些的所有过程包括了眼睛的动作，寻找，定位，协调手完成动作；迅速决定用什么动作，把这些东西以什么样的轨迹放到哪里，并且不要碰撒旁边的其他容器。一个简单的早饭仔细分析竟然如此复杂，每一步都有明确的goal，比如取原料，是为了吃里面的东西，而不是为了打开包装，在吃饭的时候，用勺子吃了第一勺食物，是为了吃下一勺，以及最后从中获得能量。无论Phil是否享受吃饭的过程，如果当前身体的station告诉他很需要能量，需要大概吃多少，以及想吃什么，他都会按照这个指令去做的。  强化学习的特征(Features of Examples and RL) 上面5个都是我们生活中自然界的例子，所有例子经过分析都可以得出以下结论：\n 所有例子都包含interaction（作用，反应） 这些interaction都是在agent和environment之间产生的 agent要做出决定，做什么 environment是agent所处的环境，agent在其中搜寻，并达到自己的目标 environment不管是否已知，agent都要去搜索  Agent的Actions会影响未来Environment的State，以及Agent后面的选择空间，例如：\n 下棋的这一步决定，直接影响棋手下一步的走法 机器人走的下一步，会影响他的电量，和他找到充电站的消耗能量  所有这些action的结果都会在若干步后体现，而不是马上反映出来，所以目前agent能做的就是预测和计划（prediction and planning）\n有效性(Effects of Actions) 上面这些例子，所有action的结果全部无法完全预测，所以agent只能自己随时注意environment的变化，随时做出反应。 比如在Phile做饭的过程中，他要仔细盯着要拿出来多少材料，加多少牛奶，而且不能溢出来。 所有例子中的目标在某种意义上说都是非常明确地，agent通过直观的感受来判断是否向着目标前进。比如：\n 棋手知道什么样算是赢了 石油提炼工知道生产了多少油 机器人知道自己有多少电，还有多久能到充电站 Phil知道自己吃没吃饱  Agent的经验(Experience of Agent) 所有例子，agent都能根据经验提高他们的表现：\n 棋手反复训练能提高技艺 羚羊通过反复的尝试知道怎么能站稳，能奔跑 Phil天天做早餐，所以知道什么样的工序最优  那么agent后面的技能是根据前面的经验，那么刚开始的agent从哪来的经验呢？（Agent初始知识来源）\n 来自类似的任务 通过设计，人工完成 生物进化  所有这些都是agent初始化的内容，但是agent最终表现，都是要靠和environment之间的interaction完成的，这个过程逐渐修正agent的行为，执行在当前环境特异化的操作。\nConclusion 前面的高谈阔论，不过就是做早饭的一个过程，可见，我们获得了如此行动能力，和智慧是多么的复杂和令人惊叹的\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.  原文来自：https://face2ai.com/RL-RSAB-1-2-Examples/转载标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-2-examples.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍几个对应于强化学习的生活中的例子，来具体化前面提到的名词和几个重要理论在自然界中的表现。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 强化学习，强化学习举例，Agent，Environment，环境，Reaction，反应\u003c/p\u003e","title":"【强化学习】1-2 强化学习举例"},{"content":"Abstract: 本文主要介绍强化学习现阶段的情况，以及未来的去向的一种预测。 Keywords: modern Reinforcement Learning，现代强化学习，Psychology，心理学，Neuroscience，神经系统科学，mathematics，数学\n强化学习和人工智能 现代强化学习 Modern Reinforcement Learning 现在的RL可以很有效的跟其他学科结合产生出一些特定领域非常客观的结果，尤其是在工程和自然科学领域。当然RL也可以在金融等方面有所作为，称为一个有力的工具。\nRL在未来一段时间内，会成为AI或者机器学习中的主力，但需要结合一下这些学科的相互扶持，以下列举包括但不限于：\n Statistics(统计) Optimization(优化) other Mathematics(其他数学) Psychology(心理学) Neuroscience(神经系统科学)  说RL是未来的一个倾向是有原因的：\n RL最接近任何动物的学习方式 RL的核心算法有很多都是直接来自biological learning system  而且RL可以和生物研究相配合，生物研究可以提供给RL一些数据，而RL可以创建一些reward system的模型。 Psychology和Neuroscience会在14和15章中介绍，而本书主要部分是介绍RL在工程和AI中的相关内容。\n弱方法和强方法(Weak or Strong Method) 讲一段历史，1960\u0026rsquo;s 那段时间，大家认为general principle是扯淡的，根本没这东西，所以没人研究这类算法，他们认为只要数据足够多，就能产生智慧，当然现在来看，他们的看法局限性很大，但是别忘了，后面二十年的结果会导致我们现在的热门研究也有局限性，所以，不要到处鼓吹某项技术能成为永远，包括RL可能就是未来AI中的一个小角色，或者一个雏形，至于CNN什么的，就是个SVM，玩来玩去，其实也就是在没有理论指导下的随机结果。\ngeneral principle被认为是weak method，而与此相对的是strong method。\nweak方法基于：\n search learning  strong方法基于：\n specific knowledge  总结(Conclusion) 介绍了介绍RL的最后一部分，后面基本就开始深入各个分支进行介绍了。 需要指出的是，RL在AI中作用目前不知道有多重要或多不重要,本书作者说：\n It is not clear how far back the pendulum will swing, but reinforcement learning research is certainly part of the swing back toward simpler and fewer general principles of artificial intelligence.\n 这段不翻译了，对于技术科技类的内容：\u0026ldquo;信\u0026quot;的翻译不能做到 \u0026ldquo;雅达\u0026rdquo;,\u0026ldquo;雅达\u0026quot;的翻译不能做到 \u0026ldquo;信\u0026rdquo;\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.  原文来自：https://face2ai.com/RL-RSAB-1-1-4-Reinforcement-Learning/转载标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-1-4-reinforcement-learning.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍强化学习现阶段的情况，以及未来的去向的一种预测。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e modern Reinforcement Learning，现代强化学习，Psychology，心理学，Neuroscience，神经系统科学，mathematics，数学\u003c/p\u003e","title":"【强化学习】1-1-4 强化学习和人工智能"},{"content":"Abstract: 本文简要介绍强化学习的框架，以及框架中几个概念的基本关系 Keywords: agent，real-time，organism，robot，framwork\nReinforcement Learning Framework 上来就把这篇的核心知识点讲出来吧，对于一个RL任务，其框架从总体上分，包括：\n agent agent\u0026rsquo;s environment  我不知道怎么翻译agent这个词，所以就一直用英文了，代理，或者特工都不太合适，而且我总能想到Agent Hunter。。agent我们已经用了好多次了，到现在都不知道是什么，是算法，还是算法和其他的什么的合集，就像模型一样，可能用了很久都不知道所谓模型，架构到底是什么，而我们在后面会用详细的例子来形容agent是什么。 就像数学分析里面的定义一样，一个限定加命名而已。所以不要过于担心这一点。\nAgent 虽然不知道agent到底是什么，有没有枪什么的，但是我们知道他有以下几个特点：\n explicit goal(明确的目标) sense aspect of their environment(对他们的环境敏感) choose action to influnce their environment(选择action来改变environment)  即使在算法的刚开始，agent没有任何经验，比如对于一个刚学会下棋规则的人来说，他没有任何经验，但是他也要对棋局做出反应，瞎弄都可以，但是你不能楞在那，这是不可以的，agent要对环境做出action，即使是未知环境。\n如果包含planning的过程，agent不能一直planning，要平衡planning和real-time之间的关系，还有环境模型如何生成和提升等（这几句话如果不懂，不用急，因为这个是更复杂的RL，后面回头看会好一些）\n如果RL包含监督学习的部分，agent还有个任务就是判断哪个监督学习模型的能力强，哪个弱（这个同样是复杂版本的RL，也需要后面的知识来融汇贯通）\n还是继续说agent，agent不是我们想象中organism或者robot，就是agent并不是一个完整的有智慧的个体，或者一个器官，agent更像是一个复杂系统中的一个组成部分，比如对于一个完整robot系统，其中一个agent就像是电池系统，负责管理充电程度的，这个agent不和机器人的外部环境直接interact，而是和更大的系统（包含他的那个机器人系统）直接interact。这时候这个agent的environment就是robot所处的大environment，以及robot内部出了自己以外的其他部分。\nConclusion 本文有点小凌乱，介绍了RL的框架，是Agent和他的environment，以及agent的几个小特点，以及environment是什么，总体来说比较抽象，我也不知道为啥这本书开头就给出了这么多没解释的东西，但是可能作者的风格就是让你先猜一下，后面再公布答案。\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.  原文来自：https://face2ai.com/RL-RSAB-1-1-3-Reinforcement-Learning/转载标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-1-3-reinforcement-learning.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文简要介绍强化学习的框架，以及框架中几个概念的基本关系\n\u003cstrong\u003eKeywords:\u003c/strong\u003e agent，real-time，organism，robot，framwork\u003c/p\u003e","title":"【强化学习】1-1-3 强化学习基本框架"},{"content":"Abstract: 本文介绍强化学习中最重要的一个挑战—— “探索”(Exploration)还是“ 利用”(Exploitation) Keywords: Trade-Off，Exploration，Exploitation，Goal-Direct，平衡，探索，利用，目标导向\nTrade-off between Exploitation and Exploration(利用和探索之间的平衡) 在强化学习中会遇到一个伴随一生的问题，这个问题其实也出现在我们的生活中，也会遇到这种问题，当你遇到一个问题，一个你以前已经遇到过的问题，你有两种选择，第一种，按照以前的方法（其中之一）来完成这件事（Exploitation）；或者，你可以尝试另一种方法，一种全新的方法（Exploration）；前者可以获得稳定的效果，但是不一定是最优的，后者可能会得到更优的方法，但是也可能得到一个不如以前方法的效果。\n同样的情况在强化学习中会一直伴随我们，两种action，选择其中一个是困难的。在下棋的过程中，针对当前的environment，我们的agent以前有类似的经历，是按照过去的经验完成，还是创新一下，采用一种以前没有经验的方法，这个问题dilemma的，而且这两种方法都没有办法保证自己不会失效（fail） 对于一个随机性的任务，更是要经过无数的尝试，才能得到一个稳定的期望，所以那个🐶经过了这么久才能在围棋这种困难的项目上打败人类，而更早的深蓝只能在较简单的项目上打败人类（没错，是什么棋我忘了）。这里所谓的随机性的任务，通俗理解，可以想象成打麻将😆 对于Exploration 和 Exploitation之间的平衡在第二章中详细分析，这个问题经过了几十年大量数学研究，似乎还是没研究明白。 我们只需要简单的记住，我们要平衡他们就可以了。\n监督学习，非监督学习则没有这个问题，所以RL跟他们没有附属关系。\nGoal-Direct \u0026amp; Uncertain Environment（目标导向和未知环境） 强化学习的另一个重要的特征就是要面向目标就是goal-direct，这也是强化学习最吸引人的特征。在一个未知的environment里agent要做出他认为正确的action，并且对interaction做出后续的反应。\n这与其他那些妖艳的机器学习算法不同，他们“聪明的”分解问题，通过近似的解决子问题了来近似的解决的整个问题。比如监督学习中，很多算法是对最后其自身解决问题的能力是没有评估的。另外的一些方法过于沉迷于找到问题的最优解，而忘记时间的重要，走一步棋要一年，那么我敢保证，机器完胜人类，毕竟能下两盘的人不多。还有就是预测模型从哪来（人为设计了结构的CNN，还是怎么生成一个任意结构的模型）。虽然这些算法在不同问题上都有不错的表现，但是过于注意独立的子问题，是他们自身的一个Limitation。\nConclusion 今天介绍了两个强化学习的重要特点，EE的平衡，GD目标导向的思想，都决定了强化学习会在未来的研究上更近似智能。\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.  原文来自：https://face2ai.com/RL-RSAB-1-1-2-Reinforcement-Learning/转载标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-1-2-reinforcement-learning.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍强化学习中最重要的一个挑战—— “探索”(Exploration)还是“ 利用”(Exploitation)\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Trade-Off，Exploration，Exploitation，Goal-Direct，平衡，探索，利用，目标导向\u003c/p\u003e","title":"【强化学习】1-1-2 “探索”(Exploration)还是“ 利用”(Exploitation)都要“面向目标”(Goal-Direct)"},{"content":"Abstract: 本文主要介绍强化学习，监督学习，非监督学习之间的不同。 Keywords: Supervised Learning，Unsupervised Learning，Reinforcement Learning\nMachine Learning（机器学习） 上文我们曾提到强化学习是机器学习的一种，而机器学习的定义是什么我也不记得了，而可以肯定的是下面这三类算法或者三个learning都属于机器学习，机器学习是个更大的概念：\n 监督学习 非监督学习 强化学习  监督学习是最常见，也是当前比较火爆的领域，你要是不懂个CNN，神经网络的都不好意思是说自己是做研发的，这些算法都是监督学习。 非监督学习更注重通过算法来找到一些为标记的数据的背后的关系，比如常见的聚类算法。\n强化学习，最形象的过程就是学下棋，目标就是赢棋，至于如何走每一步，这就是算法要解决的问题了，不管你怎么折腾，目标明确，就是要赢棋，尽量不输。\n关于机器学习的相关知识强烈推荐，参考文献2的这本书，很详细和严谨。后面如果有时间，我也会接着写这本书的博客。\n下面我们介绍下RL对于监督学习和非监督学习的主要区别。\nReinforcement Learning v.s. Supervised Learning（强化学习和监督学习） 首先我们要介绍一个概念，knowledge，这个单词是中学学的，表示知识，对于这两类算法，可以理解为在建立模型之前已知的所有条件，这些条件包括问题类型，已知的对于此类问题有效的方法，已知数据等等，所有我们知道的，与之相关的都是knowledge，而在这些knowledge中，RL和SL(监督学习的简称)的一个最最最显著的不同就是数据，SL的每一条数据都有明确的label，也就是模型应该对这条数据的反应，而RL没有。这就产生了巨大的差别，RL每次对input的action是不知道对错的，也不会产生什么loss或者残差，而监督学习可以，而且可以用数字精确衡量，而RL最多也就是自己估计一个好坏程度，摸着石头过河。\n没错，监督学习，是有家教的好孩子，每一步都有指导方针，每天都有纠错，改错，进步，没什么随机性。而RL就是个野生的，一学期没人管，期末考试没考好，回家被揍一顿，类似于这种效果。\n你可能说，监督学习里也有随机啊，随机梯度下降，放心，那是因为愚蠢的人类目前没有找到直接一步到位的优化方法，而这种方法应该是存在的，随机过程只是优化方法无奈的一种选择，而监督学习的每一步都是有准确衡量的，错了多少，对了多少都是明确的。这就是和RL的最大区别之一。\n监督学习通过extrapolate，generalize，最后得到一个尽可能高准确率的分类结果（或者叫做response） 监督学习没有环境这种说法，所以其学习的就是不是环境和acition之间的interaction。\n如果你一定要说interaction是损失函数算出来的数值，这里我觉得也可以，但是似乎有些怪异，比如一个baby走路，爸爸妈妈的教法应该是，站起来，别摔倒，能走多远走多远，而如果要使用损失函数，就有点类似于告诉这个小baby，你每一步必须走21.286cm，多了或少了都要有“损失”哦，损失函数更像机器，RL更像人 。 RL的另一个特点是，他学习的最终目的是对所有situation都有正确并且及时的反应，不能对于一个situation没有反应或者自己错乱了，这都是不允许的。 RL从他之前的experience中产生action这个也是SL没有的，因为SL没有经验，所有信息都在模型本身里面，没有什么记忆可以谈。\nReinforcement Learning v.s. Unsupervised Learning （强化学习和非监督学习） 有人说RL不是监督学习，那就非监督学习喽，其实他们也有很大的不同，所以非监督学习并不是监督学习在机器学习领域内的补集。\n非监督学习的主要目的，就是找到无任何标记的数据的背后的隐含的关系。所以没有对错输赢这种书法，更像是听天由命。\n为出现的结构数据对于RL是有用的，但是更多的数据并不能解决RL的问题，但是更多的数据对于SL或者UL往往能产生质的飞越。\nRL不是监督学习，也不是非监督学习，RL的目标很单纯:\n MAXIMUM REWARDING SINGAL\n Conclusion 本文主要介绍强化学习和监督学习，非监督学习的区别，并说明，机器学习不是简单的分成监督非监督学习两种。\nReferences  Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011. Nasrabadi N M. Pattern recognition and machine learning[J]. Journal of electronic imaging, 2007, 16(4): 049901.  原文来自：https://face2ai.com/RL-RSAB-1-1-1-Reinforcement-Learning转载标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-1-1-reinforcement-learning.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍强化学习，监督学习，非监督学习之间的不同。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Supervised Learning，Unsupervised Learning，Reinforcement Learning\u003c/p\u003e","title":"【强化学习】1.1.1 强化学习、监督学习和非监督学习"},{"content":"Abstract: 本文介绍Reinforcement Learning的具体特点和与其他机器学习算法不同之处，本文是一个骨架性的文章，所有专有名词都保持英文原始单词，具体内容会在后续中给出详细解答。 Keywords: Reinforcement Learning，Situation，Action，Enviroment，Closed-loop，Optimal Control，Markov Decision Processes，MDPs\nReinforcement Learning 中文翻译Reinforcement Learning为强化学习，不知道为啥这么翻译，也没想去了解给这个词命名的人是否非常了解这个领域的知识，既然这么叫了那就这样吧。 上面谈到命名是为了介绍其内容，有些东西的命名，可以直接看出其内容，但是强化学习，显然不是这类的，而“土豆片”，“薯条”这种名字则可以。\n本文后面把强化学习简写成RL或者直接写英文名，希望大家理解。\nReinforcement Learning 像Machine Learning一样，名字里都有Learning这个词，表明，RL也是一个问题和解决方案的集合。\n值得注意的是，RL属于machine learning的一种。\n对于问题和解决方案的集合这种描述，我们有很多经典的例子：\n “拟合房子的面积大小和价格”的问题 “识别手写数字”的问题  这些问题对应了一系列解决方案，比如直线拟合就有最小二乘法，svm等算法来解决；识别手写数字可以有神经网络（Naive版，全连接的那种神经网络），CNN等方法。问题和方法一起构成了一个监督学习（机器学习中的一种）问题。 对于RL一样，一个问题，和一些列解决方案，找出各种方案的优缺点，提出更好的方案，这就是强化学习的内容，而与其他机器学习（监督学习，非监督学习）的具体区别在于解决问题的策略和问题的条件。\nRL在解决问题的过程中的基本策略是：\n 如何对应Situation和Action 如何最大化Rewarding Signal  如果你不懂这四个名词，不要着急，后面有详细的解释，而且，负责任的告诉你，你以后研究的所有问题里面都是围绕他们几个展开的。\n在解决RL的问题过程中又会产生一个新的问题，这个问题是在我们还没有看到一个具体的例子之前就能想到的，也是一个基础问题，就是我们的RL学习过程是个closed-loop的过程，通俗点说，像一个死循环，就是当我们用一个输入调整了模型以后，这个模型又会反过来影响这个输入产生的结果，具体点就是这个系统是不稳定的，当你输入在时间 $t_0$ 输入数据 $A_0$ 时产生的结果是 $R_0$ 但是当你在 $t_1$ 输入数据 $A_0$ 时产生的结果是 $R_1$ 并且在大多数情况下， $R_0\\neq R_1$ 而专业一些的用词就是： input会影响这个agent的action，而这个action会改变这个agent，当input再次来袭的时候，新的action会不同，因为这个agent已经变了。 有点绕，没关系，后面看了具体的例子，或者有具体的数据，我会提醒大家回来看这里的。有点抽象，但是复合逻辑。\n这个过程中有一个和其他机器学习算法巨大的差别：agent或者learner并不知道每次Input对应的“正确的”Action是什么，也不知道哪个 “Action”能产生最大的Rewarding，而监督学习是知道的。 并且这个Action影响本次迭代的Rewarding并且影响下一个Situation。\n这里其实可以想象一下下棋，不管什么棋，学习下棋就是一个RL的过程，我这里不想提某狗的例子是因为，我觉得这只狗已经被一些俗人写的烂大街了（一条好狗被糟蹋了），蹭热度，蹭关键词与我无益，所以，想象一下象棋或者五子棋，每一步都是一个action，而最后的输赢才是唯一的结果，每个action没有一个确定结果与其对应，为了衡量action的好坏，给出一个Rewarding，然后action做出以后，当前situation已经和action之前的不一样了（比如棋局中的形式就是situation） 简单的例子，漏洞百出，但是可以大概这么理解。\nReinforcement Learning Features 上面大概介绍了一些内容，可能云里雾里，没关系，都是些名词而已，后面内容会详细描述每一个细节，我们要知道的是RL的一些特征，使其与众不同的特征：\n Closed-Loop是基本的学习方式 学习过程中在每一步并不直接知道应该采用哪个Action Action的结果，Reward Signal并不是实时产生的，需要等待一些步骤后，才能知道。  Markov Decision Processes Markov Decision Processes是一个非常经典的全面的对RL中Optimal Control问题的理解，但是要等到第三章才会完整的描述，其中会详细介绍，Agent，Environment等具体的形式，以及如何达到我们的RL目的。其中的Environment非常主要，因为我们的Goal（目的，目标）就是environment的一个state（状态）。 在第三章中，MDPs将会介绍：\n Sensation Action Goal  等部分的具体内容，而且会明确的直接解决一个RL问题，而不是将原始问题转化为小问题，比如下棋就是要最后获胜而不是把棋局分成几种，分开解决，再合成最后的方案，RL要直接解决问题，而不是细分再整合这种硬套路。\n总结 中英文穿插写，比较让人讨厌，但是没办法，我觉得如果不看原版书籍，就记住一些翻译版本的关键词是非常有害的。 本文很多关键词和具体过程都没办法现在给出，所以，如果你觉得迷惑，没关系，看懂多少都无所谓，毕竟是个开头介绍，当你看完第一个例子就会豁然开朗，第一个例子在1.3中出现，敬请期待吧。\n原文来自：https://face2ai.com/RL-RSAB-1-1-0-Reinforcement-Learning转载标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-1-0-reinforcement-learning.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍Reinforcement Learning的具体特点和与其他机器学习算法不同之处，本文是一个骨架性的文章，所有专有名词都保持英文原始单词，具体内容会在后续中给出详细解答。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Reinforcement Learning，Situation，Action，Enviroment，Closed-loop，Optimal Control，Markov Decision Processes，MDPs\u003c/p\u003e","title":"【强化学习】1.1.0 强化学习介绍"},{"content":"Abstract: 本文是 《Reinforcement Learning: An Introduction》 的第一篇，介绍本书以及本系列的主要写作内容 Keywords: 学习的本质，诱发与结果(cause and effect)，计算近似，直接建模，强化学习\n开篇废话 本系列主要介绍强化学习，使用的教材为：\n内容主要来自本教材以及其引用的论文，但是不限于此。 此外，了解我的同学可能知道我喜欢说些没用的废话，本系列该环节省略。本系列部分专业名词保持英文原文，不是为了装x，因为翻译怕不准影响后续学习。\nNature of Learning(学习的本质) 做人工智能的，或者吹人工智能的所有人应该都曾经尝试思考过，学习的本质是什么，因为我们的“智能”来自学习，可能也有一些遗传因素，我们并不了解学习或者智能的本质是什么，但是我们可以从我们的生活或者其他一些动物的行为和现象中得到一些启示：\n 和环境的Interaction(我想翻译成:“相互影响”，但是觉得不太对）  没错，所有有智能的动物，的所有智能都是来自周围自然环境的，如果把一个人从出生就放在培养基中，完全隔绝所有能和我们行为互动的因素，那么这个人很有可能就是一个有机体，不具备任何智能的器官的组合。 而我们从出生就是开始和自然环境互动，一个婴儿，挥动胳膊，蹬一下腿，或者摇摇头，这些看起来毫无规律和目的的动作就已经跟周围产生了Interaction\nCause and Effect (诱发与结果) action的结果是我们从自然环境中获得反馈，所以，我可以负责任的告诉你，BP(反向传导)和现在的依靠BP训练的所有神经网络就是若干年前的SVM没什么可以吹的，所以CNN的开创者们呼吁冷静对待BP是有道理的，而且，可以肯定的是，你的智慧不是BP来的。\n我们的智慧的获得的一个主要过程就是，我们能够总结，或者学习到，我们的动作（action）能够产生什么结果，然后，为了获得某个结果，我们需要做什么。这也是我们获得智慧的主要来源就是这一些列的过程。\n目前近乎所有关于学习和智能的基础思想都是 —— 从interaction中学习。\nComputational Approach or Directly Theorizing (计算近似 vs 直接建模) 人工智能的最终目的就是让机器有动物或者人一样的智慧，而达到这个目的目前有两种主流方案\n 直接建模 近似计算  直接建模的思想比较简单，就是用复杂的算法或者机制，直接产生智慧，目前来看有难度，毕竟智慧不是“鸡兔同笼”或者 “疯狂的泳池管理员”这种数学模型。 另一种做法是通过用算法迭代，近似，自动的生成一个模型，来模拟一种行为（智慧的一部分），而这个过程主要有两步：\n explore 抽象的情况 evaluate 多种学习结果的 Effectiveness  这两步就是AI研究人员和工程人员的主要研发对象了。他们的主要工作就是制造一部机器（算法），解决类似的问题，这个机器(算法)必须有以下特征：\n Evaluating \u0026ndash; 评估设计(通过数学分析) Computational \u0026ndash; 计算近似结果  设计这个机器的过程，就是 “强化学习”\n强化学习只研究 goal-directed 学习过程，主要研究对象是Interaction。而不是其他例如残差，或者其他损失函数，这些机器学习关注的目标。\n总结 深度学习关注interaction，是一种goal-directed的过程，和其他机器学习算法有本质的不同，更接近人或者智慧生物的学习过程。\n原文地址：https://www.face2ai.com/RL-RSAB-1-0-Introduction转载请标明出处\n","permalink":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-0-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是 《Reinforcement Learning: An Introduction》 的第一篇，介绍本书以及本系列的主要写作内容\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 学习的本质，诱发与结果(cause and effect)，计算近似，直接建模，强化学习\u003c/p\u003e","title":"【强化学习】1.0 强化学习介绍"},{"content":"Abstract: In this section, we introduct what is Machine Learning, pattern recognition, and some relative concepts. Keywords: \u0026ldquo;Pattern Recognition and Machine Learning\u0026rdquo;, Introduction, Machine Learning\n  Search for Pattern in Data Pattern Recognition: Automatic Discover regularities  Rule \u0026amp; Heuristics (poor result) Machine Learning Algorithm $y(\\vec{x})$ Training / Learning Phase (train set) Generalization (test set)   Pre-Process  Feature Extraction Speed Up Dimensionality Reduction   Supervised Learning  Classification Regression   Unsupervised Learning  Clustering Density Estimation Visualization   Reinforcement Learning  States / Action Exploration Exploitation   Tools:  Probability Decision Theory Information Theory    ","permalink":"https://go.face2ai.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/machinelearning-prml-1-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e In this section, we introduct what is Machine Learning, pattern recognition, and some relative concepts.\n\u003cstrong\u003eKeywords:\u003c/strong\u003e \u0026ldquo;Pattern Recognition and Machine Learning\u0026rdquo;, Introduction, Machine Learning\u003c/p\u003e","title":"【PRML】Introduction 介绍"},{"content":"Abstract: 本文介绍流回调 Keywords: 流回调\n流回调 流回调是一种特别的技术，有点像是事件的函数，这个回调函数被放入流中，当其前面的任务都完成了，就会调用这个函数，但是比较特殊的是，在回调函数中，需要遵守下面的规则\n 回调函数中不可以调用CUDA的API 不可以执行同步  流函数有特殊的参数规格，必须写成下面形式参数的函数;\nvoid CUDART_CB my_callback(cudaStream_t stream, cudaError_t status, void *data) { printf(\u0026#34;callback from stream %d\\n\u0026#34;, *((int *)data)); } 然后使用：\ncudaError_t cudaStreamAddCallback(cudaStream_t stream,cudaStreamCallback_t callback, void *userData, unsigned int flags); 加入流中。 本文完整的代码在github:https://github.com/Tony-Tan/CUDA_Freshman（欢迎随手star😝 ） 部分代码\n// // // void CUDART_CB my_callback(cudaStream_t stream,cudaError_t status,void * data) { printf(\u0026#34;call back from stream:%d\\n\u0026#34;,*((int *)data)); } // // // // // int main(int argc,char **argv) { //  //  //  //asynchronous calculation  int iElem=nElem/N_SEGMENT; cudaStream_t stream[N_SEGMENT]; for(int i=0;i\u0026lt;N_SEGMENT;i++) { CHECK(cudaStreamCreate(\u0026amp;stream[i])); } cudaEvent_t start,stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start,0); for(int i=0;i\u0026lt;N_SEGMENT;i++) { int ioffset=i*iElem; CHECK(cudaMemcpyAsync(\u0026amp;a_d[ioffset],\u0026amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); CHECK(cudaMemcpyAsync(\u0026amp;b_d[ioffset],\u0026amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(\u0026amp;a_d[ioffset],\u0026amp;b_d[ioffset],\u0026amp;res_d[ioffset],iElem); CHECK(cudaMemcpyAsync(\u0026amp;res_from_gpu_h[ioffset],\u0026amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i])); CHECK(cudaStreamAddCallback(stream[i],my_callback,(void *)(stream+i),0)); } //timer  CHECK(cudaEventRecord(stop, 0)); int counter=0; while (cudaEventQuery(stop)==cudaErrorNotReady) { counter++; } //  //  //  //  //  // } 结果如下： 总结 本文介绍了本系列的最后一个小功能，流回调，下面部分就是中级提高篇了，要好好练习前面的，不然后面会懵逼哦！\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-5-%E6%B5%81%E5%9B%9E%E8%B0%83.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍流回调\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 流回调\u003c/p\u003e","title":"【CUDA 基础】6.5 流回调"},{"content":"Abstract: 本文介绍如何进行GPU和CPU的重叠执行 Keywords: 重叠GPU和CPU的执行\n重叠GPU和CPU的执行 除了上文说到的重叠数据传输和核函数的同时执行，另一个最主要的问题就是使用GPU的同时CPU也进行计算，这就是我们本文关注的重点。 本文示例过程如下：\n 内核调度到各自的流中 CPU在等待事件的同时进行计算  具体代码如下：\ncudaEvent_t start,stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start,0); for(int i=0;i\u0026lt;N_SEGMENT;i++) { int ioffset=i*iElem; CHECK(cudaMemcpyAsync(\u0026amp;a_d[ioffset],\u0026amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); CHECK(cudaMemcpyAsync(\u0026amp;b_d[ioffset],\u0026amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(\u0026amp;a_d[ioffset],\u0026amp;b_d[ioffset],\u0026amp;res_d[ioffset],iElem); CHECK(cudaMemcpyAsync(\u0026amp;res_from_gpu_h[ioffset],\u0026amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i])); } //timer  CHECK(cudaEventRecord(stop, 0)); int counter=0; while (cudaEventQuery(stop)==cudaErrorNotReady) { counter++; } printf(\u0026#34;cpu counter:%d\\n\u0026#34;,counter); 本文完整的代码在github:https://github.com/Tony-Tan/CUDA_Freshman（欢迎随手star😝 ） 运行结果是：\n可见在事件stop执行之前，CPU是一直在工作的，这就达到一种并行的效果 代码中关键的一点是\ncudaEventQuery(stop) 是非阻塞的，否则，不能继续cpu的计算\n总结 本文很短，但是，作为一个非常不错的例子，展示了GPU和CPU之间的并行。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-4-%E9%87%8D%E5%8F%A0gpu%E5%92%8Ccpu%E7%9A%84%E6%89%A7%E8%A1%8C.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍如何进行GPU和CPU的重叠执行\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 重叠GPU和CPU的执行\u003c/p\u003e","title":"【CUDA 基础】6.4 重叠GPU和CPU的执行"},{"content":"Abstract: 本文介绍如何利用流的重叠来隐藏主机到设备的数据传输延迟 Keywords: 深度优先，广度优先\n重叠内核执行和数据传输 前面一节我们主要研究多个内核在不同流中的不同行为，主要使用的工具是NVVP，NVVP是可视化的非常实用的工具，值得大家深入研究一下。 Fermi架构和Kepler架构下有两个复制引擎队列，也就是数据传输队列，一个从设备到主机，一个从主机到设备。所以读取和写入是不经过同一条队列的，这样的好处就是这两个操作可以重叠完成了，注意，只有方向不同的时候才能数据操作。同向的时候不能进行此操作。 应用程序中，还需要检查数据传输和内核执行之间的关系，分为以下两种：\n 如果内核使用数据A，那么对A进行数据传输必须要安排在内核启动之前，且必须在同一个流中 如果内核完全不使用数据A，那么内核执行和数据传输可以位于不同的流中重叠执行。  第二种情况就是重叠内核执行和数据传输的基本做法，当数据传输和内核执行被分配到不同的流中时，CUDA执行的时候默认这是安全的，也就是程序编写者要保证他们之间的依赖关系。 但是第一种情况也可以进行重叠，只要对核函数进行一定的分割，我们用向量加法来完成本文的研究。\n使用深度优先调度重叠 向量加法的内核我们很熟悉了\n__global__ void sumArraysGPU(float*a,float*b,float*res,int N) { int idx=blockIdx.x*blockDim.x+threadIdx.x; if(idx \u0026lt; N) //for delay  { for(int j=0;j\u0026lt;N_REPEAT;j++) res[idx]=a[idx]+b[idx]; } } 我们这一章的重点都不是在核函数上，所以，我们使用这种非常简单的内核函数。但是不同的是，我们使用N_REPEAT进行多次冗余计算，原因是为了延长线程的执行时间，方便nvvp捕捉运行数据。 向量加法的过程是：\n 两个输入向量从主机传入内核 内核运算，计算加法结果 将结果（一个向量）从设备回传到主机  由于这个问题就是一个一步问题，我们没办法让内核和数据传输重叠，因为内核需要全部的数据，但是，我们如果思考一下，向量加法之所以能够并发执行，因为每一位都互不干扰，那么我们可以把向量分块，然后每一个块都是一个上面的过程，并且A块中的数据只用于A块的内核，而跟B，C，D内核没有关系，于是我们来把整个过程分成 N_SEGMENT 份，也就是 N_SEGMENT 个流分别执行，在主机代码中流的使用如下：\ncudaStream_t stream[N_SEGMENT]; for(int i=0;i\u0026lt;N_SEGMENT;i++) { CHECK(cudaStreamCreate(\u0026amp;stream[i])); } cudaEvent_t start,stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start,0); for(int i=0;i\u0026lt;N_SEGMENT;i++) { int ioffset=i*iElem; CHECK(cudaMemcpyAsync(\u0026amp;a_d[ioffset],\u0026amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); CHECK(cudaMemcpyAsync(\u0026amp;b_d[ioffset],\u0026amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(\u0026amp;a_d[ioffset],\u0026amp;b_d[ioffset],\u0026amp;res_d[ioffset],iElem); CHECK(cudaMemcpyAsync(\u0026amp;res_from_gpu_h[ioffset],\u0026amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i])); } //timer  CHECK(cudaEventRecord(stop, 0)); CHECK(cudaEventSynchronize(stop)); 其中和前面唯一有区别的就是\nfor(int i=0;i\u0026lt;N_SEGMENT;i++) { int ioffset=i*iElem; CHECK(cudaMemcpyAsync(\u0026amp;a_d[ioffset],\u0026amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); CHECK(cudaMemcpyAsync(\u0026amp;b_d[ioffset],\u0026amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(\u0026amp;a_d[ioffset],\u0026amp;b_d[ioffset],\u0026amp;res_d[ioffset],iElem); CHECK(cudaMemcpyAsync(\u0026amp;res_from_gpu_h[ioffset],\u0026amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i])); } 数据传输使用异步方式，注意异步处理的数据要声明称为固定内存，不能是分页的，如果是分页的可能会出现未知错误。 编译后使用nvvp查看结果如下：\n如果使用非固定的主机内存，会产生下面的错误（别问我咋知道的。😜）\n分成四份，数据传输和内核执行时重叠的。 观察nvvp结果：\n 不同流中内核相互重叠 内核和数据传输重叠  同时图中也有两种阻塞行为：\n 内核被前面的数据传输阻塞 主机到设备的数据传输被同一方向上的前面的数据传输阻塞  同样这里使用多个流的时候需要注意虚假依赖的问题。\nGMU网格管理单元是Kepler架构引入了一个新的网格和调度控制系统，GMU可以暂停新网格调度，使网格排队等待且暂停网格直到他们准备好执行。使得运行时变得灵活。同时GMU也创建多个硬件工作队列，减少虚假内存的影响。\n使用广度优先调度重叠 同样的，我们看完深度优先之后看一下广度优先 代码：\nfor(int i=0;i\u0026lt;N_SEGMENT;i++) { int ioffset=i*iElem; CHECK(cudaMemcpyAsync(\u0026amp;a_d[ioffset],\u0026amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); CHECK(cudaMemcpyAsync(\u0026amp;b_d[ioffset],\u0026amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i])); } for(int i=0;i\u0026lt;N_SEGMENT;i++) { int ioffset=i*iElem; sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(\u0026amp;a_d[ioffset],\u0026amp;b_d[ioffset],\u0026amp;res_d[ioffset],iElem); } for(int i=0;i\u0026lt;N_SEGMENT;i++) { int ioffset=i*iElem; CHECK(cudaMemcpyAsync(\u0026amp;res_from_gpu_h[ioffset],\u0026amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i])); } nvvp结果：\n在Fermi以后架构的设备，不太需要关注工作调度顺序，因为多个工作队列足以优化执行过程，而Fermi架构则需要关注一下。\n总结 本文介绍了如何使用流隐藏数据传输的延迟，这是后面非常有用的一种技术，来加速数据密集型应用。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-3-%E9%87%8D%E5%8F%A0%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C%E5%92%8C%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍如何利用流的重叠来隐藏主机到设备的数据传输延迟\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 深度优先，广度优先\u003c/p\u003e","title":"【CUDA 基础】6.3 重叠内核执行和数据传输"},{"content":"Abstract: 本文介绍内核的并发执行，以及相关的知识 Keywords: 流，事件，深度优先，广度优先，硬件工作队列，默认流阻塞行为\n并发内核执行 继续前面的内容，上文中我们说到了流，事件和同步等的概念，以及一些函数的用法，接下来的几个例子，介绍并发内核的几个基本问题，包括不限于以下几个方面：\n 使用深度优先或者广度优先方法的调度工作 调整硬件工作队列 在Kepler设备和Fermi设备上避免虚假的依赖关系 检查默认流的阻塞行为 在非默认流之间添加依赖关系 检查资源使用是如何影响并发的  非空流中的并发内核 本文我们开始使用NVIDIA提供的另一个可视化工具nvvp进行性能分析，其最大用途在于可视化并发核函数的执行，第一个例子中我们就能清楚地看到各个核函数是如何执行的，本例子中使用了同一个核函数，并将其复制多份，并确保每个核函数的计算要消耗足够的时间，保证执行过程能够被性能分析工具准确的捕捉到。 我们的核函数是：\n__global__ void kernel_1() { double sum=0.0; for(int i=0;i\u0026lt;N;i++) sum=sum+tan(0.1)*tan(0.1); } __global__ void kernel_2() { double sum=0.0; for(int i=0;i\u0026lt;N;i++) sum=sum+tan(0.1)*tan(0.1); } __global__ void kernel_3() { double sum=0.0; for(int i=0;i\u0026lt;N;i++) sum=sum+tan(0.1)*tan(0.1); } __global__ void kernel_4() { double sum=0.0; for(int i=0;i\u0026lt;N;i++) sum=sum+tan(0.1)*tan(0.1); } 四个核函数，N是100，tan计算在GPU中应该有优化过的高速版本，但是就算优化，这个也是相对耗时的，足够我们进行观察了。 接着我们按照上节课的套路，创建流，把不同的核函数或者指令放到不同的流中，然后看一下他们的表现。 本文完整的代码在github:https://github.com/Tony-Tan/CUDA_Freshman（欢迎随手star😝 ）\n我们本章主要关注主机代码，下面是创建流的代码：\ncudaStream_t *stream=(cudaStream_t*)malloc(n_stream*sizeof(cudaStream_t)); for(int i=0;i\u0026lt;n_stream;i++) { cudaStreamCreate(\u0026amp;stream[i]); } 首先声明一个流的头结构，是malloc的注意后面要free掉 然后为每个流的头结构分配资源，也就是Create的过程，这样我们就有n_stream个流可以使用了，接着，我们添加核函数到流，并观察运行效果\ndim3 block(1); dim3 grid(1); cudaEvent_t start,stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); cudaEventRecord(start); for(int i=0;i\u0026lt;n_stream;i++) { kernel_1\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_2\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_3\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_4\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); } cudaEventRecord(stop); CHECK(cudaEventSynchronize(stop)); float elapsed_time; cudaEventElapsedTime(\u0026amp;elapsed_time,start,stop); printf(\u0026#34;elapsed time:%f ms\\n\u0026#34;,elapsed_time); 这不是完整的代码，这个循环是将每个核函数都放入不同的流之中，也就是假设我们有10个流，那么这10个流中每个流都要按照上面的顺序执行这4个核函数。 注意如果没有\ncudaEventSynchronize(stop) nvvp将会无法运行，因为所有这些都是异步操作，不会等到操作完再返回，而是启动后自动把控制权返回主机，如果没有一个阻塞指令，主机进程就会执行完毕推出，这样就跟设备失联了，nvvp也会相应的报错。 然后我们创建两个事件，然后记录事件之间的时间间隔。这个间隔是不太准确的，因为是异步的。 运行结果如下：\n使用nvvp检测，结果如下：\nFermi GPU 上的虚假依赖关系 虚假依赖我们在上文中讲到过了，这种情况通常出现在只有在比较古老的Fermi架构上出现，原因是其只有一个硬件工作队列，由于我们现在很难找到Fermi架构的GPU了，所以，只能看看书上给出的nvvp结果图了：\n虚假依赖的问题我们在流和事件概述已经描述了引起此问题的理论原因，这里就不再解释了。 如果你手头只有老机器，这种虚假依赖关系也是可以解决的，原理就是使用广度优先的方法，组织各任务的方式如下：\n// dispatch job with breadth first way for (int i = 0; i \u0026lt; n_streams; i++) kernel_1\u0026lt;\u0026lt;\u0026lt;grid, block, 0, streams[i]\u0026gt;\u0026gt;\u0026gt;(); for (int i = 0; i \u0026lt; n_streams; i++) kernel_2\u0026lt;\u0026lt;\u0026lt;grid, block, 0, streams[i]\u0026gt;\u0026gt;\u0026gt;(); for (int i = 0; i \u0026lt; n_streams; i++) kernel_3\u0026lt;\u0026lt;\u0026lt;grid, block, 0, streams[i]\u0026gt;\u0026gt;\u0026gt;(); for (int i = 0; i \u0026lt; n_streams; i++) kernel_4\u0026lt;\u0026lt;\u0026lt;grid, block, 0, streams[i]\u0026gt;\u0026gt;\u0026gt;(); 这样逻辑图就不是:\n而是\n这样了，这就可以从抽象模型层面避免问题。 广度优先的nvvp结果是：\n注意，以上结论都是我从书上原封不动弄下来的。\n使用OpenMP的调度操作 OpenMP是一种非常好用的并行工具，比pthread更加好用，但是没有pthread那么灵活，这里我们不光要让核函数或者设备操作用多个流处理，同时也让主机在多线程下工作，我们尝试使用每个线程来操作一个流：\nomp_set_num_thread(n_stream); #pragma omp parallel  { int i=omp_get_thread_num(); kernel_1\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_2\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_3\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_4\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); } 解释下代码\nomp_set_num_thread(n_stream); #pragma omp parallel 调用OpenMP的API创建n_stream个线程，然后宏指令告诉编译器下面大括号中的部分就是每个线程都要执行的部分，有点类似于核函数，或者叫做并行单元。\n其他代码和上面常规代码都一样，但是注意，OpenMP在mac上支持的不是很好，需要安装GCC，在Linux和Windows下配置非常简单，Linux下只要连接库函数就行，Windows下如果使用vs系列IDE直接在属性中打开一个开关就可以，我们来观察一下运行结果，注意，这段代码没有使用cmake管理，而是使用命令行编译执行的：\nnvcc -O3 -Xcompiler -fopenmp stream_omp.cu -o stream_omp -lgomp -I ../include/ CUDA进阶系列还会有OpenMP和CUDA配合的部分，后面会详细说。\n用环境变量调整流行为 Kepler支持的最大Hyper-Q 工作队列数是32 ，但是在默认情况下并不是全部开启，而是被限制成8个，原因是每个工作队列只要开启就会有资源消耗，如果用不到32个可以把资源留给需要的8个队列，修改这个配置的方法是修改主机系统的环境变量。 对于Linux系统中，修改方式如下：\n#For Bash or Bourne Shell: export CUDA_DEVICE_MAX_CONNECTIONS=32 #For C-Shell: setenv CUDA_DEVICE_MAX_CONNECTIONS 32 另一种修改方法是直接在程序里写，这种方法更好用通过底层驱动修改硬件配置：\nsetenv(\u0026#34;CUDA_DEVICE_MAX_CONNECTIONS\u0026#34;, \u0026#34;32\u0026#34;, 1); 然后我们把前面的深度优先的代码改一下，加入上面这句指令,并把n_stream改成16，就可以得到如下结果：\n16个流，8个工作队列的结果：\n调用\nsetenv(\u0026#34;CUDA_DEVICE_MAX_CONNECTIONS\u0026#34;, \u0026#34;32\u0026#34;, 1); 修改成32个队列 16个流，32个工作队列的结果：\nGPU资源的并发限制 限制内核并发数量的最根本的还是GPU上面的资源，资源才是性能的极限，性能最高无非是在不考虑算法进化的前提下，资源利用率最高的结果。当每个内核的线程数增加的时候，内核级别的并行数量就会下降，比如，我们把\ndim3 block(1); dim3 grid(1); 升级到\ndim3 block(16,32); dim3 grid(32); 4个流，nvvp结果是：\n默认流的阻塞行为 默认流也就是空流对于飞空流中的阻塞流是有阻塞作用的，这句话有点难懂，首先我们没有声明流的那些GPU操作指令，核函数是在空流上执行的，空流是阻塞流，同时我们声明定义的流如果没有特别指出，声明的也是阻塞流，换句话说，这些流的共同特点，无论空流与非空流，都是阻塞的。 那么这时候空流（默认流）对非空流的阻塞操作就要注意一下了。\nfor(int i=0;i\u0026lt;n_stream;i++) { kernel_1\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_2\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_3\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(); kernel_4\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); } 注意，kernel_3是在空流（默认流）上的，从NVVP的结果中可以看出，所有kernel_3 启动以后，所有其他的流中的操作全部被阻塞：\n创建流间依赖关系 流之间的虚假依赖关系是需要避免的，而经过我们设计的依赖又可以保证流之间的同步性，避免内存竞争，这时候我们要使用的就是事件这个工具了，换句话说，我们可以让某个特定流等待某个特定的事件，这个事件可以再任何流中，只有此事件完成才能进一步执行等待此事件的流继续执行。 这种事件往往不用于计时，所以可以在生命的时候声明成 cudaEventDisableTiming 的同步事件：\ncudaEvent_t * event=(cudaEvent_t *)malloc(n_stream*sizeof(cudaEvent_t)); for(int i=0;i\u0026lt;n_stream;i++) { cudaEventCreateWithFlag(\u0026amp;event[i],cudaEventDisableTiming); } 在流中加入指令:\nfor(int i=0;i\u0026lt;n_stream;i++) { kernel_1\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_2\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_3\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); kernel_4\u0026lt;\u0026lt;\u0026lt;grid,block,0,stream[i]\u0026gt;\u0026gt;\u0026gt;(); cudaEventRecord(event[i],stream[i]); cudaStreamWaitEvent(stream[n_stream-1],event[i],0); } 这时候，最后一个流（第5个流）都会等到前面所有流中的事件完成，自己才会完成，nvvp结果如下\n总结 本文研究了如何使用并发内核提高应用整体的效率，以及流阻塞的相关知识。 下一篇我们介绍一个更加实用的技术——通过流来屏蔽数据传输延迟。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-2-%E5%B9%B6%E5%8F%91%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍内核的并发执行，以及相关的知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 流，事件，深度优先，广度优先，硬件工作队列，默认流阻塞行为\u003c/p\u003e","title":"【CUDA 基础】6.2 并发内核执行"},{"content":"Abstract: 本文介绍计算机在计算中有可能引发的有效数字缺失 Keywords: 有效数字缺失\n有效数字缺失 对计算机硬件或者底层功能足够了解，虽然表面教条枯燥，但是我们能在其中学到很多有可能出现，但是不容易察觉的陷阱\n有效数字 在了解有效数字缺失之前，我们有必要确定下什么是有效数字，无论是小数还是大数，我们都能写成科学计数法的形式，在十进制下我们可以把所有数字写成： $$ d_1.d_2\\cdots d_n \\times 10^p $$ 的这种形式，其中 $d_1\\neq 0$，有效数字是 $d_1.d_2\\cdots d_n$ 共 $n$ 位，注意我们这里之研究数学上的有效数字，物理中测量还有另一套说法，我们不考虑. 有效数字我们知道是什么了，那么什么情况下会缺失呢？数字不会凭空消失，一定是做了某种计算后，原本有 $n$ 位有效数字，结果变成了小于 $n$ 位，这就是缺失，而缺失不可能发生在右侧，也就是 $d_n$ 这一侧，原因是，如果这些位消失了，我们依然可以补充0作为有效数字，所以我们要注意左侧的数字。 比如： $$ \\begin{aligned} \u0026amp;123.4567\\ -\u0026amp;123.4566\\ \u0026amp;\\hline\\ =\u0026amp;000.0001 \\end{aligned} $$\n这时候，我们原本的7位有效数字，瞬间变成1位，有问题么？没有问题，因为这是准确结果，但是从数值上看，有效数字减少了。 但是，如果减法的两个操作数，减数或者被减数，并不是一个确定的数，而是一个表达式，包含无限的小数呢？\n“减法”=“陷阱” 考虑 $\\sqrt{9.01}-3$ 这个表达式的值。 这个例子很简答，输入matlab能够直接得到答案，$\\sqrt{9.01}\\approx 3.001 662$ 所以 $\\sqrt{9.01}-3\\approx |0.001 662$ 减法之前有七位有效数字，结果有4位有效数字，也就是少了3位，这看起来没啥。 然后我们假定，我们有一种特殊的计算机，其浮点数最多只能保存十进制的3位有效数字，（书中说是三位计算机，表达不是很准确，会和三个二进制位混淆，所以我们说这个计算机只能计算3位十进制）那么我们如果按照上面的计算过程，那么会得到： $$ \\sqrt{9.01}\\approx 3.001 662=3.00\\ \\sqrt{9.01}-3=0 $$ 结果是0，第一步我们把3.001662保存成3.00 的原因是计算机只能保存3位十进制数，所以这个结果是截断误差造成的，然后相减得到了|0.00，理想情况，有效数字是1.66，可见一个都不对。 我们这里所说的有效数字是和真实计算结果数字上的比较，不是比较大小，只考虑尾数，不考虑指数。 两个相近的数相减，可能会造成有效数字的缺失 但是你可能说，没办法，这是机器的限制，你那个能存一百位有效数字的计算机来就可以了，如果你这么想，那就没得聊了， 有没有办法？当然有办法！\n解决方法 解决办法的核心思想是：把相近的两个数变成不相近的！ 同样使用只有三维有效数字的计算机 $$ \\sqrt{9.01}-3=\\frac{(\\sqrt{9.01}-3)(\\sqrt{9.01}+3)}{\\sqrt{9.01}+3}=\\frac{|0.01}{3.00+3}=\\frac{1}{6}=1.67\\times 10^{-3} $$ 上面这种方法叫做共轭等式，可以避免两个相近的数字相减。下面我们看另一个例子，主要技巧也是用一些恒等式： $$ E_1=\\frac{1-\\text{cos}x}{\\text{sin}^2x} $$ 明显得当$x\\to 0$ 的时候 $\\text{cos}x\\to 1$ 那么就有可能造成有效数字缺失，与上面类似的，我们也有共轭等式： $$ E_2=\\frac{(1-\\text{cos}x)(1+\\text{cos}x)}{\\text{sin}^2x(1+\\text{cos}x)}=\\frac{1}{(1+\\text{cos}x)}=E_1 $$ 然后我们就写个代码来观察下 $E_1$ 和 $E_2$ 在 $x\\to 0$ 的时候的表现，是否 $E_2$ 更准确，我们写一段python程序：\nimport math x=1.0 for i in range(0,16): x=x/10 cos_x=math.cos(x) sin_x=math.sin(x) e1=(1- cos_x)/pow(sin_x,2) e2 = (1 ) / (1 + cos_x) print \u0026#39;|%1.16f|%1.16f|%1.16f|\u0026#39; % (x,e1,e2) 运行结果是：\n   $x$ $E_1$ $E_2$     0.1000000000000000 0.5012520862885769 0.5012520862885712   0.0100000000000000 0.5000125002084805 0.5000125002083363   0.0010000000000000 0.5000001249921894 0.5000001250000208   0.0001000000000000 0.4999999986279311 0.5000000012500000   0.0000100000000000 0.5000000413868522 0.5000000000125000   0.0000010000000000 0.5000444502913370 0.5000000000001250   0.0000001000000000 0.4996003610813219 0.5000000000000012   0.0000000100000000 0.0000000000000000 0.5000000000000000   0.0000000010000000 0.0000000000000000 0.5000000000000000   0.0000000001000000 0.0000000000000000 0.5000000000000000   0.0000000000100000 0.0000000000000000 0.5000000000000000   0.0000000000010000 0.0000000000000000 0.5000000000000000   0.0000000000001000 0.0000000000000000 0.5000000000000000   0.0000000000000100 0.0000000000000000 0.5000000000000000   0.0000000000000010 0.0000000000000000 0.5000000000000000   0.0000000000000001 0.0000000000000000 0.5000000000000000    从 0.00000001开始 $E_1$ 崩溃，而 $E_2$ 保持稳定。 到0.00000001崩溃的原因是cos在这个位置截断后就是1.0：\n同样，我们这套方法的核心就是不要见相近的数字，只要有减法就要小心，注意前后两个数是否相近，另一个例子是二次方程的求解，对于公式法我们必须要计算： $$ x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a} $$ 其中要注意的是 $$ x_1=\\frac{-b+\\sqrt{b^2-4ac}}{2a} $$ 对应的方法就是把这个计算改成： $$ x_1=\\frac{(-b+\\sqrt{b^2-4ac})\\cdot(+b+\\sqrt{b^2-4ac})}{2a(+b+\\sqrt{b^2-4ac})}=\\frac{-4ac}{2a(+b+\\sqrt{b^2-4ac})}=\\frac{-2c}{(b+\\sqrt{b^2-4ac})} $$ 当然这里面还有问题就是 $b^2-4ac$ 的部分依然存在减法 而且 $b$ 是负数的时候，依然会存在 $b+\\sqrt{b^2-4ac}$ 相近的情况，尤其是 $ac\u0026laquo;b^2$ 的时候，这时候计算最好使用： $$ x_1=\\frac{-b+\\sqrt{b^2-4ac}}{2a}\\ x_2=\\frac{2c}{-b+\\sqrt{b^2-4ac}} $$\n总结 有效数字可能会带来很小的误差，也可以带来很大的误差，所以，不管什么时候，我们能避免减法就避免减法，尤其减法的两个数字接近的时候，这个时候更危险。\n","permalink":"https://go.face2ai.com/math/math-numerical-analysis-0-4-significant-digit.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍计算机在计算中有可能引发的有效数字缺失\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 有效数字缺失\u003c/p\u003e","title":"【数值分析】0.4 有效数字缺失"},{"content":"Abstract: 本文介绍CUDA中流和事件的理论描述。 Keywords: 流，事件\n流和事件概述 前面几章我们一直围绕GPU设备展开，我们的代码除了在核函数的配置的部分研究过主机端执行的代码，其他部分基本都是在设备代码上进行的，这一章我们就从主机端来讲讲如何优化CUDA应用。 CUDA流：一系列异步CUDA操作，比如我们常见的套路，在主机端分配设备主存（cudaMalloc），主机向设备传输数据（cudaMemcpy），核函数启动，复制数据回主机（Memcpy）这些操作中有些是异步的，执行顺序也是按照主机代码中的顺序执行的（但是异步操作的结束不一定是按照代码中的顺序的）。 流能封装这些异步操作，并保持操作顺序，允许操作在流中排队。保证其在前面所有操作启动之后启动，有了流，我们就能查询排队状态了。 我们上面举得一般情况下的操作基本可以分为以下三种：\n 主机与设备间的数据传输 核函数启动 其他的由主机发出的设备执行的命令  流中的操作相对于主机来说总是异步的，CUDA运行时决定何时可以在设备上执行操作。我们要做的就是控制这些操作在其结果出来之前，不启动需要调用这个结果的操作。 一个流中的不同操作有着严格的顺序。但是不同流之间是没有任何限制的。多个流同时启动多个内核，就形成了网格级别的并行。 CUDA流中排队的操作和主机都是异步的，所以排队的过程中并不耽误主机运行其他指令，所以这就隐藏了执行这些操作的开销。 CUDA编程的一个典型模式是，也就是我们上面讲到的一般套路：\n 将输入数据从主机复制到设备上 在设备上执行一个内核 将结果从设备移回主机  一般的生产情况下，内核执行的时间要长于数据传输，所以我们前面的例子大多是数据传输更耗时，这是不实际的。当重叠核函数执行和数据传输操作，可以屏蔽数据移动造成的时间消耗，当然正在执行的内核的数据需要提前复制到设备上的，这里说的数据传输和内核执行是同时操作的是指当前传输的数据是接下来流中的内核需要的。这样总的执行时间就被缩减了。 流在CUDA的API调用可以实现流水线和双缓冲技术。 CUDA的API也分为同步和异步的两种：\n 同步行为的函数会阻塞主机端线程直到其完成 异步行为的函数在调用后会立刻把控制权返还给主机。  异步行为和流式构建网格级并行的支柱。 虽然我们从软件模型上提出了流，网格级并行的概念，但是说来说去我们能用的就那么一个设备，如果设备空闲当然可以同时执行多个核，但是如果设备已经跑满了，那么我们认为并行的指令也必须排队等待——PCIe总线和SM数量是有限的，当他们被完全占用，流是没办法做什么的，除了等待\n我们接下来就要研究多种计算能力的设备上的流是如何运行的。\nCUDA流 我们的所有CUDA操作都是在流中进行的，虽然我们可能没发现，但是有我们前面的例子中的指令，内核启动，都是在CUDA流中进行的，只是这种操作是隐式的，所以肯定还有显式的，所以，流分为：\n 隐式声明的流，我们叫做空流 显式声明的流，我们叫做非空流  如果我们没有特别声明一个流，那么我们的所有操作是在默认的空流中完成的，我们前面的所有例子都是在默认的空流中进行的。 空流是没办法管理的，因为他连个名字都没有，似乎也没有默认名，所以当我们想控制流，非空流是非常必要的。 基于流的异步内核启动和数据传输支持以下类型的粗粒度并发\n 重叠主机和设备计算 重叠主机计算和主机设备数据传输 重叠主机设备数据传输和设备计算 并发设备计算（多个设备）  CUDA编程和普通的C++不同的就是，我们有两个“可运算的设备”也就是CPU和GPU这两个东西，这种情况下，他们之间的同步并不是每一步指令都互相通信执行进度的，设备不知道主机在干啥，主机也不是完全知道设备在干啥。但是数据传输是同步的，也就是主机要等设备接收完数据才干别的，也就是说你爸给你寄了一袋大米，然后老人家啥也不做，拨通电话跟你保持通话不停的问你收到了么？直到你回答收到了，这就是同步的。内核启动就是异步的，你爸爸又要给你钱花，去银行给你汇了五百块钱，银行说第二天到账，他就可以回家该干嘛干嘛了，而不需要在银行等一晚，第二天你收到了，打个电话说一声就行了，这就是异步的。异步操作，可以重叠主机计算和设备计算。 前面用的cudaMemcpy就是个同步操作，我们还提到过隐式同步——从设备复制结果数据回主机，要等设备执行完。当然数据传输有异步版本：\ncudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t count,cudaMemcpyKind kind, cudaStream_t stream = 0); 值得注意的就是最后一个参数，stream表示流，一般情况设置为默认流，这个函数和主机是异步的，执行后控制权立刻归还主机，当然我们需要声明一个非空流：\ncudaError_t cudaStreamCreate(cudaStream_t* pStream); 这样我们就有一个可以被管理的流了，这段代码是创建了一个流，有C++经验的人能看出来，这个是为一个流分配必要资源的函数，给流命名声明流的操作应该是：\ncudaStream_t a; 定义了一个叫a的流，但是这个流没法用，相当于只有了名字，资源还是要用cudaStreamCreate分配的。 接下来必须要特别注意： 执行异步数据传输时，主机端的内存必须是固定的，非分页的！！ 执行异步数据传输时，主机端的内存必须是固定的，非分页的！！ 执行异步数据传输时，主机端的内存必须是固定的，非分页的！！\n讲内存模型的时候我们说到过，分配方式：\ncudaError_t cudaMallocHost(void **ptr, size_t size); cudaError_t cudaHostAlloc(void **pHost, size_t size, unsigned int flags); 主机虚拟内存中分配的数据在物理内存中是随时可能被移动的，我们必须确保其在整个生存周期中位置不变，这样在异步操作中才能准确的转移数据，否则如果操作系统移动了数据的物理地址，那么我们的设备可能还是回到之前的物理地址取数据，这就会出现未定义的错误。\n在非空流中执行内核需要在启动核函数的时候加入一个附加的启动配置：\nkernel_name\u0026lt;\u0026lt;\u0026lt;grid, block, sharedMemSize, stream\u0026gt;\u0026gt;\u0026gt;(argument list); pStream参数就是附加的参数，使用目标流的名字作为参数，比如想把核函数加入到a流中，那么这个stream就变成a。 前面我们为一个流分配资源，当然后面就要回收资源，回收方式：\ncudaError_t cudaStreamDestroy(cudaStream_t stream); 这个回收函数很有意思，由于流和主机端是异步的，你在使用上面指令回收流的资源的时候，很有可能流还在执行，这时候，这条指令会正常执行，但是不会立刻停止流，而是等待流执行完成后，立刻回收该流中的资源。这样做是合理的也是安全的。 当然，我们可以查询流执行的怎么样了，下面两个函数就是帮我们查查我们的流到哪了：\ncudaError_t cudaStreamSynchronize(cudaStream_t stream); cudaError_t cudaStreamQuery(cudaStream_t stream); 这两条执行的行为非常不同，cudaStreamSynchronize会阻塞主机，直到流完成。cudaStreamQuery则是立即返回，如果查询的流执行完了，那么返回cudaSuccess否则返回cudaErrorNotReady。 下面这段示例代码就是典型多个流中调度CUDA操作的常见模式：\nfor (int i = 0; i \u0026lt; nStreams; i++) { int offset = i * bytesPerStream; cudaMemcpyAsync(\u0026amp;d_a[offset], \u0026amp;a[offset], bytePerStream, streams[i]); kernel\u0026lt;\u0026lt;grid, block, 0, streams[i]\u0026gt;\u0026gt;(\u0026amp;d_a[offset]); cudaMemcpyAsync(\u0026amp;a[offset], \u0026amp;d_a[offset], bytesPerStream, streams[i]); } for (int i = 0; i \u0026lt; nStreams; i++) { cudaStreamSynchronize(streams[i]); } 第一个for中循环执行了nStreams个流，每个流中都是“复制数据，执行核函数，最后将结果复制回主机”这一系列操作。 下面的图就是一个简单的时间轴示意图，假设nStreams=3，所有传输和核启动都是并发的：\nH2D是主机到设备的内存传输，D2H是设备到主机的内存传输。显然这些操作没有并发执行，而是错开的，原因是PCIe总线是共享的，当第一个流占据了主线，后来的就一定要等待，等待主线空闲。编程模型和硬件的实际执行时有差距了。 上面同时从主机到设备涉及硬件竞争要等待，如果是从主机到设备和从设备到主机同时发生，这时候不会产生等待，而是同时进行。 内核并发最大数量也是有极限的，不同计算能力的设备不同，Fermi设备支持16路并发，Kepler支持32路并发。设备上的所有资源都是限制并发数量的原因，比如共享内存，寄存器，本地内存，这些资源都会限制最大并发数。\n流调度 从编程模型看，所有流可以同时执行，但是硬件毕竟有限，不可能像理想情况下的所有流都有硬件可以使用，所以硬件上如何调度这些流是我们理解流并发的关键\n虚假的依赖关系 在Fermi架构上16路流并发执行但是所有流最终都是在单一硬件上执行的，Fermi只有一个硬件工作队列，所以他们虽然在编程模型上式并行的，但是在硬件执行过程中是在一个队列中（像串行一样）。当要执行某个网格的时候CUDA会检测任务依赖关系，如果其依赖于其他结果，那么要等结果出来后才能继续执行。单一流水线可能会导致虚假依赖关系：\n这个图就是虚假依赖的最准确的描述，我们有三个流，流中的操作相互依赖，比如B要等待A的结果，Z要等待Y的结果，当我们把三个流塞到一个队列中，那么我们就会得到紫色箭头的样子，这个硬件队列中的任务可以并行执行，但是要考虑依赖关系，所以，我们按照顺序会这样执行：\n 执行A，同时检查B是否有依赖关系，当然此时B依赖于A而A没执行完，所以整个队列阻塞 A执行完成后执行B，同时检查C，发现依赖，等待 B执行完后，执行C同时检查，发现P没有依赖，如果此时硬件有多于资源P开始执行 P执行时检查Q，发现Q依赖P，所以等待  这种一个队列的模式，会产生一种，虽然P依赖B的感觉，虽然不依赖，但是B不执行完，P没办法执行，而所谓并行，只有一个依赖链的头和尾有可能并行，也就是红圈中任务可能并行，而我们的编程模型中设想的并不是这样的。\nHyper-Q技术 解决上面虚假依赖的最好办法就是多个工作队列，这样就从根本上解决了虚假依赖关系，Hyper-Q就是这种技术，32个硬件工作队列同时执行多个流，这就可以实现所有流的并发，最小化虚假依赖：\n流的优先级 3.5以上的设备可以给流优先级，也就是优先级高的（数字上更小的，类似于C++运算符优先级） 优先级只影响核函数，不影响数据传输，高优先级的流可以占用低优先级的工作。 下面函数创建一个有指定优先级的流\ncudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags,int priority); 不同的设备有不同的优先级等级，下面函数可以查询当前设备的优先级分布情况：\ncudaError_t cudaDeviceGetStreamPriorityRange(int *leastPriority, int *greatestPriority); leastPriority表示最低优先级（整数，远离0） greatestPriority表示最高优先级（整数，数字较接近0） 如果设备不支持优先级返回0\nCUDA事件 CUDA事件不同于我们前面介绍的内存事务，不要搞混，事件也是软件层面上的概念。事件的本质就是一个标记，它与其所在的流内的特定点相关联。可以使用时间来执行以下两个基本任务：\n 同步流执行 监控设备的进展 流中的任意点都可以通过API插入事件以及查询事件完成的函数，只有事件所在流中其之前的操作都完成后才能触发事件完成。默认流中设置事件，那么其前面的所有操作都完成时，事件才出发完成。 事件就像一个个路标，其本身不执行什么功能，就像我们最原始测试c语言程序的时候插入的无数多个printf一样。  创建和销毁 事件的声明如下：\ncudaEvent_t event; 同样声明完后要分配资源：\ncudaError_t cudaEventCreate(cudaEvent_t* event); 回收事件的资源\ncudaError_t cudaEventDestroy(cudaEvent_t event); 如果回收指令执行的时候事件还没有完成，那么回收指令立即完成，当事件完成后，资源马上被回收。\n记录事件和计算运行时间 事件的一个主要用途就是记录事件之间的时间间隔。 事件通过下面指令添加到CUDA流：\ncudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0); 在流中的事件主要左右就是等待前面的操作完成，或者测试指定流中操作完成情况，下面和流类似的事件测试指令（是否出发完成）会阻塞主机线程知道事件被完成。\ncudaError_t cudaEventSynchronize(cudaEvent_t event); 同样，也有异步版本：\ncudaError_t cudaEventQuery(cudaEvent_t event); 这个不会阻塞主机线程，而是直接返回结果和stream版本的类似。 另一个函数用在事件上的是记录两个事件之间的时间间隔：\ncudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop); 这个函数记录两个事件start和stop之间的时间间隔，单位毫秒，两个事件不一定是同一个流中。这个时间间隔可能会比实际大一些，因为cudaEventRecord这个函数是异步的，所以加入时间完全不可控，不能保证两个事件之间的间隔刚好是两个事件之间的。 一段简单的记录事件时间间隔的代码\n// create two events cudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); // record start event on the default stream cudaEventRecord(start); // execute kernel kernel\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(arguments); // record stop event on the default stream cudaEventRecord(stop); // wait until the stop event completes cudaEventSynchronize(stop); // calculate the elapsed time between two events float time; cudaEventElapsedTime(\u0026amp;time, start, stop); // clean up the two events cudaEventDestroy(start); cudaEventDestroy(stop); 这段代码显示，我们的事件被插入到空流中，设置两个事件作为标记，然后记录他们之间的时间间隔。 cudaEventRecord是异步的，所以间隔不准，这是特别要注意的。\n流同步 在研究线程并行的时候我们就发现并行这种一旦开始就万马奔腾的模式，想要控制就要让大家到一个固定的位置停下来，就是同步，同步好处是保证代码有可能存在内存竞争的地方降低风险，第二就是相互协调通信，当然坏处就是效率会降低，原因很简单，就是当部分线程等待的时候，设备有一些资源是空闲的，所以这会带来性能损耗。 同样，在流中也有同步，下面我们就研究一下流同步。 流分成阻塞流和非阻塞流，在非空流中所有操作都是非阻塞的，所以流启动以后，主机还要完成自己的任务，有时候就可能需要同步主机和流之间的进度，或者同步流和流之间的进度。 从主机的角度，CUDA操作可以分为两类：\n 内存相关操作 内核启动  内核启动总是异步的，虽然某些内存是同步的，但是他们也有异步版本。 前面我们提到了流的两种类型：\n 异步流（非空流） 同步流（空流/默认流）  没有显式声明的流式默认同步流，程序员声明的流都是异步流，异步流通常不会阻塞主机，同步流中部分操作会造成阻塞，主机等待，什么都不做，直到某操作完成。 非空流并不都是非阻塞的，其也可以分为两种类型：\n 阻塞流 非阻塞流  虽然正常来讲，非空流都是异步操作，不存在阻塞主机的情况，但是有时候可能被空流中的操作阻塞。如果一个非空流被声明为非阻塞的，那么没人能阻塞他，如果声明为阻塞流，则会被空流阻塞。 有点晕，就是非空流有时候可能需要在运行到一半和主机通信，这时候我们更希望他能被阻塞，而不是不受控制，这样我们就可以自己设定这个流到底受不受控制，也就是是否能被阻塞，下面我们研究如何使用这两种流。\n阻塞流和非阻塞流 cudaStreamCreate创建的是阻塞流，意味着里面有些操作会被阻塞，直到空流中默写操作完成。 空流不需要显式声明，而是隐式的，他是阻塞的，跟所有阻塞流同步。 下面这个过程很重要： 当操作A发布到空流中，A执行之前，CUDA会等待A之前的全部操作都发布到阻塞流中，所有发布到阻塞流中的操作都会挂起，等待，直到在此操作指令之前的操作都完成，才开始执行。 有点复杂，因为这涉及到代码编写的过程和执行的过程，两个过程混在一起说，肯定有点乱，我们来个例子压压惊就好了：\nkernel_1\u0026lt;\u0026lt;\u0026lt;1, 1, 0, stream_1\u0026gt;\u0026gt;\u0026gt;(); kernel_2\u0026lt;\u0026lt;\u0026lt;1, 1\u0026gt;\u0026gt;\u0026gt;(); kernel_3\u0026lt;\u0026lt;\u0026lt;1, 1, 0, stream_2\u0026gt;\u0026gt;\u0026gt;(); 上面这段代码，有三个流，两个有名字的，一个空流，我们认为stream_1和stream_2是阻塞流，空流是阻塞的，这三个核函数都在阻塞流上执行，具体过程是，kernel_1被启动，控制权返回主机，然后启动kernel_2，但是此时kernel_2 不会并不会马山执行，他会等到kernel_1执行完毕，同理启动完kernel_2 控制权立刻返回给主机，主机继续启动kernel_3,这时候kernel_3 也要等待，直到kernel_2执行完，但是从主机的角度，这三个核都是异步的，启动后控制权马上还给主机。 然后我们就想创建一个非阻塞流，因为我们默认创建的是阻塞版本：\ncudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags); 第二个参数就是选择阻塞还是非阻塞版本：\ncudaStreamDefault;// 默认阻塞流 cudaStreamNonBlocking: //非阻塞流，对空流的阻塞行为失效。 如果前面的stream_1和stream_2声明为非阻塞的，那么上面的调用方法的结果是三个核函数同时执行。\n隐式同步 前面几章核函数计时的时候，我们说过要同步，并且提到过cudaMemcpy 可以隐式同步，也介绍了\ncudaDeviceSynchronize; cudaStreamSynchronize; cudaEventSynchronize; 这几个也是同步指令，可以用来同步不同的对象，这些是显式的调用的；与上面的隐式不同。 隐式同步的指令其最原始的函数功能并不是同步，所以同步效果是隐式的，这个我们需要非常注意，忽略隐式同步会造成性能下降。所谓同步就是阻塞的意思，被忽视的隐式同步就是被忽略的阻塞，隐式操作常出现在内存操作上，比如：\n 锁页主机内存分布 设备内存分配 设备内存初始化 同一设备两地址之间的内存复制 一级缓存，共享内存配置修改  这些操作都要时刻小心，因为他们带来的阻塞非常不容易察觉\n显式同步 显式同步相比就更加光明磊落了，因为一条指令就一个作用，没啥副作用，常见的同步有：\n 同步设备 同步流 同步流中的事件 使用事件跨流同步  下面的函数就可以阻塞主机线程，直到设备完成所有操作：\ncudaError_t cudaDeviceSynchronize(void); 这个函数我们前面常用，但是尽量少用，这个会拖慢效率。 然后是流版本的，我们可以同步流，使用下面两个函数：\ncudaError_t cudaStreamSynchronize(cudaStream_t stream); cudaError_t cudaStreamQuery(cudaStream_t stream); 这两个函数，第一个是同步流的，阻塞主机直到完成，第二个可以完成非阻塞流测试。也就是测试一下这个流是否完成。 我们提到事件，事件的作用就是在流中设定一些标记用来同步，和检查是否执行到关键点位（事件位置），也是用类似的函数\ncudaError_t cudaEventSynchronize(cudaEvent_t event); cudaError_t cudaEventQuery(cudaEvent_t event); 这两个函数的性质和上面的非常类似。 事件提供了一个流之间同步的方法：\ncudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event); 这条命令的含义是，指定的流要等待指定的事件，事件完成后流才能继续，这个事件可以在这个流中，也可以不在，当在不同的流的时候，这个就是实现了跨流同步。 如下图\n可配置事件 CDUA提供了一种控制事件行为和性能的函数：\ncudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags); 其中参数是：\ncudaEventDefault cudaEventBlockingSync cudaEventDisableTiming cudaEventInterprocess 其中cudaEventBlockingSync指定使用cudaEventSynchronize同步会造成阻塞调用线程。cudaEventSynchronize默认是使用cpu周期不断重复查询事件状态，而当指定了事件是cudaEventBlockingSync的时候，会将查询放在另一个线程中，而原始线程继续执行，直到事件满足条件，才会通知原始线程，这样可以减少CPU的浪费，但是由于通讯的时间，会造成一定的延迟。 cudaEventDisableTiming表示事件不用于计时，可以减少系统不必要的开支也能提升cudaStreamWaitEvent和cudaEventQuery的效率 cudaEventInterprocess表明可能被用于进程之间的事件\n总结 这一篇理论多，验证少，如果看不懂，不妨先看后面的例子，再回来研究理论。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-1-%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E6%A6%82%E8%BF%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA中流和事件的理论描述。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 流，事件\u003c/p\u003e","title":"【CUDA 基础】6.1 流和事件概述"},{"content":"Abstract: 本文是第六章的概述，本章也是Freshman的最后一个章节。 Keywords: 流，事件，网格级并行，同步机制，NVVP\n流和并发 本文是Freshman系列的最后一篇，考虑到接下来要说的是比较高级的内容，所以把其划分到下个系列中，作为进阶内容介绍，所以本章是初级阶段的收尾。\n本章内容 本章主要介绍下面内容：\n 理解流和事件的本质 理解网格级并发 重叠内核执行和数据传输 重叠CPU执行和GPU执行 理解同步机制 调整流的优先级 注册设备回调函数 通过NVIDIA可视化性能分析器显示应用程序执行时间轴  一般来说CUDA程序有两个几倍的并发：\n 内核级并行 网格级并行  我们前面说有都是在研究内核级别的并行，通过同一内核多线程的并行来完成并行计算，提高内核级别并行我们前面用了基本所有的篇幅介绍了以下三种途径：\n 编程模型 执行模型 内存模型  这三个角度是优化内核级并行的最主要也是最基础的方法，更高级的方法虽然高级但是提升效率幅度绝没有这三种基础角度来的更有效率。 本章我们在内核之上研究并行，也就是多个内核的并行，这在一个完整应用中是很常见的，实际中的应用程序多半都不是单个内核的，多个内核最大程度的并行也就是最大限度的使用GPU设备，是提高整个应用效率的关键。\n总结 本章我们考虑只在一个设备上并行内核，使用CUDA流实现网格级并发，还会使用NVVP显示内核并行执行可视化。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-6-0-%E6%B5%81%E5%92%8C%E5%B9%B6%E5%8F%91.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是第六章的概述，本章也是Freshman的最后一个章节。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 流，事件，网格级并行，同步机制，NVVP\u003c/p\u003e","title":"【CUDA 基础】6.0 流和并发"},{"content":"Abstract: 本文介绍实数在计算机内的浮点数表示方法 Keywords: 浮点表示，IEEE754\n实数的浮点表示 好久没更新数值分析了，上一篇我们研究了十进制和二进制之间的转换，根本目的就是想让我们从我们熟悉的十进制计算体系转换到二进制的计算机的计算体系来。我们称之为浮点数，浮点数有很多版本，当然众多版本中有很多都是用在特定设备上的，IEEE有明确的关于浮点数的标准，我们今天介绍的是在IEEE754标准下的浮点数，如果你能做CPU你也可以做你自己的浮点数标准，然后让一群人按照你的标准开发程序，那都是有可能的，但是如果我们使用通用的CPU，我们还是按照标准来比较合理。 我们一直以来学习计算做大量的计算题都是基于有限位精确度的，并且我们很少研究当我们做所谓的四舍五入的时候，对数字本身有什么影响，比如我们最初学的无限循环小数，$\\frac{10}{3}=0.333\\dots$ 我们在学习这个数字的近似表示的时候，我们会说如果想保留两位小数，那么我们舍去第三位后面的循环，直接得到 $0.33$ 这时候，$0.33$ 和 $0.333\\dots$ 不是一个数字，而是两个完全不同的实数，中间存在差距 $0.000 333\\dots$ 这就是我们在10进制中计算过程中产生的误差，这种舍入是无法避免的，毕竟在实数中无限位的数字非常之多。 如果你说这个误差非常小可以忽略，小学的时候是这么讲的，但是当我们用舍入后的结果和一个很大的数字相乘的时候，或者在高斯消元，微分方程的求解这些简单的过程中，这个极小误差就会被放大。这些问题同样存在于二进制中，因为我们用无法用有限的位来表示无限的小数，我们必须时刻注意小心计算机的小误差使得计算结果不可靠的危险，因为这可以帮你避免在一个不错的模型下而得不到好结果时是误差再作祟。\n浮点格式 还记得科学计数法么？十进制科学计数法有一定的规则，把数字写成规定的一个大于1小于10的实数和10的整数幂的乘法形式： $$ \\pm 1.aaaaa\\dots\\times 10^m $$ 同样在二进制浮点数中也是一样规定： $$ \\pm 1.aaaaa\\dots\\times 2^p $$ 进制改变我们把10变成了2，但是如果想把上面这种表示写入内存，就需要提前规定，比如aaaa这些a要占多少位，p要占多少位， $\\pm$ 怎么表示，于是IEEE就给出了下面这个标准：\n   精度 符号 指数 尾数     单精度 1 8 23   双精度 1 11 52   长双精度 1 15 64    上面就是在内存中不同浮点数类型所占的内存位数，注意是位，而不是字,字节等其他单位。 符号位占一位这个好解释，指数是指p，单精度指数最多有8位，尾数是指 $aaaa\\dots$ ，其在单精度中最多有23位空间，注意每一位上只能是0或者1，因为这是二进制下的表示。 符号位1表示负号，0表示正号 举个例子： $$ +1.01\\times 2^{101} $$ 以单精度浮点数保存在内存中是这样的：\n   0 00000101 01000000000000000000000     符号位 指数 尾数    这是完整的在单精度浮点数中内存的样子 接着我们来看一个定义\n 定义0.1 机器精度对应的数字，记做 $\\varepsilon_{mach}$ 是1和比1大的最小的小浮点数之间的距离，对于IEEE双精度浮点数表示： $$ \\varepsilon_{mach}=2^{-52} $$\n 这是关于机器精度的定义，这个和我们在十进制科学计数法中关于有效数字有关系，注意这里是1和比1大的最小数字之间的差距，这个1是不能变的，因为是二进制，如果你说2和比2大的最小数字，在浮点数中我们要退一位，从机器内存储的角度看，还是和1比较，同样方法用于任何数字，精度和数字大小无关，也就是和指数没有关系，只和尾数有关系，或者说和尾数所占位数的长度有关，这一点是可以理解的，最大精度由最末尾的位来决定。 那么问题又来了，十进制数我们在小数无限的时候取近似有四舍五入的法则，那么二进制在无限小数取近似的时候是用什么法则呢？ 取近似就是截断无限长的小数尾巴，我们有多种方式：\n 截断，直接丢弃一定长度后面的数字而不管后面的数字是什么 舍入，十进制中四舍五入就是舍入  截断不可取是因为会造成系统性误差，截断后总是朝着更小的方向迈进，我们希望整个系统是无偏的，虽然有误差，我们希望不要有系统误差，这个在数理统计里面也会介绍。 相比之下舍入就好的多，因为他有可能向上也有可能向下，也就是近似结果可能大于精确值，也可能小于精确值，当我们保证两种概率相同的时候，那么这就是个无偏的系统。 二进制舍入规则，对于双精度浮点数，52位尾数：\n 53位是0的时候，舍去52后面的数字 53位是1的时候，且52位之后不是 $1000\\dots$ ，52位加1，然后舍去52位后面的数字 当52位后面的数字是 $1000\\dots$ 的时候，根据52位的数字选择是进位还是直接舍去，如果52位是1，则52位加1后截断否则直接截断  第3条看着有点怪，但是却是非常重要的一条，因为第53开始是 $1000\\dots$ 的数字刚好是进位和不进位的中间，如果规定这个数字向任何一方都会导致一方比另一方的概率变大，这样就会出现偏差，虽然很小，但是为了避免这个误差出现，我们观察52位，我们假定52位在大量样本下是均匀分布的，那么我们以此作为进位与否的标志，就能得到一个无偏的方法。 当存在系统误差的时候，计算结果会出现漂移现象，就是朝着偏大或者偏小一直移动。 不得不说，第3条准则是非常重要，其保证不会出现漂移。\n 定义0.2 将IEEE双精度浮点数字记做 $x$ 利用舍入最近法则记做 $\\text{fl}(x)$\n 接下来我们就要研究当把一个十进制数转换到二级制浮点数的时候舍入所造成的误差。 例子： 十进制数9.4表示成二进制数是 $1.0010\\overline{1100}$ 其在第53位开始往后是 $\\overline{1100}$ 因为53位是1，且54位也是1，所以我们要在52位上加1，然后舍去53位及之后的所有尾数，那么按照这个过程，我们得到的实际的数字是：\n 计算截断部分的十进制数 $$ 0.\\overline{1100}\\times2^3\\times2^{-52}=0.4\\times 2^{-48} $$ 计算包含误差后的值 $$ fl(9.4)=9.4+2^{-52}\\times 2^{3}-0.4\\times 2^{-48}\\ =9.4+0.2\\times 2^{-49} $$  我们存入计算机的十进制数和实际在计算机中的这个数并不一致，而且多数情况下是不一致的，通过舍入保存的数字和实际的十进制数之间的差，我们称为舍入误差。 上面例子中我们存入计算基的是9.4，但是由于计算机的本身的局限性，只能得到 $9.4+0.2\\times 2^{-49}$ 这个近似值。\n这是绝度的误差，当指数变大的的时候这个数字会变得非常大，所以舍入误差具体值对我们来说没什么参考意义，但是相对误差就很有意义了，因为相对误差抛弃了指数，而考虑比值。\n 定义0.3 令 $x_c$ 是计算机版本的 $x$ 精确度量，那么：\n  绝对误差： $|x-x_c|$ 相对误差：$\\frac{|x-x_c|}{x}$ 其中 $x\\neq 0$  一个结论，在IEEE计算机中相对误差不会超过机器精度的一半： $$ \\frac{|x-x_c|}{x}\\leq \\frac{1}{2}\\varepsilon_{mach} $$\n这个证明比较简单，因为我们的尾数最后一位是进位的时候，我们原始数字和近似后的数字相差在第53位而且肯定小于其一半，所以就有了 $\\frac{1}{2}$ 也可以通过计算验证，这里就不详细讲了，带个数字进去就能验证这个结论。\n机器表示 到目前为止，我们学了怎么得到一个要在内存中存储的数字，接着我们就要看看如何在计算机中实现。 双精度浮点在内存中占64位，也就是八字节： $$ se_1e_2\\cdots e_11b_1b_2\\cdots b_52 $$\n 第一位s是符号位的个没什么说的 第二位开始的11位表示指数，为正数；考虑到指数也有正负，这里没有设置符号位，而是通过让指数去叠加 $2^{10}-1=1023$ 来得到，这样指数范围就是 $[-1022,1023]$ 之间。由于特殊目的，没有使用 $[0,2047]$ 这个我们后面会说 后面的 $b$ 就是尾数了  指数中的1023称为双精度格式的指数偏差，起作用就是把负指数转化成正数保存，单精度指数偏差是127，长双精度是16383 通过MATLAB的format hex使用16个连续的16进制数表示64位的双精度浮点数在计算机中的样子，比如数字1，在计算机中的样子如下： 我们看一下1.0的表示：\n对应的二进制：\n   0 01111111111 0000 $\\cdots$ 0     符号位 指数（加了1023） 尾数52位    9.4的表示：\n9.4的二进制大家自己画画吧 我们接着要说三个特殊的数字，指数有11位，那么可以表示0~2047共2048个数，但是我们这里只有-1022~1023是指数可取值，共有2046个数，剩下两个数字-1023和1024做了什么；按照指数要加1023的规则，我们说的-1023和1024在内存中是以0，和2047存在的，那么我们规定下面的特殊情况：\n   机器数 例子 十六进制数     +Inf 1/0 7FF0 0000 0000 0000   -Inf -1/0 FFF0 0000 0000 0000   Nan 0/0 FFFx xxxx xxxx xxxx    其中x表示0或者1的任意组合且不全为0 当指数是在内存中是2047的时候，如果尾数都是0，那么表示 $\\infty$ Inf，否则表示 NaN（Not a Number）不是一个数字，$-\\infty$ 对应的就是把Inf的符号位设置为1，也就是从7FF变成了FFF\n指数都是1的研究完了，还有一个0，当指数是0的时候，数字解释为非标准的浮点数字： $$ \\pm0.b_1b_2\\dots b_52\\times 2^{-1022} $$ 小数点前面被设置为0，这是非标准的，这种与规定不符的数字，我们叫做异常(subnormal)浮点数. 根据这个异常我们的指数最小时 -1023，此时小数点前面没数字，那么当我们的尾数是 $0000\\cdots 0001$ 的时候，那么此时是双精度浮点数能表示的最小的数字 $2^{-52}\\times 2^{-1022}=2^{-1074}$ 不能更小了。 另一个重要的特征是我们有两种0的表示，+0和-0相差一个符号位：\n浮点数加法 浮点数大概的规则和表达我们说完了，接着就看看浮点数怎么计算了，最基本的从加法开始，加法的过程如下：\n 将两个数字的对齐，所谓对齐就是对齐小数点。 在特定的寄存器存储完成加法，寄存器能存储多于52的精度 计算完成后进行舍入，变回52位尾数。  来个例子，我们用 $2^{-53}+1$ 来描述描述一下过程： $$ \\begin{aligned} \u0026amp;1.\\boxed{00\\cdots 0} \\times 2^0+1.\\boxed{00\\cdots 0}\\times 2^{-53}\\ =\u0026amp;1.\\boxed{0000000000000000000000000000000000000000000000000000}\\times 2^0\\ +\u0026amp;0.\\boxed{0000000000000000000000000000000000000000000000000000}1\\times 2^0\\ \u0026amp;\\hline\\ =\u0026amp;1.\\boxed{0000000000000000000000000000000000000000000000000000}1\\times 2^0\\ =\u0026amp;1.\\boxed{0000000000000000000000000000000000000000000000000000}\\times 2^0 \\end{aligned} $$\n上面的过程的实验结果如下: 两个数字相加后没变化，有这个性质的最大浮点数是： $2^{-53}$ 其他更大的数字加到1上都会比1大。 注意区别相对大小和绝对大小， $\\varepsilon_{math}$ 是相对误差的精度，不是绝对误差，也就是说比 $\\varepsilon_{math}$ 的数字并不能直接省略，而是要看这两个数的相对大小，这也就是大数吃小数可以而小数吃小数却不行的原因。 舍入不差常常带来意想不到的结果，比如 $9.4-9.0-0.4$ 的结果并不是 $0$\n具体原因是每一步都存在舍入误差，所以，最后的结果产生了误差累积，且到了影响最后舍入结果的程度，所以会造成以上结果。\n这些特殊的结果的原因就是舍入带来的，这也是计算机计算和精确计算之间的差别。 本文中所有描述的过程在实际计算中都是可以预测的，给出的舍入原则是一个通用的原则，也可以在不同的环境下使用不同原则的舍入原则。 不同舍入原则可以一起用来判断计算的稳定性。 舍入误差是非常凶险的 ，一个小小的舍入误差可能使得整个计算功亏一篑，这看起来不可思议，但却影响深远，后面我们会研究如何处理这种情况\n总结 本文主要研究在计算中最常使用的浮点数的性质和在计算机中的各种相关规则，舍入误差是重点。\n","permalink":"https://go.face2ai.com/math/math-numerical-analysis-0-3-float.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍实数在计算机内的浮点数表示方法\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 浮点表示，IEEE754\u003c/p\u003e","title":"【数值分析】0.3 实数的浮点表示"},{"content":"Abstract: 本文介绍线程束洗牌指令的用法 Keywords: 线程束洗牌指令\n线程束洗牌指令 前面介绍了共享内存，常量内存，只读内存的使用，今天我们来研究一个比较特殊的机制，名字也很特殊，叫做线程束洗牌指令。 支持线程束洗牌指令的设备最低也要3.0以上， 洗牌指令，shuffle instruction作用在线程束内，允许两个线程见相互访问对方的寄存器。这就给线程束内的线程相互交换信息提供了了一种新的渠道，我们知道，核函数内部的变量都在寄存器中，一个线程束可以看做是32个内核并行执行，换句话说这32个核函数中寄存器变量在硬件上其实都是邻居，这样就为相互访问提供了物理基础，线程束内线程相互访问数据不通过共享内存或者全局内存，使得通信效率高很多，线程束洗牌指令传递数据，延迟极低，切不消耗内存 线程束洗牌指令是线程束内线程通讯的极佳方式。 我们先提出一个叫做束内线程的概念，英文名lane，简单的说，就是一个线程束内的索引，所以束内线程的ID在 $【0,31】$ 内，且唯一，唯一是指线程束内唯一，一个线程块可能有很多个束内线程的索引，就像一个网格中有很多相同的threadIdx.x 一样，同时还有一个线程束的ID，可以通过以下方式计算线程在当前线程块内的束内索引，和线程束ID：\nunsigned int LaneID=threadIdx.x%32; unsigned int warpID=threadIdx.x/32; 根据上面的计算公式，一个线程块内的threadIdx.x=1,33,65等对应的laneID都是1\n线程束洗牌指令的不同形式 线程束洗牌指令有两组：一组用于整形变量，另一种用于浮点型变量。一共有四种形式的洗牌指令。 在线程束内交换整形变量，其基本函数如下：\nint __shfl(int var,int srcLane,int width=warpSize); 这个指令必须好好研究一下，因为这里的输入非常之乱，谁乱？var乱，一个int值，这个变量很明显是当前线程中的一个变量，作为输入，其传递的给函数的并不是这个变量存储的值，而是这个变量名，换句话说，当前线程中有var这个变量，比如我们说1号线程的var值是1，那么2号线程中的var值不一定是1，所以，这个__shfl返回的就是var值，哪个线程var值呢？srcLane这个线程的，srcLane并不是当前线程的束内线程，而是结合with计算出来的相对线程位置，比如我想得到3号线程内存的var值，而且width=16，那么就是，0~15的束内线程接收0+3位置处的var值，也就是3号束内线程的var值，16~32的束内线程接收16+3=19位置处的var变量。 这个是非常重要的，虽然有些困难，但是却相当灵活。width的默认参数是32.srcLane我们后面简单的叫他束内线程，注意我们必须心理明白只有width是默认值的时候，他才是真正的束内线程。 图示如下\n接着是另一个指令，其主要特点是从与调用线程相关的线程中复制数据。\nint __shfl_up(int var,unsigned int delta,int with=warpSize); 这个函数的作用是调用线程得到当前束内线程编号减去delta的编号的线程内的var值，with和__shfl中都一样，默认是32，作用效果如下：\n如果是with其他值，我们可以根据前面的讲解，把线程束再分成若干个大小为with的块，进行上图的操作。 最左边两个元素没有前面的delta号线程，所以不做任何操作，保持原值。\n同样下一个指令是上面的反转版本：\nint __shfl_down(int var,unsigned int delta,int with=warpSize); 作用和参数和up一模一样，图示如下：\n最后一个洗牌指令比较夸张，也是很灵活的一个指令\nint __shfl_xor(int var,int laneMask,int with=warpSize); xor是异或操作，这个指令如果学过硬件或者c语言学的比较扎实的人可能知道，这是电路里面最最最最最重要的操作，没有之一，什么是异或？逻辑中我们假设只有0，1两种信号，用 “^”表示异或:\n0^0=0; 1^0=1; 0^1=1; 1^1=0; 二元操作，只要两个不同就会得到真，否则为假 那么我们的__shfl_xor操作就是包含抑或操作在内的洗牌指令，怎么算呢？ 如果我们输入的laneMask是1，其对应的二进制是 $000\\cdots001$ ,当前线程的索引是0~31之间的一个数，那么我们用laneMask与当前线程索引进行抑或操作得到的就是目标线程的编号了，这里laneMask是1，那么我们把1与0~31分别抑或就会得到：\n000001^000000=000001; 000001^000001=000000; 000001^000010=000011; 000001^000011=000010; 000001^000100=000101; 000001^000101=000100; . . . 000001^011110=011111; 000001^011111=011110; 这就是当前线程的束内线程编号和目标线程束内县城编号之间的对应关系，画成图会非常犀利：\n这就是4个线程束洗牌指令对整形的操作了。对应的浮点型不需要该函数名，而是只要把var改成float就行了，函数就会自动重载了。\n线程束内的共享内存数据 接下来我们用代码实现以下，看看每一个指令的作用效果，洗牌指令可以用于下面三种整数变量类型中：\n 标量变量 数组 向量型变量  跨线程束值的广播 这个就是 __shfl函数作用结果了，代码如下\n__global__ void test_shfl_broadcast(int *in,int*out,int const srcLans) { int value=in[threadIdx.x]; value=__shfl(value,srcLans,BDIM); out[threadIdx.x]=value; } 这里面的过程就不用说了，注意var参数对应value就是我们要找的目标，srcLane这里是2，所以，我们取得了2号书内线程的value值给了当前线程，于是所有束内线程的value都是2了： 计算结果：\n线程束内上移 这里使用__shfl_up指令进行上移。代码如下\n__global__ void test_shfl_up(int *in,int*out,int const delta) { int value=in[threadIdx.x]; value=__shfl_up(value,delta,BDIM); out[threadIdx.x]=value; } 运行结果：\n线程束内下移 这里使用__shfl_down指令进行上移。代码如下\n__global__ void test_shfl_down(int *in,int*out,int const delta) { int value=in[threadIdx.x]; value=__shfl_down(value,delta,BDIM); out[threadIdx.x]=value; } 运行结果：\n线程束内环绕移动 然后是循环移动，我们修改__shfl中的参数，把静态的目标改成一个动态的目标，如下：\n__global__ void test_shfl_wrap(int *in,int*out,int const offset) { int value=in[threadIdx.x]; value=__shfl(value,threadIdx.x+offset,BDIM); out[threadIdx.x]=value; } 当offset=2的时候，得到结果：\n前14个元素的值是可以预料到的，但是14号，15号并没有像__shfl_down那样保持不变，而是获得了0号和1号的值，那么我们有必要相信，__shfl中计算目标线程编号的那步有取余操作，对with取余，我们真正得到的数据来自\nsrcLane=srcLane%width; 这样就说的过去了，同理我们通过将srclane设置成-2的话就能得到对应的向上的环绕移动。\n跨线程束的蝴蝶交换 接着我们看看__shfl_xor像我说的这个操作非常之灵活，可以组合出任何你想要的到的变换，我们先来个简单的就是我们上面讲原理的时候得到的结论：\n__global__ void test_shfl_xor(int *in,int*out,int const mask) { int value=in[threadIdx.x]; value=__shfl_xor(value,mask,BDIM); out[threadIdx.x]=value; } mask我们设置成1，然后就能得到下面的结果：\n忍不住画了个叉，哈哈 这些都是预料之中的，接着我们看点高级的。也解释下为什么说可以操作数组，好吧我之前也蒙了。\n跨线程束交换数组值 我们要交换数组了，假如线程内有数组，然后我们交换数组的位置，我们可以用下面代码实现一个简单小数组的例子：\n__global__ void test_shfl_xor_array(int *in,int*out,int const mask) { //1.  int idx=threadIdx.x*SEGM; //2.  int value[SEGM]; for(int i=0;i\u0026lt;SEGM;i++) value[i]=in[idx+i]; //3.  value[0]=__shfl_xor(value[0],mask,BDIM); value[1]=__shfl_xor(value[1],mask,BDIM); value[2]=__shfl_xor(value[2],mask,BDIM); value[3]=__shfl_xor(value[3],mask,BDIM); //4.  for(int i=0;i\u0026lt;SEGM;i++) out[idx+i]=value[i]; } 有逻辑的地方代码就会变得复杂，我们从头看，首先我们定义了一个宏SEGM为4，然后每个线程束包含一个SEGM大小的数组，当然，这些数据数存在寄存器中的，如果数组过大可能会溢出到本地内存中，不用担心，也在片上，这个数组比较小，寄存器足够了。 我们看每一步都做了什么\n 计算数组的起始地址，因为我们的输入数据是一维的，每个线程包含其中长度为SEGM的一段数据，所以，这个操作就是计算出当前线程对应的数组的起始位置 声明数组，在寄存器中开辟地址，这句编译时就会给他们分配地址，然后从全局内存中读数据。 计算当前线程中数组中的元素，与要交换的目标的线程的之间的抑或，此时mask为1，那么就相当于将多个寄存器变量进行跨线程束的蝴蝶交换 将寄存器内的交换结果写会到全局内存  这个看起来有点复杂，但是其实就是把上面的蝴蝶交换重复执行。\n大蝴蝶~ 跨线程束不是跨越线程束，而是横跨当前线程束的意思，这个标题有点让人迷惑。\n跨线程束使用数组索引交换数值 接下来这个是个扩展了，交换了两个之间的一对值，并且这里是我们第一次写设备函数，也就是只能被核函数调用的函数：\n__inline__ __device__ void swap(int *value,int laneIdx,int mask,int firstIdx,int secondIdx) { bool pred=((laneIdx%(2))==0); if(pred) { int tmp=value[firstIdx]; value[firstIdx]=value[secondIdx]; value[secondIdx]=tmp; } value[secondIdx]=__shfl_xor(value[secondIdx],mask,BDIM); if(pred) { int tmp=value[firstIdx]; value[firstIdx]=value[secondIdx]; value[secondIdx]=tmp; } } __global__ void test_shfl_swap(int *in,int* out,int const mask,int firstIdx,int secondIdx) { //1.  int idx=threadIdx.x*SEGM; int value[SEGM]; for(int i=0;i\u0026lt;SEGM;i++) value[i]=in[idx+i]; //2.  swap(value,threadIdx.x,mask,firstIdx,secondIdx); //3.  for(int i=0;i\u0026lt;SEGM;i++) out[idx+i]=value[i]; } 这个过程有点复杂，这里面每一句指令的意思都很明确，而且与上面数组交换类似\n 和上面数组交换类似，不赘述 交换数组内的first和second，然后xor在second位置的元素，然后再次重新交换first和second的元素 写入全局变量。  2的描述看起来简单，但是实际上比较麻烦，我们画个图来表示：\n对照代码每一步变换的过程都画了绿线，所以看起来还是好理解的，运行结果：\n使用线程束洗牌指令的并行规约 前面我们已经很详细的介绍过归约算法了，从线程块之间到线程间的归约我们都进行了研究，包括使用共享内存以及各种展开方式，今天我们使用线程束洗牌指令完成归约，主要目标就是减少线程间数据传递的延迟，达到更快的效率： 我们主要考虑三个层面的归约：\n 线程束级归约 线程块级归约 网格级归约  一个线程块有过个线程束，每个执行自己的归约，每个线程束不适用共享内存，而是使用线程束洗牌指令，代码如下：\n__inline__ __device__ int warpReduce(int localSum) { localSum += __shfl_xor(localSum, 16); localSum += __shfl_xor(localSum, 8); localSum += __shfl_xor(localSum, 4); localSum += __shfl_xor(localSum, 2); localSum += __shfl_xor(localSum, 1); return localSum; } __global__ void reduceShfl(int * g_idata,int * g_odata,unsigned int n) { //set thread ID  __shared__ int smem[DIM]; unsigned int idx = blockDim.x*blockIdx.x+threadIdx.x; //convert global data pointer to the  //1. \tint mySum=g_idata[idx]; int laneIdx=threadIdx.x%warpSize; int warpIdx=threadIdx.x/warpSize; //2. \tmySum=warpReduce(mySum); //3. \tif(laneIdx==0) smem[warpIdx]=mySum; __syncthreads(); //4. \tmySum=(threadIdx.x\u0026lt;DIM)?smem[laneIdx]:0; if(warpIdx==0) mySum=warpReduce(mySum); //5.  if(threadIdx.x==0) g_odata[blockIdx.x]=mySum; } 代码解释:\n 从全局内存读取数据，计算线程束ID和当前线程的束内线程ID 计算当前线程束内的归约结果，使用的xor，这里需要动手计算下每个线程和这些2的幂次计算的结果因为每个线程束只有32个线程，所以二进制最高位就是16，那么xor 16 是计算0+16，1+17，2+18，这些位置的和,计算完成后前16位是结果，16到31是一样结果，重复了一边，同理xor 8是计算0+8，1+9，2+10,\u0026hellip;,前8位结果有效，后面是复制前面的答案，最后就得到当前线程束的归约结果。 然后把线程束结果存储到共享内存中 然后继续2中的过程计算3中得到的数据，完整的重复 5.将最后结果存入全局内存  其他几个核函数前面文章都介绍过，我们通过实践可以看出使用线程束洗牌指令进行的归约效率最高。主要原因是使用寄存器进行数据交换而不需要任何位置的内存介入。 本文完整的代码在github:https://github.com/Tony-Tan/CUDA_Freshman（欢迎随手star😝 ）\n总结 本文介绍线程束洗牌指令的一些用法，其吸引人的地方就是不需要通过内存进行线程间数据交换，具有非常高的性能。 至此我们已经完成了第五章的学习，后面我们进入流和事件相关知识的学习。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-5-6-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%B4%97%E7%89%8C%E6%8C%87%E4%BB%A4.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍线程束洗牌指令的用法\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 线程束洗牌指令\u003c/p\u003e","title":"【CUDA 基础】5.6 线程束洗牌指令"},{"content":"Abstract: 本文介绍另外两种内存——常量内存，只读缓存 Keywords: CUDA常量内存，CUDA只读缓存\n常量内存 常量内存 本文介绍常量内存和只读缓存，常量内存是专用内存，他用于只读数据和线程束统一访问某一个数据，常量内存对内核代码而言是只读的，但是主机是可以修改（写）只读内存的，当然也可以读。 注意，常量内存并不是在片上的，而是在DRAM上，而其有在片上对应的缓存，其片上缓存就和一级缓存和共享内存一样， 有较低的延迟，但是容量比较小，合理使用可以提高内和效率，每个SM常量缓存大小限制为64KB。 我们可以发现，所有的片上内存，我们是不能通过主机赋值的，我们只能对DRAM上内存进行赋值。 每种内存访问都有最优与最坏的访问方式，主要原因是内存的硬件结构和底层设计原因，比如全局内存按照连续对去访问最优，交叉访问最差，共享内存无冲突最优，都冲突就会最差，其根本原因在于硬件设计，而我们的常量内存的最优访问模式是线程束所有线程访问一个位置，那么这个访问是最优的。如果要访问不同的位置，就要编程串行了，作为对比，这种情况相当于全局内存完全不连续，共享内存的全部冲突。 数学上，一个常量内存读取成本与线程束中线程读取常量内存地址个数呈线性关系。 常量内存的声明方式：\n__constant 常量内存变量的生存周期与应用程序生存周期相同，所有网格对声明的常量内存都是可以访问的，运行时对主机可见，当CUDA独立编译被使用的，常量内存跨文件可见，这个要后面才会介绍。 初始化常量内存使用一下函数完成\ncudaError_t cudaMemcpyToSymbol(const void *symbol, const void * src, size_t count, size_t offset, cudaMemcpyKind kind) 和我们之前使用的copy到全局内存的函数类似，参数也类似，包含传输到设备，以及从设备读取，kind的默认参数是传输到设备。\n使用常量内存实现一维模板 在数值分析中经常使用模板操作，好吧，我们还没学数值分析的这一课，但是说个其他名词你肯定特别熟悉——卷积，一维的卷积就是我们今天要写的例子，所谓模板操作就是把一组数据中中间连续的若干个产生一个输出：\n紫色的输入数据通过某些运算产生一个输出——绿色的块，这个在图像处理里面用处非常多，当然图像中使用的是二维的数据，我们来看一下公式： $$ f\u0026rsquo;(x)\\approx c_3(f(x+4h)-f(x-4h))+c_2(f(x+3h)-f(x-3h))\\ +c_1(f(x+2h)-f(x-2h))+c_0(f(x+h)-f(x-h)) $$ 此公式计算函数的导数，使用八阶差分来近似. 注意，这里的参数写的跟书中的不一样，也就是系数 $c_0$ 对应原文 $c_3$ 分析算法，整个算法中我们的 $c_i,0\u0026lt;i\u0026lt;4$ 被所有线程使用，对于某一固定计算我们可以把他声明到寄存器中，但是如果数量较大或者不针对某一组 $c$ 的时候，就需要动态的传递，那么此时，常量内存会是最好的选择，因为其对于核函数的只读属性，线程束内以广播形式访问，也就是32个线程只需要一个内存事务就能完成读取，这样效率是非常高的。 我们观察上面图片，可以发现，每个输入数据要被使用8次，如果每次都从全局内存读，这个延迟过高，所以结合上一篇讲到的共享内存，用共享内存缓存输入数据，得到下面的代码：\n__global__ void stencil_1d(float * in,float * out) { //1.  __shared__ float smem[BDIM+2*TEMP_RADIO_SIZE]; //2.  int idx=threadIdx.x+blockDim.x*blockIdx.x; //3.  int sidx=threadIdx.x+TEMP_RADIO_SIZE; smem[sidx]=in[idx]; //4.  if (threadIdx.x\u0026lt;TEMP_RADIO_SIZE) { if(idx\u0026gt;TEMP_RADIO_SIZE) smem[sidx-TEMP_RADIO_SIZE]=in[idx-TEMP_RADIO_SIZE]; if(idx\u0026lt;gridDim.x*blockDim.x-BDIM) smem[sidx+BDIM]=in[idx+BDIM]; } __syncthreads(); //5.  if (idx\u0026lt;TEMP_RADIO_SIZE||idx\u0026gt;=gridDim.x*blockDim.x-TEMP_RADIO_SIZE) return; float temp=.0f; //6.  #pragma unroll  for(int i=1;i\u0026lt;=TEMP_RADIO_SIZE;i++) { temp+=coef[i-1]*(smem[sidx+i]-smem[sidx-i]); } out[idx]=temp; //printf(\u0026#34;%d:GPU :%lf,\\n\u0026#34;,idx,temp); } 完整的代码在github:https://github.com/Tony-Tan/CUDA_Freshman（欢迎随手star😝 ）\n这里我们要面对一个填充问题，无论是观察公式还是图示，我们会发现，如果模板大小是9，那么我们输出的前4个数据是没办法计算的因为要使用第-1，-2，-3，-4位置的数据，最后4个数据也是不能计算的，因为他要使用 $n+1,n+2,n+3,n+4$ 的数据，这些数据也是没有的，为了保证计算过程中访问不会越界，我们把输入数据两端进行扩充，也就是把上面虽然没有，但是要用的数据填充到输入数据中，当我们我们要处理的是中间的某段数据的时候，那么填充位的数据就来自前面的块对应的输入数据，或者后面线程块对应的输入数据，这就是上面代码最困难的地方，这里懂了其他就一马平川了~\n 定义共享内存，注意是填充后的，所以才会加了模板的两个半径 计算全局线程索引 计算当前线程在填充后对应的共享内存的位置，因为前TEMP_RADIO_SIZE数据是填充位，所以我们从第TEMP_RADIO_SIZE位置进行操作。 判断是当前块对应的输入数据是否是首块，或者是尾块，这时候前面或者后面没有数据可以用来填充，如果是中间块，那么从对位置读取数据进行填充 判断是否是输出的前后无法计算的那几个位置 进行差分，使用宏指令，让编译器自己展开循环  困难在4，主要就是填充，因为如果输入数据当做整体，我们按照串行思路来做，我们可以不填充，而只把输出的前面或者最后的几个数据无效掉就好，但是如果把输入数据分块处理，那么就涉及从全局内存相邻的位置取数据，而且要判断是否越界。\n与只读缓存的比较 以上是常量内存和常量缓存的操作，我们作为对比，展示下只读缓存对应的操作，只读缓存拥有从全局内存读取数据的专用带宽,所以，如果内核函数是带宽限制型的，那么这个帮助是非常大的，不同的设备有不同的只读缓存大小，Kepler SM有48KB的只读缓存，只读缓存对于分散访问的更好，当所有线程读取同一地址的时候常量缓存最好，只读缓存这时候效果并不好，只读换粗粒度为32. 实现只读缓存可以使用两种方法\n 使用__ldg函数 全局内存的限定指针  使用__ldg()的方法：\n__global__ void kernel(float* output, float* input) { ... output[idx] += __ldg(\u0026amp;input[idx]); ... } 使用限定指针的方法：\nvoid kernel(float* output, const float* __restrict__ input) { ... output[idx] += input[idx]; } 使用只读缓存需要更多的声明和控制，在代码非常复杂的情况下以至于编译器都没办法保证制度缓存使用是否安全的情况下，建议使用 __ldg()函数，更容易控制。 只读缓存独立存在，区别于常量缓存，常量缓存喜欢小数据，而只读内存加载的数据比较大，可以再非统一模式下访问，我们修改上面的代码，得到只读缓存版本：\n__global__ void stencil_1d_readonly(float * in,float * out,const float* __restrict__ dcoef) { __shared__ float smem[BDIM+2*TEMP_RADIO_SIZE]; int idx=threadIdx.x+blockDim.x*blockIdx.x; int sidx=threadIdx.x+TEMP_RADIO_SIZE; smem[sidx]=in[idx]; if (threadIdx.x\u0026lt;TEMP_RADIO_SIZE) { if(idx\u0026gt;TEMP_RADIO_SIZE) smem[sidx-TEMP_RADIO_SIZE]=in[idx-TEMP_RADIO_SIZE]; if(idx\u0026lt;gridDim.x*blockDim.x-BDIM) smem[sidx+BDIM]=in[idx+BDIM]; } __syncthreads(); if (idx\u0026lt;TEMP_RADIO_SIZE||idx\u0026gt;=gridDim.x*blockDim.x-TEMP_RADIO_SIZE) return; float temp=.0f; #pragma unroll  for(int i=1;i\u0026lt;=TEMP_RADIO_SIZE;i++) { temp+=dcoef[i-1]*(smem[sidx+i]-smem[sidx-i]); } out[idx]=temp; //printf(\u0026#34;%d:GPU :%lf,\\n\u0026#34;,idx,temp); } 唯一的不同就是多了一个参数，这个参数在主机内是定义的全局内存\nfloat * dcoef_ro; CHECK(cudaMalloc((void**)\u0026amp;dcoef_ro,TEMP_RADIO_SIZE * sizeof(float))); CHECK(cudaMemcpy(dcoef_ro,templ_,TEMP_RADIO_SIZE * sizeof(float),cudaMemcpyHostToDevice)); stencil_1d_readonly\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(in_dev,out_dev,dcoef_ro); CHECK(cudaDeviceSynchronize()); iElaps=cpuSecond()-iStart; printf(\u0026#34;stencil_1d_readonly Time elapsed %f sec\\n\u0026#34;,iElaps); CHECK(cudaMemcpy(out_gpu,out_dev,nBytes,cudaMemcpyDeviceToHost)); checkResult(out_cpu,out_gpu,nxy); 执行结果：\n总结 常量内存和只读缓存：\n 对于核函数都是只读的 SM上的资源有限，常量缓存64KB，只读缓存48KB 常量缓存对于统一读取（读同一个地址）执行更好 只读缓存适合分散读取 ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-5-5-%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍另外两种内存——常量内存，只读缓存\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA常量内存，CUDA只读缓存\u003c/p\u003e","title":"【CUDA 基础】5.5 常量内存"},{"content":"Abstract: 本文介绍使用共享内存进行矩阵转置以减少内存的交叉访问 Keywords: 合并，转置\n合并的全局内存访问 还记得我们矩阵转置的例子么，在全局内存部分介绍的：4.4核函数可达到的带宽 在4.4中我们当时只有共享内存这一种工具可以使用，为了达到最高效率，我们要配合一级缓存，二级缓存进行编程，来提高转置的效率，因为转置只能在行读取列写入或者列读取行写入之间选择一个，这样就必然会引发非合并的访问，虽然我们利用一级缓存的性质可以提高性能，但是我们今天会介绍我们的新工具共享内存，在共享内存中完成转置后写入全局内存，这样就可以避免交叉访问了。\n基准转置内核 在介绍我们的神奇共享内存之前，我们最好先研究出来一下我们的问题的极限在哪，换句话说，我们需要清楚的知道我们最慢的情况（最简单的方式能达到的速度）以及最快的理论速度，理论速度可能会达不到，但是可以接近，最慢速度肯定可以超越，你永远可以写出更慢的程序，所以我们用最简单的方法作为下界，而用正行读取，然后不经变换的写入来作为上限，这一招我们在前面使用过，就是在4.4中，那次我们突破极限了（哈哈，很有可能是计时有问题），但是正常来讲，极限是最好的参考值。 完整的代码在github:https://github.com/Tony-Tan/CUDA_Freshman（欢迎随手star😝 ） 上限：\n__global__ void copyRow(float * in,float * out,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx=ix+iy*nx; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { out[idx]=in[idx]; } } 下限是我们的too young too naive版本，就是最常规的方法：\n__global__ void transformNaiveRow(float * in,float * out,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx_row=ix+iy*nx; int idx_col=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { out[idx_col]=in[idx_row]; } } 这两段代码中第一段并没有转置的功能，只是为了测试上限，第二段是naive的转置，前面也讲过，这里就直接贴结果了 copyRow的cpu计时和nvprof结果： transformNaiveRow的cpu计时和nvprof结果：\n我们可以得到下表：\n   核函数 CPU计时 nvprof计时     copyRow 0.001442 s 1.4859 ms   transformNaiveRow 0.003964 s 3.9640 ms    可以看出计cpu计时还是比较准的，在数据量比较大情况下，我们现在的矩阵大小是 $2^{12}\\times 2^{12}$ 的大小。 然后是加载和存储全局内存请求的平均事务数（越少越好） copyRow: transformNaiveRow: 接着我们就开始用共享内存进行操作了。\n使用共享内存的矩阵转置 为了避免交叉访问，我们可以使用二维共享内存缓存原始矩阵数据，然后从共享内存中读取一列存储到全局内存中，因为共享内存按列读取不会导致交叉访问那么严重的延迟，所以这种想法是可以提高效率的，但是前面一篇我们说这种案列访问共享内存会造成冲突，所以我们先来按照最简单的方式来使用共享内存，来测试下看看有多少性能提高：\n上面这个图就是我们的转置过程\n 从全局内存读取数据（按行）写入共享内存（按行） 从共享内存读取一列写入全局内存的一行  如果从串行的角度看，我们有一个二维矩阵其存储在内存的时候是一维的，一般我们是把二维矩阵按照逐行的方式放入一维内存中，转置的过程可以理解为把逐行从上到下数据改成逐列，从左到右的数据。 由于过程很简单，我们直接看代码，这里我们可以使用正方形的共享内存也可以使用矩形的共享内存，我们按照一般情况下的情况使用矩形的共享内存。\n__global__ void transformSmem(float * in,float* out,int nx,int ny) { __shared__ float tile[BDIMY][BDIMX]; unsigned int ix,iy,transform_in_idx,transform_out_idx; // 1 \tix=threadIdx.x+blockDim.x*blockIdx.x; iy=threadIdx.y+blockDim.y*blockIdx.y; transform_in_idx=iy*nx+ix; // 2 \tunsigned int bidx,irow,icol; bidx=threadIdx.y*blockDim.x+threadIdx.x; irow=bidx/blockDim.y; icol=bidx%blockDim.y; // 3 \tix=blockIdx.y*blockDim.y+icol; iy=blockIdx.x*blockDim.x+irow; // 4 \ttransform_out_idx=iy*ny+ix; if(ix\u0026lt;nx\u0026amp;\u0026amp; iy\u0026lt;ny) { tile[threadIdx.y][threadIdx.x]=in[transform_in_idx]; __syncthreads(); out[transform_out_idx]=tile[icol][irow]; } } 结合下图，看一下我们的代码过程：\n解释代码，这段代码是通用代码，并不需要矩阵为正方形（结合上面的图，过程会更清晰）：\n 计算当前块中的线程的全局坐标（相对于整个网格），计算对应的一维线性内存的位置 bidx表示block idx也就是在这个快中的线程的坐标的线性位置（把块中的二维线程位置按照逐行排布的原则，转换成一维的），然后进行转置，也就是改成逐列排布的方式，计算出新的二维坐标，逐行到逐列排布的映射就是转置的映射，这只完成了很多块中的一块，而关键的是我们把这块放回到哪 计算出转置后的二维全局线程的目标坐标，注意这里的转置前的行位置是计算出来的是转置后的列的位置，这就是转置的第二步。 计算出转置后的二维坐标对应的全局内存的一维位置 读取全局内存，写入共享内存，然后按照转置后的位置写入  上面5个过程可以理解为两步\n 把共享内存内的矩阵进行转置，如果你傻乎乎的真的分配两块共享内存，然后转置，就有点傻了，当然也不赞成你按照行读取全局内存然后按列写入（我开始就是这么想的），而是建立一种转置的映射关系，完成分块转置中的某一个块的转置。 找到上面转置完了的块的新位置，然后写入全局内存。  看一下cpu计时\nnvprof结果;\n从0.003964 s到0.001803 s 看起来还不错。速度提升了一倍。 然后看一下平均内存事务:\nstroe过程的事务数从32降低到4，还是很显著的。我们看一下共享内存的事务数，来看看是否有冲突：\n毫不意外，16路冲突，store没冲突，主要来自load，解决这个问题，我们使用的方法只有填充\n使用填充共享内存的矩阵转置 解决共享内存冲突的最好办法就是填充，我们来填充下我们的共享内存；\n__global__ void transformSmemPad(float * in,float* out,int nx,int ny) { __shared__ float tile[BDIMY][BDIMX+IPAD]; unsigned int ix,iy,transform_in_idx,transform_out_idx; ix=threadIdx.x+blockDim.x*blockIdx.x; iy=threadIdx.y+blockDim.y*blockIdx.y; transform_in_idx=iy*nx+ix; unsigned int bidx,irow,icol; bidx=threadIdx.y*blockDim.x+threadIdx.x; irow=bidx/blockDim.y; icol=bidx%blockDim.y; ix=blockIdx.y*blockDim.y+icol; iy=blockIdx.x*blockDim.x+irow; transform_out_idx=iy*ny+ix; if(ix\u0026lt;nx\u0026amp;\u0026amp; iy\u0026lt;ny) { tile[threadIdx.y][threadIdx.x]=in[transform_in_idx]; __syncthreads(); out[transform_out_idx]=tile[icol][irow]; } } 为一的不同就是IPAD，我们主要看指标： 运行结果：\n可见执行速度从0.0018降低到0.0015 nvprof结果： 我们主要关注共享内存事务,全局事务没有改变。 还有冲突，我们改变IPAD试试（从1到2）\n哈哈，冲突不见了，执行CPU计时：\n冲突解决了但是似乎速度没有显著提升。\n使用展开的矩阵转置 在共享内存进行填充消除冲突后，我们还想继续提高性能，那么我们使用前面提到了另一个大杀器，展开循环，节省大量的线程块，提高带宽利用率： 废话不多说，直接上代码，然后再解释：\n__global__ void transformSmemUnrollPad(float * in,float* out,int nx,int ny) { __shared__ float tile[BDIMY*(BDIMX*2+IPAD)]; //1. \tunsigned int ix,iy,transform_in_idx,transform_out_idx; ix=threadIdx.x+blockDim.x*blockIdx.x*2; iy=threadIdx.y+blockDim.y*blockIdx.y; transform_in_idx=iy*nx+ix; //2. \tunsigned int bidx,irow,icol; bidx=threadIdx.y*blockDim.x+threadIdx.x; irow=bidx/blockDim.y; icol=bidx%blockDim.y; //3. \tunsigned int ix2=blockIdx.y*blockDim.y+icol; unsigned int iy2=blockIdx.x*blockDim.x*2+irow; //4. \ttransform_out_idx=iy2*ny+ix2; if(ix+blockDim.x\u0026lt;nx\u0026amp;\u0026amp; iy\u0026lt;ny) { unsigned int row_idx=threadIdx.y*(blockDim.x*2+IPAD)+threadIdx.x; tile[row_idx]=in[transform_in_idx]; tile[row_idx+BDIMX]=in[transform_in_idx+BDIMX]; //5 \t__syncthreads(); unsigned int col_idx=icol*(blockDim.x*2+IPAD)+irow; out[transform_out_idx]=tile[col_idx]; out[transform_out_idx+ny*BDIMX]=tile[col_idx+BDIMX]; } } 没错，一个线程干两个线程的事，或者说一个块干两个块的事，其实结合前面的共享内存转置的详细说明，这个就很好理解了，但是我还是再写一遍，以免有遗漏，或者解释不清楚。 当然，看图更清楚，注意与上面的代码进行对比：\n 计算当前块中的线程的全局坐标——一维线性内存的位置 在共享内存内进行转置 计算出转置后的二维全局线程的目标坐标，注意这里的转置前的行位置是计算出来的是转置后的列的位置，这就是转置的第二步。 计算出转置后的二维坐标对应的全局内存的一维位置，注意这里不是一次计算一个块，而是计算两个块，换个理解方法，我们把原来的块x方向扩大一倍，然后再对这个大块分成两个小块（，B），每个小块中的对应位置差BDIMX，然后对其中A，B中数据按行写入共享内存。 将4中读取到共享内存中的数据，按照转置后的位置写入全局内存。  这个过程看起来不麻烦，写起来有点麻烦，但是有一个技巧，就是画图，你画个图就基本明白了。 我们看一下nvprof的结果，以及全局内存的事务。 nvprof：\n全局内存事务： 可以看到在我们当前的环境下，展开并没有提速，我们可以展开更多试试，这里就不做试验了。\n增大并行性 增大并行性的办法是调整核函数的配置，我们调整成 $16\\times 16$ 的块，可以得到下面的结果：\n调整成 $8\\times 8$ 的块的结果如下 这个要考大家不停的试才能找到当前设备最优的。\n总结 本文使用共享内存避免交叉的全局内存访问，优化了矩阵转置的速度。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-5-4-%E5%90%88%E5%B9%B6%E7%9A%84%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍使用共享内存进行矩阵转置以减少内存的交叉访问\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 合并，转置\u003c/p\u003e","title":"【CUDA 基础】5.4 合并的全局内存访问"},{"content":"Abstract: 本文介绍使用共享内存进行归约，并比较全局内存归约与共享内存归约之间的性能差距 Keywords: 共享内存，归约\n减少全局内存访问 逻辑是非常重要的，一旦你学会了逻辑，很多假的东西你可以轻松的识别出来，这会使你更加强大而不会被任何人或者组织洗脑。 废话少说，开始今天的博客。 使用共享内存的主要原因就是减少对全局内存的访问，来减少不必要的延迟，第三章我们学过了归约，可以参考：\n https://face2ai.com/CUDA-F-3-4-避免分支分化/ https://face2ai.com/CUDA-F-3-5-展开循环/  这两篇博客包含我们前面使用全局内存进行归约的各种技术，我们几天也要用其中一部分代码作为比较，来体现我们共享内存的优势。 我们要集中解决下面两个问题：\n 如何重新安排数据访问模式以避免线程束分化 如何展开循环以保证有足够的操作使指令和内存带宽饱和  本文我们通过对比研究前面的部分代码，来分析为何要使用共享内存，以及如何使用共享内存。\n使用共享内存的并行归约 我们首先来回忆全局内存下的，完全展开的归约计算：\n__global__ void reduceGmem(int * g_idata,int * g_odata,unsigned int n) { //set thread ID \tunsigned int tid = threadIdx.x; unsigned int idx = blockDim.x*blockIdx.x+threadIdx.x; //boundary check \tif (tid \u0026gt;= n) return; //convert global data pointer to the \tint *idata = g_idata + blockIdx.x*blockDim.x; //in-place reduction in global memory \tif(blockDim.x\u0026gt;=1024 \u0026amp;\u0026amp; tid \u0026lt;512) idata[tid]+=idata[tid+512]; __syncthreads(); if(blockDim.x\u0026gt;=512 \u0026amp;\u0026amp; tid \u0026lt;256) idata[tid]+=idata[tid+256]; __syncthreads(); if(blockDim.x\u0026gt;=256 \u0026amp;\u0026amp; tid \u0026lt;128) idata[tid]+=idata[tid+128]; __syncthreads(); if(blockDim.x\u0026gt;=128 \u0026amp;\u0026amp; tid \u0026lt;64) idata[tid]+=idata[tid+64]; __syncthreads(); //write result for this block to global mem \tif(tid\u0026lt;32) { volatile int *vmem = idata; vmem[tid]+=vmem[tid+32]; vmem[tid]+=vmem[tid+16]; vmem[tid]+=vmem[tid+8]; vmem[tid]+=vmem[tid+4]; vmem[tid]+=vmem[tid+2]; vmem[tid]+=vmem[tid+1]; } if (tid == 0) g_odata[blockIdx.x] = idata[0]; } 下面这步是计算当前线程的索引位置：\nunsigned int idx = blockDim.x*blockIdx.x+threadIdx.x; 当前线程块对应的数据块首地址\nint *idata = g_idata + blockIdx.x*blockDim.x; 然后是展开循环的部分，tid是当前线程块中线程的标号，主要区别于全局编号idx：\nif(blockDim.x\u0026gt;=1024 \u0026amp;\u0026amp; tid \u0026lt;512) idata[tid]+=idata[tid+512]; __syncthreads(); if(blockDim.x\u0026gt;=512 \u0026amp;\u0026amp; tid \u0026lt;256) idata[tid]+=idata[tid+256]; __syncthreads(); if(blockDim.x\u0026gt;=256 \u0026amp;\u0026amp; tid \u0026lt;128) idata[tid]+=idata[tid+128]; __syncthreads(); if(blockDim.x\u0026gt;=128 \u0026amp;\u0026amp; tid \u0026lt;64) idata[tid]+=idata[tid+64]; __syncthreads(); 这一步把是当前线程块中的所有数据归约到前64个元素中，接着使用如下代码，将最后64个元素归约成一个\nif(tid\u0026lt;32) { volatile int *vmem = idata; vmem[tid]+=vmem[tid+32]; vmem[tid]+=vmem[tid+16]; vmem[tid]+=vmem[tid+8]; vmem[tid]+=vmem[tid+4]; vmem[tid]+=vmem[tid+2]; vmem[tid]+=vmem[tid+1]; } 注意这里声明了一个volatile变量，如果我们不这么做，编译器不能保证这些数据读写操作按照代码中的顺序执行（参考5.1中关于编译器数据传输部分的说明），所以必须要这么做。 然后我们执行以下这段代码，虽然前面执行过了，我们还是执行以下，观察下结果： 完整的可执行代码依旧在GitHub上可以找到 github:https://github.com/Tony-Tan/CUDA_Freshman点个星星也不会很累，拜托啦😆\n这里我们假装看不到别的，只看我们的核函数，可见器质性时间是4.25ms左右 然后我们对上面的代码进行改写，改写成共享内存的版本，来看代码：\n__global__ void reduceSmem(int * g_idata,int * g_odata,unsigned int n) { //set thread ID  __shared__ int smem[DIM]; unsigned int tid = threadIdx.x; //unsigned int idx = blockDim.x*blockIdx.x+threadIdx.x; \t//boundary check \tif (tid \u0026gt;= n) return; //convert global data pointer to the \tint *idata = g_idata + blockIdx.x*blockDim.x; smem[tid]=idata[tid]; __syncthreads(); //in-place reduction in global memory \tif(blockDim.x\u0026gt;=1024 \u0026amp;\u0026amp; tid \u0026lt;512) smem[tid]+=smem[tid+512]; __syncthreads(); if(blockDim.x\u0026gt;=512 \u0026amp;\u0026amp; tid \u0026lt;256) smem[tid]+=smem[tid+256]; __syncthreads(); if(blockDim.x\u0026gt;=256 \u0026amp;\u0026amp; tid \u0026lt;128) smem[tid]+=smem[tid+128]; __syncthreads(); if(blockDim.x\u0026gt;=128 \u0026amp;\u0026amp; tid \u0026lt;64) smem[tid]+=smem[tid+64]; __syncthreads(); //write result for this block to global mem \tif(tid\u0026lt;32) { volatile int *vsmem = smem; vsmem[tid]+=vsmem[tid+32]; vsmem[tid]+=vsmem[tid+16]; vsmem[tid]+=vsmem[tid+8]; vsmem[tid]+=vsmem[tid+4]; vsmem[tid]+=vsmem[tid+2]; vsmem[tid]+=vsmem[tid+1]; } if (tid == 0) g_odata[blockIdx.x] = smem[0]; } 唯一的不同就是多了一个共享内存的声明，以及各线程将全局写入共享内存，以及后面的同步指令：\nsmem[tid]=idata[tid]; __syncthreads(); 这一步过后同步保证该线程块内的所有线程，都执行到此处后继续向下进行，这是可以理解的，因为我们的归约只针对本块内，当然如果想跨几个块执行，可能同步这里就有问题了，这个是上一节课要讨论的，这里就不过多解释了，我们接着就看到一个volatile类型的指针，指向共享内存，对最后64个归约结果进行归约，整个过程和全局内存一毛一样，只不过一个在全局内存操作，一个在共享内存操作，得到相同的结果，我们也来看一下运行结果。\n还是那张图，对比来看，速度提高了不少了。看一下\ngld_transactions gst_transactions 这两个指标的结果\n可以看出使用共享内存的，比使用全局内存的高到不知道哪里去了\n使用展开的并行归约 可能看到上面的截图你已经知道我接下来要并行4块了，对于前面说的，使用共享内存不能并行四块，是因为没办法同步读四个块，这里我们还是用老方法进行并行四个块，就是在写入共享内存之前进行归约，4个块变成一个，然后把这一个存入共享内存，进行常规的共享内存归约:\n__global__ void reduceUnroll4Smem(int * g_idata,int * g_odata,unsigned int n) { //set thread ID  __shared__ int smem[DIM]; unsigned int tid = threadIdx.x; unsigned int idx = blockDim.x*blockIdx.x*4+threadIdx.x; //boundary check \tif (tid \u0026gt;= n) return; //convert global data pointer to the  int tempSum=0; if(idx+3 * blockDim.x\u0026lt;=n) { int a1=g_idata[idx]; int a2=g_idata[idx+blockDim.x]; int a3=g_idata[idx+2*blockDim.x]; int a4=g_idata[idx+3*blockDim.x]; tempSum=a1+a2+a3+a4; } smem[tid]=tempSum; __syncthreads(); //in-place reduction in global memory \tif(blockDim.x\u0026gt;=1024 \u0026amp;\u0026amp; tid \u0026lt;512) smem[tid]+=smem[tid+512]; __syncthreads(); if(blockDim.x\u0026gt;=512 \u0026amp;\u0026amp; tid \u0026lt;256) smem[tid]+=smem[tid+256]; __syncthreads(); if(blockDim.x\u0026gt;=256 \u0026amp;\u0026amp; tid \u0026lt;128) smem[tid]+=smem[tid+128]; __syncthreads(); if(blockDim.x\u0026gt;=128 \u0026amp;\u0026amp; tid \u0026lt;64) smem[tid]+=smem[tid+64]; __syncthreads(); //write result for this block to global mem \tif(tid\u0026lt;32) { volatile int *vsmem = smem; vsmem[tid]+=vsmem[tid+32]; vsmem[tid]+=vsmem[tid+16]; vsmem[tid]+=vsmem[tid+8]; vsmem[tid]+=vsmem[tid+4]; vsmem[tid]+=vsmem[tid+2]; vsmem[tid]+=vsmem[tid+1]; } if (tid == 0) g_odata[blockIdx.x] = smem[0]; } 这段代码就是多了其他三块的求和：\nunsigned int idx = blockDim.x*blockIdx.x*4+threadIdx.x; //boundary check if (tid \u0026gt;= n) return; //convert global data pointer to the int tempSum=0; if(idx+3 * blockDim.x\u0026lt;=n) { int a1=g_idata[idx]; int a2=g_idata[idx+blockDim.x]; int a3=g_idata[idx+2*blockDim.x]; int a4=g_idata[idx+3*blockDim.x]; tempSum=a1+a2+a3+a4; } 这一步在3.5中已经介绍过了为什么能加速了，因为可以通过增加三步计算而减少之前的3个线程块的计算，这是非常大的减少。同时多步内存加载也可以使内存带宽达到更好的使用。 结果可想而知：\n吞吐量指标：\nnvprof --metrics dram_read_throughput ./reduce_integer_shared_memory 无论是指标还是运行速度，都有非常显著的提升。 我们这里总结下展开的优势：\n I/O得到了更多的并行，就是我上面说的更好的利用带宽，增加了吞吐量 全局内存存储事务减少到 $\\frac{1}{4}$ ，这个主要针对最后一步，将结果存入全局内存 整体性能巨幅提升  使用动态共享内存的并行归约 然后我们看一下动态版本，其实动态版本没啥可看的，只是写法上有点不同，把宏改成核函数配置参数，注意其单位是字节就好，也就是不要忘了sizeof()就行了。 这里不啰嗦了。\n有效带宽 对比一下数据，回顾一下我们的有效带宽，其计算公式(4.4中有详细介绍)： $$ 有效带宽=\\frac{(读字节数 + 写字节数)\\times 10^{-9}}{运行时间}\\tag{1} $$ 可以研究三个核函数的有效带宽，这里就不再一个个计算了，因为这个在4.4中已经教大家做了，我们可以自己算一下，并得出结论\n总结 本文主要高速大家如何使用共享内存加速归约，以及结合了共享内存的展开能更高的提高效率，注意线程块内的同步，这个是重要的。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-5-3-%E5%87%8F%E5%B0%91%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍使用共享内存进行归约，并比较全局内存归约与共享内存归约之间的性能差距\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 共享内存，归约\u003c/p\u003e","title":"【CUDA 基础】5.3 减少全局内存访问"},{"content":"Abstract: 本文主要研究几个关于共享内存的例子，以此来了解共享内存的性质，为我们的核函数加速 Keywords: 行主序，列主序，填充与无填充，从线程索引体映射数据元素\n共享内存的数据布局 本文我们主要研究共享内存的数据布局，通过代码实现，来观察运行数据，换句话说，我们主要研究上一篇中的放西瓜，取西瓜，以及放冬瓜等的一些列操作对性能的影响，以及如何才能使效率最大化。 几个例子包括以下几个主题：\n 方阵与矩阵数组 行主序与列主序 静态与动态共享内存的声明 文件范围与内核范围的共享内存 内存填充与无内存填充  当使用共享内存设计核函数的时候下面两个概念是非常重要的：\n 跨内存存储体映射数据元素 从线程索引到共享内存偏移的映射  当上面这些主题和概念都得到很好地理解，设计一个高效的使用共享内存的核函数就没什么问题了，其可以避免存储体冲突并充分利用共享内存的优势。 注意，从几何上讲，方形属于矩形，这里我们说的矩形时指长方形。\n方形共享内存 我们前面说过我们的线程块可以是一维二维和三维的，对应的线程编号是threadIdx.x, threadIdx.y以及threadIdx.z，为了对应一个二维的共享内存，我们假设我们使用二维的线程块，那么对于一个二维的共享内存\n#define N 32 ... __shared__ int x[N][N]; ... 当我们使用二维块的时候，很有可能会使用下面这种方式来索引x的数据：\n#define N 32 ... __shared__ int x[N][N]; ... int a=x[threadIdx.y][threadIdx.x]; 当然这个索引就是 $(y,x)$ 对应的，我们也可以用 $(x,y)$ 来索引。 在CPU中，如果用循环遍历二维数组，尤其是双层循环的方式，我们倾向于内层循环对应x，因为这样的访问方式在内存中是连续的，因为CPU的内存是线性存储的，但是GPU的共享内存并不是线性的，而是二维的，分成不同存储体的，并且，并行也不是循环，那么这时候，问题完全不同，没有任何可比性。 回顾放西瓜的例子以及存储体冲突的特性，容易想到，我们最应该避免的是存储体冲突，那么对应的问题就来了，我们每次执行一个线程束，对于二维线程块，一个线程束是按什么划分的呢？是按照threadIdx.x 维进行划分还是按照threadIdx.y维进行划分的呢？ 这句话有点迷糊？那我再啰嗦一遍，因为这个很关键，我们每次执行的是一个线程束，线程束里面有很多线程，对于一个二维的块，切割线程束有两种方法，顺着y切，那么就是threadIdx.x固定（变化慢），而threadIdx.y是连续的变化，顺着x切相反；CUDA明确的告诉你，我们是顺着x切的，也就是一个线程束中的threadIdx.x 连续变化。 我们的数据是按照行放进存储体中的这是固定的，所以我们希望，这个线程束中取数据是按照行来进行的，所以\nx[threadIdx.y][threadIdx.x]; 这种访问方式是最优的，threadIdx.x在线程束中体现为连续变化的，而对应到共享内存中也是遍历共享内存的同一行的不同列\n上面这个确实有点绕，我们可以画画图，多想象一下CUDA的运行原理，这个就好理解了，说白了就是不要一个线程束中访问一列共享内存，而是要访问一行。\n对照上图，我们把一个int类型（四字节）的1024个元素的数组放到共享内存A中，每个int的索引对应到蓝框中，假设我们的块大小是 $(32,32)$ 那么我们第一个线程束就是 threadIdx.y=0,threadIdx.x=0\u0026hellip;\u0026hellip;31,如果我们使用\nA[threadIdx.x][threadIdx.y]; 的索引方式，就会得到绿框的数据，可想而知，这冲突达到了最大，效率最低、 果我们使用\nA[threadIdx.y][threadIdx.x]; 我们就会得到红色框中的数据，无冲突，一个事务完成。\n本文全部代码在GitHub上可下载使用:https://github.com/Tony-Tan/CUDA_Freshman\n行主序访问和列主序访问 行主序访问和列主序访问我们上面已经把原理基本介绍清楚了，我们下面看实现后的试验，这里我们研究的访问，包括读和写，也就是加载和存储。\n我们定义块的尺寸为\n#define BDIMX 32 #define BDIMY 32 核函数只完成简单的两个操作：\n 将全局线程索引值存入二维共享内存 从共享内存中按照行主序读取这些值并存到全局内存中  项目完整的代码在24_shared_memory_read_data这个文件夹下，下文我们只贴部分代码。 核函数如下\n__global__ void setRowReadRow(int * out) { __shared__ int tile[BDIMY][BDIMX]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; tile[threadIdx.y][threadIdx.x]=idx; __syncthreads(); out[idx]=tile[threadIdx.y][threadIdx.x]; }  定义一个共享内存，大小为 $32\\times 32$ 计算当前线程的全局位置的值idx 将idx这个无符号整数值写入二维共享内存tile【threadIdx.y】【threadIdx.x】中 同步 将共享内存tile【threadIdx.y】【threadIdx.x】中的值写入全局内存对应的idx位置处  核函数的内存工作：\n 共享内存的写入 共享内存的读取 全局内存的写入  这个核函数按照行主序读和写，所以对于共享内存没有读写冲突 另一种方法就是按照列主序访问了，核函数代码如下：\n__global__ void setColReadCol(int * out) { __shared__ int tile[BDIMY][BDIMX]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; tile[threadIdx.x][threadIdx.y]=idx; __syncthreads(); out[idx]=tile[threadIdx.x][threadIdx.y]; } 原理不再赘述，我们直接看运行结果： 对于使用nvprof如果出现 ======== Error: unified memory profiling failed.错误，是因为系统的保护机制，所以使用sudo权限来执行即可，如果sudo找不到你的nvprof，你可以用完整路径，或则添加到环境变量：\n可见行主序的平均时间是 $1.552\\mu s$ 而列主序是 $2.4640\\mu s$ 注意如果直接使用来方法即cpu计时，那么会非常不准，比如我们红色方框内就是cpu计时的结果，原因是数据量太小，运行时间太短，误差相对就太大了，这显然是错误，很有可能我们前面也出现过理论和实际不符的情况也是因为计时有问题。\n接下来我们看看检测存储体冲突的指标，会是什么数据：\nshared_load_transactions_per_request shared_store_transactions_per_request  shared_load_transactions_per_request 结果：  nvprof --metrics shared_load_transactions_per_request ./shared_memory_read_data 可以看到load过程行主序1个事务，而列主序32个\n shared_store_transactions_per_request 结果：  nvprof --metrics shared_store_transactions_per_request ./shared_memory_read_data 同样行主序的事务是1，而列主序的事务是32\n注意，我们这个设备是4-byte宽的，上面第二张图中有相关信息。\n按行主序写和按列主序读 接下来我们来点混合的，我们按照行主序写入，按照列主序读取，声明下，这个核函数本身没什么意义，就像前面的核函数，其根本作用就是为了我们测试各种指标用到，所以大家不要过度关心输入输出是什么，而是关系不同的读写方法，在指标和性能上的差异。 按照列主序读：\nout[idx]=tile[threadIdx.x][threadIdx.y]; 按照行主序写：\ntile[threadIdx.y][threadIdx.x]=idx; 核函数跟上面差不多，我就不贴了，直接看看结果：\nshared_load_transactions_per_request: shared_store_transactions_per_request 可以看出load的时候。也就是读取共享内存，然后写入全局内存的这步，是有冲突的，32路冲突是显然的。 同样的试验可以测试按照行主序读和列主序写，这里就不啰嗦一遍了，因为一毛一样。\n动态共享内存 共享内存没有malloc但是也可以到运行时才分配，具体机制我没去了解，是不是共享内存也分堆和栈，但是我们有必要了解这个方法，因为写过C++程序的都知道，基本上我们的大部分变量是要靠动态分配手动管理的，CUDA好的一点就是动态的共享内存，不需要手动回收。 我们看核函数：\n__global__ void setRowReadColDyn(int * out) { extern __shared__ int tile[]; unsigned int row_idx=threadIdx.y*blockDim.x+threadIdx.x; unsigned int col_idx=threadIdx.x*blockDim.y+threadIdx.y; tile[row_idx]=row_idx; __syncthreads(); out[row_idx]=tile[col_idx]; } 其中extern用于表明这个共享内存是运行时才知道的\nextern __shared__ int tile[]; 一个int型的共享内存长度不知道，什么时候才能知道？当然是运行的时候：\nsetRowReadColDyn\u0026lt;\u0026lt;\u0026lt;grid,block,(BDIMX)*BDIMY*sizeof(int)\u0026gt;\u0026gt;\u0026gt;(out); 核函数配置参数，网格大小，块大小，第三个参数就是共享内存的大小了，那么问题来了，我们可不可以声明多个动态共享内存呢？是否可以继续添加参数呢？这个我还没试验，可以留作一个思考。 运行结果当然也没什么出奇的：\nshared_load_transactions_per_request:\nshared_store_transactions_per_request:\n动态静态运行结果没什么差别，冲突不变。\n填充静态声明的共享内存 填充我们在前面博客大概提到了，我们通过改变声明的共享内存大小来填充一些位置，比如最后一列，我们声明了这个尺寸的共享内存，其会自动对应到CUDA模型上的二维共享内存存储体，换句话说，所谓填充是在声明的时候产生的， 声明一个二维共享内存，或者一维共享内存，编译器会自动将其重新整理到一个二维的空间中，这个空间包含32个存储体，每个存储体宽度一定，换句话说，你声明一个二维存储体，编译器会把声明的二维转换成一维线性的，然后再重新整理成二维按照32个存储体，4-Byte/8-Byte宽的内存分布，然后再进行运算的。 这就是我们填充存储体的最根本原理。 原理明白了，下面的核函数就没啥了，就是加了个常量：\n__global__ void setRowReadColIpad(int * out) { __shared__ int tile[BDIMY][BDIMX+IPAD]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; tile[threadIdx.y][threadIdx.x]=idx; __syncthreads(); out[idx]=tile[threadIdx.x][threadIdx.y]; } 运行结果：\n冲突不见了！同志们，填充起作用了！\n填充动态声明的共享内存 动态填充和静态填充似乎没什么区别，一个在核函数内声明，一个在参数中体现，代码如下：\n__global__ void setRowReadColDynIpad(int * out) { extern __shared__ int tile[]; unsigned int row_idx=threadIdx.y*(blockDim.x+1)+threadIdx.x; unsigned int col_idx=threadIdx.x*(blockDim.x+1)+threadIdx.y; tile[row_idx]=row_idx; __syncthreads(); out[row_idx]=tile[col_idx]; } 调用代码：\nsetRowReadColDynIpad\u0026lt;\u0026lt;\u0026lt;grid,block,(BDIMX+IPAD)*BDIMY*sizeof(int)\u0026gt;\u0026gt;\u0026gt;(out); 填充示意图： 唯一要注意的就是索引，当填充了以后我们的行不变，列加宽了，所以索引的时候要：\nunsigned int row_idx=threadIdx.y*(blockDim.x+1)+threadIdx.x; unsigned int col_idx=threadIdx.x*(blockDim.x+1)+threadIdx.y; 有些童鞋可能能对这个索引，有点迷糊，这个时候你不要想硬件上也就是转换后的存储体内是什么样的，我们之关心我们逻辑上的布局就好，也就是上图的样子，因为到存储体会自动的转换过去转换回来，和图上的方式是一一对应的，如果这几个你一起考虑必然会混乱，而不明白为啥要threadIdx.x*(blockDim.x+1)+threadIdx.y\n然后就没什么了，结果如下：\n冲突不见了，同志们，填充起作用了！\n方形共享内存内核性能的比较 接下来就是比较各个内核，我们直接运行：\nsudo / $\\mu$ sr/local/cuda/bin/nvprof ./shared_memory_read_data 11 可以观察到所有核函数的运行时间\n   Type Time(%) Time Calls Avg Min Max Name     GPU activities: 16.12% 2.5600 $\\mu$ s 1 2.5600 $\\mu$ s 2.5600 $\\mu$ s 2.5600 $\\mu$ s setColReadCol(int*)    13.30% 2.1120 $\\mu$ s 1 2.1120 $\\mu$ s 2.1120 $\\mu$ s 2.1120 $\\mu$ s warmup(int*)    12.90% 2.0480 $\\mu$ s 1 2.0480 $\\mu$ s 2.0480 $\\mu$ s 2.0480 $\\mu$ s setRowReadCol(int*)    12.70% 2.0170 $\\mu$ s 1 2.0170 $\\mu$ s 2.0170 $\\mu$ s 2.0170 $\\mu$ s setRowReadColDyn(int*)    12.70% 2.0160 $\\mu$ s 1 2.0160 $\\mu$ s 2.0160 $\\mu$ s 2.0160 $\\mu$ s setColReadRow(int*)    9.27% 1.4720 $\\mu$ s 1 1.4720 $\\mu$ s 1.4720 $\\mu$ s 1.4720 $\\mu$ s setRowReadColIpad(int*)    8.88% 1.4090 $\\mu$ s 1 1.4090 $\\mu$ s 1.4090 $\\mu$ s 1.4090 $\\mu$ s setRowReadRow(int*)    8.47% 1.3450 $\\mu$ s 1 1.3450 $\\mu$ s 1.3450 $\\mu$ s 1.3450 $\\mu$ s setRowReadColDynIpad(int*)    具体内存里有什么我这里就不演示代码了，缩小声明的内存大小，然后打印出来，就能看到结果了。\n所有上面这些，只需要明确并且熟练于心的就是我们声明的是我们编程的模型，实际存储的是另一种形状的二维存储体布局，这两个数据分布是一一对应的，我们在写核函数写功能的时候只要考虑我们的编程模型即可得到正确答案，但是优化就要考虑编程模型和存储体分布之间的关系，适当的填充会得到好的结果。\n矩形共享内存 以上是正方形，长方形类似，只要掌握了编程模型和存储体之间的对应关系这个过程，其实来个三角形的内存都是无所谓的，但是为了配合我们还是写一下吧，从简。 长方形共享内存：\n#define BDIMX_RECT 32 #define BDIMY_RECT 16 不一样长就是举矩形了，这时候我们就要考虑非正方形的索引问题了，这时候不能简单的交换坐标了，而是需要用我上面说到的，先转换成线性的，然后再重新计算行和列的坐标\n行主序访问和列主序访问 这个和正方形的完全一样，我们就不再啰嗦了，只是调整了下共享内存的尺寸而已，其他没有变化。我们关注的是按行读按列写或者按列读按行写这种需要坐标转换的情况。\n行主序写操作和列主序读操作 没错，这就是复杂的情况，我们先贴代码，然后好研究一下：\n__global__ void setRowReadColRect(int * out) { __shared__ int tile[BDIMY_RECT][BDIMX_RECT]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; unsigned int icol=idx%blockDim.y; unsigned int irow=idx/blockDim.y; tile[threadIdx.y][threadIdx.x]=idx; __syncthreads(); out[idx]=tile[icol][irow]; } 解释： 先贴图 注意红色为存储体空间，蓝色小标1，2，3，4表示四个不同的模型。\nunsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; 这句就是计算线性位置，因为我们把不同的尺寸的共享内存映射到线性空间图中 1-\u0026gt;3 的过程；接着，\nunsigned int icol=idx%blockDim.y; unsigned int irow=idx/blockDim.y; 这一步是3-\u0026gt;2 的过程，很多同学可能不明白，其实我也不明白，但是我仔细研究了一下，我们并没有改变原始tile的形状，我们只是改变了数组的索引顺序，原始的我们是按照1中的绿线从左到右逐行进行的，然后我们通过除法和取余，得到的新坐标是按照3中的绿色箭头从上到下，逐列进行的。所以当1中x方向坐标增加一个，对应于3中y方向坐标加一个，但是数组的形状不变，这就成了一个按照行写入，一个按照列读取。 只是这样就不是前面方形的32个冲突了，而是16个冲突，因为每进行一行32个元素，对应的是两列，那么就是16个冲突 运行结果：\n可见如我们所推测的16路冲突在load的时候出现了。\n动态声明的共享内存 接着就是动态版本的了，基本没区别， 代码：\n__global__ void setRowReadColRectDyn(int * out) { extern __shared__ int tile[]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; unsigned int icol=idx%blockDim.y; unsigned int irow=idx/blockDim.y; unsigned int col_idx=icol*blockDim.x+irow; tile[idx]=idx; __syncthreads(); out[idx]=tile[col_idx]; } 启动核函数：\nsetRowReadColRectDynPad\u0026lt;\u0026lt;\u0026lt;grid_rect,block_rect,(BDIMX+1)*BDIMY*sizeof(int)\u0026gt;\u0026gt;\u0026gt;(out); 和方形的类似，结果如下：\n结果同静态内存一致\n填充静态声明的共享内存 然后我们填充内存，填充内存的时候就需要注意索引计算了，但是并不需要做调整。 那么我们的代码是：\n__global__ void setRowReadColRectPad(int * out) { __shared__ int tile[BDIMY_RECT][BDIMX_RECT+IPAD]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; unsigned int icol=idx%blockDim.y; unsigned int irow=idx/blockDim.y; tile[threadIdx.y][threadIdx.x]=idx; __syncthreads(); out[idx]=tile[icol][irow]; } 结果是，在填充了一列的时候，我们只产生了两路冲突 填充两列的时候，代码为\n__global__ void setRowReadColRectPad(int * out) { __shared__ int tile[BDIMY_RECT][BDIMX_RECT+IPAD*2]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; unsigned int icol=idx%blockDim.y; unsigned int irow=idx/blockDim.y; tile[threadIdx.y][threadIdx.x]=idx; __syncthreads(); out[idx]=tile[icol][irow]; } 运行结果，没有冲突\n如果不明白为什么，可以自己画个图，从长方形转换到存储体空间上，然后按照执行的过程比划比划\n填充动态声明的共享内存 动态填充和方形的动态填充类似，tile没有二维索引了，所以需要计算出二维索引对应一维位置，其他情况类似，代码如下：\n__global__ void setRowReadColRectDynPad(int * out) { extern __shared__ int tile[]; unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; unsigned int icol=idx%blockDim.y; unsigned int irow=idx/blockDim.y; unsigned int row_idx=threadIdx.y*(IPAD+blockDim.x)+threadIdx.x; unsigned int col_idx=icol*(IPAD+blockDim.x)+irow; tile[row_idx]=idx; __syncthreads(); out[idx]=tile[col_idx]; } 索引转换，和二维坐标对应于一维坐标，这些都是前面用过的技术，观察结果，对于添加1列，得出两路冲突，和上面静态填充一致 矩形共享内存内核性能的比较 矩形的所有核函数运行结果比较\n   Type Time(%) Time Calls Avg Min Max Name     GPU activities: 66.27% 12.512us 1 12.512us 12.512us 12.512us setRowReadColRectPad(int*)    8.81% 1.6640us 1 1.6640us 1.6640us 1.6640us setRowReadColRect(int*)    8.81% 1.6640us 1 1.6640us 1.6640us 1.6640us setRowReadColRectDyn(int*)    8.47% 1.6000us 1 1.6000us 1.6000us 1.6000us warmup(int*)    7.63% 1.4400us 1 1.4400us 1.4400us 1.4400us setRowReadColRectDynPad(int*)    总结 本文通过一组实验，得到了关于共享内存的存储体内分布情况，以及如何使用动态的共享内存，以及填充来避免冲突。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-5-2-%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E7%9A%84%E6%95%B0%E6%8D%AE%E5%B8%83%E5%B1%80.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要研究几个关于共享内存的例子，以此来了解共享内存的性质，为我们的核函数加速\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 行主序，列主序，填充与无填充，从线程索引体映射数据元素\u003c/p\u003e","title":"【CUDA 基础】5.2 共享内存的数据布局"},{"content":"Abstract: 本文为CUDA内存的概述，介绍共享内存的模型，分配，访问，配置，同步等内容 Keywords: 模型，分配，访问，配置，同步\nCUDA共享内存概述 这里首先要进一步说明一下，前面我们在说缓存的时候说其是可编程的，这是不准确的，应该说是可以控制的，而我们今天要说的共享内存才是真正意义上的可编程的。 废话不多说了，一套CUDA内容写到现在，一大半已经进行完了，希望我们在一个系列完成后都能有所成长，而不是纯粹的阅读或者码字。 GPU内存按照类型（物理上的位置）可以分为\n 板载内存 片上内存  全局内存是较大的板载内存，延迟高，共享内存是片上的较小的内存，延迟低，带宽高。前面我我们讲过工厂的例子，全局内存就是原料工厂，要用车来运输原料，共享内存是工厂内存临时存放原料的房间，取原料路程短速度快。 共享内存是一种可编程的缓存，共享内存通常的用途有：\n 块内线程通信的通道 用于全局内存数据的可编程管理的缓存 告诉暂存存储器，用于转换数据来优化全局内存访问模式  本章我们研究两个例子：\n 归约核函数 矩阵转置核函数  共享内存 共享内存（shared memory，SMEM）是GPU的一个关键部分，物理层面，每个SM都有一个小的内存池，这个线程池被次SM上执行的线程块中的所有线程所共享。共享内存使同一个线程块中可以相互协同，便于片上的内存可以被最大化的利用，降低回到全局内存读取的延迟。 共享内存是被我们用代码控制的，这也是是他称为我们手中最灵活的优化武器。 结合我们前面学习的一级缓存，二级缓存，今天的共享内存，以及后面的只读和常量缓存，他们的关系如下图：\nSM上有共享内存，L1一级缓存，ReadOnly 只读缓存，Constant常量缓存。所有从Dram全局内存中过来的数据都要经过二级缓存，相比之下，更接近SM计算核心的SMEM，L1，ReadOnly，Constant拥有更快的读取速度，SMEM和L1相比于L2延迟低大概20~30倍，带宽大约是10倍。 下面我们了解下共享内存的生命周期和读取性质。 共享内存是在他所属的线程块被执行时建立，线程块执行完毕后共享内存释放，线程块和他的共享内存有相同的生命周期。 对于每个线程对共享内存的访问请求\n 最好的情况是当前线程束中的每个线程都访问一个不冲突的共享内存，具体是什么样的我们后面再说，这种情况，大家互不干扰，一个事务完成整个线程束的访问，效率最高 当有访问冲突的时候，具体怎么冲突也要后面详细说，这时候一个线程束32个线程，需要32个事务。 如果线程束内32个线程访问同一个地址，那么一个线程访问完后以广播的形式告诉大家  后面的全章内容都是基本围绕如何避免访问冲突，高效的是有共享内存来展开的。\n注意我们刚才说的共享内存的生命周期是和其所属的线程块相同的，这个共享内存是编程模型层面上的。物理层面上，一个SM上的所有的正在执行的线程块共同使用物理的共享内存，所以共享内存也成为了活跃线程块的限制，共享内存越大，或者块使用的共享内存越小，那么线程块级别的并行度就越高。 共享内存，高端有限资源，合理使用！\n接着说说可编程，矩阵乘法的串行形式，最简单的方式是三层循环，通过调整循环可以获得更好的缓存命中率，这个题在找工作笔试的时候有，当时在大学笔试工作的时候，我会很傻x的在笔试习题上写上注释，可以通过调整循环顺序提高缓存命中率，但是现在想一下，CPU的缓存是不可控制的，你只能调整自己的程序来适应它。 GPU高端的一点，就是你不止有一个缓存可以编程控制，而是有好几个。\n共享内存分配 分配和定义共享内存的方法有多种，动态的声明，静态的声明都是可以的。可以在核函数内，也可以在核函数外（也就是本地的和全局的，这里是说变量的作用域，在一个文件中），CUDA支持1，2，3维的共享内存声明，当然多了不知道支不支持，可能新版本支持，但是要去查查手册，一般情况下我们就假装最多只有三维。 声明共享内存通过关键字：\n__shared__ 声明一个二维浮点数共享内存数组的方法是：\n__shared__ float a[size_x][size_y]; 这里的size_x,size_y和声明c++数组一样，要是一个编译时确定的数字，不能是变量。 如果想动态声明一个共享内存数组，可以使用extern关键字，并在核函数启动时添加第三个参数。 声明:\nextern __shared__ int tile[]; 在执行上面这个声明的核函数时，使用下面这种配置：\nkernel\u0026lt;\u0026lt;\u0026lt;grid,block,isize*sizeof(int)\u0026gt;\u0026gt;\u0026gt;(...); isize就是共享内存要存储的数组的大小。比如一个十个元素的int数组，isize就是10. 注意，动态声明只支持一维数组。\n共享内存存储体和访问模式 声明和定义是代码层面上的产生了共享内存，接下来我们看看共享内存是怎么存储以及是如何访问的。上一章我们研究了全局内存，并且明确的学习了带宽和延迟是如何对核函数造成性能影响的。共享内存是用来隐藏全局内存延迟以及提高带宽性能的主要武器之一。掌握武器的办法就是了解武器的工作原理和各个部件的特性。\n内存存储体 共享内存是一个一维的地址空间，注意这句话的意思是，共享内存的地址是一维的，也就是和所有我们前面提到过的内存一样，都是线性的，二维三维更多维的地址都要转换成一维的来对应物理上的内存地址。 共享内存有个特殊的形式是，分为32个同样大小的内存模型，称为存储体，可以同时访问。32个存储体的目的是对应一个线程束中有32个线程，这些线程在访问共享内存的时候，如果都访问不同存储体（无冲突），那么一个事务就能够完成，否则（有冲突）需要多个内存事务了，这样带宽利用率降低。 是否有冲突，以及冲突如何发生我们下面介绍。\n存储体冲突 当多个线程要访问一个存储体的时候，冲突就发生了，注意这里是说访问同一个存储体，而不是同一个地址，访问同一个地址不存在冲突（广播形式）。当发生冲突就会有等待和更多的事务产生，这是严重影响效率的。 线程束访问共享内存的时候有下面3种经典模式：\n 并行访问，多地址访问多存储体 串行访问，多地址访问同一存储体 广播访问，单一地址读取单一存储体  并行访问是最常见，也是效率较高的一种，但是也可以分为完全无冲突，和小部分冲突的情况，完全无冲突是理想模式，线程束中所有线程通过一个内存事务完成自己的需求，互不干扰，效率最高，当有小部分冲突的时候，大部分不冲突的部分可以通过一个内存事务完成，冲突的被分割成另外的不冲突的事务被执行，这样效率稍低。 上面的小部分冲突变成完全冲突就是串行模式了，这是最糟糕的形式，所有线程访问同一个存储体，注意不是同一个地址，是同一个存储体，一个存储体有很多地址。这时就是串行访问。 广播访问是所有线程访问一个地址，这时候，一个内存事务执行完毕后，一个线程得到了这个地址的数据，他会通过广播的形式告诉其他所有线程，虽然这个延迟相比于完全的并行访问并不慢，但是他只读取了一个数据，带宽利用率很差。\n最优访问模式（并行不冲突）： 不规则的访问模式（并行不冲突）：\n不规则的访问模式（并行可能冲突，也可能不冲突）\n这时候又两种可能\n 冲突：这时候就要等待了 不冲突：访问同一个存储体的线程都要访问同一个地址，通过广播解决问题。  以上就是产生冲突的根本原因，我们通过调整数据，代码，算法，最好规避冲突，提高性能。\n访问模式 共享内存的存储体和地址有什么关系呢？这个关系决定了访问模式。内存存储体的宽度随设备计算能力不同而变化，有以下两种情况：\n 2.x计算能力的设备，为4字节（32位） 3.x计算能力的设备，为8字节（64位）  怎么理解宽度呢，我们假设我们这有三十二个水桶，每个水桶当做一个存储体，桶的口大小是固定的，假设我们用桶装西瓜，每个桶的口最多同时能拿出四个西瓜，能拿出四个西瓜，宽度就是4，能拿出八个就是八，这就是宽度的通俗解释。 然后我们将一排西瓜编号，从0开始，一直到n然后我们有三十二个编了号的桶（0~31号），摆成一排，然后往桶里同时装西瓜，因为一次只能装四个西瓜，那么我们把0~3号西瓜装到0号桶，4~7号习惯装入1号桶，以此类推，当装到第31号桶的时候，我们装 124~127号西瓜；然后我们每个桶里都有四个西瓜了，接着我们将128~131号西瓜装入0号桶，开始下一轮装西瓜。 这就是共享内存的存储体的访问模式， 那么我们已知西瓜的编号怎么知道西瓜在哪个桶里呢? 公式如下 $$ 桶号=\\frac{西瓜编号\\div4}{32}%32 $$ 当然这是我们的例子，转换成内存地址的话，西瓜编号对应的就是字节的地址，宽度就是4个西瓜，4字节（8字节），32个桶对应32个存储体： $$ 存储体索引=\\frac{字节地址\\div4}{存储体数}%存储体数 $$ 上面公式中 $%$ 表示取余。 我们来看个正规的图：\n这个和我们的西瓜图内容一致，知识第一个byte address放成直线了，而我把西瓜放成了二维的，实际上一维的是准确的。 同一个线程束中的两个线程访问同一个地址不会发生冲突，一个线程读取后广播告诉有相同需求的线程。但是对于写入，这个就不确定了，结果不可预料。 上面我们介绍的存储体宽度是4的情况，宽度是8的情况同样，但是宽度变宽了，其带宽就有变宽了，比如，我们之前一次只能取四个西瓜，现在可以取八个西瓜了，这时候如果有两个线程访问同一个存储体，按照我们前面的解释，一种是访问同一个地址，这时候通过广播来解决冲突，还有一种冲突是需要用等待解决的，当桶变宽了，如果一个线程想要桶里左边的西瓜，而一个线程想要右边的西瓜，这时候是不冲突的，因为桶是够宽的。 或者我们可以理解为更宽的桶，在桶中间又进行了一次间隔，左右两边各一个空间，读取不影响，如果两个线程都要左边的西瓜则等待，如果一个要左边的一个要右边的，这时候可以同时进行不冲突。 把桶换成存储体就是\n下图显示64位宽的存储体无冲突访问的一种情况，每个bank被划分成了两部分\n下图是另一种无冲突方式：\n一种冲突方式，两个线程访问同一个小桶：\n另一种冲突方式，三个线程访问同一个小桶\n内存填充 存储体冲突会严重影响共享内存的效率，那么当我们遇到严重冲突的情况下，可以使用填充的办法让数据错位，来降低冲突。 假如我们当前存储体内的数据罗列如下，这里假设共4个存储体，实际是32个\n当我们的线程束访问bank0中的不同数据的时候就会发生一个5线程的冲突，这时候我们假如我们分配内存时候的声明是：\n__shared__ int a[5][4]; 这时候我们的就会得到上面的图中的这种内存布局，但是当我们声明的时候改成\n__shared__ int a[5][5]; 就会产生这个效果，在编程时候加入一行填充物\n然后编译器会将这个二维数组重新分配到存储体，因为存储体一共就4个，我们每一行有5个元素，所以有一个元素进入存储体的下一行，这样，所有元素都错开了，就不会出现冲突了。\n这个例子可能有点难懂，比如，我们还是举例子装西瓜，当我们装到第31个的时候，我们不想把32号33号34号35号放到0号桶里，怎么办，叫四个冬瓜过来占位置，那么其就应该放在31号后面，加4列冬瓜。 共享内存在确定大小的时候，比如编译的时候，就已经被确定好每个地址在哪个存储体中了，想要改变分布，就在声明共享内存的时候调整就行，跟将要存储到共享内存中的数据没有关系。 注意：共享内存声明时，就决定了每个地址所在的存储体，想要调整每个地址对应的存储体，就要扩大声明的共享内存的大小，至于扩大多少，就要根据我们前面的公式好好计算了。这段是本文较难理解的一段。\n访问模式配置 访问模式查询：可以通过以下语句，查询是4字节还是8字节：\ncudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig * pConfig); 返回的pConfig可以是下面的结果：\ncudaSharedMemBankSizeFourByte cudaSharedMemBankSizeEightByte 在可以配置的设备上，可以用下面函数来配置新的存储体大小：\ncudaError_t cudaDeviceSetShareMemConfig(cudaSharedMemConfig config); 其中 config可以是：\ncudaSharedMemBankSizeDefault cudaSharedMemBankSizeFourByte cudaSharedMemBankSizeEightByte 不同的核函数启动之间，更改共享内存的配置，可能需要一个隐式的设备同步点，更改共享内存存储体的大小不会增加共享内存的使用，也不会影响内核函数的占用率，但其对性能可能有重大的影响。大的存储体可能有更高的带宽，大可能导致更多的冲突，要根据具体情况进行分析。\n配置共享内存 每个SM上有64KB的片上内存，共享内存和L1共享这64KB，并且可以配置。CUDA为配置一级缓存和共享内存提供以下两种方法：\n 按设备进行配置 按核函数进行配置  配置函数：\ncudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig); 其中配置参数如下：\ncudaFuncCachePreferNone: no preference(default) cudaFuncCachePreferShared: prefer 48KB shared memory and 16 KB L1 cache cudaFuncCachePreferL1: prefer 48KB L1 cache and 16 KB shared memory cudaFuncCachePreferEqual: prefer 32KB L1 cache and 32 KB shared memory 那种更好全看核函数：\n 共享内存使用较多，那么更多的共享内存更好 更多的寄存器使用，L1更多更好。  另一个函数是通过不同核函数自动配置的。\ncudaError_t cudaFuncSetCacheConfig(const void* func,enum cudaFuncCacheca cheConfig); 这里的func是核函数指针，当我们调用某个核函数时，次核函数已经配置了对应的L1和共享内存，那么其如果和当前配置不同，则会重新配置，否则直接执行。 一级缓存和共享内存都在同一个片上，但是行为大不相同，共享内存靠的的是存储体来管理数据，而L1则是通过缓存行进行访问。我们对共享内存有绝对的控制权，但是L1的删除工作是硬件完成的。 GPU缓存比CPU的更难理解，GPU使用启发式算法删除数据，由于GPU使用缓存的线程更多，所以数据删除更频繁而且不可预知。共享内存则可以很好的被控制，减少不必要的误删造成的低效，保证SM的局部性。\n同步 同步是并行的重要机制，其主要目的就是防止冲突。同步基本方法：\n 障碍 内存栅栏  障碍是所有调用线程等待其余调用线程达到障碍点。 内存栅栏，所有调用线程必须等到全部内存修改对其余线程可见时才继续进行。 有点蒙圈？没事，我们下来了解下理解这两个概念的预备知识。\n弱排序内存模型 CUDA采用宽松的内存模型，也就是内存访问不一定按照他们在程序中出现的位置进行的。宽松的内存模型，导致了更激进的编译器。 一下这一点非常重要：\n GPU线程在不同的内存，比如SMEM，全局内存，锁页内存或对等设备内存中，写入数据的顺序是不一定和这些数据在源代码中访问的顺序相同，当一个线程的写入顺序对其他线程可见的时候，他可能和写操作被执行的实际顺序不一致。 指令之间相互独立，线程从不同内存中读取数据的顺序和读指令在程序中的顺序不一定相同。 换句话说，核函数内连续两个内存访问指令，如果独立，其不一定哪个先被执行。 在这种混乱的情况下，为了可控，必须使用同步技术，否则真就是一千只脱了缰的哈士奇，万马奔腾的场景了。\n 显示障碍 CUDA中，障碍点设置在核函数中，注意这个指令只能在核函数中调用，并只对同一线程块内线程有效。\nvoid __syncthreads();  __syncthreads()作为一个障碍点，他保证在同一线程块内所有线程没到达此障碍点时，不能继续向下执行。 同一线程块内此障碍点之前的所有全局内存，共享内存操作，对后面的线程都是可见的。 这个也就能解决同一线程块内，内存竞争的问题，同步，保证先后顺序，不会混乱。 避免死锁情况出现，比如下面这种情况，就会导致内核死锁：  if (threadID % 2 == 0) { __syncthreads(); } else { __syncthreads(); } 只能解决一个块内的线程同步，想做块之间的，只能通过核函数的执行和结束来进行块之间的同步。（把要同步的地方作为核函数的结束，来隐式的同步线程块）  内存栅栏 内存栅栏能保证栅栏前的内核内存写操作对栅栏后的其他线程都是可见的，有以下三种栅栏：块，网格，系统。\n 线程块内：  void __threadfence_block(); 保证同一块中的其他线程对于栅栏前的内存写操作可见 2. 网格级内存栅栏\nvoid __threadfence(); 挂起调用线程，直到全局内存中所有写操作对相同的网格内的所有线程可见\n系统级栅栏，夸系统，包括主机和设备，  void __threadfence_system(); 挂起调用线程，以保证该线程对全局内存，锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程可见。\nVolatile修饰符 volatile声明一个变量，防止编译器优化，防止这个变量存入缓存，如果恰好此时被其他线程改写，那就会造成内存缓存不一致的错误，所以volatile声明的变量始终在全局内存中。\n总结 本文有点长，但是作为概览，我们后面都要围绕这篇展开，没有什么代码，纯理论的东西，多读多理解，多查资料多翻书。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-5-1-cuda%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E6%A6%82%E8%BF%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文为CUDA内存的概述，介绍共享内存的模型，分配，访问，配置，同步等内容\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 模型，分配，访问，配置，同步\u003c/p\u003e","title":"【CUDA 基础】5.1 CUDA共享内存概述"},{"content":"Abstract: 本文是第五章关于CUDA共享内存和常量内存的概述 Keywords: 共享内存，常量内存\n共享内存和常量内存 本文是CUDA第五章的概论，来给出本章的大概思路，文章短小，不说废话。\n共享内存和常量内存 在本章中，我们要学习：\n 数据在共享内存中的安排 二维共享内存到线性全局内存的索引转换 解决不同访问模式中的存储体中的冲突 在共享内存中缓存数据以减少对全局内存的访问 使用共享内存避免非合并全局内存的访问 常量缓存和只读缓存之间的差异 线程束洗牌指令编程  前面我们主要研究了全局内存的使用，如何通过不同的方式提高全局内存的访问效率。虽然未对其的内存访问是没有问题的，因为现代GPU都有一级缓存了。但是跨全局内存的非合并内存访问，还是会导致带宽利用率不佳的效果。但是非合并内存访问在实际应用时无法避免，在这时可能使用共享内存，那么共享内存就是提高效率的关键。\n总结 本章我们主要研究如何使用共享内存进行编程，数据在共享内存中如何被存储，数据元素是怎样使用不同的访问模式被映射到内存存储体（硬件）上的，以及使用共享内存提高核函数性能的方法\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-5-0-%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%92%8C%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是第五章关于CUDA共享内存和常量内存的概述\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 共享内存，常量内存\u003c/p\u003e","title":"【CUDA 基础】5.0 共享内存和常量内存"},{"content":"Abstract: 本文介绍二进制数字的相关知识 Keywords: 数制，二进制，十进制\n二进制数字 世界一片混乱，大家都说自己是受害者，而当人们接受了不对称不完整的信息后会产生极大的错误判断和所谓的民族情绪，个人认为：你看到的都是别人想让你看到的，而你分析出来的才有可能是真实的。 今天研究二进制，我们在《陶哲轩实分析》中只学了自然数的定义就没有继续了，原因是我觉得那个系列写起来太废时间，而且过多的是整理工作，自己的思考比较少，所以就暂时停止了，是继续还是就此打住，我觉得打住的可能性较大。 自然数的定义可以参考：https://face2ai.com/Math-Analysis-2-0-The-Natural-Numbers/\n数是一个不存在实体的概念，而我们写的1，2，3，只是一种虚拟概念的一种可见的映射，换句话说，这种实体不只有一种，而且只是一个幻影，一个标志，其本身是没有意义的，只是一个标志，注意我说的是1，2，3是数的标志。当然我们也可以用I，II，III的标志。 上面这句迷糊的话想表示的就是数十个虚无的概念，没有实体，我们人类为了表示，记录简单，自己发明了1，2，3这些符号来使数可见。 当然我们说的二进制，也是数的一种标志，00，01，10和1，2，3是一一对应的，每一组对应都表示同一个数。 计算机为什么使用二进制这是计算机基础里面研究的，我就不说了，二进制表现如下： $$ \\cdots b_3b_2b_1b_0.b_{-1}b_{-2}\\cdots $$ 我们下面将注意力转移到上图的 $f$ 中，也就是如何进行数制转换。\n十进制转化为二进制 我们分别处理整数和小数。 整数部分的转化，我们用除（以）2取余的方法： 例如将 $(52)_{10}$ 转换成二进制： $$ 52\\div2=26\\cdots 1\\ 26\\div2=13\\cdots 0\\ 13\\div2=6\\cdots 1\\ 6\\div2=3\\cdots 0\\ 3\\div2=1\\cdots 1\\ 1\\div2=0\\cdots 1\\ $$\n所以 $(52){10}=(110101)2$ 这个我记得高中就学过，教材上称之为辗转相除法（够辗转的），其实说白了就是利用了整数除法的性质来解下面的方程 证明： $$ x{0{10}}=a_{m}2^{m}+\\cdots+a_{3}2^{3}+a_{2}2^{2}+a_{1}2^{1}+a_{0}2^{0}\\ \\text{suppose: }x_{0_{10}}=2\\times x_{1_{10}}+y\\ 2\\times x_{1_{10}}+y_{1_{10}}=a_{m}2^{m}+\\cdots+a_{3}2^{3}+a_{2}2^{2}+a_{1}2^{1}+a_{0}2^{0}\\ \\text{so: } \\ y_{1_{10}}=a_{0}\\ y_{2_{10}}=a_{1}\\ \\vdots\\ y_{m+1_{10}}=a_{m}\\ $$ 小数部分道理一样，只是把除以二，变成了除以二分之一，也就是乘以二，下面我们将 $(0.7)_{10}$ 转化为二进制： $$ 0.7\\times2=0.4+1\\ 0.4\\times2=0.8+0\\ 0.8\\times2=0.6+1\\ 0.6\\times2=0.2+1\\ 0.2\\times2=0.4+0\\ 0.4\\times2=0.8+0\\ \\vdots\\ $$\n我们就得到了一个循环小数 $0.101100110\\cdots$ 表示成传统形式就是 $0.1\\overline{0110}$ 备注，有些时候渲染成htnl会偏移，横线位于 $0110$ 上方。\n二进制转化为十进制 二进制转十进制就更简单了，对于有限位数，就算乘法和加法就行，但是对于有循环的部分我们就需要数学技巧了。 整数比较简单，我们只举个栗子，不做特殊证明，$(10101)_2$ 转化成十进制：\n$$ 1\\times 2^4+0\\times 2^3+1\\times 2^2+0\\times 2^1+1\\times 2^0=21_{10} $$\n小数部分如果是有限的，只需要把上面的基换成 $\\frac{1}{2}$ 即可，这里就省略了，但是对于循环小数，我们来点数学技巧，举个例子 $0.\\overline{1011}$ (横线在1011上)：\n$$ x_{10}=0.\\overline{1011}2\\ x{10}\\times 2^4=0.\\overline{1011}2\\times (2^4)=1011.\\overline{1011}2\\ x{10}\\times2^4-x{10}=1011_2=11_{10}\\ x_{10}=\\frac{11}{15} $$\n一个简单的数学技巧，主要用于小数点后面紧跟着循环体，但是如果小数点后面没有紧跟着循环体怎么办，当然是移位啦，比如 $0.10\\overline{101}$ (横线在101上)我们的做法是：\n$$ x_{10}=0.10\\overline{101}2\\ x{10}\\times 2^2=10.\\overline{101}2\\ x{10}\\times 2^2-10=0.\\overline{101}_2\\ $$\n然后用上面的方法处理 $0.\\overline{101}$ 即可\n2进制是计算机的根基，目前计算机都是基于2进制的，但是2进制写起来实在太长，为了简短，我们可以找一个2的幂次，但是和10差不多的基，所以计算机里还有8进制和16进制，这两个是和10最接近的2的幂次进制。16进制比较常用，四位2进制就是一个16进制，四是2的幂次，所以16进制使用较多,2进制只包含两个数字 $0,1$ 对应的十进制就是我们熟悉的 $0\\cdots9$ 十六进制则是在 $0\\cdots 9$ 的基础上扩展了 $a,b,c,d,e,f$\n总结 今天研究了二进制，不算研究，就是复习了一下数制之间的转化。\n","permalink":"https://go.face2ai.com/math/math-numerical-analysis-0-2-binary.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍二进制数字的相关知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 数制，二进制，十进制\u003c/p\u003e","title":"【数值分析】0.2 二进制数字"},{"content":"Abstract: 本文主要介绍数理统计学简史，以及数理统计学怎么应用 Keywords: 数理统计，数理统计应用\n数理统计理论导言 开篇废话 某乎上有人说《数理统计学教程》非常适合入门，实际情况是看了其中大多数内容两边，例子和讲解及其清晰，但是有些证明比较复杂，比较不好理解，并不是说作者给出的证明方法冗杂，而是那些结论本身证明就十分困难，所以内心就会有个矛盾，如果深入研究证明，会发现看不懂，因为涉及到一些技巧和知识可能我们并没有掌握，如果不看，又怕错过什么，这可能就是自学的另一个麻烦，当然，我在写博客的时候会把证明全写出来，并把每一步都解释清楚，当然这很耗时，但是应该会有很大的帮助。 这是导言，但是我觉得这个导言比后面的每一课都重要，但是我上学的时候是不看导言的，但是后来开始自学就开始看导言和介绍了，因为需要在里面找到本书的大致结构和安排，而本书的导言上来就是干货，而且是big picture级别的。\n所本文对于本系列的理解十二分重要！\n什么是数理统计学 没有定义 大多数数理统计学教材都对数理统计学的性质，任务，应用等做了详细的描述，这些在业界没有问题，大家都比较认可，但是如果试图用简单的语言正式定义数理统计学，那么就会引起不少麻烦，因为少量语言没办法准确的描述，这时候你无论落下谁的观点，都会被打击，所以定义什么是数理统计的定义，这里没有，当然如果你在哪本神书中看到相关的定义，请仔细揣摩，别被忽悠，当然，也别背，很有肯能记住一个错的定义。本文就是想通过描述一些数理统计的实质内容，来告诉大家我们研究来研究去，数理统计到底是个啥东西（至于定义是“专家”们喜欢卖弄的，我们不研究）\n研究内容 研究一件事简单的说可以用两种方法，一种是研究其所有前提，推导出结论，也就是正向推导，另一种方法是观察实验结果，来反推原理。 说个真事，应该是我小学五年级或者初一的时候，在学三角的时候，我觉得三角形角和边之间一定存在着某种关系，于是我用量角器，尺子开始在纸上画三角形，量数据，当时我记得似乎已经知道固定其中若干个变量，然后只改变一个或者两个变量，来统计结果，具体做法可能忘了，而且不可能算出余弦定理，因为我连sin和cos都没学，都十几年前的事了，这个思路就是观察结果，推原理的过程。 在观察和实验的方式下研究问题，有固定的几个步骤： 第一步：通过观察或实验收集必要的数据（小谭年轻的时候画三角，量某边（角）的长度（度数）） 第二步：对采集的数据进行分析，对研究的问题作出某种形式的推论（小谭的方法是找规律，某种形式的推论就是角的大小和边之间的关系） 这两步有数学问题，也有别的问题，比如画三角形就是绘画问题。我们关注的是数学问题，为了解决这些数学问题发展起来的理论和方法就构成了数理统计学的内容。数理统计学是数学分支是没问题的，其研究怎么用有效的方法去收集和使用带有随机性影响的数据。\n数据必须带有随机性 数据没有随机性，其不能成为数理统计的研究对象，比如我们记录了去年每一天的花销（假定全部正确无误，没有任何随机因素在其中），然后研究平均每天花了多少，这个问题就不是数理统计问题，这时候求平均值问题，数据是准确无误的，所以没有随机性。区别数理统计方法和其他数学方法的办法就是确定数据是否有随机性。 作为对比，确定一批产品的合格率，从一批产品中抽样，得到 $N$ 个产品，其中 $m$ 个废品，废品率是 $p=\\frac{m}{N}$ 这个就有随机性在里面，因为抽样会得到随机结果，因为抽到哪个产品是偶然的。 这是我们观察的时候得到的随机因素，在实验过程中随机性更多，比如我们在进行物理化学实验的时候，每一个操作都有可能带入误差，比如控制温度和气压，混入一定量溶液，这些过程都有随机误差在里面，在不同的条件以及误差的影响下，我们得到两个结论 $t_1,t_2$ 其中 $t_1$ 优于 $t_2$ 那么我们是否能说 $t_1$ 优于 $t_2$ 的原因是因为不同的条件，还是误差也起到了至关重要的作用。这也要用数理统计的方法进行分析。\n有效的方式收集数据 有效地才是重要的，理解有效的可以从两个方面：\n 这组数据是数学上可以处理的，或者用我们可以接受的模型来处理的，太过复杂或者复杂到不可理解的数据，对于我们来说等同于无效 数据重要尽可能的包含研究问题的有关信息  举个例子，调查某地区10000个家庭的收入情况，理论上我们不能每家每户都调查，不现实切不实际，所以我们想要抽取一部分样本，比如100户，那么有效性就要得到保证了，就是这个样本大小100是否能给我们足够的信息。以及这100户如何选取才能保证有效性，比如我们调的一百户都是什么比尔盖茨家族，摩根家族之类的，这个就扯淡了，或者一百户都是没有劳动能力的低收入家庭，这些数据对于我们研究整个地区所包含的信息太少，或者说不全面，这是无效的，所以如何挑选这100户也变得非常重要。一种有效地方法是随机选取，前提是保证每一户被抽取的可能性相同，但是这要求样本应该要大一些，而100够不够是个问题，另一种就是先分类，分成高收入家庭，和低收入家庭，按比例从两个类中随机挑选不同的样本。后一种方法我们认为比前一种随机挑选要有效。 在设计实验时要考虑可实施的性质，比如在一个产品实验中我们考虑温度和压强对产品质量的影响，我们可能需要实验来测试，如果我们选择4个温度和4个压强，就要进行16次试验，这可能还是可以接受的，但是如果我们要考察三个元素，而且每个元素可能的取值不止四个，那么这个试验次数基本就是不可接受的，所以选择哪些元素，每个元素取哪些值，这个都是需要谨慎思考和研究的，并且需要充分的使用学科背景知识，尽可能的挑选对问题有实质性反应的信息。\n上面两个例子并不是我随便说的，而是陈老师书上写的，并且这两个问题对应于统计学中两个重要分支：\n 抽样理论 实验设计(试验设计)  这两个分支对应于上面例子的那个系列问题的研究。\n有效地使用数据 当我们得到数据了以后就要使用数据了，我们收集数据的目的不是为了收集数据然后存储起来，我们收集数据的目的是解决一个相对应的问题的，这就使得使用数据需要更加谨慎，因为通过使用数据我们要得出问题的结论，这个结论我们在统计上称之为 “推断” ，强调一点，我们手机来的有效地数据直观上不可能看出我们问题的答案，如果能看出来，那么这就不是一个统计学问题，而是一个看数字的结论的问题。我们要做的是在有效的数据上，使用某种有效地方法，尽可能的提炼数据中能够解答问题的有关信息。尽可能是指我们不可能完全提炼出所有的有关信息，因为这些数据（信息）在收集的时候包含随机性，随机的添加无用信息，随机的丢掉有用信息，我们的数理统计方法就是要尽可能的缩小这些随机错误对我们结果的干扰，但是不可能完全消除。 关于“推断”的相关问题我们会在后面连续讨论，并且这也是数理统计中的一个核心问题。 接下来我们要说“模型”，但是并没有严格的定义这个概念，我们就把它当做一个算法或者一套算法来对待，后面会有严格的定义： 为有效地使用数据进行统计推断，涉及不少数学问题，我们需要建立一个数学模型，并且制定某些准则，才能根据模型和准则得出这个统计方法的优劣；比如对于某一组数据，我们用A，B两个模型进行建模，考虑他们的性质 $n$ 并且理论上来将， $n$ 越大越好，在这个对比中 $n_A \u0026gt; n_B$ 那么我们就说在某种衡量标准下，A模型优于B模型。 举个例子，称重，我们称一个物体9次，得到9个值 $x_0,\\cdots,x_8$ ，每个值都不同，因为称量，观察都有误差被引进，那么我们建立以下三种模型来降低误差干扰：\n 使用平局值 $\\overline{x}=\\frac{1}{9}\\sum_{i=0}^{8}x_i$ 作为物体的重量 将这些测量值排序： $x^{0}\\leq \\cdots x^{8}$ 然后取中间值 $x^{5}$ 作为物体重量 依旧使用2中的排序结果，而将 $\\frac{x^{0}+x^{8}}{2}$ 作为物体重量  这就是3个不同的模型，但是哪种好可能每个人都有不同的看法，比如你可能认为1中均值比较好，也有可能认为2中的中位数好，当然3也可以好，但是每一个在什么条件下最优是不一样的，在什么条件下，为什么最优成为数理统计学的中心内容，这个研究过程需要使用大量的数学，概率的工具，在后面我们看到在不同的随机（随机性的概率结构，或者统计模型）影响下，在不同的衡量指标下，都可以作为此条件下的最优结果。\n数理统计是数学问题 一直以来不少人都想说数理统计学不属于数学分支，当然我也不知道他们想把概率弄去哪个分支，不会被加入到CS中去吧😆 。但是数理统计只处理在收集和使用过程中带随机性影响的数据中的数学问题，所以，他就是一个数学分支。 接下来我们说说数理统计学研究的对象是什么 简单来说是数据，数理统计学的应用不局限于数学各学科，物理化学生物计算机航天等，只要你能根据你们学科的知识，设计有效地试验，并且采集到有效地数据，给出有效地模型。 接下来数理统计学就可以在不知道任何背景下，通过数据和模型对问题进行分析和研究，并且这步是完全数学的——这就是数理统计学研究的对象，这些超脱于试验的数据和给定的模型。 一个数理统计学者可以不问相关专业的知识，但是这有非常多的坏处。因为只有对问题有深入的研究才能帮助提出有效的模型以及有效收集数据的方法。 比如一个遗传学方面的学者很了解数理统计，那么会对其研究有非常大的帮助，相反一个数理统计学者不了解遗传学，可以肯定他不会在遗传方面有什么成就。 数理统计的方法的使用不需要高深的数理统计学理论知识，这也使得统计能够被很多行业的研究者从业者使用，而他们所使用的准确的是时统计学的方法，或者叫做应用统计，这些并不需要他们有很多的数理统计知识。美国将统计学中涉及数学基础的部分叫做数理统计学，这是狭义的定义，我们国家是为了区分和社会学科的统计学，才有了数理统计这个名字，所以我们的数理统计不光有数学基础，还有应用，这相当于美国等国的统计学。\n数理统计学的应用 应用有很多，我们说多了啰嗦，说少了有没啥意思，简单的说，人类所有活动里都有数理统计，这么说一点不过分。 农业上某个指标对产量的额影响，这个问题可以用数理统计解决。 工业上，同样的在某个生产过程，某个因素对产量质量的影响，也要考数理统计分析。 天气预报，金融，地质，社会学，等也有很多应用。\n总结 本文是第一篇，应该也是非常非常重要的一篇，希望大家不止局限于本文，而是要查阅相关更多的书籍资料，博客和别人的讲述最多只能给你一个骨架，让你不会跑偏，吃肉还是要去看书。\n原文地址：https://www.face2ai.com/Math-Statistics-Basis-1-1-Introduction转载请标明出处\n","permalink":"https://go.face2ai.com/math/math-statistics-basis-1-1-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍数理统计学简史，以及数理统计学怎么应用\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 数理统计，数理统计应用\u003c/p\u003e","title":"【数理统计理论基础】 1.1 导言"},{"content":"Abstract: 本文介绍多项式求值的相关内容。 Keywords: 多项式求值，霍纳方法\n多项式求值 在高中的时候就学过这类知识，但是当时确实不知道怎么用，因为那时候多项式分解成递归形式是数学课上讲的，那时候的教材刚把Basic语言放在书里，老师当然是数学老师，虽然高中数学老师在数学方面非常厉害，但是编程当然不行啦，所以他没说过这个是用在计算机里面的，可能他也不清楚，或者我没听到。但是从今天的课程中，我们能发现，这种递归的分解多项式能有效减少计算量。\n多项式求值 计算机最核心部分就是中央计算单元，只能进行加法和乘法的计算，以及逻辑计算，为什么？你去看看数字电路的数，乘法器和加法器是最好实现的，而通过加法和乘法又可以实现多项式计算（减法和除法被我们看成加法和乘法的一种形式），多项式可以用来近似各种函数，所以得出揭露： 计算机计算各类函数都可以转换成多项式计算。 所以多项式之于计算机数学计算，相当于砖之于房子。 每一块砖都达到了最结实状态，房子才能达到最坚固状态。 每个多项式性能最优，整体才能性能最优。 所以我们首先就研究多项式\n三种方法 方法1: \n$$ P(x)=2x^4+3x^3-3x^2+5x-1 $$ 当我们把所有这些都放在寄存器里了，包括 $x$ 的值（这步告诉我们不考虑内存事务的延迟，只考虑计算），假设 $x=\\frac{1}{2}$ 我们常规的计算是： $$ P(\\frac{1}{2})=2\\times \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2} +3\\times \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2} -3\\times \\frac{1}{2}\\times \\frac{1}{2} +5\\times \\frac{1}{2}-1 $$\n这是最直观的做法，所有人都会。\n统计下计算次数： 乘法—— 10次 加法—— 4 次 计算机完成减法和加法的速度是一致的，这里不需要担心减法的影响，当然除法就有些特殊了，他比乘法要慢很多很多（详情可以参考《深入理解计算机系统》）\n方法2: \n我们发现这里有不少是重复计算的，减少重复计算是我们提高性能最直接的方法，我们观察发现 $$ x^4=x\\times x^3\\ x^3=x\\times x^2\\ \\vdots $$ 那么我们先计算 $$ \\frac{1}{2}\\times \\frac{1}{2} $$ 并保存为结果啊，然后计算 $\\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2}$ 时，就可以计算 $$ \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2}=\\frac{1}{2}\\times a $$ 消耗一个多于存储空间但是计算量能减少不少。 只计算了 $\\frac{1}{2}$ 相乘的3次，加上系数相乘的4次，以及4次加法。\n统计下计算次数： 乘法—— 7 次 加法—— 4 次 乘法降低了3次，加法次数不变，只减少了3次乘法，用处大吗？ 如果你就计算一次，当然没啥影响，nm级别的优化等于没优化，但是如果大量使用几十亿或者更多次，那就有影响了，而几十亿次好像经常被使用。 或者从相对加速的角度看，我们忽略加法计算，原因是加法计算的耗时相对于乘法来说比较小，我们忽略其影响，然后我们得到了 $\\frac{10-7}{10}\\times 100%=30%$ 的性能提升,这个看起来就很可观了。 那么还能提高么？ 答案当然是能： 方法3: \n调整多项式的形式： $$ \\begin{aligned} P(x) \u0026amp;=2x^4+3x^3-3x^2+5x-1\\ \u0026amp;=-1+x(5-3x+3x^2+2x^3)\\ \u0026amp;=-1+x(5-x(3+3x+2x^2))\\ \u0026amp;=-1+x(5-x(3+x(3+2x)))\\ \\end{aligned} $$\n这个分解方式被称为 嵌套乘法 或者 霍纳方法 其直观表现就是 $n$ 阶多项式可以分解成只有 $n$ 次乘法，以及 $n$ 次加法的形式。\n统计下计算次数： 乘法—— 4 次 加法—— 4 次 深入理解这个例子 这个方法不止解决了多项式问题，作为一个例子体现了我们科学计算方法中的所有特征：\n 计算机在简单计算的时候速度极快 简单计算会被多次重复执行，所以高效的简单计算非常重要 最好的计算方法并不是最显而易见的那种。  20世纪后五十年，结合计算机硬件，常见问题已经开发出了许多有效的求解方法（也就是利用计算机帮我们高效的完成数学计算）。 根据霍纳方法，我们可以直接把任意多项式套用到 $$ c_1+x(c_2+x(c_3+x(\\dots))) $$ 中去，但是有时候需要点特殊形式，比如后面的插值可能就要用到这种形式 $$ c_1+(x-r_1)(c_2+(x-r_2)(c_3+(x-r_3)(\\dots))) $$ 这里面新加的参数 $r_i \\text{ for }i=1,2,3,\\dots$ 被叫做基点，令其全为0，就回到了我们上面的霍纳方法的最初形式了。\n编程实现 然后我们用Matlab实现以下：\nfunctiony=nest(d,c,x,b)if nargin\u0026lt;4, b=zeros(d,1);end y=c(d+1); for i=d👎1 y=y.*(x-b(i))+c(i); end 运行结果：\n\u0026gt;\u0026gt; nest(4,[-1 5 -3 3 2],1/2,[0 0 0 0]) ans = 1.2500 Matlab的代码比较简单，虽然我们写过matlab的代码，但是这段似乎挺简单，但是这语法，写C太久，看别的语言总感觉语法混乱/(ㄒoㄒ)/~~\nif nargin\u0026lt;4, b=zeros(d,1);end 这句是判断参数的，如果小于4， $b$ 用0填充 Matlab还可以同时输入多个 $x$ 真的很神奇 😓\n\u0026gt;\u0026gt; nest(4,[-1 5 -3 3 2],[-2 -1 0 1 2]) ans = -15 -10 -1 6 53 一个插值的例子，也就是 $b$ 不为空\n$$ P(x)=1+x(\\frac{1}{2}+(x-2)(\\frac{1}{2})+(x-3)(-\\frac{1}{2})) $$\n\u0026gt;\u0026gt; nest(3,[1 1/2 1/2 -1/2],1,[0 2 3]) ans = 0 总结 多项式是所有计算的基础，但是第一课就写他的原因更多的是其优化开发过程非常具有代表性，代表了数值分析的基本过程和特性，所以作为第一课共大家参考。\n","permalink":"https://go.face2ai.com/math/math-numerical-analysis-0-1-polynomial-evaluation.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍多项式求值的相关内容。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 多项式求值，霍纳方法\u003c/p\u003e","title":"【数值分析】0.1 多项式求值"},{"content":"Abstract: 数值分析课程介绍 Keywords: 数值分析\n数值分析介绍 数值分析并没有在我们那张人工智能基础数学地图上 但是根据MIT和Stanford人工智能课程安排，数值分析这门课都是在大一或者大二的必修课，换句话说，学完线性代数和微积分，就要学数值分析，为什么数值分析没在我们的数学地图上，但却被这些大学如此之重视呢？当然我上大学也是大二就开了这么课。 我们前面学到的线性代数也好，微积分也好，概率也好，我们都是在用笔计算，博客中也有反映从头到尾，没看到我写程序吧。并且我尝试了写微积分的博客，尝试了几次都放弃了，原因就是基本都是用笔计算的东西，需要写大量的计算公式，我就放弃了。 那么当代计算主要肯定不是用手算出来的，如何连接计算机和数学各大分支就变成了重要中的重要。 没错，数值分析就是这个桥梁，数值分析。 本系列的主要目的是阐述并讨论计算机上求解数学问题的方法，从最基础的乘法加法开始，通过分析计算结果，调整计算顺序，得到更准更快的算法，并且分析计算结果的误差。从基础理论逐步过渡到更加复杂的概念。 章节安排： 0. 基础知识 - 0.1 多项式求值\n 求解方程 方程组 插值 最小二乘 数值微分和积分 微分方程 边值问题 偏微分方程 随机数和应用 三角插值和FFT 压缩 特征值与奇异值 最优化  章节关系： 上图表示个个章节关系，注意本书中的所有问题我们可能都要研究下，但是微分方程等可能会放在后面，因为相关理论我们还没研究。 我们的参考书为：《Numerical Analysis Second Edition》，Timothy Sauer著。 之前没注意有没有英文版，所以用的是翻译版。\n总结 本系列预计每天更新，每篇应该都不长，所有代码使用Matlab编写。当然经过调整或者不调整，就直接可以在octave下运行，当然我鼓励大家使用octave。 关于编程工具，我们目前做的是上层工作，以模型开发为主，所以需要快速实现模型的编程，试验阶段高效并没有那么重要，当模型有优秀表现的时候，我们在使用C或者CUDA进行二次开发，结合我们的CUDA系列完成最终工作。 学习数值分析和CUDA都是为了使得计算结果更快，但一个从算法角度，加速分析结果，一个从硬件和技术角度完成加速，两种最重要的途径，我们都要掌握。手持利剑，身怀秘籍，才是绝世高手。\n","permalink":"https://go.face2ai.com/math/math-numerical-analysis-0-0-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数值分析课程介绍\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 数值分析\u003c/p\u003e","title":"【数值分析】数值分析介绍(Big Picture)"},{"content":"Abstract: MIT AI本课课程介绍 Keywords:\n人工智能课程安排 本文介绍麻省理工EECS学院，AI专业的本科课程安排，相比于斯坦福，MIT给出的更直接，所以直接原文搬过来，稍微解释下大家就清楚了。\n计算机科学工程学士学位 Bachelor of Science in Computer Science and Engineering Departmental Program 忽略公共课程，直接来专业的，公共课程什么哲学或者艺术的，不在我们参考范围内。\nDepartmental Requirements (必须要学的) Units 6.0001Introduction to Computer Science Programming in Python6 6.042[J]Mathematics for Computer Science12 6.UATOral Communication (CI-M) 19 Select one of the following （选一个）:12 6.01Introduction to EECS via Robotics 6.02Introduction to EECS via Communications Networks 6.03Introduction to EECS via Medical Technology Computer Science Requirements（CS必须的） 6.004Computation Structures12 6.006Introduction to Algorithms12 6.009Fundamentals of Programming12 6.031Elements of Software Construction15 6.033Computer System Engineering (CI-M)12 6.034Artificial Intelligence12 or\u0026nbsp;6.036Introduction to Machine Learning 6.045[J]Automata, Computability, and Complexity12 or\u0026nbsp;6.046[J]Design and Analysis of Algorithms Advanced Undergraduate Subjects 下面这些应该不是全部需要学习的，应该是根据自己的需要来选择，根据研究方向，选择你需要的课程\n6.023[J]Fields, Forces and Flows in Biological Systems12 6.025[J]Medical Device Design (CI-M)12 6.035Computer Language Engineering12 6.047Computational Biology: Genomes, Networks, Evolution12 6.061Introduction to Electric Power Systems12 6.101Introductory Analog Electronics Laboratory (CI-M)12 6.111Introductory Digital Systems Laboratory12 6.115Microcomputer Project Laboratory (CI-M)12 6.131Power Electronics Laboratory (CI-M)12 6.172Performance Engineering of Software Systems18 6.175Constructive Computer Architecture12 6.301Solid-State Circuits12 6.302Feedback System Design12 6.602Fundamentals of Photonics12 6.701Introduction to Nanoelectronics12 6.717[J]Design and Fabrication of Microelectromechanical Systems12 6.801Machine Vision12 6.802[J]Foundations of Computational and Systems Biology12 6.803The Human Intelligence Enterprise12 6.804[J]Computational Cognitive Science12 6.806Advanced Natural Language Processing12 6.813User Interface Design and Implementation12 6.814Database Systems12 6.815Digital and Computational Photography12 6.816Multicore Programming12 6.819Advances in Computer Vision12 6.837Computer Graphics12 6.905Large-scale Symbolic Systems12  Independent Inquiry Subjects 6.035Computer Language Engineering12 6.047Computational Biology: Genomes, Networks, Evolution12 6.100Electrical Engineering and Computer Science Project12 6.111Introductory Digital Systems Laboratory12 6.1151Microcomputer Project Laboratory - Independent Inquiry (CI-M)15 6.129[J]Biological Circuit Engineering Laboratory (CI-M)12 6.1311Power Electronics Laboratory - Independent Inquiry (CI-M)15 6.141[J]Robotics: Science and Systems (CI-M)12 6.161Modern Optics Project Laboratory (CI-M)12 6.163Strobe Project Laboratory (CI-M)12 6.170Software Studio12 6.172Performance Engineering of Software Systems18 6.182Psychoacoustics Project Laboratory (CI-M)12 6.805[J]Foundations of Information Policy (CI-M)12 6.806Advanced Natural Language Processing12 6.811[J]Principles and Practice of Assistive Technology12 6.819Advances in Computer Vision12 6.905Large-scale Symbolic Systems12  MIT很讲究的一点是把你的打怪升级图贴出来了\n以及一张表\n总结 本文只列出了MIT本科CS专业（包含AI的部分的相关课程，供自学人员参考） 参考http://catalog.mit.edu/degree-charts/computer-science-engineering-course-6-3/\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-mit-undergraduate-programs-course-6-3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e MIT AI本课课程介绍\n\u003cstrong\u003eKeywords:\u003c/strong\u003e\u003c/p\u003e","title":"【麻省理工学院】人工智能课程安排"},{"content":"Abstract: 本文翻译自Stanford大学AI专业的课程选取表，为广大AI自学者提供自学建议 Keywords: Stanford Artificial Intelligence Track，人工智能课程，自学机器学习，自学人工智能，机器学习数学课程\n人工智能课程安排 来自远方的选课表 看我的博客的人应该大概有印象，我其实这些都是自学的，我也相信，上网找资料找博客的人也有一大部分是准备自学的，自学的好处是可以根据自己时间能力和方向自己安排学什么，怎么学，包括看书，看公开课等很多方式，但是缺点就是，我们可能知道大概学什么，但是精确到每一个科目还是有些拿不准，换句话说就是学到什么样的深度把握不好，因为每一门课都可以深入到相当专业的程度，那么学到什么程度就可以开始下一门基础课了呢，比如人工智能，我们数学分析要学多少，实分析要不要，泛函要不要，这些问题都困扰着我，常见的办法就是去zhihu提问，当然答案靠不靠谱就不一定了。 所以今天我就找了一下Stanford大学的AI专业的选课表（我自己翻译的，我觉得就是个这东西）或者说是课程安排。我只找了一部分比较重要的，完整的可以参考： https://cs.stanford.edu/degrees/ug/ProgramSheets.shtml\n当然如果你认为这是美帝的课表不符合你的能力或者你更喜欢中国的课程安排，你也可以去查查清华北大的课程安排，当然我没尝试，毕竟这种东西有人认为应该跟大家分享，也有人觉得这是机密，不过这是个好思路，找名校的课程安排，来安排自己的学习。\n数学 数学是我这个博客目前主要介绍的内容，毕竟是基础，所以怎么强调都不为过。我以前总结的图是这样的; 当然这个也不是错的，但可能不太科学，stage 0 和1应该是都要掌握的，但是我们来看看Stanford是怎么安排数学课的： 这些就是他们要掌握的数学基础，红框里面选修两门 可见微积分1，2，3和计算方法以及概率是必须要会的，他们的微积分3好像是我们的数学分析，详细内容可以查一下他们的课程内容 https://explorecourses.stanford.edu/search?view=catalog\u0026amp;filter-coursestatus-Active=on\u0026amp;page=0\u0026amp;catalog=\u0026amp;academicYear=\u0026amp;q=\u0026amp;collapse=\n输入课程号码就可以找到详细的资料了。\n计算机 接着是计算机基础课： 少得可怜，就三门课，当然也是必修，计算机组成和系统，计算机系统原理，设计和分析算法。 咋没有C语言呢。。\n深入研究 这是最长的，当然他们也不是全都学，根据自己的方向，和兴趣，可以从里面进行选择，比如自然语言还是图像，或者是研究AI方法的，都有自己要学的东西，要求也不一样，A要求最高，C相对低。\n选修 然后就是自己发挥的课程了，根据自己的兴趣选择 凸优化，认知神经学都在这里，这里应该算是更加深入了，而不是更加不重要了。 换句话说就是专业性更强了，不像微积分那么大众了\n总结 这篇短文主要内容在图表，相信大家懂点英文都能看懂，重要的发现自己喜欢的和需要的。原文和我整理的表格连接如下：\n原文PDF CS_AI_1718PS\n表格\n","permalink":"https://go.face2ai.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/machinelearning-stanford-artificial-intelligence-track.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文翻译自Stanford大学AI专业的课程选取表，为广大AI自学者提供自学建议\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Stanford Artificial Intelligence Track，人工智能课程，自学机器学习，自学人工智能，机器学习数学课程\u003c/p\u003e","title":"【斯坦福大学】人工智能课程安排"},{"content":"Abstract: 本文为数理统计理论基础的第一篇，作为概览，从总体介绍数理统计系列的学习线路和相关博客写作的大体思路 Keywords: 统计学基础，数理统计学基础\n数理统计理论 概览 开篇废话 好几天没有更新博客了，前一段时间有点用力过猛，而且最近正在补习英语，所以博客更新的没有前面那么频繁了，博客现在每天流量有100多的IP，大部分来自CSDN，有几天特别想把博客流量做大，但是后来一想，这不是就是取经路上被妖怪色诱了么，我的目的是分享和深入研究，每天要是为了流量总想着怎么SEO那就有点太不务正业了，所以，流量什么的就随他去吧，我要干的还是研究要研究的东西，以及分享一些我认为有意思的想法。 作为人工智能或者机器学习类学科的基础知识，微积分，线性代数相对来讲，相对容易些，注意我说的是对于AI和ML研究者的要求，要是深入研究，难到不知道哪里去了，相对来说概率和统计相对难一些，本系列主要针对的是统计的理论部分，国外把这称为数理统计学，就是统计的数学分析部分，当然应用部分是不包含在内的，但是国内的分发是统计学是一门人文学科，所以我们数学的统计学就叫数理统计学了，包括应用，当然，我们本系列不讲应用，有专门的一个系列讲应用，应用部分有大部分应该是需要编程的，如果可能我应该会使用CUDA并完成一个独立的项目。当然这是后面的事。我们本系列把目光聚焦在统计学的数学分析上。\n数理统计知识关系 没错，我犹豫了很久最后还是用了陈希孺先生的《数理统计学教程》，这本书说实话，有些难度，不适合自学，因为没人告诉你哪部分要学，哪部分不要学，虽然前言中有建议，但是建议学习的部分中还是有一些不建议学习的内容，所以这就是有老师和没老师代的区别，当然，我考虑了很久，还是放弃了前面概率论时候用的教材《概率统计》，原因是，这本书太详细，以至于，我觉得有点墨迹，但是这本书绝对适合入门，虽然是英文的，但是基本能读懂。 本系列的主要参考就是《数理统计学教程》当然可能会参考《数理统计引论》，不过非常不建议大家去研究数理统计学引论，这本书不适合入门。参考一些知识就行了，啃下来可能是后面的事了。 我们的知识点结构图如下：\n你可以找到完整版： https://github.com/Tony-Tan/MachineLearningMath\n这里的五个层大概是从概率理论到实际使用的结构分析，注意概率和统计的关系，他们是两回事，但是统计使用了概率作为工具，千万不要认为他们是一回事，因为我们平时看到统计总是跟在概率后面的，这并不意味着统计是概率的分支或者什么的。 我们主要会学习下面这些知识点：\n 点估计 假设检验 区间检验 贝叶斯统计 线性模型 多元分析  每一部分的深入程度不同，比如多元分析，可能后面我们会单独用一系列来研究。 本系列只能作为入门了解基础知识点使用，如果你的需求是考研，或者从事专门的数理统计学研究工作，那么这些文章对你应该是没什么帮助的。\n总结 本文作为数理统计学基础的第一篇，从全局上介绍了一下我们本系列的大致内容，希望大家继续支持。\n","permalink":"https://go.face2ai.com/math/math-statistics-basis-0-0-big-picture.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文为数理统计理论基础的第一篇，作为概览，从总体介绍数理统计系列的学习线路和相关博客写作的大体思路\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 统计学基础，数理统计学基础\u003c/p\u003e","title":"【数理统计理论基础】 概览"},{"content":"Abstract: 使用统一内存的CUDA程序——向量加法 Keywords: 统一内存，Uniform Memory\n使用统一内存的向量加法 本文是前面关于统一内存的补充 参考：https://face2ai.com/CUDA-F-4-2-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/\n统一内存矩阵加法 统一内存的基本思路就是减少指向同一个地址的指针，比如我们经常见到的，在本地分配内存，然后传输到设备，然后在从设备传输回来，使用统一内存，就没有这些显式的需求了，而是驱动程序帮我们完成。 具体的做法就是:\nCHECK(cudaMallocManaged((float**)\u0026amp;a_d,nByte)); CHECK(cudaMallocManaged((float**)\u0026amp;b_d,nByte)); CHECK(cudaMallocManaged((float**)\u0026amp;res_d,nByte)); 使用cudaMallocManaged 来分配内存，这种内存在表面上看在设备和主机端都能访问，但是内部过程和我们前面手动copy过来copy过去是一样的，也就是memcopy是本质，而这个只是封装了一下。\n我们来看看完整的代码：\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; void sumArrays(float * a,float * b,float * res,const int size) { for(int i=0;i\u0026lt;size;i+=4) { res[i]=a[i]+b[i]; res[i+1]=a[i+1]+b[i+1]; res[i+2]=a[i+2]+b[i+2]; res[i+3]=a[i+3]+b[i+3]; } } __global__ void sumArraysGPU(float*a,float*b,float*res,int N) { int i=blockIdx.x*blockDim.x+threadIdx.x; if(i \u0026lt; N) res[i]=a[i]+b[i]; } int main(int argc,char **argv) { // set up device  initDevice(0); int nElem=1\u0026lt;\u0026lt;24; printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *res_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d,*res_d; CHECK(cudaMallocManaged((float**)\u0026amp;a_d,nByte)); CHECK(cudaMallocManaged((float**)\u0026amp;b_d,nByte)); CHECK(cudaMallocManaged((float**)\u0026amp;res_d,nByte)); initialData(a_d,nElem); initialData(b_d,nElem); //CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice));  //CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice));  dim3 block(512); dim3 grid((nElem-1)/block.x+1); double iStart,iElaps; iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec\\n\u0026#34;,grid.x,block.x,iElaps); //CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));  sumArrays(b_d,b_d,res_h,nElem); checkResult(res_h,res_d,nElem); cudaFree(a_d); cudaFree(b_d); cudaFree(res_d); free(res_h); return 0; } 注意我们注释掉的，这就是省去的代码部分、 运行结果： 就这个代码而言，使用统一内存还是手动控制，运行速度差不多。 这里有一个新概念叫页面故障，我们分配的这个统一内存地址是个虚拟地址，对应了主机地址和GPU地址，当我们的主机访问这个虚拟地址的时候，会出现一个页面故障，当CPU要访问位于GPU上的托管内存时，统一内存使用CPU页面故障来出发设备到CPU的数据传输，这里的故障不是坏掉了，而是一种通信方式，类似于中断。 故障数和传输数据的大小直接相关。 使用\nnvprof --unified-memory-profiling per-process-device ./sum_arrays_uniform_memory 可以查看到实际参数\n也可以使用 nvvp来查看，效果类似。\n总结 虽然统一内存管理给我们写代码带来了方便而且速度也很快，但是实验表明，手动控制还是要优于统一内存管理，换句话说，人脑的控制比编译器和目前的设备更有效，所以，为了效率，大家还是手动控制内存吧，把命运掌握在自己手里。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-4-5-%E4%BD%BF%E7%94%A8%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%9A%84%E5%90%91%E9%87%8F%E5%8A%A0%E6%B3%95.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 使用统一内存的CUDA程序——向量加法\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 统一内存，Uniform Memory\u003c/p\u003e","title":"【CUDA 基础】4.5 使用统一内存的向量加法"},{"content":"Abstract: 本文通过矩阵转置这一个例子，调整，优化核函数，使其达到最优的内存带宽 Keywords: 带宽，吞吐量，矩阵转置\n核函数可达到的带宽 下面是废话，与本文知识无关，可以直接跳到下面红字处开始本文知识的学习。 废话继续，这两天没更新博客了，上一篇是转发的MIT人工智能实验室的研究指南，也就是告诉刚入学的研究生怎么做研究，要怎么积累，那篇文章发表在1988年，MIT的AI实验室网站目前仍然能检索的到，通读全文，感受很多，也学会了很多东西，当一个健康的框架搭好了以后，后面的好功能会源源不断的涌现，教育也是，当一套体系形成，那么就会有源源不断的人才和成果出现，相反，如果体系本身漏洞百出，根基不稳，短时间真的改不了，人也一样，价值观一旦确定，这个人的人生也就基本定型了——正所谓三岁看老。 今天废话有点多，如果没兴趣，可以直接跳到这里 上一章我们研究怎么通过调整线程网格结构和核函数来达到SM的最高利用率，今天我们来研究如何达到内存带宽的最大利用率。 还是要提那个老例子，但是说实话，这的很形象，也很有用，记住这个例子基本就能了解CUDA的优化大概要从哪入手了： 一条大路（内存读取总线）连接了工厂生产车间（GPU）和材料仓库（全局内存），生产车间又有很多的工作小组（SM），材料仓库有很多小库房（内存分块），工作小组同时生产相同的产品互不干扰（并行），我们有车从材料仓库开往工厂车间，什么时候发车，运输什么由工作小组远程电话指挥（内存请求），发车前，从材料仓库装货的时候，还要听从仓库管理员的分配，因为可能同一间库房可能只允许一个车来拿材料（内存块访问阻塞），然后这些车单向的开往工厂，这时候就是交通问题了，如果我们的路是单向（从仓库到工厂）8车道，每秒钟能通过16辆车，那么我们把这个指标称为带宽。当然我们还有一条路是将成品运输到成品仓库，这也是一条路，与原料库互不干扰，和材料仓库到工厂的路一样，也有宽度，也是单向的，如果这条路堵住，和仓库到工厂的路堵住一样，此时工厂要停工等待。 最理想的状态是，路上全是车，并且全都高速行驶，工厂里的所有工人都在满负荷工作，没有等待，这就是优化的最终目标，如果这个目标达到了，还想进一步提高效率，那么你就只能优化你的工艺了（算法） 上面的这个就是粗糙的GPU工作过程。例子还是比较贴切的，但是有点描述粗糙，多读两遍应该会有点收获的。 内存延迟是影响核函数的一大关键，内存延迟，也就是从你发起内存请求到数据进入SM的寄存器的整个时间。 内存带宽，也就是SM访问内存的速度，它以单位时间内传输的字节数进行测量。 上一节我们用了两种方法改善内核性能：\n 最大化线程束的数量来隐藏内存延迟，维持更多的正在执行的内存访问达到更好的总线利用率 通过适当的对齐和合并访问，提高带宽效率  然而，当前内核本身的内存访问方式就有问题，上面两种优化相当于给一个拖拉机优化空气动力学外观，杯水车薪。 我们本文要做的就是看看这个核函数对应的问题，其极限效率是多少，在理想效率之下，我们来进行优化，我们本文那矩阵转置来进行研究，看看如何把一种看起来没办法优化的内核，重新设计让它达到更好的性能。\n内存带宽 多数内核对带宽敏感，也就是说，工人们生产效率特别高，而原料来的很慢，这限制了生产速度。去哪聚内存中数据的安排方式和线程束的访问方式都对带宽有显著影响。一般有如下两种带宽\n 理论带宽 有效带宽  理论带宽就是硬件设计的绝对最大值，硬件限制了这个最大值为多少，比如对于不使用ECC的Fermi M2090来说，理论峰值 117.6 GB/s 有效带宽是核函数实际达到的带宽，是测量带宽，可以用下面公式计算: $$ 有效带宽=\\frac{(读字节数 + 写字节数)\\times 10^{-9}}{运行时间}\\tag{1} $$ 注意吞吐量和带宽的区别，吞吐量是衡量计算核心效率的，用的单位是每秒多少十亿次浮点运算(gflops)，有效吞吐量其不止和有效带宽有关，还和带宽的利用率等因素有关，当然最主要的还是设备的运算核心。 当然，也有内存吞吐量这种说法这种说法就是单位时间上内存访问的总量，用单位 GB/s 表示，这个值越大表示读取到的数据越多，但是这些数据不一定是有用的。 接下来我们研究如何调整核函数来提高有效带宽\n矩阵转置问题 矩阵转置(点击查看详情)就是交换矩阵的坐标，我们本文研究有二维矩阵，转置结果如下：\n使用串行编程很容易实现：\nvoid transformMatrix2D_CPU(float * MatA,float * MatB,int nx,int ny) { for(int j=0;j\u0026lt;ny;j++) { for(int i=0;i\u0026lt;nx;i++) { MatB[i*nx+j]=MatA[j*nx+i]; } } } 这段代码应该比较容易懂，这是串行解决的方法，必须要注意的是，我们所有的数据，结构体也好，数组也好，多维数组也好，所有的数据，在内存硬件层面都是一维排布的，所以我们这里也是使用一维的数组作为输入输出，那么从真实的角度看内存中的数据就是下面这样的：\n通过这个图能得出一个结论，转置操作：\n 读：原矩阵行进行读取，请求的内存是连续的，可以进行合并访问 写：写到转置矩阵的列中，访问是交叉的  图中的颜色需要大家注意一下，读的过程同一颜色可以看成是合并读取的，但是转置发生后写入的过程，是交叉的。 交叉访问是使得内存访问变差的罪魁。但是作为矩阵转置本身，这个是无法避免的。但是在这种无法避免的交叉访问前提下，我们怎么能提升效率就变成了一个有趣的课题。 我们接下来所有方法都会有按照行读取和按照列读取的版本，来对比效率，看看是交叉读有优势，还是交叉写有优势。 如果按照我们上文的观点，如果按照下面两种方法进行读\n最初的想法肯定是：按照图一合并读更有效率，因为写的时候不需要经过一级缓存，所以对于有一级缓存的程序，合并的读取应该是更有效率的。如果你这么想，恭喜你，你想的不对（我当时也是这么想的）。 我们需要补充下关于一级缓存的作用，上文我们讲到合并，可能第一印象就是一级缓存是缓冲从全局内存里过来的数据一样，但是我们忽略了一些东西，就是内存发起加载请求的时候，会现在一级缓存里看看有没有这个数据，如果有，这个就是一个命中，这和CPU的缓存运行原理是一样的，如果命中了，就不需要再去全局内存读了，如果用在上面这个例子，虽然按照列读是不合并的，但是使用一级缓存加载过来的数据在后面会被使用，我们必须要注意虽然，一级缓存一次读取128字节的数据，其中只有一个单位是有用的，但是剩下的并不会被马上覆盖，粒度是128字节，但是一级缓存的大小有几k或是更大，这些数据很有可能不会被替换，所以，我们按列读取数据，虽然第一行只用了一个，但是下一列的时候，理想情况是所有需要读取的元素都在一级缓存中，这时候，数据直接从缓存里面读取，美滋滋！\n为转置核函数设置上限和下限 在优化之前，我们要给自己一个目标，也就是理论上极限是多少，比如我们测得理论极限是10，而我们已经花了一天时间优化到了9.8，就没必要再花10天优化到9.9了，因为这已经很接近极限了，如果不知道极限，那么就会在无限的接近中浪费时间。 我们本例子中的瓶颈在交叉访问，所以我们假设没有交叉访问，和全是交叉访问的情况，来给出上限和下限：\n 行读取，行存储来复制矩阵(上限) 列读取，列存储来复制矩阵(下限)  __global__ void copyRow(float * MatA,float * MatB,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx=ix+iy*nx; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx]=MatA[idx]; } } __global__ void copyCol(float * MatA,float * MatB,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx]=MatA[idx]; } } 我们使用命令行编译，开启一级缓存：\nnvcc -O3 -arch=sm_35 -Xptxas -dlcm=ca -I ../include/ transform_matrix2D.cu -o transform_matrix2D 可以得到：\n   核函数 试验1 试验2 试验3 平均值     上限 0.001611 0.001614 0.001606 0.001610   下限 0.004191 0.004210 0.004205 0.004202    这个时间是三次测试出来的平均值，基本可以肯定在当前数据规模下，上限在0.001610s，下限在0.004202s，不可能超过上限，当然如果你能跌破下限也算是人才了！ 另外，我们我们全文用的主函数我只在此列举一次，完成代码库在https://github.com/Tony-Tan/CUDA_Freshman\nint main(int argc,char** argv) { printf(\u0026#34;strating...\\n\u0026#34;); initDevice(0); int nx=1\u0026lt;\u0026lt;12; int ny=1\u0026lt;\u0026lt;12; int nxy=nx*ny; int nBytes=nxy*sizeof(float); int transform_kernel=0; if(argc\u0026gt;=2) transform_kernel=atoi(argv[1]); //Malloc  float* A_host=(float*)malloc(nBytes); float* B_host=(float*)malloc(nBytes); initialData(A_host,nxy); //cudaMalloc  float *A_dev=NULL; float *B_dev=NULL; CHECK(cudaMalloc((void**)\u0026amp;A_dev,nBytes)); CHECK(cudaMalloc((void**)\u0026amp;B_dev,nBytes)); CHECK(cudaMemcpy(A_dev,A_host,nBytes,cudaMemcpyHostToDevice)); CHECK(cudaMemset(B_dev,0,nBytes)); int dimx=32; int dimy=32; // cpu compute  double iStart=cpuSecond(); transformMatrix2D_CPU(A_host,B_host,nx,ny); double iElaps=cpuSecond()-iStart; printf(\u0026#34;CPU Execution Time elapsed %f sec\\n\u0026#34;,iElaps); // 2d block and 2d grid  dim3 block(dimx,dimy); dim3 grid((nx-1)/block.x+1,(ny-1)/block.y+1); dim3 block_1(dimx,dimy); dim3 grid_1((nx-1)/(block_1.x*4)+1,(ny-1)/block_1.y+1); iStart=cpuSecond(); switch(transform_kernel) { case 0: copyRow\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; case 1: copyCol\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; case 2: transformNaiveRow\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; case 3: transformNaiveCol\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; case 4: transformNaiveColUnroll\u0026lt;\u0026lt;\u0026lt;grid_1,block_1\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; case 5: transformNaiveColUnroll\u0026lt;\u0026lt;\u0026lt;grid_1,block_1\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; case 6: transformNaiveRowDiagonal\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; case 7: transformNaiveColDiagonal\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,nx,ny); break; default: break; } CHECK(cudaDeviceSynchronize()); iElaps=cpuSecond()-iStart; printf(\u0026#34; Time elapsed %f sec\\n\u0026#34;,iElaps); CHECK(cudaMemcpy(B_host,B_dev,nBytes,cudaMemcpyDeviceToHost)); checkResult(B_host,B_host,nxy); cudaFree(A_dev); cudaFree(B_dev); free(A_host); free(B_host); cudaDeviceReset(); return 0; } switch部分可以写成函数指针的方式，但是问题不大（原文写的应该是函数指针的方式）。 我的笔记本是1050ti的显卡，这个表可能是主机版本的1050ti的指标，可以看出其理论贷款是112GB/s 我们使用公式(1)来算一下两种极限的带宽： $$ \\text{copyRow}=\\frac{1\\times2^{12+12}\\times 4\\times 2\\times 10^{-9}}{0.001610}=\\frac{0.134217728}{0.001610}=83.3650 \\text{ GB/s}\\ \\text{copyCol}=\\frac{1\\times2^{12+12}\\times 4\\times 2\\times 10^{-9}}{0.004202}=\\frac{0.134217728}{0.004202}=31.9414 \\text{ GB/s} $$\n朴素转置：读取行与读取列 接下来我们看最naive的两种转置方法，不加任何优化，也就是我们一瞬间就想到的方案：\n__global__ void transformNaiveRow(float * MatA,float * MatB,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx_row=ix+iy*nx; int idx_col=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx_col]=MatA[idx_row]; } } __global__ void transformNaiveCol(float * MatA,float * MatB,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx_row=ix+iy*nx; int idx_col=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx_row]=MatA[idx_col]; } } 运行时间：\n   核函数 试验1 试验2 试验3 平均值     transformNaiveRow 0.004008 0.004005 0.004012 0.004008   transformNaiveCol 0.002126 0.002118 0.002124 0.002123    $$ \\text{transformNaiveRow}=\\frac{1\\times2^{12+12}\\times 4\\times 2\\times 10^{-9}}{0.001610}=\\frac{0.134217728}{0.004008}= 33.4874 \\text{ GB/s}\\ \\text{transformNaiveCol}=\\frac{1\\times2^{12+12}\\times 4\\times 2\\times 10^{-9}}{0.004202}=\\frac{0.134217728}{0.002123}= 63.2207 \\text{ GB/s} $$\n使用按列读取效果更好，这和我们前面分析的基本一致。 下面是使用一级缓存的加载存储吞吐量    核函数 加载吞吐量 存储吞吐量     copyRow 81.263 40.631   copyCol 120.93 120.93   transformNaiveRow 31.717 126.87   transformNaiveCol 243.64 30.454    按列读取的高吞吐量的原因就是上面我们说的缓存命中，这里也能看到吞吐量是可以超过带宽的，因为带宽衡量的是从全局内存到SM的速度极限，而吞吐量是SM获得数据的总量除以时间，而这些数据可以来自一级缓存，而不必千里迢迢从主存读取。 这里有个疑问：虽然交叉读取缓存命中率高了，但是似乎并没有减少从主存读取数据的数据量，那为什么速度会有提高呢？ 我认为应该是延迟隐藏部分出的问题，导致了交叉读取效率变高，当然只是我的猜测后面还要验证一下。\n展开转置：读取行与读取列 接下来这个是老套路了，有效地隐藏延迟，从展开操作开始：\n__global__ void transformNaiveRowUnroll(float * MatA,float * MatB,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x*4; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx_row=ix+iy*nx; int idx_col=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx_col]=MatA[idx_row]; MatB[idx_col+ny*1*blockDim.x]=MatA[idx_row+1*blockDim.x]; MatB[idx_col+ny*2*blockDim.x]=MatA[idx_row+2*blockDim.x]; MatB[idx_col+ny*3*blockDim.x]=MatA[idx_row+3*blockDim.x]; } } __global__ void transformNaiveColUnroll(float * MatA,float * MatB,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x*4; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx_row=ix+iy*nx; int idx_col=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx_row]=MatA[idx_col]; MatB[idx_row+1*blockDim.x]=MatA[idx_col+ny*1*blockDim.x]; MatB[idx_row+2*blockDim.x]=MatA[idx_col+ny*2*blockDim.x]; MatB[idx_row+3*blockDim.x]=MatA[idx_col+ny*3*blockDim.x]; } } 结果如图\n   核函数 试验1 试验2 试验3 平均值     transformNaiveRowUnroll 0.001544 0.001550 001541 0.001545   transformNaiveColUnroll 0.001545 0.001539 0.001546 0.001543    这里出现了尴尬的一幕，没错，我们突破上限了，上限是按行合并读取，合并存储，不存在交叉的情况，这种理想情况不可能发生在转置中，所以我们说这是上限。而我们使用展开的交叉访问居然得到了比上限更快的速度，所以我断定，如果把上限展开，速度肯定会更快，但是我们这里还把他叫做上限，虽然并不是真正的上限。 想要知道真正的上限是什么，就要从硬件角度算理论上限，实际测出来的上限很有可能不正确。\n对角转置：读取行与读取列 接下来我们使用一点新技巧，这个技巧的来源是DRAM的特性导致的，还记得我们例子中对原料仓库的描述么，那里面有很多小库房，这些小库房同时可能只允许一台车拿东西，在DRAM中内存是分区规划的，如果过多的访问同一个区，会产生排队的现象，也就是要等待，为了避免这种情况，我们最好均匀的访问DRAM的某一段，DRAM的分区是每256个字节算一个分区，所以我们最好错开同一个分区的访问，方法就是调整块的ID，这时候你可能有问题了，我们并不知道块的执行顺序，那应该怎么调呢，这个问题没有啥官方解释，我自己的理解是，硬件执行线程块必然是按照某种规则进行的，按照123执行，可能要比按照随机执行好，因为想要随机执行，还要有生成随机顺序这一步，根本没必要，我们之所以说块的执行顺序不确定，其实是为了避免大家把它理解为确定顺序，而实际上可能有某些原因导致顺序错乱，但是这个绝对不是硬件设计时故意而为之的。 我们这个对角转置的目的就是使得读取DRAM位置均匀一点，别都集中在一个分区上，方法是打乱线程块，因为连续的线程块可能访问相近的DRAM地址。 我们的方案是使用一个函数 $f(x,y)=(m,n)$ 一个一一对应的函数，将原始笛卡尔坐标打乱。 注意，所有这些线程块的顺序什么的都是在编程模型基础上的，跟硬件没什么关系，这些都是逻辑层面的，实际上线程块ID对应的是哪个线程块也是我们自己规定的而已。 说实话，这个代码有点难理解，当然你也不用死记硬背这种用法，似乎没有程序员被代码，甚至入门的过程都不用背，我们要理解的就是线程块ID和线程块之间的对应，以及新ID和原始ID的对应，以及新ID对应的块， 原始的线程块ID 新设计的线程块ID\n__global__ void transformNaiveRowDiagonal(float * MatA,float * MatB,int nx,int ny) { int block_y=blockIdx.x; int block_x=(blockIdx.x+blockIdx.y)%gridDim.x; int ix=threadIdx.x+blockDim.x*block_x; int iy=threadIdx.y+blockDim.y*block_y; int idx_row=ix+iy*nx; int idx_col=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx_col]=MatA[idx_row]; } } __global__ void transformNaiveColDiagonal(float * MatA,float * MatB,int nx,int ny) { int block_y=blockIdx.x; int block_x=(blockIdx.x+blockIdx.y)%gridDim.x; int ix=threadIdx.x+blockDim.x*block_x; int iy=threadIdx.y+blockDim.y*block_y; int idx_row=ix+iy*nx; int idx_col=ix*ny+iy; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatB[idx_row]=MatA[idx_col]; } } 这个速度还没有展开的版本快，甚至没有naive的交叉读取速度快，但书上说的是效率有提高，可能是CUDA升级后的原因吧，或者其他原因的影响，但是DRAM分区会出现排队这种现象值得注意。\n瘦块来增加并行性 接下来老套路，调整一下线程块的尺寸我们看看有没有啥变化，当然，我们以naive的列读取作为对照。\n   block尺寸 测试1 测试2 测试3 平均值     (32,32) 0.002166 0.002122 0.002125 0.002138   (32,16) 0.001677 0.001696 0.001703 0.001692   (32,8) 0.001925 0.001929 0.001925 0.001926   (64,16) 0.002117 0.002146 0.002113 0.002125   (64,8) 0.001949 0.001945 0.001945 0.001946   (128,8) 0.002228 0.002230 0.002229 0.002229    这是简单的实验结果，可见（32，16）的这种模式效率最高\n总结 本文主要讲解内存带宽对效率的影响以及如何有效地通过调整读取方式来突破内存存储瓶颈，这是我们优化CUDA程序非常重要的手段\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-4-4-%E6%A0%B8%E5%87%BD%E6%95%B0%E5%8F%AF%E8%BE%BE%E5%88%B0%E7%9A%84%E5%B8%A6%E5%AE%BD.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文通过矩阵转置这一个例子，调整，优化核函数，使其达到最优的内存带宽\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 带宽，吞吐量，矩阵转置\u003c/p\u003e","title":"【CUDA 基础】4.4 核函数可达到的带宽"},{"content":"Abstract: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY October, 1988 Keywords:\n如何在AI实验室做研究 https://dspace.mit.edu/bitstream/handle/1721.1/41487/AI_WP_316.pdf\n by a whole bunch of current, former, and honorary MIT AI Lab graduate students David Chapman, Editor Version 1.3, September, 1988.\n Abstract This document presumptuously purports to explain how to do research. We give heuristics that may be useful in picking up the specific skills needed for research (reading, writing, programming) and for understanding and enjoying the process itself (methodology, topic and advisor selection, and emotional factors). Copyright 1987, 1988 by the authors. A. I. Laboratory Working Papers are produced for internal circulation, and may contain information that is, for example, too preliminary or too detailed for formal publication. It is not intended that they should be considered papers to which reference can be made in the literature.\nIntroduction What is this? There\u0026rsquo;s no guaranteed recipe for success at research. This document collects a lot of informal rules-of-thumb advice that may help.\nWho\u0026rsquo;s it for? This document is written for new graduate students at the MIT AI Laboratory. However, it may be useful to many others doing research in AI at other institutions. People even in other fields have found parts of it useful.\nHow do I use it? It\u0026rsquo;s too long to read in one sitting. It\u0026rsquo;s best to browse. Most people have found that it\u0026rsquo;s useful to flip through the whole thing to see what\u0026rsquo;s in it and then to refer back to sections when they are relevant to their current research problems. The document is divided roughly in halves. The first several sections talk about the concrete skills you need: reading, writing, programming, and so on. The later sections talk about the process of research: what it\u0026rsquo;s like, how to go at it, how to choose an advisor and topic, and how to handle it emotionally. Most readers have reported that these later sections are in the long run more useful and interesting than the earlier ones.\n  Section 3 is about getting grounded in AI by reading. It points at the most important journals and has some tips on how to read.\n  4 is about becoming a member of the AI community: getting connected to a network of people who will keep you up to date on what\u0026rsquo;s happening and what you need to read.\n  5 is about learning about fields related to AI. You\u0026rsquo;ll want to have a basic understanding of several of these and probably in-depth understanding of one or two.\n  6 is about keeping a research notebook.\n  7 is about writing papers and theses; about writing and using comments on drafts; and about getting published.\n  8 is about giving research talks.\n  9 is about programming. AI programming may be different from the sorts you\u0026rsquo;re used to.\n  10 is about the most important choice of your graduate career, that of your advisor. Different advisors have different styles; this section gives some heuristics for finding one who will suit you. An advisor is a resource you need to know how to use; this section tells you how.\n  11 is about theses. Your thesis, or theses, will occupy most of your time during most of your graduate student career. The section gives advice on choosing a topic and avoiding wasting time.\n  12 is on research methodology. This section mostly hasn\u0026rsquo;t been written yet.\n  13 is perhaps the most important section: it\u0026rsquo;s about emotional factors in the process of research. It tells how to deal with failure, how to set goals, how to get unstuck, how to avoid insecurity, maintain self-esteem, and have fun in the process.\n  This document is still in a state of development; we welcome contributions and comments. Some sections are very incomplete. 【Annotations in brackets and italics indicate some of the major incompletions】. We appreciate contributions; send your ideas and comments to Zvona@ai.ai.mit.edu.\nReading AI Many researchers spend more than half their time reading. You can learn a lot more quickly from other people\u0026rsquo;s work than from doing your own. This section talks about reading within AI; section covers reading about other subjects.\nThe time to start reading is now. Once you start seriously working on your thesis you\u0026rsquo;ll have less time, and your reading will have to be more focused on the topic area. During your first two years, you\u0026rsquo;ll mostly be doing class work and getting up to speed on AI in general. For this it suffices to read textbooks and published journal articles. (Later, you may read mostly drafts; see section .)\nThe amount of stuff you need to have read to have a solid grounding in the field may seem intimidating, but since AI is still a small field, you can in a couple years read a substantial fraction of the significant papers that have been published. What\u0026rsquo;s a little tricky is figuring out which ones those are. There are some bibliographies that are useful: for example, the syllabi of the graduate AI courses. The reading lists for the AI qualifying exams at other universities-particularly Stanford-are also useful, and give you a less parochial outlook. If you are interested in a specific subfield, go to a senior grad student in that subfield and ask him what are the ten most important papers and see if he\u0026rsquo;ll lend you copies to Xerox. Recently there have been appearing a lot of good edited collections of papers from a subfield, published particularly by Morgan-Kauffman.\nThe AI lab has three internal publication series, the Working Papers, Memos, and Technical Reports, in increasing order of formality. They are available on racks in the eighth floor play room. Go back through the last couple years of them and snag copies of any that look remotely interesting. Besides the fact that a lot of them are significant papers, it\u0026rsquo;s politically very important to be current on what people in your lab are doing.\nThere\u0026rsquo;s a whole bunch of journals about AI, and you could spend all your time reading them. Fortunately, only a few are worth looking at. The principal journal for central-systems stuff is Artificial Intelligence, also referred to as \u0026ldquo;the Journal of Artificial Intelligence\u0026rdquo;, or \u0026ldquo;AIJ\u0026rdquo;. Most of the really important papers in AI eventually make it into AIJ, so it\u0026rsquo;s worth scanning through back issues every year or so; but a lot of what it prints is really boring. Computational Intelligence is a new competitor that\u0026rsquo;s worth checking out. Cognitive Science also prints a fair number of significant AI papers. Machine Learning is the main source on what it says. IEEE PAMI is probably the best established vision journal; two or three interesting papers per issue. The International Journal of Computer Vision (IJCV) is new and so far has been interesting. Papers in Robotics Research are mostly on dynamics; sometimes it also has a landmark AIish robotics paper. IEEE Robotics and Automation has occasional good papers.\nIt\u0026rsquo;s worth going to your computer science library (MIT\u0026rsquo;s is on the first floor of Tech Square) every year or so and flipping through the last year\u0026rsquo;s worth of AI technical reports from other universities and reading the ones that look interesting.\nReading papers is a skill that takes practice. You can\u0026rsquo;t afford to read in full all the papers that come to you. There are three phases to reading one. The first is to see if there\u0026rsquo;s anything of interest in it at all. AI papers have abstracts, which are supposed to tell you what\u0026rsquo;s in them, but frequently don\u0026rsquo;t; so you have to jump about, reading a bit here or there, to find out what the authors actually did. The table of contents, conclusion section, and introduction are good places to look. If all else fails, you may have to actually flip through the whole thing. Once you\u0026rsquo;ve figured out what in general the paper is about and what the claimed contribution is, you can decide whether or not to go on to the second phase, which is to find the part of the paper that has the good stuff. Most fifteen page papers could profitably be rewritten as one-page papers; you need to look for the page that has the exciting stuff. Often this is hidden somewhere unlikely. What the author finds interesting about his work may not be interesting to you, and vice versa. Finally, you may go back and read the whole paper through if it seems worthwhile.\nRead with a question in mind. \u0026ldquo;How can I use this?\u0026rdquo; \u0026ldquo;Does this really do what the author claims?\u0026ldquo;\u0026ldquo;What if\u0026hellip;?\u0026rdquo; Understanding what result has been presented is not the same as understanding the paper. Most of the understanding is in figuring out the motivations, the choices the authors made (many of them implicit), whether the assumptions and formalizations are realistic, what directions the work suggests, the problems lying just over the horizon, the patterns of difficulty that keep coming up in the author\u0026rsquo;s research program, the political points the paper may be aimed at, and so forth.\nIt\u0026rsquo;s a good idea to tie your reading and programming together. If you are interested in an area and read a few papers about it, try implementing toy versions of the programs being described. This gives you a more concrete understanding.\nMost AI labs are sadly inbred and insular; people often mostly read and cite work done only at their own school. Other institutions have different ways of thinking about problems, and it is worth reading, taking seriously, and referencing their work, even if you think you know what\u0026rsquo;s wrong with them.\nOften someone will hand you a book or paper and exclaim that you should read it because it\u0026rsquo;s (a) the most brilliant thing ever written and/or (b) precisely applicable to your own research. Usually when you actually read it, you will find it not particularly brilliant and only vaguely applicable. This can be perplexing. \u0026ldquo;Is there something wrong with me? Am I missing something?\u0026rdquo; The truth, most often, is that reading the book or paper in question has, more or less by chance, made your friend think something useful about your research topic by catalyzing a line of thought that was already forming in their head.\nGetting connected After the first year or two, you\u0026rsquo;ll have some idea of what subfield you are going to be working in. At this point-or even earlier-it\u0026rsquo;s important to get plugged into the Secret Paper Passing Network. This informal organization is where all the action in AI really is. Trend-setting work eventually turns into published papers-but not until at least a year after the cool people know all about it. Which means that the cool people have a year\u0026rsquo;s head start on working with new ideas.\nHow do the cool people find out about a new idea? Maybe they hear about it at a conference; but much more likely, they got it through the Secret Paper Passing Network. Here\u0026rsquo;s how it works. Jo Cool gets a good idea. She throws together a half-assed implementation and it sort of works, so she writes a draft paper about it. She wants to know whether the idea is any good, so she sends copies to ten friends and asks them for comments on it. They think it\u0026rsquo;s cool, so as well as telling Jo what\u0026rsquo;s wrong with it, they lend copies to their friends to Xerox. Their friends lend copies to their friends, and so on. Jo revises it a bunch a few months later and sends it to AAAI. Six months later, it first appears in print in a cut-down five-page version (all that the AAAI proceedings allow). Jo eventually gets around to cleaning up the program and writes a longer revised version (based on the feedback on the AAAI version) and sends it to the AI Journal. AIJ has almost two years turn-around time, what with reviews and revisions and publication delay, so Jo\u0026rsquo;s idea finally appears in a journal form three years after she had it-and almost that long after the cool people first found out about it. So cool people hardly ever learn about their subfield from published journal articles; those come out too late.\nYou, too, can be a cool people. Here are some heuristics for getting connected:\n  There\u0026rsquo;s a bunch of electronic mailing lists that discuss AI subfields like connectionism or vision. Get yourself on the ones that seem interesting.\n  Whenever you talk about an idea you\u0026rsquo;ve had with someone who knows the field, they are likely not to give an evaluation of your idea, but to say, \u0026ldquo;Have you read X?\u0026rdquo; Not a test question, but a suggestion about something to read that will probably be relevant. If you haven\u0026rsquo;t read X, get the full reference from your interlocutor, or better yet, ask to borrow and Xerox his copy.\n  When you read a paper that excites you, make five copies and give them to people you think will be interested in it. They\u0026rsquo;ll probably return the favor.\n  The lab has a number of on-going informal paper discussion groups on various subfields. These meet every week or two to discuss a paper that everyone has read.\n  Some people don\u0026rsquo;t mind if you read their desks. That is, read the papers that they intend to read soon are heaped there and turn over pretty regularly. You can look over them and see if there\u0026rsquo;s anything that looks interesting. Be sure to ask before doing this; some people do mind. Try people who seem friendly and connected.\n  Similarly, some people don\u0026rsquo;t mind your browsing their filing cabinets. There are people in the lab who are into scholarship and whose cabinets are quite comprehensive. This is often a faster and more reliable way to find papers than using the school library.\n  Whenever you write something yourself, distribute copies of a draft of it to people who are likely to be interested. (This has a potential problem: plagiarism is rare in AI, but it does happen. You can put something like \u0026ldquo;Please do not photocopy or quote\u0026rdquo; on the front page as a partial prophylactic.) Most people don\u0026rsquo;t read most of the papers they\u0026rsquo;re given, so don\u0026rsquo;t take it personally when only a few of the copies you distribute come back with comments on them. If you go through several drafts-which for a journal article you should-few readers will read more than one of them. Your advisor is expected to be an exception.\n  When you finish a paper, send copies to everyone you think might be interested. Don\u0026rsquo;t assume they\u0026rsquo;ll read it in the journal or proceedings spontaneously. Internal publication series (memos and technical reports) are even less likely to be read.\n  The more different people you can get connected with, the better. Try to swap papers with people from different research groups, different AI labs, different academic fields. Make yourself the bridge between two groups of interesting people working on related problems who aren\u0026rsquo;t talking to each other and suddenly reams of interesting papers will flow across your desk.\n  When a paper cites something that looks interesting, make a note of it. Keep a log of interesting references. Go to the library every once in a while and look the lot of them up. You can intensively work backward through a \u0026ldquo;reference graph\u0026rdquo; of citations when you are hot on the trail of an interesting topic. A reference graph is a web of citations: paper A cites papers B and C, B cites C and D, C cites D, and so on. Papers that you notice cited frequently are always worth reading. Reference graphs have weird properties. One is that often there are two groups of people working on the same topic who don\u0026rsquo;t know about each other. You may find yourself close to closure on searching a graph and suddenly find your way into another whole section. This happens when there are different schools or approaches. It\u0026rsquo;s very valuable to understand as many approaches as possible-often more so than understanding one approach in greater depth.\n  Hang out. Talk to people. Tell them what you\u0026rsquo;re up to and ask what they\u0026rsquo;re doing. (If you\u0026rsquo;re shy about talking to other students about your ideas, say because you feel you haven\u0026rsquo;t got any, then try talking to them about the really good-or unbelievably foolish-stuff you\u0026rsquo;ve been reading. This leads naturally into the topic of what one might do next.) There\u0026rsquo;s an informal lunch group that meets in the seventh floor playroom around noon every day. People tend to work nights in our lab, and so go for dinner in loose groups. Invite yourself along.\n  If you interact with outsiders much-giving demos or going to conferences-get a business card. Make it easy to remember your name.\n  At some point you\u0026rsquo;ll start going to scientific conferences. When you do, you will discover fact that almost all the papers presented at any conference are boring or silly. (There are interesting reasons for this that aren\u0026rsquo;t relevant here.) Why go to them then? To meet people in the world outside your lab. Outside people can spread the news about your work, invite you to give talks, tell you about the atmosphere and personalities at a site, introduce you to people, help you find a summer job, and so forth. How to meet people? Walk up to someone whose paper you\u0026rsquo;ve liked, say \u0026ldquo;I really liked your paper\u0026rdquo;, and ask a question.\n  Get summer jobs away at other labs. This gives you a whole new pool of people to get connected with who probably have a different way of looking at things. One good way to get summer jobs at other labs is to ask senior grad students how. They\u0026rsquo;re likely to have been places that you\u0026rsquo;d want to go and can probably help you make the right connections.\n  Learning other fields It used to be the case that you could do AI without knowing anything except AI, and some people still seem to do that. But increasingly, good research requires that you know a lot about several related fields. Computational feasibility by itself doesn\u0026rsquo;t provide enough constraint on what intelligence is about. Other related fields give other forms of constraint, for example experimental data, which you can get from psychology. More importantly, other fields give you new tools for thinking and new ways of looking at what intelligence is about. Another reason for learning other fields is that AI does not have its own standards of research excellence, but has borrowed from other fields. Mathematics takes theorems as progress; engineering asks whether an object works reliably; psychology demands repeatable experiments; philosophy rigorous arguments; and so forth. All these criteria are sometimes applied to work in AI, and adeptness with them is valuable in evaluating other people\u0026rsquo;s work and in deepening and defending your own.\nOver the course of the six or so years it takes to get a PhD at MIT, you can get a really solid grounding in one or two non-AI fields, read widely in several more, and have at least some understanding of the lot of them. Here are some ways to learn about a field you don\u0026rsquo;t know much about:\n  Take a graduate course. This is solidest, but is often not an efficient way to go about it.\n  Read a textbook. Not a bad approach, but textbooks are usually out of date, and generally have a high ratio of words to content.\n  Find out what the best journal in the field is, maybe by talking to someone who knows about it. Then skim the last few years worth and follow the reference trees. This is usually the fastest way to get a feel of what is happening, but can give you a somewhat warped view.\n  Find out who\u0026rsquo;s most famous in the field and read their books.\n  Hang out with grad students in the field.\n  Go to talks. You can find announcements for them on departmental bulletin boards.\n  Check out departments other than MIT\u0026rsquo;s. MIT will give you a very skewed view of, for example, linguistics or psychology. Compare the Harvard course catalog. Drop by the graduate office over there, read the bulletin boards, pick up any free literature.\n  Now for the subjects related to AI you should know about.\n  Computer science is the technology we work with. The introductory graduate courses you are required to take will almost certainly not give you an adequate understanding of it, so you\u0026rsquo;ll have to learn a fair amount by reading beyond them. All the areas of computer science-theory, architectures, systems, languages, etc.\u0026mdash;are relevant.\n  Mathematics is probably the next most important thing to know. It\u0026rsquo;s critical to work in vision and robotics; for central-systems work it usually isn\u0026rsquo;t directly relevant, but it teaches you useful ways of thinking. You need to be able to read theorems, and an ability to prove them will impress most people in the field. Very few people can learn math on their own; you need a gun at your head in the form of a course, and you need to do the problem sets, so being a listener is not enough. Take as much math as you can early, while you still can; other fields are more easily picked up later. Computer science is grounded in discrete mathematics: algebra, graph theory, and the like. Logic is very important if you are going to work on reasoning. It\u0026rsquo;s not used that much at MIT, but at Stanford and elsewhere it is the dominant way of thinking about the mind, so you should learn enough of it that you can make and defend an opinion for yourself. One or two graduate courses in the MIT math department is probably enough. For work in perception and robotics, you need continuous as well as discrete math. A solid background in analysis, differential geometry and topology will provide often-needed skills. Some statistics and probability is just generally useful.\n  Cognitive psychology mostly shares a worldview with AI, but practitioners have rather different goals and do experiments instead of writing programs. Everyone needs to know something about this stuff. Molly Potter teaches a good graduate intro course at MIT.\n  Developmental psychology is vital if you are going to do learning work. It\u0026rsquo;s also more generally useful, in that it gives you some idea about which things should be hard and easy for a human-level intelligence to do. It also suggests models for cognitive architecture. For example, work on child language acquisition puts substantial constraints on linguistic processing theories. Susan Carey teaches a good graduate intro course at MIT.\n  \u0026ldquo;Softer\u0026rdquo; sorts of psychology like psychoanalysis and social psychology have affected AI less, but have significant potential. They give you very different ways of thinking about what people are. Social \u0026ldquo;sciences\u0026rdquo; like sociology and anthropology can serve a similar role; it\u0026rsquo;s useful to have a lot of perspectives. You\u0026rsquo;re on your own for learning this stuff. Unfortunately, it\u0026rsquo;s hard to sort out what\u0026rsquo;s good from bad in these fields without a connection to a competent insider. Check out Harvard: it\u0026rsquo;s easy for MIT students to cross-register for Harvard classes.\n  Neuroscience tells us about human computational hardware. With the recent rise of computational neuroscience and connectionism, it\u0026rsquo;s had a lot of influence on AI. MIT\u0026rsquo;s Brain and Behavioral Sciences department offers good courses on vision (Hildreth, Poggio, Richards, Ullman) motor control (Hollerbach, Bizzi) and general neuroscience (9.015, taught by a team of experts).\n  Linguistics is vital if you are going to do natural language work. Besides that, it exposes a lot of constraint on cognition in general. Linguistics at MIT is dominated by the Chomsky school. You may or may not find this to your liking. Check out George Lakoff\u0026rsquo;s recent book Women, Fire, and Dangerous Things as an example of an alternative research program.\n  Engineering, especially electrical engineering, has been taken as a domain by a lot of AI research, especially at MIT. No accident; our lab puts a lot of stock in building programs that clearly do something, like analyzing a circuit. Knowing EE is also useful when it comes time to build a custom chip or debug the power supply on your Lisp machine.\n  Physics can be a powerful influence for people interested in perception and robotics.\n  Philosophy is the hidden framework in which all AI is done. Most work in AI takes implicit philosophical positions without knowing it. It\u0026rsquo;s better to know what your positions are. Learning philosophy also teaches you to make and follow certain sorts of arguments that are used in a lot of AI papers. Philosophy can be divided up along at least two orthogonal axes. Philosophy is usually philosophy of something; philosophy of mind and language are most relevant to AI. Then there are schools. Very broadly, there are two very different superschools: analytic and Continental philosophy. Analytic philosophy of mind for the most part shares a world view with most people in AI. Continental philosophy has a very different way of seeing which takes some getting used to. It has been used by Dreyfus to argue that AI is impossible. More recently, a few researchers have seen it as compatible with AI and as providing an alternative approach to the problem. Philosophy at MIT is of the analytical sort, and of a school that has been heavily influenced by Chomsky\u0026rsquo;s work in linguistics.\n  This all seems like a lot to know about, and it is. There\u0026rsquo;s a trap here: thinking \u0026ldquo;if only I knew more X, this problem would be easy,\u0026rdquo; for all X. There\u0026rsquo;s always more to know that could be relevant. Eventually you have to sit down and solve the problem.\nNotebooks Most scientists keep a research notebook. You should too. You\u0026rsquo;ve probably been told this in every science class since fifth grade, but it\u0026rsquo;s true. Different systems work for different people; experiment. You might keep it online or in a spiral notebook or on legal pads. You might want one for the lab and one for home.\nRecord in your notebook ideas as they come up. Nobody except you is going to read it, so you can be random. Put in speculations, current problems in your work, possible solutions. Work through possible solutions there. Summarize for future reference interesting things you read.\nRead back over your notebook periodically. Some people make a monthly summary for easy reference.\nWhat you put in your notebook can often serve as the backbone of a paper. This makes life a lot easier. Conversely, you may find that writing skeletal papers-title, abstract, section headings, fragments of text-is a useful way of documenting what you are up to, even when you have no intention of ever making it into a real paper. (And you may change your mind later.)\nYou may find useful Vera Johnson-Steiner\u0026rsquo;s book Notebooks of the Mind, which, though mostly not literally about notebooks, describes the ways in which creative thought emerges from the accumulation of fragments of ideas.\nWriting There\u0026rsquo;s a lot of reasons to write.\n  You are required to write one or two theses during your graduate student career: a PhD and maybe an MS, depending on your department.\n  Writing a lot more than that gives you practice.\n  Academia runs on publish-or-perish. In most fields and schools, this starts in earnest when you become a professor, but most graduate students in our lab publish before they graduate. Publishing and distributing papers is good politics and good publicity.\n  Writing down your ideas is the best way to debug them. Usually you will find that what seemed perfectly clear in your head is in fact an incoherent mess on paper.\n  If your work is to benefit anyone other than yourself, you must communicate it. This is a basic responsibility of research. If you write well more people will read your work!\n  AI is too hard to do by yourself. You need constant feedback from other people. Comments on your papers are one of the most important forms of that.\n  Anything worth doing is worth doing well.\n  Read books about how to write. Strunk and White\u0026rsquo;s Elements of Style gives the basic dos and don\u0026rsquo;ts. Claire Cook\u0026rsquo;s The MLA\u0026rsquo;s Line By Line (Houghton Mifflin) is about editing at the sentence level. Jacques Barzun\u0026rsquo;s Simple and Direct: A Rhetoric for Writers (Harper and Row, 1985) is about composition.\n  When writing a paper, read books that are well-written, thinking in background mode about the syntactic mechanics. You\u0026rsquo;ll find yourself absorbing the author\u0026rsquo;s style.\n  Learning to write well requires doing a lot of it, over a period of years, and getting and taking seriously criticism of what you\u0026rsquo;ve written. There\u0026rsquo;s no way to get dramatically better at it quickly.\n  Writing is sometimes painful, and it can seem a distraction from doing the \u0026ldquo;real\u0026rdquo; work. But as you get better at it, it goes faster, and if you approach it as a craft, you can get a lot of enjoyment out of the process for its own sake.\n  You will certainly suffer from writer\u0026rsquo;s block at some point. Writer\u0026rsquo;s block has many sources and no sure cure. Perfectionism can lead to writer\u0026rsquo;s block: whatever you start to write seems not good enough. Realize that writing is a debugging process. Write something sloppy first and go back and fix it up. Starting sloppy gets the ideas out and gets you into the flow. If you \u0026ldquo;can\u0026rsquo;t\u0026rdquo; write text, write an outline. Make it more and more detailed until it\u0026rsquo;s easy to write the subsubsubsections. If you find it really hard to be sloppy, try turning the contrast knob on your terminal all the way down so you can\u0026rsquo;t see what you are writing. Type whatever comes into your head, even if it seems like garbage. After you\u0026rsquo;ve got a lot of text out, turn the knob back up and edit what you\u0026rsquo;ve written into something sensible. Another mistake is to imagine that the whole thing can be written out in order. Usually you should start with the meat of the paper and write the introduction last, after you know what the paper really says. Another cause of writer\u0026rsquo;s block is unrealistic expectations about how easy writing is. Writing is hard work and takes a long time; don\u0026rsquo;t get frustrated and give up if you find you write only a page a day.\n  Perfectionism can also lead to endless repolishing of a perfectly adequate paper. This is a waste of time. (It can also be a way of semideliberately avoiding doing research.) Think of the paper you are writing as one statement in a conversation you are having with other people in the field. In a conversation not everything goes perfectly; few expect that what they say in a single utterance will be the whole story or last word in the interchange.\n  Writing letters is good practice. Most technical papers would be improved if the style was more like a letter to a friend. Keeping a diary is also a way to practice writing (and lets you be more stylistically experimental than technical papers). Both practices have other substantial benefits.\n  It\u0026rsquo;s a common trap to spend more time hacking the formatter macro than the content. Avoid this. LaTeX is imperfect, but it has most of the macrology you want. If that\u0026rsquo;s not enough, you can probably borrow code from someone else who has wanted to do the same thing. Most sites (including MIT) maintain a library of locally-written extensions.\n  Know what you want to say. This is the hardest and most important factor in writing clearly. If you write something clumsy and can\u0026rsquo;t seem to fix it, probably you aren\u0026rsquo;t sure what you really want to say. Once you know what to say, just say it.\n  Make it easy for the reader to find out what you\u0026rsquo;ve done. Put the sexy stuff up front, at all levels of organization from paragraph up to the whole paper. Carefully craft the abstract. Be sure it tells what your good idea is. Be sure you yourself know what it is! Then figure out how to say it in a few sentences. Too many abstracts tell what the paper is generally about and promise an idea without saying what it is.\n  Don\u0026rsquo;t \u0026ldquo;sell\u0026rdquo; what you\u0026rsquo;ve done with big words or claims. Your readers are good people; honesty and self-respect suffice. Contrariwise, don\u0026rsquo;t apologize for or cut down your own work.\n  Often you\u0026rsquo;ll write a clause or sentence or paragraph that you know is bad, but you won\u0026rsquo;t be able to find a way to fix it. This happens because you\u0026rsquo;ve worked yourself into a corner and no local choice can get you out. You have to back out and rewrite the whole passage. This happens less with practice.\n  Make sure your paper has an idea in it. If your program solves problem X in 10 ms, tell the reader why it\u0026rsquo;s so fast. Don\u0026rsquo;t just explain how your system is built and what it does, also explain why it works and why it\u0026rsquo;s interesting.\n  Write for people, not machines. It\u0026rsquo;s not enough that your argument be correct, it has to be easy to follow. Don\u0026rsquo;t rely on the reader to make any but the most obvious deductions. That you explained how the frobnitz worked in a footnote on page seven is not a justification when the reader gets confused by your introducing it without further explanation on page twenty-three. Formal papers are particularly hard to write clearly. Do not imitate math texts; their standard of elegance is to say as little as possible, and so to make the reader\u0026rsquo;s job as hard as possible. This is not appropriate for AI.\n  After you have written a paper, delete the first paragraph or the first few sentences. You\u0026rsquo;ll probably find that they were content-free generalities, and that a much better introductory sentence can be found at the end of the first paragraph of the beginning of the second.\n  If you put off writing until you\u0026rsquo;ve done all the work, you\u0026rsquo;ll lose most of the benefit. Once you start working on a research project, it\u0026rsquo;s a good idea to get into the habit of writing an informal paper explaining what you are up to and what you\u0026rsquo;ve learned every few months. Start with the contents of your research notebook. Take two days to write it-if it takes longer, you are being perfectionistic. This isn\u0026rsquo;t something you are judged on; it\u0026rsquo;s to share with your friends. Write DRAFT-NOT FOR CITATION on the cover. Make a dozen copies and give them to people who are likely to be interested (including your advisor!). This practice has most of the benefits of writing a formal paper (comments, clarity of thought, writing practice, and so forth), but on a smaller scale, and with much less work invested. Often, if your work goes well, these informal papers can be used later as the backbone of a more formal paper, from an AI Lab Working Paper to a journal article.\nOnce you become part of the Secret Paper Passing Network, you\u0026rsquo;ll find that people give you copies of draft papers that they want comments on. Getting comments on your papers is extremely valuable. You get people to take the time to write comments on yours by writing comments on theirs. So the more people\u0026rsquo;s papers you write comments on, the more favors are owed you when you get around to writing one good politics. Moreover, learning to critique other people\u0026rsquo;s papers will help your own writing.\nWriting useful comments on a paper is an art.\n  To write really useful comments, you need to read the paper twice, once to get the ideas, and the second time to mark up the presentation.\n  If someone is making the same mistake over and over, don\u0026rsquo;t just mark it over and over. Try to figure out what the pattern is, why the person is doing it, and what they can do about it. Then explain this explicitly at length on the front page and/or in person.\n  The author, when incorporating your comments, will follow the line of least resistance, fixing only one word if possible, or if not then one phrase, or if not then one sentence. If some clumsiness in their text means that they have to back up to the paragraph level, or that they have to rethink the central theme of a whole section, or that the overall organization of the paper is wrong, say this in big letters so they can\u0026rsquo;t ignore it.\n  Don\u0026rsquo;t write destructive criticism like \u0026ldquo;garbage\u0026rdquo; on a paper. This contributes nothing to the author. Take the time to provide constructive suggestions. It\u0026rsquo;s useful to think about how you would react to criticism of your own paper when providing it for others.\n  There are a variety of sorts of comments. There are comments on presentation and comments on content. Comments on presentation vary in scope. Copy-edits correct typos, punctuation, misspellings, missing words, and so forth. Learn the standard copy-editing symbols. You can also correct grammar, diction, verbosity, and muddied or unclear passages. Usually people who make grammatical mistakes do so consistently, using comma splices for example; take the time to explain the problem explicitly. Next there are organizational comments: ideas out of order at various scales from clauses through sentences and paragraphs to sections and chapters; redundancy; irrelevant content; missing arguments.\nComments on content are harder to characterize. You may suggest extensions to the author\u0026rsquo;s ideas, things to think about, errors, potential problems, expressions of admiration. \u0026ldquo;You ought to read X because Y\u0026rdquo; is always a useful comment.\nIn requesting comments on a paper, you may wish to specify which sorts are most useful. For an early draft, you want mostly comments on content and organization; for a final draft, you want mostly comments on details of presentation. Be sure as a matter of courtesy to to run the paper through a spelling corrector before asking for comments.\nYou don\u0026rsquo;t have to take all the suggestions you get, but you should take them seriously. Cutting out parts of a paper is particularly painful, but usually improves it. Often if you find yourself resisting a suggestion it is because while it points out a genuine problem with your paper the solution suggested is unattractive. Look for a third alternative.\nGetting your papers published counts. This can be easier than it seems. Basically what reviewers for AI publications look for is a paper that (a) has something new to say and (b) is not broken in some way. If you look through an IJCAI proceedings, for example, you\u0026rsquo;ll see that standards are surprisingly low. This is exacerbated by the inherent randomness of the reviewing process. So one heuristic for getting published is to keep trying. Here are some more:\n  Make sure it is readable. Papers are rejected because they are incomprehensible or ill-organized as often as because they don\u0026rsquo;t have anything to say.\n  Circulate drafts for a while before sending it in to the journal. Get and incorporate comments. Resist the temptation to hurry a result into publication; there isn\u0026rsquo;t much competition in AI, and publication delays will outweigh draft-commenting delays anyway.\n  Read some back issues of the journal or conference you are submitting to to make sure that the style and content of your paper are appropriate to it.\n  Most publications have an \u0026ldquo;information for authors,\u0026rdquo; a one page summary of what they want. Read it.\n  The major conferences choose prize papers on the basis of excellence both of content and presentation from among those accepted. Read them.\n  It\u0026rsquo;s often appropriate to send a short, early report on a piece of work to a conference and then a longer, final version to a journal.\n  Papers get rejected-don\u0026rsquo;t get dejected.\n  The reviewing process differs greatly between journals and conferences. To get quick turn-around time, conferences must review quickly. There is no time for contemplation or for interaction. If you get bounced, you lose. But with a journal, you can often argue with the editor, and with the referee through the editor.\n  Referees should be helpful. If you get an obnoxious referee report, you should complain to the program chair or editor. Don\u0026rsquo;t expect much feedback from conference referee reports. But from journals, you can often get excellent suggestions. You don\u0026rsquo;t have to do all of them, but if you don\u0026rsquo;t you should explain why, and realize that it may take further negotiation. In any case, no matter which side of the reviewing process you are on, be polite. You are going to be working with the people whose papers you review as part of a community for the rest of your professional life.\n  MIT AI Lab Memos are generally of publishable or near-publishable quality. De facto, Technical Reports are almost always revised versions of theses. Working Papers can be and often are very informal. They are a good way to get a lot of copies made of a paper you\u0026rsquo;d want to send to a bunch of colleagues anyway. You publish one of these internal documents by getting a form from the Publications Office (just off the eighth floor playroom) and getting two faculty members to sign it.\n  Like all else in research, paper writing always takes a lot longer than you expect. Papers for publication have a particularly insidious form of this disease, however. After you finally finish a paper, you send it in for publication. Many months later it comes back with comments, and you have to revise it. Then months after that the proofs come back for correction. If you publish several forms of the paper, like a short conference version and a long journal version, this may go through several rounds. The result is that you are still working on a paper years after you thought you were through with it and after the whole topic has become utterly boring. This suggests a heuristic: don\u0026rsquo;t do some piece of research you don\u0026rsquo;t care for passionately on the grounds that it won\u0026rsquo;t be hard to get a publication out of it: the pain will be worse than you expect.\nTalks Talks are another form of communication with your colleagues, and most of what we said about writing is true of talking also. An ability to stand in front of an audience and give a talk that doesn\u0026rsquo;t make the audience fall asleep is crucial for success in terms of recognition, respect and eventually a job. Speaking ability is not innate-you can start out graduate life as a terrible public speaker and end up as a sparkling wit so long as you practice, practice, practice, by actually giving talks to groups of people.\nSome ways to learn and practice speaking:\n  Patrick Winston has a great short paper on how to give talks. He also gives a lecture based on it every January which simultaneously illustrates and describes his heuristics.\n  If you feel you are a bad speaker, or if you want to be a good one, take a course on public speaking. An intro acting class is also useful.\n  If your advisor\u0026rsquo;s students have regular research meetings, volunteer to talk about your stuff.\n  The MIT AI lab has a series of semiformal talks known as the Revolving Seminar. Volunteer to give one if you have something worth turning into an AI memo or a conference paper.\n  Learn enough about the Lab\u0026rsquo;s various robotics projects so when your relatives or friends from out of town come you can give them a tour and a little 60 second talk in front of each robot about it. (Your relatives and non-AI friends will usually love this; they won\u0026rsquo;t be so impressed by the intricacies of your TMS.)\n  Since revising a talk is generally much easier than revising a paper, some people find that this is a good way to find the right way to express their ideas. (Mike Brady once remarked that all of his best papers started out as talks.)\n  Practice the talk in an empty room, preferably the one in which you will deliver it. Studies of context effects in memory suggest that you will remember what you are going to say better if you have practiced in the room you deliver in. Practice runs let you debug the mechanics of a talk: what to say during each slide, moving overlays around smoothly, keeping notes and slides synchronized, estimating the length of the entire talk. The less time you spend fumbling around with your equipment, the more time you have left to communicate.\n  Practicing with a mirror or tape or video recorder is another alternative. The lab has all three. They might help debug your voice and body language, too.\n  For a relatively formal talk-especially your Oral Exam-do a practice run for half a dozen friends and have them critique it.\n  Watch the way other people give talks. There are a lot of talks given by people visiting MIT. Attending such talks is a good way to get a taste of areas you aren\u0026rsquo;t so familiar with, and if the talk turns out to be boring, you can amuse yourself by analyzing what the speaker is doing wrong. (Going to a seminar is also a way to cure the mid-afternoon munchies)\n  Cornering one of your friends and trying to explain your most recent brainstorm to him is a good way both to improve your communication skills, and to debug your ideas.\n  Some key things to remember in planning and delivering a talk:\n  You can only present one \u0026ldquo;idea\u0026rdquo; or \u0026ldquo;theme\u0026rdquo; in a talk. In a 20 minute or shorter talk the idea must be crystal clear and cannot have complicated associated baggage. In a 30 or 45 minute talk the idea can require some buildup or background. In an hour talk the idea can be presented in context, and some of the uglies can be revealed. Talks should almost never go on for more than an hour (though they often do).\n  The people in the audience want to be there; they want to learn what you have to say. They aren\u0026rsquo;t just waiting for an excuse to attack you, and will feel more comfortable if you are relaxed.\n  Take at least one minute per overhead. Some people vary in their rate, but a common bug is to think that you can do it faster than that and still be clear. You can\u0026rsquo;t.\n  Don\u0026rsquo;t try to cram everything you know into a talk. You need to touch on just the high points of your ideas, leaving out the details.\n  AI talks are usually accompanied by overhead transparencies, otherwise known as \u0026ldquo;slides\u0026rdquo;. They should be kept simple. Use few words and big type. If you can\u0026rsquo;t easily read your slides when you are standing and they are on the floor, they\u0026rsquo;re too small. Draw pictures whenever possible. Don\u0026rsquo;t stand in front of the screen. Don\u0026rsquo;t point at the overhead if it is possible to point directly at the screen. If you must point at the overhead, don\u0026rsquo;t actually touch the transparency since you will make it jerk around.\n  Programming Not every AI thesis involves code, and there are important people in AI who have never written a significant program, but to a first approximation you have to be able to program to do AI. Not only does most AI work involve writing programs, but learning to program gives you crucial intuitions into what is and isn\u0026rsquo;t computationally feasible, which is the major source of constraint AI contributes to cognitive science.\nAt MIT, essentially all AI programming is done in Common Lisp. If you don\u0026rsquo;t know it, learn it. Learning a language is not learning to program, however; and AI programming involves some techniques quite different from those used for systems programming or for other applications. You can start by reading Abelson and Sussman\u0026rsquo;s Structure and Interpretation of Computer Programs and doing some of the exercises. That book isn\u0026rsquo;t about AI programming per se, but it teaches some of the same techniques. Then read the third edition of Winston and Horn\u0026rsquo;s Lisp book; it\u0026rsquo;s got a lot of neat AI programs in it. Ultimately, though, programming, not reading, is the best way to learn to program.\nThere is a lot of Lisp programming culture that is mostly learned by apprenticeship. Some people work well writing code together; it depends strongly on the personalities involved. Jump at opportunities to work directly with more experienced programmers. Or see if you can get one of them to critique your code. It\u0026rsquo;s also extremely useful to read other people\u0026rsquo;s code. Ask half a dozen senior grad students if you can get the source code for their programs. They\u0026rsquo;ll probably complain a bit, and make noises about how their coding style is just awful, and the program doesn\u0026rsquo;t really work, and then give you the code anyway. Then read it through carefully. This is time consuming; it can take as long to read and fully understand someone else\u0026rsquo;s code as it would take you to write it yourself, so figure on spending a couple of weeks spread over your first term or two doing this. You\u0026rsquo;ll learn a whole lot of nifty tricks you wouldn\u0026rsquo;t have thought of and that are not in any textbook. You\u0026rsquo;ll also learn how not to write code when you read pages of incomprehensible uncommented gibberish.\nAll the standard boring things they tell you in software engineering class are true of AI programming too. Comment your code. Use proper data abstraction unless there is a compelling reason not to. Segregate graphics from the rest of your code, so most of what you build is Common Lisp, hence portable. And so on.\nOver your first couple years, you should write your own versions of a bunch of standard AI building blocks, such as\n a truth maintenance system a means-ends planner a unification rule system a few interpreters of various flavors an optimizing compiler with flow analysis a frame system with inheritance several search methods an explanation-based learner  whatever turns you on. You can write stripped-down but functional versions of these in a few days. Extending an existing real version is an equally powerful alternative. It\u0026rsquo;s only when you\u0026rsquo;ve written such things that you really understand them, with insight into when they are and aren\u0026rsquo;t useful, what the efficiency issues are, and so forth.\nUnlike most other programmers, AI programmers rarely can borrow code from each other. (Vision code is an exception.) This is partly because AI programs rarely really work. (A lot of famous AI programs only worked on the three examples in the author\u0026rsquo;s thesis, though the field is less tolerant of this sloppiness than it once was.) The other reason is that AI programs are usually thrown together in a hurry without concern for maximum generality. Using Foobar\u0026rsquo;s \u0026ldquo;standard\u0026rdquo; rule interpreter may be very useful at first, and it will give you insight into what\u0026rsquo;s wrong if it doesn\u0026rsquo;t have quite the functionality you need, or that it\u0026rsquo;s got too much and so is too inefficient. You may be able to modify it, but remember that understanding someone else\u0026rsquo;s code is very time consuming. It\u0026rsquo;s sometimes better to write your own. This is where having done the half-dozen programming projects in the last paragraph becomes real handy. Eventually you get so you can design and implement a custom TMS algorithm (say) in an afternoon. (Then you\u0026rsquo;ll be debugging it on and off for the next six weeks, but that\u0026rsquo;s how it is.) Sometimes making a standard package work can turn into a thesis in itself.\nLike papers, programs can be over-polished. Rewriting code till it\u0026rsquo;s perfect, making everything maximally abstract, writing macros and libraries, and playing with operating system internals has sucked many people out their theses and out of the field. (On the other hand, maybe that\u0026rsquo;s what you really wanted to be doing for a living anyway.)\nAdvisors At MIT there are two kinds of advisors, academic advisors and thesis advisors.\nAcademic advisors are simple so we\u0026rsquo;ll dispose of them first. Every graduate student is assigned a faculty member as academic advisor, generally in his or her area, though it depends on current advisor loads. The function of the academic advisor is to represent the department to you: to tell you what the official requirements are, to get on your case if you are late satisfying them, and to OK your class schedule. If all goes well, you only have to see your academic advisor in that capacity twice a year on registration day. On the other hand, if you are having difficulties, your academic advisor may be able to act as advocate for you, either in representing you to the department or in providing pointers to sources of assistance.\nThe thesis advisor is the person who supervises your research. Your choice of thesis advisor is the most important decision you\u0026rsquo;ll make as a graduate student, more important than that of thesis topic area. To a significant extent, AI is learned by apprenticeship. There is a lot of informal knowledge both of technical aspects of the field and of the research process that is not published anywhere.\nMany AI faculty members are quite eccentric people. The grad students likewise. The advisor-advisee relationship is necessarily personal, and your personality quirks and your advisor\u0026rsquo;s must fit well enough that you can get work done together.\nDifferent advisors have very different styles. Here are some parameters to consider.\n  How much direction do you want? Some advisors will hand you a well-defined thesis-sized problem, explain an approach, and tell you to get to work on it. If you get stuck, they\u0026rsquo;ll tell you how to proceed. Other advisors are hands-off; they may give you no help in choosing a topic at all, but can be extremely useful to bounce ideas off of once you find one. You need to think about whether you work better independently or with structure.\n  How much contact do you want? Some advisors will meet with you weekly for a report on your progress. They may suggest papers to read and give you exercises and practice projects to work. Others you may not talk to more than twice a term.\n  How much pressure do you want? Some advisors will exert more than others.\n  How much emotional support do you want? Some can give more than others.\n  How seriously do you want to take your advisor? Most advisors will suggest thesis topics fairly regularly. Some can be depended on to produce suggestions that, if carried out diligently, will almost certainly produce an acceptable, if perhaps not very exciting thesis. Others throw out dozens of off-the-wall ideas, most of which will go nowhere, but one in ten of which, if pursued with vision, can result in ground-breaking work. If you choose such an advisor, you have to act as the filter.\n  What kind of research group does the advisor provide? Some professors create an environment in which all their students work together a lot, even if they are not all working on the same project. Many professors get together with their all their students for weekly or biweekly meetings. Will that be useful to you? Are the advisor\u0026rsquo;s students people you get along with? Some students find that they construct important working relationships with students from other research groups instead.\n  Do you want to be working on a part of a larger project? Some professors divide up a big system to be built into pieces and assign pieces to individual students. That gives you a group of people that you can talk to about the problem as a whole.\n  Do you want cosupervision? Some thesis projects integrate several areas of AI, and you may want to form strong working relationships with two or more professors. Officially, you\u0026rsquo;ll have just one thesis supervisor, but that doesn\u0026rsquo;t have to reflect reality.\n  Is the advisor willing to supervise a thesis on a topic outside his main area of research? Whether or not you can work with him or her may be more important to both of you than what you are working on. Robotics faculty at MIT have supervised theses on qualitative physics and cognitive modeling; faculty in reasoning have supervised vision theses. But some faculty members are only willing to supervise theses on their own area of interest. This is often true of junior faculty members who are trying to build tenure cases; your work counts toward that.\n  Will the advisor fight the system for you? Some advisors can keep the department and other hostile entities off your back. The system works against certain sorts of students (notably women and eccentrics), so this can be very important.\n  Is the advisor willing and able to promote your work at conferences and the like? This is part of his or her job, and can make a big difference for your career.\n  The range of these parameters varies from school to school. MIT in general gives its students a lot more freedom than most schools can afford to.\nFinding a thesis advisor is one of the most important priorities of your first year as a graduate student. You should have one by the end of the first year, or early in the second year at the latest. Here are some heuristics on how to proceed:\n  Read the Lab\u0026rsquo;s research summary. It gives a page or so description of what each of the faculty and many of the graduate students are up to.\n  Read recent papers of any faculty member whose work seems at all interesting.\n  Talk to as many faculty members as you can during your first semester. Try to get a feel for what they are like, what they are interested in, and what their research and supervision styles are like.\n  Talk to grad students of prospective advisors and ask what working for him or her is like. Make sure you talk to more than one student who works with a particular advisor as each advisor has a large spectrum of working styles and levels of success in interaction with his or her students. You could be misled either way by a single data point. Talk to his or her first year advisees and his seventh year advisees too.\n  Most or all faculty member\u0026rsquo;s research group meetings are open to new grad students, and they are a very good way of getting an idea of what working with them is like.\n  AI is unusual as a discipline in that much of the useful work is done by graduate students, not people with doctorates, who are often too busy being managers. This has a couple of consequences. One is that the fame of a faculty member, and consequently his tenure case, depends to a significant extent on the success of his students. This means that professors are highly motivated to get good students to work for them, and to provide useful direction and support to them. Another consequence is that, since to a large degree students\u0026rsquo; thesis directions are shaped by their advisors, the direction and growth of the field as a whole depends a great deal on what advisors graduate students pick.\nAfter you\u0026rsquo;ve picked and advisor and decided what you want from him or her, make sure he or she knows. You advisor may hear \u0026ldquo;I\u0026rsquo;d like to work with you\u0026rdquo; as \u0026ldquo;Please give me a narrowly specified project to do,\u0026rdquo; or \u0026ldquo;I\u0026rsquo;ve got stuff I\u0026rsquo;d like to do and I want you to sign it when I\u0026rsquo;m done,\u0026rdquo; or something else. Don\u0026rsquo;t let bad communication get you into a position of wasting a year either spinning your wheels when you wanted close direction or laboring under a topic that isn\u0026rsquo;t the thing you had your heart set on.\nDon\u0026rsquo;t be fully dependent on your advisor for advice, wisdom, comments, and connections. Build your own network. You can probably find several people with different things to offer you, whether they\u0026rsquo;re your official advisor or not. It\u0026rsquo;s important to get a variety of people who will regularly review your work, because it\u0026rsquo;s very easy to mislead yourself (and often your advisor as well) into thinking you are making progress when you are not, and so zoom off into outer space. The network can include graduate students and faculty at your own lab at others.\nIt is possible that you will encounter racist, sexist, heterosexist, or other harrassment in your relationships with other students, faculty members, or, most problematically, your advisor. If you do, get help. MIT\u0026rsquo;s ODSA publishes a brochure called \u0026ldquo;STOP Harrassment\u0026rdquo; with advice and resources. The Computer Science Women\u0026rsquo;s Report, available from the LCS document room, is also relevant.\nSome students in the lab are only nominally supervised by a thesis advisor. This can work out well for people who are independent self-starters. It has the advantage that you have only your own neuroses to deal with, not your advisor\u0026rsquo;s as well. But it\u0026rsquo;s probably not a good idea to go this route until you\u0026rsquo;ve completed at least one supervised piece of work, and unless you are sure you can do without an advisor and have a solid support network.\nThe thesis Your thesis, or theses, will occupy most of your time during most of your career as a graduate student. The bulk of that time will be devoted to research, or even to choosing a topic, rather than to the actual writing.\nThe Master\u0026rsquo;s thesis is designed as practice for the PhD thesis. PhD-level research is too hard to embark on without preparation. The essential requirement of a Master\u0026rsquo;s thesis is that it literally demonstrate mastery: that you have fully understood the state of the art in your subfield and that you are capable of operating at that level. It is not a requirement that you extend the state of the art, nor that the Master\u0026rsquo;s thesis be publishable. There is a substantial machismo about theses in our lab, however, so that many Master\u0026rsquo;s theses do in fact contribute significantly to the field, and perhaps half are published. This is not necessarily a good thing. Many of us burn out on our Master\u0026rsquo;s work, so that it is notorious that MIT Master\u0026rsquo;s theses are often better than the PhD theses. This defeats the preparatory intent of the Master\u0026rsquo;s. The other factor is that doing research that contributes to the field takes at least two years, and that makes the graduate student career take too damn long. You may not feel in a hurry now, but after you\u0026rsquo;ve been around the Lab for seven years you\u0026rsquo;ll want out badly. The mean time from entrance to finishing the Master\u0026rsquo;s is two and a half years. However, the CS department is strongly encouraging students to reduce this period. If a Master\u0026rsquo;s topic turns out to be a blockbuster, it can be split into parts, one for the Master\u0026rsquo;s and one for a PhD.\nTo get some idea of what constitutes a Master\u0026rsquo;s thesis-sized piece of research, read several recent ones. Keep in mind that the ones that are easy to get at are the ones that were published or made into tech reports because someone thought they extended the state of the art-in other words, because they did more than a Master\u0026rsquo;s thesis needs to. Try also reading some theses that were accepted but not published. All accepted theses can be found in one of the MIT libraries. PhD theses are required to extend the state of the art. PhD thesis research should be of publishable quality. MIT machismo operates again, so that many PhD theses form the definitive work on a subarea for several years. It is not uncommon for a thesis to define a new subarea, or to state a new problem and solve it. None of this is necessary, however.\nIn general, it takes about two to three years to do a PhD thesis. Many people take a year or two to recover from the Master\u0026rsquo;s and to find a PhD topic. It\u0026rsquo;s good to use this period to do something different, like being a TA or getting a thorough grounding in a non-AI field or starting a rock and roll band. The actual writing of the PhD thesis generally takes about a year, and an oft-confirmed rule of thumb is that it will drag on for a year after you are utterly sick of it.\nChoosing a topic is one of the most difficult and important parts of thesis work.\n  A good thesis topic will simultaneously express a personal vision and participate in a conversation with the literature.\n  Your topic must be one you are passionate about. Nothing less will keep you going. Your personal vision is your reason for being a scientist, an image or principle or idea or goal you care deeply about. It can take many forms. Maybe you want to build a computer you can talk to. Maybe you want to save the world from stupid uses of computers. Maybe you want to demonstrate the unity of all things. Maybe you want to found colonies in space. A vision is always something big. Your thesis can\u0026rsquo;t achieve your vision, but it can point the way.\n  At the same time, science is a conversation. An awful lot of good people have done their best and they\u0026rsquo;re written about it. They\u0026rsquo;ve accomplished a great deal and they\u0026rsquo;ve completely screwed up. They\u0026rsquo;ve had deep insights and they\u0026rsquo;ve been unbelievably blind. They\u0026rsquo;ve been heros and cowards. And all of this at the same time. Your work will be manageable and comprehensible if it is framed as a conversation with these others. It has to speak to their problems and their questions, even if it\u0026rsquo;s to explain what\u0026rsquo;s wrong with them. A thesis topic that doesn\u0026rsquo;t participate in a conversation with the literature will be too big or too vague, or nobody will be able to understand it.\n  The hardest part is figuring out how to cut your problem down to a solvable size while keeping it big enough to be interesting. \u0026ldquo;Solving AI breadth-first\u0026rdquo; is a common disease; you\u0026rsquo;ll find you need to continually narrow your topic. Choosing a topic is a gradual process, not a discrete event, and will continue up to the moment you declare the thesis finished. Actually solving the problem is often easy in comparison to figuring out what exactly it is. If your vision is a fifty-year project, what\u0026rsquo;s the logical ten-year subproject, and what\u0026rsquo;s the logical one-year subproject of that? If your vision is a vast structure, what\u0026rsquo;s the component that gets most tellingly to its heart, and what demonstration would get most tellingly to the heart of that component?\n  An important parameter is how much risk you can tolerate. Often there is a trade-off between the splashiness of the final product and the risk involved in producing it. This isn\u0026rsquo;t always true, though, because AI has a high ratio of unexplored ideas to researchers.\n  An ideal thesis topic has a sort of telescoping organization. It has a central portion you are pretty sure you can finish and that you and your advisor agree will meet the degree requirements. It should have various extensions that are successively riskier and that will make the thesis more exciting if they pan out. Not every topic will fit this criterion, but it\u0026rsquo;s worth trying for.\n  Some people find that working on several potential thesis projects at once allows them to finish the one that works out and abandon the ones that fail. This decreases the risk. Others find that the substantial thrashing overhead this engenders is too high, and choose a single topic before starting any work in earnest.\n  You may only be interested in a particular subfield, in which case your thesis topic search is narrowed. You may find, though, that there\u0026rsquo;s no faculty member who can supervise a topic in that field whom you are comfortable working with. You may also find that there doesn\u0026rsquo;t seem to be a natural topic to work on in that field, whereas you have good ideas about something else.\n  Choosing a Master\u0026rsquo;s topic can be harder than choosing the PhD topic, because it has to be done before you know very much and before you\u0026rsquo;ve built much self-confidence.\n  One parameter of PhD topic choice is whether to continue working in the same subfield as your Master\u0026rsquo;s, perhaps extending or building on that work, or to switch to another subfield. Staying in the same field simplifies things and probably will take one to two years off the total time to graduation, especially if a PhD-sized topic becomes obvious during the course of the Master\u0026rsquo;s work. But it may leave you \u0026ldquo;typecast\u0026rdquo; as someone who does shape-from-shading or circuit analysis; changing fields gives you breadth.\n  Topics can be placed in a spectrum from flakey to cut-and-dried. Flakier theses open up new territory, explore previously unresearched phenomena, or suggest heuristic solutions to problems that are known to be very hard or are hard to characterize. Cut-and-dried theses rigorously solve well-characterized problems. Both are valuable; where you situate yourself in this spectrum is a matter of personal style.\n  The \u0026ldquo;further work\u0026rdquo; sections of papers are good sources of thesis topics.\n  Whatever you do, it has to have not been done before. Also, it\u0026rsquo;s not a good idea to work on something that someone else is doing simultaneously. There\u0026rsquo;s enough turf out there that there\u0026rsquo;s no need for competition. On the other hand, it\u0026rsquo;s common to read someone else\u0026rsquo;s paper and panic because it seems to solve your thesis problem. This happens most when you\u0026rsquo;re halfway through the process of making your topic specific and concrete. Typically the resemblance is actually only superficial, so show the paper to some wise person who knows your work and ask them what they think.\n  Not all MIT AI Lab theses are about AI; some are hardware or programming language theses. This is OK.\n  Once you\u0026rsquo;ve got a thesis topic, even when it\u0026rsquo;s a bit vague, you should be able to answer the question \u0026ldquo;what\u0026rsquo;s the thesis of your thesis?\u0026rdquo; What are you trying to show? You should have one-sentence, one-paragraph, and five-minute answers. If you don\u0026rsquo;t know where you are going, people won\u0026rsquo;t take you seriously, and, worse, you\u0026rsquo;ll end up wandering around in circles.\nWhen doing the work, be able to explain simply how each part of your theory and implementation is in service of the goal.\nMake sure once you\u0026rsquo;ve selected a topic that you get a clear understanding with your advisor as to what will constitute completion. If you and he have different expectations and don\u0026rsquo;t realize it, you can lose badly. You may want to formulate an explicit end-test, like a set of examples that your theory or program will be able to handle. Do this for yourself anyway, even if your advisor doesn\u0026rsquo;t care. Be willing to change this test if circumstances radically change.\nTry a simplified version of the thesis problem first. Work examples. Thoroughly explore some concrete instances before making an abstract theory.\nThere are a number ways you can waste a lot of time during the thesis. Some activities to avoid (unless they are central to the thesis): language design, user-interface or graphics hacking, inventing new formalisms, overoptimizing code, tool building, bureaucracy. Any work that is not central to your thesis should be minimized.\nThere is a well-understood phenomenon known as \u0026ldquo;thesis avoidance,\u0026rdquo; whereby you suddenly find fixing obscure bugs in an obsolete operating system to be utterly fascinating and of paramount importance. This is invariably a semiconscious way of getting out of working on one\u0026rsquo;s thesis. Be aware that\u0026rsquo;s what you are doing. (This document is itself an example of thesis avoidance on the part of its authors.)\nResearch methodology 【This section is weak. Please contribute!】\nA research methodology defines what the activity of research is, how to proceed, how to measure progress, and what constitutes success. AI methodology is a jumbled mess. Different methodologies define distinct schools which wage religious wars against each other.\nMethods are tools. Use them; don\u0026rsquo;t let them use you. Don\u0026rsquo;t fall for slogans that raise one above the others: \u0026ldquo;AI research needs to be put on firm foundations;\u0026rdquo; \u0026ldquo;Philosophers just talk. AI is about hacking;\u0026rdquo; \u0026ldquo;You have to know what\u0026rsquo;s computed before you ask how.\u0026rdquo; To succeed at AI, you have to be good at technical methods and you have to be suspicious of them. For instance, you should be able to prove theorems and you should harbor doubts about whether theorems prove anything.\nMost good pieces of AI delicately balance several methodologies. For example, you must walk a fine line between too much theory, possibly irrelevant to any real problem, and voluminous implementation, which can represent an incoherent munging of ad-hoc solutions. You are constantly faced with research decisions that divide along a boundary between \u0026ldquo;neat\u0026rdquo; and \u0026ldquo;scruffy.\u0026rdquo; Should you take the time to formalize this problem to some extent (so that, for example, you can prove its intractability), or should you deal with it in its raw form, which ill-defined but closer to reality? Taking the former approach leads (when successful) to a clear, certain result that will usually be either boring or at least will not Address the Issues; the latter approach runs the risk of turning into a bunch of hacks. Any one piece of work, and any one person, should aim for a judicious balance, formalizing subproblems that seem to cry for it while keeping honest to the Big Picture.\nSome work is like science. You look at how people learn arithmetic, how the brain works, how kangaroos hop, and try to figure it out and make a testable theory. Some work is like engineering: you try to build a better problem solver or shape-from algorithm. Some work is like mathematics: you play with formalisms, try to understand their properties, hone them, prove things about them. Some work is example-driven, trying to explain specific phenomena. The best work combines all these and more.\nMethodologies are social. Read how other people attacked similar problems, and talk to people about how they proceeded in specific cases.\nEmotional factors Research is hard. It is easy to burn out on it. An embarrassingly small fraction of students who start PhD programs in AI finish. AT MIT, almost all those who do not finish drop out voluntarily. Some leave because they can make more money in industry, or for personal reasons; the majority leave out of frustration with their theses. This section tries to explain how that can happen and to give some heuristics that may help. Forewarned is forearmed: mostly it\u0026rsquo;s useful to know that the particular sorts of tragedies, aggravations, depressions and triumphs you go through in research are necessary parts of the process, and are shared with everyone else who does it.\nAll research involves risk. If your project can\u0026rsquo;t fail, it\u0026rsquo;s development, not research. What\u0026rsquo;s hard is dealing with project failures. It\u0026rsquo;s easy to interpret your project failing as your failing; in fact, it proves that you had the courage to do something difficult.\nThe few people in the field who seem to consistently succeed, turning out papers year after year, in fact fail as often as anyone else. You\u0026rsquo;ll find that they often have several projects going at once, only a few of which pan out. The projects that do succeed have usually failed repeatedly, and many wrong approaches went into the final success.\nAs you work through your career, you\u0026rsquo;ll accumulate a lot of failures. But each represents a lot of work you did on various subtasks of the overall project. You\u0026rsquo;ll find that a lot of the ideas you had, ways of thinking, even often bits of code you wrote, turn out to be just what\u0026rsquo;s needed to solve a completely different problem several years later. This effect only becomes obvious after you\u0026rsquo;ve piled up quite a stack of failures, so take it on faith as you collect your first few that they will be useful later.\nResearch always takes much, much longer than it seems it ought to. The rule of thumb is that any given subtask will take three times as long as you expect. (Some add, \u0026quot; even after taking this rule into account.\u0026rdquo;)\nCrucial to success is making your research part of your everyday life. Most breakthroughs occur while you are in the shower or riding the subway or windowshopping in Harvard Square. If you are thinking about your research in background mode all the time, ideas will just pop out. Successful AI people generally are less brilliant than they are persistent. Also very important is \u0026ldquo;taste,\u0026rdquo; the ability to differentiate between superficially appealing ideas and genuinely important ones.\nYou\u0026rsquo;ll find that your rate of progress seems to vary wildly. Sometimes you go on a roll and get as much done in a week as you had in the previous three months. That\u0026rsquo;s exhilarating; it\u0026rsquo;s what keeps people in the field. At other times you get stuck and feel like you can\u0026rsquo;t do anything for a long time. This can be hard to cope with. You may feel like you\u0026rsquo;ll never do anything worthwhile again; or, near the beginning, that you don\u0026rsquo;t have what it takes to be a researcher. These feelings are almost certainly wrong; if you were admitted as a student at MIT, you\u0026rsquo;ve got what it takes. You need to hang in there, maintaining high tolerance for low results.\nYou can get a lot more work done by regularly setting short and medium term goals, weekly and monthly for instance. Two ways you can increase the likelihood of meeting them are to record them in your notebook and to tell someone else. You can make a pact with a friend to trade weekly goals and make a game of trying to meet them. Or tell your advisor.\nYou\u0026rsquo;ll get completely stuck sometimes. Like writer\u0026rsquo;s block, there\u0026rsquo;s a lot of causes of this and no one solution.\n  Setting your sights too high leads to paralysis. Work on a subproblem to get back into the flow.\n  You can get into a positive feedback loop in which doubts about your ability to do the work eat away at your enthusiasm so that in fact you can\u0026rsquo;t get anything done. Realize that research ability is a learned skill, not innate genius.\n  If you find yourself seriously stuck, with nothing at all happening for a week or more, promise to work one hour a day. After a few days of that, you\u0026rsquo;ll probably find yourself back in the flow.\n  It\u0026rsquo;s hard to get started working in the morning, easy to keep going once you\u0026rsquo;ve started. Leave something easy or fun unfinished in the evening that you can start with in the morning. Start the morning with real work-if you start by reading your mail, you may never get to something more productive.\n  Fear of failure can make work hard. If you find yourself inexplicably \u0026ldquo;unable\u0026rdquo; to get work done, ask whether you are avoiding putting your ideas to the test. The prospect of discovering that your last several months of work have been for naught may be what\u0026rsquo;s stopping you. There\u0026rsquo;s no way to avoid this; just realize that failure and wasted work are part of the process.\n  Read Alan Lakien\u0026rsquo;s book How to Get Control of Your Time and Your Life, which is recommended even by people who hate self-help books. It has invaluable techniques for getting yourself into productive action.\n  Most people find that their personal life and their ability to do research interact. For some, work is a refuge when everything else is going to hell. Others find themselves paralyzed at work when life is in turmoil for other reasons. If you find yourself really badly stuck, it can be helpful to see a psychotherapist. An informal survey suggests that roughly half of the students in our lab see one at some point during their graduate careers.\nOne factor that makes AI harder than most other types of work is that there are no generally accepted standards of progress or of how to evaluate work. In mathematics, if you prove a theorem, you\u0026rsquo;ve done something; and if it was one that others have failed to prove, you\u0026rsquo;ve done something exciting. AI has borrowed standards from related disciplines and has some of its own; and different practitioners, subfields, and schools put different emphases on different criteria. MIT puts more emphasis on the quality of implementations than most schools do, but there is much variation even within this lab. One consequence of this is that you can\u0026rsquo;t please all the people all the time. Another is that you may often be unsure yourself whether you\u0026rsquo;ve made progress, which can make you insecure. It\u0026rsquo;s common to find your estimation of your own work oscillating from \u0026ldquo;greatest story ever told\u0026rdquo; to \u0026ldquo;vacuous, redundant, and incoherent.\u0026rdquo; This is normal. Keep correcting it with feedback from other people.\nSeveral things can help with insecurity about progress. Recognition can help: acceptance of a thesis, papers you publish, and the like. More important, probably, is talking to as many people as you can about your ideas and getting their feedback. For one thing, they\u0026rsquo;ll probably contribute useful ideas, and for another, some of them are bound to like it, which will make you feel good. Since standards of progress are so tricky, it\u0026rsquo;s easy to go down blind alleys if you aren\u0026rsquo;t in constant communication with other researchers. This is especially true when things aren\u0026rsquo;t going well, which is generally the time when you least feel like talking about your work. It\u0026rsquo;s important to get feedback and support at those times.\nIt\u0026rsquo;s easy not to see the progress you have made. \u0026ldquo;If I can do it, it\u0026rsquo;s trivial. My ideas are all obvious.\u0026rdquo; They may be obvious to you in retrospect, but probably they are not obvious to anyone else. Explaining your work to lots of strangers will help you keep in mind just how hard it is to understand what now seems trivial to you. Write it up.\nA recent survey of a group of Noble Laureates in science asked about the issue of self-doubt: had it been clear all along to these scientists that their work was earth-shattering? The unanimous response (out of something like 50 people) was that these people were constantly doubting the value, or correctness, of their work, and they went through periods of feeling that what they were doing was irrelevant, obvious, or wrong. A common and important part of any scientific progress is constant critical evaluation, and is some amount of uncertainty over the value of the work is an inevitable part of the process.\nSome researchers find that they work best not on their own but collaborating with others. Although AI is often a pretty individualistic affair, a good fraction of people work together, building systems and coauthoring papers. In at least one case, the Lab has accepted a coauthored thesis. The pitfalls here are credit assignment and competition with your collaborator. Collaborating with someone from outside the lab, on a summer job for example, lessens these problems.\nMany people come to the MIT AI Lab having been the brightest person in their university, only to find people here who seem an order of magnitude smarter. This can be a serious blow to self-esteem in your first year or so. But there\u0026rsquo;s an advantage to being surrounded by smart people: you can have someone friendly shoot down all your non-so-brilliant ideas before you could make a fool of yourself publicly. To get a more realistic view of yourself, it is important to get out into the real world where not everyone is brilliant. An outside consulting job is perfect for maintaining balance. First, someone is paying you for your expertise, which tells you that you have some. Second, you discover they really need your help badly, which brings satisfaction of a job well done.\nContrariwise, every student who comes into the Lab has been selected over about 400 other applicants. That makes a lot of us pretty cocky. It\u0026rsquo;s easy to think that I\u0026rsquo;m the one who is going to solve this AI problem for once and for all. There\u0026rsquo;s nothing wrong with this; it takes vision to make any progress in a field this tangled. The potential pitfall is discovering that the problems are all harder than you expected, that research takes longer than you expected, and that you can\u0026rsquo;t do it all by yourself. This leads some of us into a severe crisis of confidence. You have to face the fact that all you can do is contribute your bit to a corner of a subfield, that your thesis is not going to solve the big problems. That may require radical self-reevaluation; often painful, and sometimes requiring a year or so to complete. Doing that is very worthwhile, though; taking yourself less seriously allows you to approach research in a spirit of play.\nThere\u0026rsquo;s at least two emotional reasons people tolerate the pain of research. One is a drive, a passion for the problems. You do the work because you could not live any other way. Much of the best research is done that way. It has severe burn-out potential, though. The other reason is that good research is fun. It\u0026rsquo;s a pain a lot of the time, but if a problem is right for you, you can approach it as play, enjoying the process. These two ways of being are not incompatible, but a balance must be reached in how seriously to take the work.\nIn getting a feeling for what research is like, and as inspiration and consolation in times of doubt, it\u0026rsquo;s useful to read some of the livelier scientific autobiographies. Good ones are Gregory Bateson\u0026rsquo;s Advice to a Young Scientist, Freeman Dyson\u0026rsquo;s Disturbing the Universe, Richard Feynmann\u0026rsquo;s Surely You Are Joking, Mr. Feynmann!, George Hardy\u0026rsquo;s A Mathematician\u0026rsquo;s Apology, and Jim Watson\u0026rsquo;s The Double Helix.\nA month or two after you\u0026rsquo;ve completed a project such as a thesis, you will probably find that it looks utterly worthless. This backlash effect is the result of being bored and burned-out on the problem, and of being able to see in retrospect that it could have been done better-which is always the case. Don\u0026rsquo;t take this feeling seriously. You\u0026rsquo;ll find that when you look back at it a year or two later, after it is less familiar, you\u0026rsquo;ll think \u0026ldquo;Hey! That\u0026rsquo;s pretty clever! Nice piece of work!\u0026rdquo;\nEndnote This document incorporates ideas, text, and comments from Phil Agre, Jonathan Amsterdam, Jeff Anton, Alan Bawden, Danny Bobrow, Kaaren Bock, Jennifer Brooks, Rod Brooks, David Chapman, Jim Davis, Bruce Donald, Ken Forbus, Eric Grimson, Ken Haase, Dan Huttenlocher, Leslie Kaelbling, Mike Lowry, Patrick Sobalvarro, Jeff Shrager, Daniel Weise, and Ramin Zabih. We\u0026rsquo;d like to thank all the people who gave us the wisdom that we pass on in this document (and which, incidentally, got us through our theses), especially our advisors.\nSome of the ideas herein were lifted from \u0026ldquo;On Being a Researcher\u0026rdquo; by John Backus and \u0026ldquo;How to Get a PhD in AI,\u0026rdquo; by Alan Bundy, Ben du Boulay, Jim Howe, and Gordon Plotkin.\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-mit-how-to-research.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e  MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY October, 1988\n\u003cstrong\u003eKeywords:\u003c/strong\u003e\u003c/p\u003e","title":"【转】How to do Research At the MIT AI Lab"},{"content":"Abstract: 本文介绍狄莫弗与斯特林之间的联系，以及结果进一步的优化 Keywords: 二项分布，斯特林公式\n数理统计学简史 狄莫弗初步结果的改进，与斯特林的联系 初步结果的改进，与斯特林的联系 上文我们说的(5),(6)两个结果结局了喀明的问题，但是还不能给 $P_d$ 找了一个近似的公式，而找到这个近似公式，是狄莫弗对后世发挥重要贡献的的工作。 这方面的进一步推动来于斯特林，他在数学上以其关于阶乘的渐近公式而知名， 在1725年，喀明把狄莫弗的结果告知了斯特林，这激发了斯特林的兴趣，然后他使用级数做出了两个关于 $b(m)$ 的表达式： $$ b^2(m)=\\frac{2}{\\pi(2m+1)}[1+\\frac{1}{4(m+\\frac{3}{2})}+\\frac{1}{32(m+\\frac{3}{2})(m+\\frac{5}{2})}+\\dots]\\tag{7} $$ 以及 $$ b^{-2}(m)=\\pi m[1+\\frac{1}{4m+1}+\\frac{9}{32(m+1)(m+2)}+\\dots]\\tag{8} $$\n证明特别复杂，这里不描述，值得注意的是，这是 $\\pi$ 第一次引入此类公式，这也显示，二项分布的正态逼近这一重要论题中，斯特林功不可没，但是现在教科书上基本只写狄莫弗，所以我们有必要知道这些事实，所以教科书的客观与否，影响的是一代人。 注意 $m=\\frac{N}{2}$ 令 $N\\to \\infty$ 在 (8) 式左边只取主项1，就能得到下面的重要结论： $$ b(m)\\sim \\sqrt{\\frac{2}{\\pi N}}\\tag{9} $$ 这个结果意义重大，因为这个式子跟前文提到的(5)有相似之处，而(5) 式中 $\\sim$ 只表示两边接近的意思，对 $N\\to \\infty$ 时两边比值趋近于1的结论没有任何证明作用。而(9)则能指出这个结论，如果狄莫弗只停留在(5)上，这个结果如果再继续和后面的过程联系在一起很有可能最后会得到错误的答案，而错失追求真相的机会。但是从另一个方面，(9)在极限条件下是正解，但是在 $N$ 较小的情况下(5)式反而更准一些，比如当 $N=6$ 的时候。 下面做一个数值上的对比\n   m (5)的结果 (9)的结果 准确结果     $m=3$ $0.3256035$ $0.3257350$ $0.3125000$   $m=12$ $0.2302365$ $0.2303294$ $0.2255859$    值得庆幸的是狄莫弗和斯特林有联系，而且斯特林把结果告知了狄莫弗，很快狄莫弗发现（9）可以通过瓦里斯在1655年得到的下述无穷乘积结果： $$ \\text{lim}_{N\\to \\infty} \\sqrt{\\frac{1}{2N+1}}\\frac{2\\cdot 4\\cdot6\\dots 2N}{1\\cdot 3\\cdot 5\\dots(2N-1)}=\\sqrt{\\frac{\\pi}{2}}\\tag{10} $$ 从得出(9)的时间来看，认为狄莫弗注意到瓦里斯公式的关系是出于斯特林的结果提示的。如果狄莫弗能更早的发现和瓦里斯公式的关系，则他可以更早的提出公式(9),进而更早的完成整项工作，也能避免进入到 (5)的死胡同。 斯特林公式最早发表于 1730年 斯特林当年做出了一个关键结果，阶乘公式是这个结果的一个推论。 1730年狄莫弗证明了一个比斯特林原始阶乘公式更简洁的形式，斯特林原始结果比这个复杂许多： $$ m!=\\sqrt{2\\pi}m^{m+\\frac{1}{2}}e^{-m+\\frac{1}{12m}-\\frac{1}{360m^3}+\\dots}\\tag{11} $$ 省略掉了随 $m\\to \\infty$ 趋近于0的部分，得到教科书上常见的形式： $$ m!\\sim\\sqrt{2\\pi}m^{m+\\frac{1}{2}}e^{-m} $$\n总结 斯特林公式在我们学习概率论基础的时候是选学部分，现在看来证明确实有些复杂，但是作为二项分布近似计算的一部分，可以说简化了很多操作。 总结文章主旨：要多读论文，尤其是别人的结果，没准就能帮助你！\n","permalink":"https://go.face2ai.com/math/math-statistics-2-3-communication-with-stirling.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍狄莫弗与斯特林之间的联系，以及结果进一步的优化\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 二项分布，斯特林公式\u003c/p\u003e","title":"【数理统计学简史】2.3 狄莫弗初步结果的改进，与斯特林的联系"},{"content":"Abstract: 本文介绍内存的访问过程，也就是从应用发起请求到硬件实现的完整操作过程，这里是优化内存瓶颈的关键之处，也是CUDA程序优化的基础。 Keywords: 内存访问模式，对齐，合并，缓存，结构体数组，数组结构体\n内存访问模式  \u0026ldquo;物有本末，事有终始，知所先后，则近道矣\u0026rdquo; ——《大学·大学之道章》\n 这句话出自大学，大学非我们现在上的大学，而我不知道我们现在的大学为什么叫大学，是否和《大学》有关系，但是大学第一章，就是讲道，让读书人懂得道：\u0026ldquo;格物，致知，诚意，正心，修身，齐家，治国，平天下\u0026rdquo;，但是我们现在似乎所有的课程都不讲这些了——可能是因为时间过去太久了，所以成了糟粕，但是我觉得对我还是有些启发的。 第一句引用的话没有官方解释，我觉得小学时候学的语文，概括中心思想，我觉得都是扯淡，一百个人有一百种想法，但是答案却是统一的，所以，我觉得这种就是培养有流水线上的机器，我只说这句话对我的启发，而且我们就从机器学习这个角度说。 举个例子，忽略时间，单从技术的角度，神经网络算是始还是终？是本还是末？我觉得不是始，也不是终。更不是本。 对于人工智能领域，我们的始是对于智慧的理解和再造，终点就是制造出人类智慧的机器，而神经网络只是我们要探索的深林里面一个大数，当有人认为这是最高峰的时候，那么他会不断地向上爬，直到最顶端，也许发现到达了我们的最终目的，但更大的可能性是周围还有更高的树。所以神经网络不是始也不是终，只是中间的一个过程，必须承认，我们走了三四十年才看到这棵这么大的树，但是如果还没爬到最顶端，就认定这个树就是终点了，似乎缺少一些谨慎。 而整个学科的本，我认为是数学，当如果哪一天证明，数学的整个体系在人工智能面前崩溃了，各种反公理，反定理的事件都在人工智能中发生了，那就证明我错了。但目前，本还是本。 废话有点多，今天我们要学习的也是CUDA中最最最重要的课程之一，当然我不会一口气写完，可能要写两天，但是以一篇的篇幅发表，力求写清楚写明白。用一些简单通俗，但是足够恰当的比喻和一些实例，让大家更容易了解。\n多数GPU程序容易受到内存带宽的限制，所以最大程度的利用全局内存带宽，提高全局加载效率（后面会详细说明），是调控内核函数性能的基本条件。如果不能正确调控全局内存使用，那么优化方案可能收效甚微。 CUDA执行模型告诉我们，CUDA执行的基本单位是线程束，所以，内存访问也是以线程束为基本单位发布和执行的，存储也一致。我们本文研究的就是这一个线程束的内存访问，不同线程的内存请求，其目标位置的不同，可以产生非常多种情况。所以本篇就是研究这些不同情况的，以及如何实现最佳的全局内存访问。 注意：访问可以是加载，也可以是存储。 注意，我们本文使用命令进行编译，省去反复修改CMakelist的麻烦.\n对齐与合并访问 全局内存通过缓存实现加载和存储的过程如下图 全局内存是一个逻辑层面的模型，我们编程的时候有两种模型考虑：一种是逻辑层面的，也就是我们在写程序的时候（包括串行程序和并行程序），写的一维（多维）数组，结构体，定义的变量，这些都是在逻辑层面的；一种是硬件角度，就是一块DRAM上的电信号，以及最底层内存驱动代码所完成数字信号的处理。 L1表示一级缓存，每个SM都有自己L1，但是L2是所有SM公用的，除了L1缓存外，还有只读缓存和常量缓存，这个我们后面会详细介绍。 核函数运行时需要从全局内存（DRAM）中读取数据，只有两种粒度，这个是关键的：\n 128字节 32字节  解释下“粒度”，可以理解为最小单位，也就是核函数运行时每次读内存，哪怕是读一个字节的变量，也要读128字节，或者32字节，而具体是到底是32还是128还是要看访问方式：\n 使用一级缓存 不使用一级缓存  对于CPU来说，一级缓存或者二级缓存是不能被编程的，但是CUDA是支持通过编译指令停用一级缓存的。如果启用一级缓存，那么每次从DRAM上加载数据的粒度是128字节，如果不适用一级缓存，只是用二级缓存，那么粒度是32字节。 还要强调一下CUDA内存模型的内存读写，我们现在讨论的都是单个SM上的情况，多个SM只是下面我们描述的情形的复制：SM执行的基础是线程束，也就是说，当一个SM中正在被执行的某个线程需要访问内存，那么，和它同线程束的其他31个线程也要访问内存，这个基础就表示，即使每个线程只访问一个字节，那么在执行的时候，只要有内存请求，至少是32个字节，所以不使用一级缓存的内存加载，一次粒度是32字节而不是更小。 在优化内存的时候，我们要最关注的是以下两个特性\n 对齐内存访问 合并内存访问  我们把一次内存请求——也就是从内核函数发起请求，到硬件响应返回数据这个过程称为一个内存事务（加载和存储都行）。 当一个内存事务的首个访问地址是缓存粒度（32或128字节）的偶数倍的时候：比如二级缓存32字节的偶数倍64，128字节的偶数倍256的时候，这个时候被称为对齐内存访问，非对齐访问就是除上述的其他情况，非对齐的内存访问会造成带宽浪费。 当一个线程束内的线程访问的内存都在一个内存块里的时候，就会出现合并访问。 对齐合并访问的状态是理想化的，也是最高速的访问方式，当线程束内的所有线程访问的数据在一个内存块，并且数据是从内存块的首地址开始被需要的，那么对齐合并访问出现了。为了最大化全局内存访问的理想状态，尽量将线程束访问内存组织成对齐合并的方式，这样的效率是最高的。下面看一个例子。\n 一个线程束加载数据，使用一级缓存，并且这个事务所请求的所有数据在一个128字节的对齐的地址段上（对齐的地址段是我自己发明的名字，就是首地址是粒度的偶数倍，那么上面这句话的意思是，所有请求的数据在某个首地址是粒度偶数倍的后128个字节里），具体形式如下图，这里请求的数据是连续的，其实可以不连续，但是不要越界就好。 上面蓝色表示全局内存，下面橙色是线程束要的数据，绿色就是我称为对齐的地址段。 如果一个事务加载的数据分布在不一个对齐的地址段上，就会有以下两种情况：  连续的，但是不在一个对齐的段上，比如，请求访问的数据分布在内存地址1~128，那么0~127和128~255这两段数据要传递两次到SM 不连续的，也不在一个对齐的段上，比如，请求访问的数据分布在内存地址0~63和128~191上，明显这也需要两次加载。 上图就是典型的一个线程束，数据分散开了，thread0的请求在128之前，后面还有请求在256之后，所以需要三个内存事务，而利用率，也就是从主存取回来的数据被使用到的比例，只有 $\\frac{128}{128\\times 3}$ 的比例。这个比例低会造成带宽的浪费，最极端的表现，就是如果每个线程的请求都在不同的段，也就是一个128字节的事务只有1个字节是有用的，那么利用率只有 $\\frac{1}{128}$    这里总结一下内存事务的优化关键：用最少的事务次数满足最多的内存请求。事务数量和吞吐量的需求随设备的计算能力变化。\n全局内存读取 注意我们说的都是读取，也就是加载过程，写或者叫做存储是另外一回事！ SM加载数据，根据不同的设备和类型分为三种路径：\n 一级和二级缓存 常量缓存 只读缓存  常规的路径是一级和二级缓存，需要使用常量和只读缓存的需要在代码中显式声明。但是提高性能，主要还是要取决于访问模式。 控制全局加载操作是否通过一级缓存可以通过编译选项来控制，当然比较老的设备可能就没有一级缓存。 编译器禁用一级缓存的选项是：\n-Xptxas -dlcm=cg 编译器启用一级缓存的选项是：\n-Xptxas -dlcm=ca 当一级缓存被禁用的时候，对全局内存的加载请求直接进入二级缓存，如果二级缓存缺失，则由DRAM完成请求。 每次内存事务可由一个两个或者四个部分执行，每个部分有32个字节，也就是32，64或者128字节一次（注意前面我们讲到是否使用一级缓存决定了读取粒度是128还是32字节，这里增加的64并不在此情况，所以需要注意）。 启用一级缓存后，当SM有全局加载请求会首先通过尝试一级缓存，如果一级缓存缺失，则尝试二级缓存，如果二级缓存也没有，那么直接DRAM。 在有些设备上一级缓存不用来缓存全局内存访问，而是只用来存储寄存器溢出的本地数据，比如Kepler 的K10,K20。 内存加载可以分为两类：\n 缓存加载 没有缓存的加载  内存访问有以下特点：\n 是否使用缓存：一级缓存是否介入加载过程 对齐与非对齐的：如果访问的第一个地址是32的倍数（前面说是32或者128的偶数倍，这里似乎产生了矛盾，为什么我现在也很迷惑） 合并与非合并，访问连续数据块则是合并的  缓存加载 下面是使用一级缓存的加载过程，图片表达很清楚，我们只用少量文字进行说明：\n  对齐合并的访问，利用率100%   对齐的，但是不是连续的，每个线程访问的数据都在一个块内，但是位置是交叉的，利用率100%   连续非对齐的，线程束请求一个连续的非对齐的，32个4字节数据，那么会出现，数据横跨两个块，但是没有对齐，当启用一级缓存的时候，就要两个128字节的事务来完成   线程束所有线程请求同一个地址，那么肯定落在一个缓存行范围（缓存行的概念没提到过，就是主存上一个可以被一次读到缓存中的一段数据。），那么如果按照请求的是4字节数据来说，使用一级缓存的利用率是 $\\frac{4}{128}=3.125%$   比较坏的情况，前面提到过最坏的，就是每个线程束内的线程请求的都是不同的缓存行内，这里比较坏的情况就是，所有数据分布在 $N$ 个缓存行上，其中 $1\\leq N\\leq 32$，那么请求32个4字节的数据，就需要 $N$ 个事务来完成，利用率也是 $\\frac{1}{N}$   CPU和GPU的一级缓存有显著的差异，GPU的一级缓存可以通过编译选项等控制，CPU不可以，而且CPU的一级缓存是的替换算法是有使用频率和时间局部性的，GPU则没有。\n没有缓存的加载 没有缓存的加载是指的没有通过一级缓存，二级缓存则是不得不经过的。 当不使用一级缓存的时候，内存事务的粒度变为32字节，更细粒度的好处是提高利用律，这个很好理解，比如你每次喝水只能选择一瓶大瓶500ml的或则一个小瓶的250ml，当你非常渴的时候需要400ml水分，喝大瓶的，比较方便，因为如果喝小瓶的一瓶不够，还需要再喝一瓶，此时大瓶的方便.但如果你需要200ml的水分的时候，小瓶的利用率就高很多。细粒度的访问就是用小瓶喝水，虽然体积小，但是每次的利用率都高了不少，针对上面使用缓存的情况5，可能效果会更好。 继续我们的图解：\n  对齐合并访问128字节，不用说，还是最理想的情况，使用4个段，利用率 $100%$   对齐不连续访问128字节，都在四个段内，且互不相同，这样的利用率也是 $100%$   连续不对齐，一个段32字节，所以，一个连续的128字节的请求，即使不对齐，最多也不会超过五个段，所以利用率是 $\\frac{4}{5}=80%$ ,如果不明白为啥不能超过5个段，请注意前提是连续的，这个时候不可能超过五段   所有线程访问一个4字节的数据，那么此时的利用率是 $\\frac{4}{32}=12.5%$   最欢的情况，所有目标数据分散在内存的各个角落，那么需要 N个内存段， 此时与使用一级缓存的作比较也是有优势的因为 $N\\times 128$ 还是要比 $N\\times 32$ 大不少，这里假设 $N$ 不会因为 $128$ 还是 $32$ 而变的，而实际情况，当使用大粒度的缓存行的时候， $N$ 有可能会减小   非对齐读取示例 下面就非对齐读取进行演示， 代码如下：\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; void sumArrays(float * a,float * b,float * res,int offset,const int size) { for(int i=0,k=offset;k\u0026lt;size;i++,k++) { res[i]=a[k]+b[k]; } } __global__ void sumArraysGPU(float*a,float*b,float*res,int offset,int n) { //int i=threadIdx.x;  int i=blockIdx.x*blockDim.x+threadIdx.x; int k=i+offset; if(k\u0026lt;n) res[i]=a[k]+b[k]; } int main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int nElem=1\u0026lt;\u0026lt;18; int offset=0; if(argc\u0026gt;=2) offset=atoi(argv[1]); printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *a_h=(float*)malloc(nByte); float *b_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte); float *res_from_gpu_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d,*res_d; CHECK(cudaMalloc((float**)\u0026amp;a_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;b_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;res_d,nByte)); CHECK(cudaMemset(res_d,0,nByte)); initialData(a_h,nElem); initialData(b_h,nElem); CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); dim3 block(1024); dim3 grid(nElem/block.x); double iStart,iElaps; iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,offset,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec --offset:%d \\n\u0026#34;,grid.x,block.x,iElaps,offset); sumArrays(a_h,b_h,res_h,offset,nElem); checkResult(res_h,res_from_gpu_h,nElem); cudaFree(a_d); cudaFree(b_d); cudaFree(res_d); free(a_h); free(b_h); free(res_h); free(res_from_gpu_h); return 0; } 编译指令：\ntony@tony-Lenovo:~/Project/CUDA_Freshman/18_sum_array_offset$ nvcc -O3 -arch=sm_35 -Xptxas -dlcm=cg -I ../include/ sum_array_offset.cu -o sum_array_offset 运行结果\n编译指令，启用一级缓存：\ntony@tony-Lenovo:~/Project/CUDA_Freshman/18_sum_array_offset$ nvcc -O3 -arch=sm_35 -Xptxas -dlcm=ca -I ../include/ sum_array_offset.cu -o sum_array_offset 这里我们使用的指标是： $$ 全局加载效率=\\frac{请求的全局内存加载吞吐量}{所需的全局内存加载吞吐量} $$\n只读缓存 只读缓存最初是留给纹理内存加载用的，在3.5以上的设备，只读缓存也支持使用全局内存加载代替一级缓存。也就是说3.5以后的设备，可以通过只读缓存从全局内存中读数据了。 只读缓存粒度32字节，对于分散读取，细粒度优于一级缓存 有两种方法指导内存从只读缓存读取：\n 使用函数 _ldg 在间接引用的指针上使用修饰符  代码：\n__global__ void copyKernel(float * in,float* out) { int idx=blockDim*blockIdx.x+threadIdx.x; out[idx]=__ldg(\u0026amp;in[idx]); } 注意函数参数，然后就能强制使用只读缓存了。\n全局内存写入 内存的写入和读取（或者叫做加载）是完全不同的，并且写入相对简单很多。一级缓存不能用在 Fermi 和 Kepler GPU上进行存储操作，发送到设备前，只经过二级缓存，存储操作在32个字节的粒度上执行，内存事物也被分为一段两端或者四段，如果两个地址在一个128字节的段内但不在64字节范围内，则会产生一个四段的事务，其他情况以此类推。 我们将内存写入也参考前面的加载分为下面这些情况：\n  对齐的，访问一个连续的128字节范围。存储操作使用一个4段事务完成：   分散在一个192字节的范围内，不连续，使用3个一段事务来搞定   对齐的，在一个64字节的范围内，使用一个两段事务完成。   非对齐写入示例 与读取情况类似，且更简单，因为始终不经过一级缓存，所以略过此实验。\n结构体数组与数组结构体 写过C语言的人对结构体都应该非常了解，结构体说白了就是基础数据类型组合出来的新的数据类型，这个新的数据类型在内存中表现是：结构中的成员在内存里对齐的依次排开，然后我们我们就有了接下来的话题，数组的结构体，和结构体的数组。 数组结构体（AoS）就是一个数组，每个元素都是一个结构体，而结构体数组（SoA）就是结构体中的成员是数组用代码表示： AoS\nstruct A a[N]; SoA\nstruct A{ int a[N]; int b[N] }a; 如果你分不清这两个名字，没关系，我也分不清，记住AoS是数组就行了，CUDA对细粒度数组是非常友好的，但是对粗粒度如结构体组成的数组就不太友好了，具体表现在，内存访问利用率低。比如当一个线程要访问结构体中的某个成员的时候，当三十二个线程同时访问的时候，SoA的访问就是连续的，而AoS则是不连续： 这样看来AoS访问效率只有 $50%$ 对比AoS和SoA的内存布局，我们能得到下面结论。\n 并行编程范式，尤其是SIMD（单指令多数据）对SoA更友好。CUDA中普遍倾向于SoA因为这种内存访问可以有效地合并。  AoS数据布局的简单数学运算 我们看一下AoS的例子\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; struct naiveStruct{ float a; float b; }; void sumArrays(float * a,float * b,float * res,const int size) { for(int i=0;i\u0026lt;size;i++) { res[i]=a[i]+b[i]; } } __global__ void sumArraysGPU(float*a,float*b,struct naiveStruct* res,int n) { //int i=threadIdx.x;  int i=blockIdx.x*blockDim.x+threadIdx.x; if(i\u0026lt;n) res[i].a=a[i]+b[i]; } void checkResult_struct(float* res_h,struct naiveStruct*res_from_gpu_h,int nElem) { for(int i=0;i\u0026lt;nElem;i++) if (res_h[i]!=res_from_gpu_h[i].a) { printf(\u0026#34;check fail!\\n\u0026#34;); exit(0); } printf(\u0026#34;result check success!\\n\u0026#34;); } int main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int nElem=1\u0026lt;\u0026lt;18; int offset=0; if(argc\u0026gt;=2) offset=atoi(argv[1]); printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; int nByte_struct=sizeof(struct naiveStruct)*nElem; float *a_h=(float*)malloc(nByte); float *b_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte_struct); struct naiveStruct *res_from_gpu_h=(struct naiveStruct*)malloc(nByte_struct); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d; struct naiveStruct* res_d; CHECK(cudaMalloc((float**)\u0026amp;a_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;b_d,nByte)); CHECK(cudaMalloc((struct naiveStruct**)\u0026amp;res_d,nByte_struct)); CHECK(cudaMemset(res_d,0,nByte_struct)); initialData(a_h,nElem); initialData(b_h,nElem); CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); dim3 block(1024); dim3 grid(nElem/block.x); double iStart,iElaps; iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte_struct,cudaMemcpyDeviceToHost)); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec\\n\u0026#34;,grid.x,block.x,iElaps); sumArrays(a_h,b_h,res_h,nElem); checkResult_struct(res_h,res_from_gpu_h,nElem); cudaFree(a_d); cudaFree(b_d); cudaFree(res_d); free(a_h); free(b_h); free(res_h); free(res_from_gpu_h); return 0; } nvcc -O3 -arch=sm_35 -Xptxas -dlcm=ca -I ../include/ AoS.cu -o AoS nvcc -O3 -arch=sm_35 -Xptxas -dlcm=cg -I ../include/ AoS.cu -o AoS SoA数据布局的简单数学运算 然后看SoA的例子\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; void sumArrays(float * a,float * b,float * res,int offset,const int size) { for(int i=0,k=offset;k\u0026lt;size;i++,k++) { res[i]=a[k]+b[k]; } } __global__ void sumArraysGPU(float*a,float*b,float*res,int offset,int n) { //int i=threadIdx.x;  int i=blockIdx.x*blockDim.x*4+threadIdx.x; int k=i+offset; if(k+3*blockDim.x\u0026lt;n) { res[i]=a[k]+b[k]; res[i+blockDim.x]=a[k+blockDim.x]+b[k+blockDim.x]; res[i+blockDim.x*2]=a[k+blockDim.x*2]+b[k+blockDim.x*2]; res[i+blockDim.x*3]=a[k+blockDim.x*3]+b[k+blockDim.x*3]; } } int main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int block_x=512; int nElem=1\u0026lt;\u0026lt;18; int offset=0; if(argc==2) offset=atoi(argv[1]); else if(argc==3) { offset=atoi(argv[1]); block_x=atoi(argv[2]); } printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *a_h=(float*)malloc(nByte); float *b_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte); float *res_from_gpu_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d,*res_d; CHECK(cudaMalloc((float**)\u0026amp;a_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;b_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;res_d,nByte)); CHECK(cudaMemset(res_d,0,nByte)); initialData(a_h,nElem); initialData(b_h,nElem); CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); dim3 block(block_x); dim3 grid(nElem/block.x); double iStart,iElaps; iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,offset,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; printf(\u0026#34;warmup Time elapsed %f sec\\n\u0026#34;,iElaps); iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,offset,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec --offset:%d \\n\u0026#34;,grid.x,block.x,iElaps,offset); sumArrays(a_h,b_h,res_h,offset,nElem); checkResult(res_h,res_from_gpu_h,nElem-4*block_x); cudaFree(a_d); cudaFree(b_d); cudaFree(res_d); free(a_h); free(b_h); free(res_h); free(res_from_gpu_h); return 0; } nvcc -O3 -arch=sm_35 -Xptxas -dlcm=ca -I ../include/ SoA.cu -o SoA nvcc -O3 -arch=sm_35 -Xptxas -dlcm=cg -I ../include/ SoA.cu -o SoA 性能调整 优化设备内存带宽利用率有两个目标：\n 对齐合并内存访问，以减少带宽的浪费 足够的并发内存操作，以隐藏内存延迟  第三章我们讲过优化指令吞吐量的核函数，实现并发内存访问量最大化是通过以下方式得到的：\n 增加每个线程中执行独立内存操作的数量 对核函数启动的执行配置进行试验，已充分体现每个SM的并行性  接下来我们就按照这个思路对程序进行优化试验：展开技术和增大并行性。\n展开技术 把前面讲到的展开技术用到向量加法上，我们来看看其对内存效率的影响： 代码\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; void sumArrays(float * a,float * b,float * res,int offset,const int size) { for(int i=0,k=offset;k\u0026lt;size;i++,k++) { res[i]=a[k]+b[k]; } } __global__ void sumArraysGPU(float*a,float*b,float*res,int offset,int n) { //int i=threadIdx.x;  int i=blockIdx.x*blockDim.x*4+threadIdx.x; int k=i+offset; if(k+3*blockDim.x\u0026lt;n) { res[i]=a[k]+b[k]; res[i+blockDim.x]=a[k+blockDim.x]+b[k+blockDim.x]; res[i+blockDim.x*2]=a[k+blockDim.x*2]+b[k+blockDim.x*2]; res[i+blockDim.x*3]=a[k+blockDim.x*3]+b[k+blockDim.x*3]; } } int main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int block_x=512; int nElem=1\u0026lt;\u0026lt;18; int offset=0; if(argc==2) offset=atoi(argv[1]); else if(argc==3) { offset=atoi(argv[1]); block_x=atoi(argv[2]); } printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *a_h=(float*)malloc(nByte); float *b_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte); float *res_from_gpu_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d,*res_d; CHECK(cudaMalloc((float**)\u0026amp;a_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;b_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;res_d,nByte)); CHECK(cudaMemset(res_d,0,nByte)); initialData(a_h,nElem); initialData(b_h,nElem); CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); dim3 block(block_x); dim3 grid(nElem/block.x); double iStart,iElaps; iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,offset,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; printf(\u0026#34;warmup Time elapsed %f sec\\n\u0026#34;,iElaps); iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,offset,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec --offset:%d \\n\u0026#34;,grid.x,block.x,iElaps,offset); sumArrays(a_h,b_h,res_h,offset,nElem); checkResult(res_h,res_from_gpu_h,nElem-4*block_x); cudaFree(a_d); cudaFree(b_d); cudaFree(res_d); free(a_h); free(b_h); free(res_h); free(res_from_gpu_h); return 0; } 编译指令。\nnvcc -O3 sum_array_offset_unrolling.cu -o sum_array_offset_unrolling -arch=sm_35 -Xptxas -dlcm=cg -I ../include/ nvprof 内存效率\n增大并行性 通过调整块的大小来实现并行性调整，也是前面讲过的套路，我们关注的还是内存利用效率 代码同上面的展开技术。 offset=11的时候\n由于数据量少，所以时间差距不大，512有最佳速度，不仅因为内存，还有并行性等多方面因素，这个前面我们也曾提到过。要看综合能力。\n本文全部代码都在Github上有完整版，请访问：https://github.com/Tony-Tan/CUDA_Freshman\n总结 这是我今年写作时间最长的一篇博客，写了三天，主要是代码比较多，结果也比较多 这里我们没用Cmake，而是用的指令，原因是方便修改编译选项，试验时间结果不明显的原因是数据量小，部分结果和书上不一致，主要是书的时间比较久了，GPU换代太快。 全局内存本篇算是比较完整了，后面还有其他内存知识，我们继续。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-4-3-%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍内存的访问过程，也就是从应用发起请求到硬件实现的完整操作过程，这里是优化内存瓶颈的关键之处，也是CUDA程序优化的基础。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 内存访问模式，对齐，合并，缓存，结构体数组，数组结构体\u003c/p\u003e","title":"【CUDA 基础】4.3 内存访问模式"},{"content":"Abstract: 本文介绍狄莫弗上一篇之后得出的关于二项分布的近似计算 Keywords: 二项分布，斯特林公式\n狄莫弗的初步结果 狄莫弗接着前面写的，狄莫弗继续他的研究，二项分布，三个参数，使用函数的写法 $b(N,p,i)$ ,三个变量明显研究起来困难，所以控制变量，只研究一个参数 $b(2m,\\frac{1}{2},m)$ 然后再研究中心项与任意一项的比，也就是 $\\frac{b(m)}{b(m+d)}$ 这里的 $b$ 进行了简化，把三个参数变成了一个参数，也就是 $b(2m,\\frac{1}{2},m)$ 狄莫弗在 1721年得到下面的结果，当 $N\\to \\infty$ 时有： $$ b(m)\\sim 2.168\\frac{(1-\\frac{1}{N})^N}{\\sqrt{N-1}}\\tag{5} $$ $$ \\text{log}(\\frac{b(m)}{b(m+d)})\\sim (m+d-\\frac{1}{2})\\text{log}(m+d-1)\\ +(m-d+\\frac{1}{2})\\text{log}(m-d+1)-2m\\text{log}m+\\text{log}(\\frac{m+d}{m})\\tag{6} $$ 狄莫弗的证明在《统计学简史》的书中p44，有狄莫弗的证明，虽然这个结论至今没什么用了，但是证明过程相当有用，因为我们这个系列的目的是让大家了解数理统计的发展，但是有些喜欢细节的同学们可以去查下原文。 （5）的结论现在我们已经在书上见不到了，因为我们有更好的近似结论了，而且(5)对狄莫弗后续研究也没啥作用， 另外（6）中的 $d$ 可以随 $N$ 变化，有一定限制，在斯特林公式下，可以明确的看到在极限情况下 $\\frac{d}{N}\\to 0$ 。虽然狄莫弗没有给出这个条件，但是他在证明的时候也没有违反这个条件。\n总结 本文虽短，但是狄莫弗的研究进入到了关键阶段，后面与斯特林沟通后，更好的结果出现了。\n","permalink":"https://go.face2ai.com/math/math-statistics-2-2-de-moivre-naive-result.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍狄莫弗上一篇之后得出的关于二项分布的近似计算\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 二项分布，斯特林公式\u003c/p\u003e","title":"【数理统计学简史】2.2 狄莫弗的初步结果"},{"content":"Abstract: 本文介绍狄莫弗研究二项概率的动机，和伯努利大数定律产生联系的过程 Keywords: 狄莫弗，狄莫弗公式，二项概率，大数定律\n狄莫弗的研究动因 亚伯拉罕·狄莫弗出生在法国一个新教徒家中，19岁时因为宗教信仰问题被抓入狱，在狱中度过了两年，为了避免迫害，21岁流亡伦敦，担任一名教师，在哪里他在教课的空余继续研习数学，主要是阅读刚出版不就的牛顿著作《自然哲学的数学原理》，他在数学领域取得多方面成就，并使他于 1697年 当选为英国皇家学会会员，这年他刚三十岁，他的一项著名结果，用他名字命名的公式： $$ (\\text{cos}\\theta+i\\text{sin}\\theta)^n=\\text{cos}n\\theta+i\\text{sin}\\theta $$ 不过当时他没把公式写成这个样子。 1718年狄莫弗出版了《机遇论》（Doctrine of Chances）此书奠定了他在概率史上的地位，此书共三个版本1718年，1738年，1756年。人们说较早期的概率史上有三部里程碑性质的著作：\n 《推测术》——伯努利 《机遇论》——狄莫弗 《概率的分析理论》——拉普拉斯  有意思的是，狄莫弗研究二项概率并不受到伯努利的影响，反而，1718年版本的《机遇论》表明，他对伯努利的工作颇有一些看法，说白了就是对伯努利的研究没啥感觉，狄莫弗注意到这个问题纯属偶然：\n 1712年,一名叫亚历山大·喀明的人向狄莫弗提出一个问题：二人在甲家的地盘赌博，A每局获胜的概率是 $p$ ,B获胜的概率是 $q=1-p$ ，赌 $N$ 局，用 $X$ 表示A获胜的局数，约定：如果 $X\\geq Np$ ,那么A付给甲 $X-Np$ 元，如果 $X\u0026lt;Np$ 那么 B付给甲 $(N-X)-Nq=Np-X$ 元，问甲所得的期望是多少。\n 提出这个问题的明显是个想开赌场的。不管结果，赢的多的一方要给甲服务费，而甲还很关系自己能收到多少服务费，所以希望狄莫弗帮忙算下期望。 根据问题，数学化后就是求下面的期望： $$ D_n=E(|X-Np|)=\\sum_{i=1}^{N}|i-Np|b(N,p,i) $$ 这里的 $b(N,p,i)$ 就是二项概率 $C^{N}{i}p^i(1-p)^{N-i}$ 的函数形式。 狄莫弗在 $Np$ 为整数的条件下得到了下面的结论： $$ D_N=2Npqb(N,q,Np)\\tag{2} $$ 并且他只给 $p=\\frac{1}{2}$ 的特例给出了证明，不过证明方法很容易推广到其他情况下，狄莫弗生成此公式在1721年提出，但是发表在1730年，现在我们可以在一般情况下证明： $$ D_N=2\\mu qb(N,p,Np)\\ \\mu=[Np]+1\\tag{3} $$ 其中 $[a]$ 这个表示为向下取整，也就是小于等于 $a$ 的最小整数。 容易验证， $Np$ 为整数时，公式(2)(3)是一致的。 这就完全回答了喀明的问题，但是 $N$ 较大的时候， $b(N,p,i)$ 不好算啊。所以狄莫想找到一种能便于计算的近似公式，他对这个问题进行讨论之前，对上面的公式做了点讨论（ 其实是从 $D_n=E(|X-Np|)=\\sum{i=1}^{N}|i-Np|b(N,p,i)$ 得到的），记： $$ K_N=E(|\\frac{X}{N}-p|)=\\frac{D_N}{N} $$ 则由（2）公式，可以得到： $$ K_N=2pqb(N,p,Np) $$ 容易证明（证明过程可以参考陈希孺《数理统计学简史》p47的注4）： $$ lim_{N\\to \\infty}b(N,p,Np)=0 $$ 这个证明可以用初等办法进行，而不必使用斯特林公式（斯特林公式在我们的基础概率论中有提到，但是当时是选学内容），由此可以得到 $lim_{N\\to \\infty}K_N=0$ 再因为 $P(|\\frac{X}{N}-p|\\geq \\varepsilon)\\leq \\varepsilon^{-1}K_N$ 可以得到 $lim_{N\\to \\infty}P(|\\frac{X}{N}-p|\\geq \\varepsilon)=0$ 。 没错，这就是伯努利的大数定理，当然这个证明方法与切比雪夫不等式证明方法类似，但是当时还没有方差这个东西。 狄莫弗继续证明了，当 $N\\to \\infty$ 的时候， $b(N,p,Np)$ 是以 $\\frac{1}{\\sqrt{N}}$ 的速度趋于 0 的，因此 $K_N$ 也是以同一速度趋于0，这可以解释为： 频率 $\\frac{X}{N}$ 估计概率 $p$ 的精度，大致上和试验次数 $N$ 的平方根成比例，而不是当时看起来和 $N$ 成比例。后面我们还要继续研究这个问题。\n","permalink":"https://go.face2ai.com/math/math-statistics-2-1-de-moivre-motive.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍狄莫弗研究二项概率的动机，和伯努利大数定律产生联系的过程\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 狄莫弗，狄莫弗公式，二项概率，大数定律\u003c/p\u003e","title":"【数理统计学简史】2.1 狄莫弗研究的动因"},{"content":"Abstract: 本文主要介绍CUDA内存管理，以及CUDA内存模型下的各种内存的特点。 Keywords: CUDA内存管理，CUDA内存分配和释放，CUDA内存传输，固定内存，零拷贝内存，统一虚拟寻址，统一内存寻址\n内存管理 迷茫和困惑会影响我们的前进，彻底摆脱也许不太可能，但是我们必须肯定信仰的力量，专注你所热爱的，就会走出迷雾。\nCUDA编程的目的是给我们的程序加速，尤其是机器学习，人工智能类的计算，CPU不能高效完成，说白了，我们在控制硬件，控制硬件的语言属于底层语言，比如C语言，最头疼的就是管理内存，python，php这些语言有自己的内存管理机制，c语言的内存管理机制——程序员管理。这样的好处是学起来特别困难，但是学会了又会觉得特别爽，因为自由，你可以随意的控制计算机的计算过程。CUDA是C语言的扩展，内存方面基本集成了C语言的方式，由程序员控制CUDA内存，当然，这些内存的物理设备是在GPU上的，而且与CPU内存分配不同，CPU内存分配完就完事了，GPU还涉及到数据传输，主机和设备之间的传输。 接下来我们要了解的是：\n 分配释放设备内存 在主机和设备间传输内存  为达到最优性能，CUDA提供了在主机端准备设备内存的函数，并且显式地向设备传递数据，显式的从设备取回数据。\n内存分配和释放 内存的分配和释放我们在前面已经用过很多次了，前面所有的要计算的例子都包含这一步：\ncudaError_t cudaMalloc(void ** devPtr,size_t count) 这个函数用过很多次了，唯一要注意的是第一个参数，是指针的指针，一般的用法是首先我们生命一个指针变量，然后调用这个函数：\nfloat * devMem=NULL; cudaError_t cudaMalloc((float**) devMem, count) 这里是这样的，devMem是一个指针，定义时初始化指向NULL，这样做是安全的，避免出现野指针，cudaMalloc函数要修改devMem的值，所以必须把他的指针传递给函数，如果把devMem当做参数传递，经过函数后，指针的内容还是NULL。 不知道这个解释有没有听明白，通俗的讲，如果一个参数想要在函数中被修改，那么一定要传递他的地址给函数，如果只传递本身，函数是值传递的，不会改变参数的值。 内存分配支持所有的数据类型，什么int，float。。。这些都无所谓，因为他是按照字节分配的，只要是正数字节的变量都能分配，当然我们根本没有半个字节的东西。 函数执行失败返回：cudaErrorMemoryAllocation. 当分配完地址后，可以使用下面函数进行初始化：\ncudaError_t cudaMemset(void * devPtr,int value,size_t count) 用法和Memset类似，但是注意，这些被我们操作的内存对应的物理内存都在GPU上。 当分配的内存不被使用时，使用下面语句释放程序。\ncudaError_t cudaFree(void * devPtr) 注意这个参数一定是前面cudaMalloc类的函数（还有其他分配函数）分配到空间，如果输入非法指针参数，会返回 cudaErrorInvalidDevicePointer 错误，如果重复释放一个空间，也会报错。 目前为止，套路基本和C语言一致。但是，设备内存的分配和释放非常影响性能，所以，尽量重复利用！\n内存传输 下面介绍点C语言没有的，C语言的内存分配完成后就可以直接读写了，但是对于异构计算，这样是不行的，因为主机线程不能访问设备内存，设备线程也不能访问主机内存，这时候我们要传送数据了：\ncudaError_t cudaMemcpy(void *dst,const void * src,size_t count,enum cudaMemcpyKind kind) 这个函数我们前面也反复用到，注意这里的参数是指针，而不是指针的指针，第一个参数dst是目标地址，第二个参数src是原始地址，然后是拷贝的内存大小，最后是传输类型，传输类型包括以下几种：\n cudaMemcpyHostToHost cudaMemcpyHostToDevice cudaMemcpyDeviceToHost cudaMemcpyDeviceToDevice  四种方式，都写在字面上来，唯一有点问题的就是有个host 到host，不知道为啥存在，估计很多人跟我想法一样，可能后面有什么高级的用法。 这个例子也不用说了，前面随便找个有数据传输的都有这两步：从主机到设备，然后计算，最后从设备到主机。 代码省略，来张图： GPU的内存采用的DDR5制式，2011三星才做出来DDR4的主机内存，但是GPU却一直在使用DDR5，这个具体原因我也不清楚，有兴趣的同学自行去查询，但是我们要说的是GPU的内存理论峰值带宽非常高，对于Fermi C2050 有144GB/s，这个值估计现在的GPU应该都超过了，CPU和GPU之间通信要经过PCIe总线，总线的理论峰值要低很多——8GB/s左右，也就是说所，管理不当，算到半路需要从主机读数据，那效率瞬间全挂在PCIe上了。 CUDA编程需要大家减少主机和设备之间的内存传输。\n固定内存 主机内存采用分页式管理，通俗的说法就是操作系统把物理内存分成一些“页”，然后给一个应用程序一大块内存，但是这一大块内存可能在一些不连续的页上，应用只能看到虚拟的内存地址，而操作系统可能随时更换物理地址的页（从原始地址复制到另一个地址）但是应用是不会差觉得，但是从主机传输到设备上的时候，如果此时发生了页面移动，对于传输操作来说是致命的，所以在数据传输之前，CUDA驱动会锁定页面，或者直接分配固定的主机内存，将主机源数据复制到固定内存上，然后从固定内存传输数据到设备上： 上图左边是正常分配内存，传输过程是：锁页-复制到固定内存-复制到设备 右边时分配时就是固定内存，直接传输到设备上。 下面函数用来分配固定内存：\ncudaError_t cudaMallocHost(void ** devPtr,size_t count) 分配count字节的固定内存，这些内存是页面锁定的，可以直接传输到设备的（翻译的原文写的是：设备可访问的，英文原文是：Since the pinned memory can be accessed directly by the device。应该是翻译问题）这样就是的传输带宽变得高很多。 固定的主机内存释放使用：\ncudaError_t cudaFreeHost(void *ptr) 我们可以测试一下固定内存和分页内存的传输效率，代码如下\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; void sumArrays(float * a,float * b,float * res,const int size) { for(int i=0;i\u0026lt;size;i+=4) { res[i]=a[i]+b[i]; res[i+1]=a[i+1]+b[i+1]; res[i+2]=a[i+2]+b[i+2]; res[i+3]=a[i+3]+b[i+3]; } } __global__ void sumArraysGPU(float*a,float*b,float*res) { int i=blockIdx.x*blockDim.x+threadIdx.x; res[i]=a[i]+b[i]; } int main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int nElem=1\u0026lt;\u0026lt;14; printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *a_h=(float*)malloc(nByte); float *b_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte); float *res_from_gpu_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d,*res_d; // pine memory malloc  CHECK(cudaMallocHost((float**)\u0026amp;a_d,nByte)); CHECK(cudaMallocHost((float**)\u0026amp;b_d,nByte)); CHECK(cudaMallocHost((float**)\u0026amp;res_d,nByte)); initialData(a_h,nElem); initialData(b_h,nElem); CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); dim3 block(1024); dim3 grid(nElem/block.x); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;,grid.x,block.x); CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); sumArrays(a_h,b_h,res_h,nElem); checkResult(res_h,res_from_gpu_h,nElem); cudaFreeHost(a_d); cudaFreeHost(b_d); cudaFreeHost(res_d); free(a_h); free(b_h); free(res_h); free(res_from_gpu_h); return 0; } 注意这个核函数将会被本篇所有程序使用，今天的关键在于主机分配内存部分，所以核函数就选个最简单的。大家看看效率就好。 使用\nnvprof ./pine_memory 如果提示错误：\nError: CUDA profiling error. 可以改用root权限执行，这时候又发现root没有nvprof程序，所以如图一样，用完整路径执行就好，或者添加到你的path里面。 结果如下： 作为对比，我们改写了代码库中第三个里的参数，使用常规内存拷贝方法，得到的时间如下：\n这个结果有点尴尬，固定内存的指标显示的是HtoH，也就是主机到主机的内存拷贝，而常规拷贝显示了HtoD。主机到设备但是看memcpy的速度能看出固定内存耗时确实少一些30:42。 同时也能看到cudaHostAlloc和cudaMalloc的时间接近，当数据增大的时候，这个就有区别了，cudaHostAlloc会慢很多。 结论： 固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。 尽量使用流来使内存传输和计算之间同时进行，第六章详细介绍这部分。\n零拷贝内存 截止到目前，我们所接触到的内存知识的基础都是：主机直接不能访问设备内存，设备不能直接访问主机内存。对于早期设备，这是肯定的，但是后来，一个例外出现了——零拷贝内存。 GPU线程可以直接访问零拷贝内存，这部分内存在主机内存里面，CUDA核函数使用零拷贝内存有以下几种情况：\n 当设备内存不足的时候可以利用主机内存 避免主机和设备之间的显式内存传输 提高PCIe传输率  前面我们讲，注意线程之间的内存竞争，因为他们可以同时访问同一个内存地址，现在设备和主机可以同时访问同一个设备地址了，所以，我们要注意主机和设备的内存竞争——当使用零拷贝内存的时候。 零拷贝内存是固定内存，不可分页。可以通过以下函数创建零拷贝内存：\ncudaError_t cudaHostAlloc(void ** pHost,size_t count,unsigned int flags) 最后一个标志参数，可以选择以下值：\n cudaHostAllocDefalt cudaHostAllocPortable cudaHostAllocWriteCombined cudaHostAllocMapped cudaHostAllocDefalt和cudaMallocHost函数一致，cudaHostAllocPortable函数返回能被所有CUDA上下文使用的固定内存，cudaHostAllocWriteCombined返回写结合内存，在某些设备上这种内存传输效率更高。cudaHostAllocMapped产生零拷贝内存。 注意，零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存，方法是：  cudaError_t cudaHostGetDevicePointer(void ** pDevice,void * pHost,unsigned flags); pDevice就是设备上访问主机零拷贝内存的指针了！ 此处flag必须设置为0，具体内容后面有介绍。 零拷贝内存可以当做比设备主存储器更慢的一个设备。 频繁的读写，零拷贝内存效率极低，这个非常容易理解，因为每次都要经过PCIe，千军万马堵在独木桥上，速度肯定慢，要是再有人来来回回走，那就更要命了。我们下面进行一个小实验，数组加法，改编自前面的代码，然后我们看看效果： 主函数代码，核函数如上节代码：\nint main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int power=10; if(argc\u0026gt;=2) power=atoi(argv[1]); int nElem=1\u0026lt;\u0026lt;power; printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *res_from_gpu_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_host,*b_host,*res_d; double iStart,iElaps; dim3 block(1024); dim3 grid(nElem/block.x); res_from_gpu_h=(float*)malloc(nByte); float *a_dev,*b_dev; CHECK(cudaHostAlloc((float**)\u0026amp;a_host,nByte,cudaHostAllocMapped)); CHECK(cudaHostAlloc((float**)\u0026amp;b_host,nByte,cudaHostAllocMapped)); CHECK(cudaMalloc((float**)\u0026amp;res_d,nByte)); initialData(a_host,nElem); initialData(b_host,nElem); //=============================================================//  iStart = cpuSecond(); CHECK(cudaHostGetDevicePointer((void**)\u0026amp;a_dev,(void*) a_host,0)); CHECK(cudaHostGetDevicePointer((void**)\u0026amp;b_dev,(void*) b_host,0)); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_dev,b_dev,res_d); CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); iElaps = cpuSecond() - iStart; //=============================================================//  printf(\u0026#34;zero copy memory elapsed %lf ms \\n\u0026#34;, iElaps); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;,grid.x,block.x); //-----------------------normal memory---------------------------  float *a_h_n=(float*)malloc(nByte); float *b_h_n=(float*)malloc(nByte); float *res_h_n=(float*)malloc(nByte); float *res_from_gpu_h_n=(float*)malloc(nByte); memset(res_h_n,0,nByte); memset(res_from_gpu_h_n,0,nByte); float *a_d_n,*b_d_n,*res_d_n; CHECK(cudaMalloc((float**)\u0026amp;a_d_n,nByte)); CHECK(cudaMalloc((float**)\u0026amp;b_d_n,nByte)); CHECK(cudaMalloc((float**)\u0026amp;res_d_n,nByte)); initialData(a_h_n,nElem); initialData(b_h_n,nElem); //=============================================================//  iStart = cpuSecond(); CHECK(cudaMemcpy(a_d_n,a_h_n,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d_n,b_h_n,nByte,cudaMemcpyHostToDevice)); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d_n,b_d_n,res_d_n); CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); iElaps = cpuSecond() - iStart; //=============================================================//  printf(\u0026#34;device memory elapsed %lf ms \\n\u0026#34;, iElaps); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;,grid.x,block.x); //--------------------------------------------------------------------  sumArrays(a_host,b_host,res_h,nElem); checkResult(res_h,res_from_gpu_h,nElem); cudaFreeHost(a_host); cudaFreeHost(b_host); cudaFree(res_d); free(res_h); free(res_from_gpu_h); cudaFree(a_d_n); cudaFree(b_d_n); cudaFree(res_d_n); free(a_h_n); free(b_h_n); free(res_h_n); free(res_from_gpu_h_n); return 0; } 结果： 我们把结果写在一个表里面：\n   数据规模n( $2^n$ ) 常规内存（us） 零拷贝内存（us）     10 2.5 3.0   12 3.0 4.1   14 7.8 8.6   16 23.1 25.8   18 86.5 98.2   20 290.9 310.5    这是通过观察运行时间得到的，当然也可以通过我们上面的nvprof得到内核执行时间：\n   数据规模n( $2^n$ ) 常规内存（us） 零拷贝内存（us）     10 1.088 4.257   12 1.056 8.00   14 1，920 24.578   16 4.544 86.63    直接上数据，图太多，没办法贴了，但是这种比较方法有点问题，因为零拷贝内存在执行内核的时候相当于还执行了内存传输工作，所以我觉得应该把内存传输也加上，那样看速度就基本差不多了，但是如果常规内存完成传输后可以重复利用，那又是另一回事了。\n但是零拷贝内存也有例外的时候，比如当CPU和GPU继承在一起的时候，你别不信，我手里就有一个，Nvidia的平板，ARM+GPU的架构，这时候，他们的物理内存公用的，这时候零拷贝内存，效果相当不错。但是如果离散架构，主机和设备之间通过PCIe连接，那么零拷贝内存将会非常耗时。\n统一虚拟寻址 设备架构2.0以后，Nvida又有新创意，他们搞了一套称为同一寻址方式（UVA）的内存机制，这样，设备内存和主机内存被映射到同一虚拟内存地址中。如图 UVA之前，我们要管理所有的设备和主机内存，尤其是他们的指针，零拷贝内存尤其麻烦，很容易乱的，写过c的人都知道，弄个五六个指针在哪其中一部分还指向相同的数据不同的地址的，十几行之后必然会混乱。有了UVA再也不用怕，一个人一个名，走到哪里都能用，通过UVA，cudaHostAlloc函数分配的固定主机内存具有相同的主机和设备地址，可以直接将返回的地址传递给核函数。 前面的零拷贝内存，可以知道以下几个方面：\n 分配映射的固定主机内存 使用CUDA运行时函数获取映射到固定内存的设备指针 将设备指针传递给核函数  有了UVA，可以不用上面的那个获得设备上访问零拷贝内存的函数了：\ncudaError_t cudaHostGetDevicePointer(void ** pDevice,void * pHost,unsigned flags); UVA来了以后，此函数基本失业了。 试验，代码：\nfloat *a_host,*b_host,*res_d; CHECK(cudaHostAlloc((float**)\u0026amp;a_host,nByte,cudaHostAllocMapped)); CHECK(cudaHostAlloc((float**)\u0026amp;b_host,nByte,cudaHostAllocMapped)); CHECK(cudaMalloc((float**)\u0026amp;res_d,nByte)); res_from_gpu_h=(float*)malloc(nByte); initialData(a_host,nElem); initialData(b_host,nElem); dim3 block(1024); dim3 grid(nElem/block.x); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_host,b_host,res_d); } UVA代码主要就是差个获取指针，UVA可以直接使用主机端的地址。\n结果：\n统一内存寻址 Nvidia的同志们还是不停的搞出新花样，CUDA6.0的时候又来了个统一内存寻址，注意不是同一虚拟寻址，提出的目的也是为了简化内存管理（我感觉是越简化越困难，因为套路多了）统一内存中创建一个托管内存池（CPU上有，GPU上也有），内存池中已分配的空间可以通过相同的指针直接被CPU和GPU访问，底层系统在统一的内存空间中自动的进行设备和主机间的传输。数据传输对应用是透明的，大大简化了代码。 就是搞个内存池，这部分内存用一个指针同时表示主机和设备内存地址，依赖于UVA但是是完全不同的技术。 统一内存寻址提供了一个“指针到数据”的编程模型，概念上类似于零拷贝，但是零拷贝内存的分配是在主机上完成的，而且需要互相传输，但是统一寻址不同。 托管内存是指底层系统自动分配的统一内存，未托管内存就是我们自己分配的内存，这时候对于核函数，可以传递给他两种类型的内存，已托管和未托管内存，可以同时传递。 托管内存可以是静态的，也可以是动态的，添加 managed 关键字修饰托管内存变量。静态声明的托管内存作用域是文件，这一点可以注意一下。 托管内存分配方式：\ncudaError_t cudaMallocManaged(void ** devPtr,size_t size,unsigned int flags=0) 这个函数和前面函数结构一致，注意函数名就好了，参数就不解释了，很明显了已经。 CUDA6.0中设备代码不能调用cudaMallocManaged，只能主机调用，所有托管内存必须在主机代码上动态声明，或者全局静态声明 。后面4.5 我们会详细的研究统一内存寻址。\n总结 本文介绍了CUDA内存管理中几种技术，注意区别他们的相同点和不同点。 代码库：https://github.com/tony-tan\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-4-2-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍CUDA内存管理，以及CUDA内存模型下的各种内存的特点。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA内存管理，CUDA内存分配和释放，CUDA内存传输，固定内存，零拷贝内存，统一虚拟寻址，统一内存寻址\u003c/p\u003e","title":"【CUDA 基础】4.2 内存管理"},{"content":"Abstract: 本文作为第二章的开篇，主要介绍狄莫弗在二项概率逼近方面的研究，顺便解决了前面一篇我们说的伯努利大数定律中 $N$ 的确定方式。 Keywords: 狄莫弗，二项概率逼近\n狄莫弗的二项概率逼近 本章开篇 前面提到的设某事件A的概率为 p未知，在同样的条件下独立进行N次试验，或者观察，其中同样的条件表明事件A出现的概率p在每次试验中都保持不变。发现事件A发生 $X$ 次, $\\frac{X}{N}$ 称为事件A在这N次试验中的的频率，用现在数学语言描述，伯努利的大数定律就是说当 $N\\to \\infty$ 的时候，频率 $\\frac{X}{N}$ 收敛于 $p$ ，然后伯努利就开始研究这个 $N$ 最少需要多大才能满足 $P(|\\frac{X}{N}-p|\\leq \\varepsilon)\\geq \\frac{c}{c+1}$ 其中 $\\varepsilon\u0026gt;0$ 为常数并且很小， $c\u0026gt;0$ 为常数并且很大。 伯努利给出的答案上文已经有描述，其侄儿尼古拉斯，也就是负责整理《推测术》的尼古拉斯也给出了自己的结论，和伯努利固定 $\\frac{c}{c+1}$ 找 $N$ 的套路不同，尼古拉斯用了下面的描述方式 $$ P_d=P(|X-N_p|\\leq d)\\tag{1} $$ 这个公式显然是上面伯努利表示法的一个变形，尼古拉斯是固定 $N$ 来找 $P_d$ 。他的得到的结果是： $$ P_d\\geq1-max(a,b)\\ a=(\\frac{[N(1-p)-d+1]Np^2}{(N_p+d)(N_p+1)(1-p)})^{\\frac{d}{2}}\\ b=(\\frac{(N_p-d+1)N(1-p)^2}{[N(1-p)+d][N(1-p)+1]p})^{\\frac{d}{2}} $$ 当要满足 $P_d\\geq \\frac{c}{c+1}$ 则要找到最小的 $N$ 使满足： $$ a\\leq (c+1)^{-1}\\ b\\leq (c+1)^{-1} $$ 上一章最后给出过具体的数字，尼古拉斯的解比伯努利的解有相当的改进，但是通过上面式子观察我们发现，$a,b$ 的计算依赖于 $p$ 这就有点不科学了，因为我们的目的就是为了推测 $p$ 是啥，当做已知数处理有点不太合理，如果用 $\\frac{X}{N}$ 代替 $p$ 更是不合理，因为我们要求的就是 $N$ 所以不太合理，另一种做法是：把 $P_d$ 作为 $p$ 的函数，在 $p=\\frac{1}{2}$ 处达到最小，因此只须对 $p=\\frac{1}{2}$ 进行证明。 尼古拉斯的解有改进，但是还是没到完美，其根本原因是 $P_d$ 是一些二项概率之和，当时的条件下，还没有处理这种问题的方法。 狄莫弗从处理二项概率入手，取得了本质的突破，其成就对后世有极大影响，所以我们用一章来记录其过程\n总结 本章主要讲解狄莫弗的研究结果，其对概率和数理统计的贡献大小不亚于伯努利。\n","permalink":"https://go.face2ai.com/math/math-statistics-2-0-de-moivre-binomial-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文作为第二章的开篇，主要介绍狄莫弗在二项概率逼近方面的研究，顺便解决了前面一篇我们说的伯努利大数定律中 $N$ 的确定方式。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 狄莫弗，二项概率逼近\u003c/p\u003e","title":"【数理统计学简史】2.0 狄莫弗的二项概率逼近"},{"content":"Abstract: 本文介绍《推测术》中最精华的第四部分，提出历史性结论——伯努利大数定律 Keywords: 大数定律，伯努利大数定律，《推测术》，强大数律，弱大数律，切比雪夫不等式\n伯努利大数定律 现在我们就要详细说说《推测术》的第四部分，包含了我们现在称之为伯努利大数定律的部分。回到前面我们从箱子里面拿球的试验设计：箱子中有a个白球，b个红球， $p=\\frac{a}{a+b}$ 有放回地从箱子中拿球 $N$ 次，记录拿到把球的次数为 $X$ 用 $\\frac{X}{N}$ 去估计 $p$ ,这个看似简单且顺理成章的想法是现今数理统计学中最重要的基本方法之一。此处暗含了一个最重要的条件就是每次拿球，每个球被拿到的概率相等。 其实拿球这事不难，每次拿球每个球有相同概率被拿到这件事非常有难度，也就是产生概率相等的随机数的过程是非常难控制的，换句话说，你怎么就能保证你的每次操作对于所有球都是一视同仁的呢？从另一个角度来看，彩票抽奖的那个装置也是非常复杂的，才能保证近似等概率。统计学家在平时需要随机数的时候，他们回去用一个叫做随机数表的东西，是一本很厚的记录随机数（0到9），用所谓“充分随机”的方法产生的，但是必须注意，到目前位为止，人们并无一种可操作的方法实现绝对的等可能，所谓随机数也常被人称为 “伪随机数”。 伯努利想要证明：用 $\\frac{X}{N}$ 估计 $p$ 可以达到事实上的确定性——他称之为道德确定性（上一篇说到的），其数学描述：任意给定两个数， $\\varepsilon\u0026gt;0$ 和 $\\eta\u0026gt;0$ ,总可以取足够大的抽取次数 $N$ 使得 ${|\\frac{X}{N}-p|\u0026gt;\\varepsilon}$ 的概率不超过 $\\eta$ ，这个说法和极限的描述很类似，从字面上将就是 $p$ 和 $\\frac{X}{N}$ 可以任意的接近，方法是通过增大抽取次数 $N$ 显然这段话是我们用现代数学语言描述的，当年没有这么套路的说法，原著上伯努利用 $\\frac{1}{a+b}$ 表示的 $\\varepsilon$ ，也就是取样的结果 $\\frac{X}{N}$ 和理论 $p$ 之间的绝对差距，小于 $\\frac{1}{a+b}$ 这个换成 $\\varepsilon$ 其实很容易，因为对于任意小的 $\\varepsilon$ 我们可以通过调整箱子内球的数量来得到更小的的 $\\frac{1}{a+b}$ 我们把 $a,b$ 扩大同样的倍数，比如 $ra,rb$ 这样整个实验是不变的 $p=\\frac{ra}{ra + rb}$ ，但是 $\\frac{1}{a+b}\\to \\frac{r}{ra+rb}$ 其次原著中要证明的是对于任意 $c\u0026gt;0$ ，只需要抽取次数 $N$ 足够大，可以得到： $$ P{|\\frac{X}{N}-p|\\leq \\varepsilon}\u0026gt;cP(|\\frac{X}{N}-p|\u0026gt;\\varepsilon)\\tag{8} $$ 和我们前面用现代语言描述的也是一致的，因为： $$ \\begin{aligned} cP(|\\frac{X}{N}-p|\u0026gt;\\varepsilon)\u0026amp;\u0026lt;P{|\\frac{X}{N}-p|\\leq \\varepsilon}\\ \u0026amp;\u0026lt;\\frac{1}{c+1} \\end{aligned}\\tag{9} $$\n是不是看不明白 $c+1$ 哪里来的？ $$ P{|\\frac{X}{N}-p|\\leq \\varepsilon}+P{|\\frac{X}{N}-p|\u0026gt; \\varepsilon}=1\\ P{|\\frac{X}{N}-p|\\leq \\varepsilon}\u0026gt;cP(|\\frac{X}{N}-p|\u0026gt;\\varepsilon)\\ $$ 等式带入不等式就能得到结论了。\n这样如果取 $c$ 充分大可使它小于 $\\eta$ 。另外要指出的是：伯努利使用的这个箱子模型使被估计的 $p$ 值只能取有理数，所以这对普遍性是个问题，但是其证明对任意 $p$ 都是有效的，所以这个试验的漏洞也就可以被忽略了。 伯努利当时比较高明的一点是他描述这个问题的时候用了(8)式，如果用他这个描述，我们用现在的的描述方法是——当N充分大，$\\frac{X}{N}$ 和 $p$ 可以任意接近: $$ lim_{N\\to \\infty}\\frac{X}{N}=p\\tag{10} $$ 上面这种现代写法在当时看有些问题的，因为我们不能排除从箱子里拿球的时候每次都拿到白球，这时候 $\\frac{X}{N}=1$ 不能收敛到一个小于1的 $p$ 所以这种提法在伯努利时代可能真的解决不了，当时还没有抽象到这个层次，毕竟当时微积分也才刚刚出现。 上面这个结论是对的，1909年波莱尔证明了其正确性，证明难度比伯努利的描述难很多。波莱尔的结论比伯努利强，所以叫做强大数定律，伯努利的则称为弱大数定律。 接下来是详细的证明过程，这里先不写详细的，只写思路，因为虽然是伯努利给出的证明，但是以我的智力只能看懂一部分，为了不打消大家的积极性，我决定忽略详细过程，只介绍一点基础的，有兴趣的同学可以参考《数理统计学简史》第24页。 伯努利用的是直接估计法：\n 首先设一个 $A_0=P(N_p\u0026lt;X\u0026lt;N_p+N_{\\varepsilon})$ 然后写一个递推关系 $A_k=P(N_p+kN_{\\varepsilon}\u0026lt;X\u0026lt;N_p+(k+1)N_{\\varepsilon}),k=1,2,\\dots$ 这样只需要证明 $N$ 充分大的时候 $A_0\\geq c(A_1+A_2+\\dots)$ 这样就可以得到 $X\u0026gt; N_p$ 的一边，同理可以得到另一边。  这是大概的证明过程，可以得到(8)中的结论。 顺带的指出，可以把伯努利的结论(9)引申一点，如果我们知道箱子中球的总数也就是 $a+b$ 的值，或者知道 $a+b$ 不超过某个值 $M$ ，则可以把(9)式(书上写的是(3)式，应该是笔误)改进成——找到一个 $p$ 的估计 $\\hat{p}(X)$ 而不是 $\\frac{X}{N}$ ，当 $N$ 充分大时有： $$ P(\\hat{p}(X)\\neq p)\u0026lt;(c+1)^{-1} $$ 但是如果 $a+b$ 的值没有范围，这个结论就不成立了，证明也在书上，更加复杂，这里也不写了，想知道的同学可以参考数理统计学简史》第25页。 其实我们可以想想，我们讲了半天，都在说 $N$ 在达到一定大小的时候，比例会接近某个概率，我们和伯努利都有一个问题就是，N到底要多大，是否有下界，在指定的精度 $\\varepsilon$ 下得出这个下界。并且可靠度不能超低于 $1-(c+1)^{-1}$ 他证明了以下的结果，定义： $$ m_1=\\text{ 不小于 } \\frac{log[c(b-1)]}{log(a+1)-log(a)}\\text{ 的最小整数 } \\ m_2=\\text{ 不小于 } \\frac{log[c(a-1)]}{log(b+1)-log(b)}\\text{ 的最小整数 } \\ N_1=\\frac{m_1(a+b)+b(a+b)(m_1-1)}{a+1}\\ N_2=\\frac{m_2(a+b)+b(a+b)(m_2-1)}{b+1} $$ 则取 $max{N_1,N_2}$ 能满足 (9) 式，伯努利给了若干数字例子，比如：$a=30,b=20(p=\\frac{3}{5}),\\varepsilon=\\frac{1}{50},c=1000$ 使用上面的结论， $N$ 至少是 25550 ,我们在基础概率论中介绍过切比雪夫不等式，也是给出N的参考值的，但是在同精度下，伯努利给出的N的大小比切比雪夫不等式给出的N小20多倍，但是这个25550这个数还是太大，当时美国一个中等城市人口也就几千人，所以学者斯蒂格勒认为，伯努利之所以长期没发布结果，是觉得这个数太大，他想找到更小的。 但是现在我们已经不关注这些地方了，大家都公认，由伯努利工作发端的大数定律已经成为整个数理统计学的基础，人们也对伯努利工作的哲学意义给予极高的评价，斯蒂格勒指出，伯努利证明了数学家不仅可以后验的认识世界，还可以用数学取菇凉他们的知识的限度。伯努利在结束《推测术》时就其结果的意义做出如下表述：\n 如果我们能把一切事物永恒的观察下去，则我们终将发现：世间的一切事物都受到因果律的支配，而我们也注定会在种种及其纷繁杂乱的事项中认识到某种必然。\n 怎么样，像哲学吧，其实是数学家说的！哈哈。 然后就是关于 $N$ 到底还能不能小一点。\n 1713年 伯努利的侄儿，尼古拉斯·伯努利在给有人的信件中报告了一个他的结果，比伯努利的结果有所改善 1733年 狄莫弗发展了用正态分布逼近二项分布的方法，这是一个意义深远的改进，我们在第二章中学习，将N继续缩小到越 6600，这已经没什么改进余地了，但还是不小。  显然大自然不想让我们轻易的看清他的面貌，这个例子也告诉我们，在平时的书刊杂志小软文中根据一个小样本得到的某种特征的比例，作为大群体中该特征的估值，其准确度和可靠性，通常还没有没什么统计学知识的公众所认为的（主观概率）准确。所以可以对他们给出的结论，看看就好，别当真。\n总结 本问是第一章最后一篇，大数定律影响了我们整个学科，对后世影响深远。 今天是高斯的诞辰，我辈继续努力吧。\n","permalink":"https://go.face2ai.com/math/math-statistics-1-7-bernouli-law-of-large-numbers.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍《推测术》中最精华的第四部分，提出历史性结论——伯努利大数定律\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 大数定律，伯努利大数定律，《推测术》，强大数律，弱大数律，切比雪夫不等式\u003c/p\u003e","title":"【数理统计学简史】1.7 伯努利大数定律"},{"content":"Abstract: 本文介绍CUDA编程的内存模型个概述，主要讲解CUDA包含的几种内存，以及各种内存的主要特点和用途，这篇作为内存部分地图一样，指导我们后面的写作和学习。 Keywords: CUDA内存模型，CUDA内存层次结构，寄存器，共享内存，本地内存，常量内存，纹理内存，全局内存\n内存模型概述 废话少说，我们直接进入主题，如果说我进入写程序的行业的印象最深刻的一本书，看过我博客的人应该能猜到，我也不止一遍的向大家推荐过《深入理解计算机系统》那本书告诉了我基本所有的计算机基础知识，编程基础知识，真的很基础，里面有CPU结构，内存管理模型，汇编等等，从知识层次来讲，非常偏底层，但是难度确实够让人难受，那本书，我估计我只看了一半，看懂的应该有一半的三分之二，也就是我只看懂了全书的三分之一，推荐有时间一定要看看，内存访问和管理是程序效率的关键点，高性能计算更是如此，上一篇举得例子关于运输原材料的例子，就是我们平时天天遇到的问题，我们希望有大量的高速度的大容量内存可以给我们的工厂（GPU核心）输送数据，但是根据我们目前的技术，大容量高速的内存不仅造价高，而且不容易生产，到目前为止（2018年5月）计算结构还是普遍采用内存模型获得最佳的延迟和带宽。 下面我们要看一条新闻，昨天还是前天看到的，刚才又搜了一下，效果大概是这样的 为了方便大家多年后理解时代背景：三周前，美国商务部制裁中兴公司（中兴公司倒闭了，你可以去查查Google）七年不允许美国公司向中国出售任何芯片类产品，中国人说这是贸易战，但是美国没说什么，直说制裁中兴公司。 新文内容是这样的： 我们把这个截图放在这，看看明年后年能不能上市，如果成功了，我算是松了口气，毕竟当年立的亩产三万八千斤的Flag到现在还没实现。 CUDA也采用的内存模型，结合了主机和设备内存系统，展现了完整的内存层次模型，其中大部分内存我们可以通过编程控制，来使我们的程序性能得到优化。 如果你之前写的程序都没怎么管理过内存，那请先练习下C语言，可能会有更好的理解。\n内存层次结构的优点 程序具有局部性特点，包括：\n 时间局部性 空间局部性  解释一下，时间局部性，就是一个内存位置的数据某时刻被引用，那么在此时刻附近也很有可能被引用，随时间流逝，该数据被引用的可能性逐渐降低。 空间局部性，如果某一内存位置的数据被使用，那么附近的数据也有可能被使用。 现代计算机的内存结构主要如下： 这个内存模型在程序局部性原则成立的时候有效。学习过串行编程的人也应该知道内存模型，速度最快的是寄存器，他能和cpu同步的配合，接着是缓存，在CPU片上，然后是主存储器，现在常见的就是内存条，显卡上也有内存芯片，然后是硬盘，这些内存设备的速度和容量相反，越快的越小，越慢的越大。 局部性是个非常有趣的事情，首先局部性的产生并不是因为设备的原因，而是程序从一开始被编写就有这个特征，与生俱来，所以当我们发现此特征后，就开始设计满足此特征硬件结构，也就是内存模型，当内存模型设计成如上结构的时候，如果你想写快速高效的程序，就要让自己的程序局部性足够好，所以这就进入了一个死循环，最后为了追求高效率，设备将越来越优化局部性，而程序也会越来越局部化。 总结下最后一层（硬盘磁带之类的）的特点：\n 每个比特位的价格要更低 容量要更高 延迟较高 处理器访问频率低  CPU和GPU的主存都是采用DRAM——动态随机存取存储器，而低延迟的内存，比如一级缓存，则采用SRAM——静态随机存取存储器。虽然底层的存储器延迟高，容量大，但是其中有数据被频繁使用的时候，就会向更高一级的层次传输，比如我们运行程序处理数据的时候，程序第一步就是把硬盘里的数据传输到主存里面。 GPU和CPU的内存设计有相似的准则和模型。但他们的区别是：CUDA编程模型将内存层次结构更好的呈献给开发者，让我们显示的控制其行为。\nCUDA内存模型 对于程序员来说，分类内存的方法有很多中，但是对于我们来说最一般的分法是：\n 可编程内存 不可编程内存  对于可编程内存，如字面意思，你可以用你的代码来控制这组内存的行为；相反的，不可编程内存是不对用户开放的，也就是说其行为在出厂后就已经固化了，对于不可编程内存，我们能做的就是了解其原理，尽可能的利用规则来加速程序，但对于通过调整代码提升速度来说，效果很一般。 CPU内存结构中，一级二级缓存都是不可编程（完全不可控制）的存储设备。 另一方面，CUDA内存模型相对于CPU来说那是相当丰富了，GPU上的内存设备有：\n 寄存器 共享内存 本地内存 常量内存 纹理内存 全局内存  上述各种都有自己的作用域，生命周期和缓存行为。CUDA中每个线程都有自己的私有的本地内存；线程块有自己的共享内存，对线程块内所有线程可见；所有线程都能访问读取常量内存和纹理内存，但是不能写，因为他们是只读的；全局内存，常量内存和纹理内存空间有不同的用途。对于一个应用来说，全局内存，常量内存和纹理内存有相同的生命周期。下图总结了上面这段话，后面的大篇幅文章就是挨个介绍这些内存的性质和使用的。\n寄存器 寄存器无论是在CPU还是在GPU都是速度最快的内存空间，但是和CPU不同的是GPU的寄存器储量要多一些，而且当我们在核函数内不加修饰的声明一个变量，此变量就存储在寄存器中，但是CPU运行的程序有些不同，只有当前在计算的变量存储在寄存器中，其余在主存中，使用时传输至寄存器。在核函数中定义的有常数长度的数组也是在寄存器中分配地址的。 寄存器对于每个线程是私有的，寄存器通常保存被频繁使用的私有变量，注意这里的变量也一定不能使共有的，不然的话彼此之间不可见，就会导致大家同时改变一个变量而互相不知道，寄存器变量的声明周期和核函数一致，从开始运行到运行结束，执行完毕后，寄存器就不能访问了。 寄存器是SM中的稀缺资源，Fermi架构中每个线程最多63个寄存器。Kepler结构扩展到255个寄存器，一个线程如果使用更少的寄存器，那么就会有更多的常驻线程块，SM上并发的线程块越多，效率越高，性能和使用率也就越高。 那么问题就来了，如果一个线程里面的变量太多，以至于寄存器完全不够呢？这时候寄存器发生溢出，本地内存就会过来帮忙存储多出来的变量，这种情况会对效率产生非常负面的影响，所以，不到万不得已，一定要避免此种情况发生。 为了避免寄存器溢出，可以在核函数的代码中配置额外的信息来辅助编译器优化，比如：\n__global__ void __lauch_bounds__(maxThreadaPerBlock,minBlocksPerMultiprocessor) kernel(...) { /* kernel code */ } 这里面在核函数定义前加了一个 关键字 lauch_bounds，然后他后面对应了两个变量：\n maxThreadaPerBlock：线程块内包含的最大线程数，线程块由核函数来启动 minBlocksPerMultiprocessor：可选参数，每个SM中预期的最小的常驻内存块参数。 注意，对于一定的核函数，优化的启动边界会因为不同的结构而不同 也可以在编译选项中加入  -maxrregcount=32 来控制一个编译单元里所有核函数使用的最大数量。\n本地内存 核函数中符合存储在寄存器中但不能进入被核函数分配的寄存器空间中的变量将存储在本地内存中，编译器可能存放在本地内存中的变量有以下几种：\n 使用未知索引引用的本地数组 可能会占用大量寄存器空间的较大本地数组或者结构体 任何不满足核函数寄存器限定条件的变量  本地内存实质上是和全局内存一样在同一块存储区域当中的，其访问特点——高延迟，低带宽。 对于2.0以上的设备，本地内存存储在每个SM的一级缓存，或者设备的二级缓存上。\n共享内存 在核函数中使用如下修饰符的内存，称为共享内存：\n__share__ 每个SM都有一定数量的由线程块分配的共享内存，共享内存是片上内存，跟主存相比，速度要快很多，也即是延迟低，带宽高。其类似于一级缓存，但是可以被编程。 使用共享内存的时候一定要注意，不要因为过度使用共享内存，而导致SM上活跃的线程束减少，也就是说，一个线程块使用的共享内存过多，导致更过的线程块没办法被SM启动，这样影响活跃的线程束数量。 共享内存在核函数内声明，生命周期和线程块一致，线程块运行开始，此块的共享内存被分配，当此块结束，则共享内存被释放。 因为共享内存是块内线程可见的，所以就有竞争问题的存在，也可以通过共享内存进行通信，当然，为了避免内存竞争，可以使用同步语句：\nvoid __syncthreads(); 此语句相当于在线程块执行时各个线程的一个障碍点，当块内所有线程都执行到本障碍点的时候才能进行下一步的计算，这样可以设计出避免内存竞争的共享内存使用程序、 注意，__syncthreads();频繁使用会影响内核执行效率。 SM中的一级缓存，和共享内存共享一个64k的片上内存（不知道现在的设备有没有提高），他们通过静态划分，划分彼此的容量，运行时可以通过下面语句进行设置：\ncudaError_t cudaFuncSetCacheConfig(const void * func,enum cudaFuncCache); 这个函数可以设置内核的共享内存和一级缓存之间的比例。cudaFuncCache参数可选如下配置：\ncudaFuncCachePreferNone//无参考值，默认设置 cudaFuncCachePreferShared//48k共享内存，16k一级缓存 cudaFuncCachePreferL1// 48k一级缓存，16k共享内存 cudaFuncCachePreferEqual// 32k一级缓存，32k共享内存 Fermi架构支持前三种，后面的设备都支持。\n常量内存 常量内存驻留在设备内存中，每个SM都有专用的常量内存缓存，常量内存使用：\n__constant__ 修饰，常量内存在核函数外，全局范围内声明，对于所有设备，只可以声明64k的常量内存，常量内存静态声明，并对同一编译单元中的所有核函数可见。 叫常量内存，显然是不能被修改的，这里不能被修改指的是被核函数修改，主机端代码是可以初始化常量内存的，不然这个内存谁都不能改就没有什么使用意义了，常量内存，被主机端初始化后不能被核函数修改，初始化函数如下：\ncudaError_t cudaMemcpyToSymbol(const void* symbol,const void *src,size_t count); 同 cudaMemcpy的参数列表相似，从src复制count个字节的内存到symbol里面，也就是设备端的常量内存。多数情况下此函数是同步的，也就是会马上被执行。 当线程束中所有线程都从相同的地址取数据时，常量内存表现较好，比如执行某一个多项式计算，系数都存在常量内存里效率会非常高，但是如果不同的线程取不同地址的数据，常量内存就不那么好了，因为常量内存的读取机制是： 一次读取会广播给所有线程束内的线程。\n纹理内存 纹理内存驻留在设备内存中，在每个SM的只读缓存中缓存，纹理内存是通过指定的缓存访问的全局内存，只读缓存包括硬件滤波的支持，它可以将浮点插入作为读取过程中的一部分来执行，纹理内存是对二维空间局部性的优化。 总的来说纹理内存设计目的应该是为了GPU本职工作显示设计的，但是对于某些特定的程序可能效果更好，比如需要滤波的程序，可以直接通过硬件完成。\n全局内存 GPU上最大的内存空间，延迟最高，使用最常见的内存，global指的是作用域和生命周期，一般在主机端代码里定义，也可以在设备端定义，不过需要加修饰符，只要不销毁，是和应用程序同生命周期的。全局内存对应于设备内存，一个是逻辑表示，一个是硬件表示、 全局内存可以动态声明，或者静态声明，可以用下面的修饰符在设备代码中静态的声明一个变量：\n__device__ 我们前面声明的所有的在GPU上访问的内存都是全局内存，或者说到目前为止我们还没对内存进行任何优化。 因为全局内存的性质，当有多个核函数同时执行的时候，如果使用到了同一全局变量，应注意内存竞争。 全局内存访问是对齐，也就是一次要读取指定大小（32，64，128）整数倍字节的内存，所以当线程束执行内存加载/存储时，需要满足的传输数量通常取决与以下两个因素：\n 跨线程的内存地址分布 内存事务的对齐方式。  一般情况下满足内存请求的事务越多，未使用的字节被传输的可能性越大，数据吞吐量就会降低，换句话说，对齐的读写模式使得不需要的数据也被传输，所以，利用率低到时吞吐量下降。1.1以下的设备对内存访问要求非常严格（为了达到高效，访问受到限制）因为当时还没有缓存，现在的设备都有缓存了，所以宽松了一些。 接下来演技如何优化全局内存访问，最大程度提高全局内存的数据吞吐量。\nGPU缓存 与CPU缓存类似，GPU缓存不可编程，其行为出厂是时已经设定好了。GPU上有4种缓存：\n 一级缓存 二级缓存 只读常量缓存 只读纹理缓存  每个SM都有一个一级缓存，所有SM公用一个二级缓存。一级二级缓存的作用都是被用来存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。Fermi，Kepler以及以后的设备，CUDA允许我们配置读操作的数据是使用一级缓存和二级缓存，还是只使用二级缓存。 与CPU不同的是，CPU读写过程都有可能被缓存，但是GPU写的过程不被缓存，只有加载会被缓存！ 每个SM有一个只读常量缓存，只读纹理缓存，它们用于设备内存中提高来自于各自内存空间内的读取性能。（常量内存，和纹理内存是否和全局内存在一个硬件片上，这个我还真不知道，要到后面我们研究硬件的时候说明，这里挖个坑）\nCUDA变量声明总结 用表格进行总结：\n   修饰符 变量名称 存储器 作用域 生命周期      float var 寄存器 线程 线程    float var[100] 本地 线程 线程   __share__ float var* 共享 块 块   __device__ float var* 全局 全局 应用程序   __constant float var* 常量 全局 应用程序    设备存储器的重要特征：\n   存储器 片上/片外 缓存 存取 范围 生命周期     寄存器 片上 n/a R/W 一个线程 线程   本地 片外 1.0以上有 R/W 一个线程 线程   共享 片上 n/a R/W 块内所有线程 块   全局 片外 1.0以上有 R/W 所有线程+主机 主机配置   常量 片外 Yes R 所有线程+主机 主机配置   纹理 片外 Yes R 所有线程+主机 主机配置    静态全局内存 CPU内存有动态分配和静态分配两种类型，从内存位置来说，动态分配在堆上进行，静态分配在站上进行，在代码上的表现是一个需要new，malloc等类似的函数动态分配空间，并用delete和free来释放。在CUDA中也有类似的动态静态之分，我们前面用的都是要cudaMalloc的，所以对比来说就是动态分配，我们今天来个静态分配的，不过与动态分配相同是，也需要显式的将内存copy到设备端，我们用下面代码来看一下程序的运行结果:\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;__device__ float devData; __global__ void checkGlobalVariable() { printf(\u0026#34;Device: The value of the global variable is %f\\n\u0026#34;,devData); devData+=2.0; } int main() { float value=3.14f; cudaMemcpyToSymbol(devData,\u0026amp;value,sizeof(float)); printf(\u0026#34;Host: copy %f to the global variable\\n\u0026#34;,value); checkGlobalVariable\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(); cudaMemcpyFromSymbol(\u0026amp;value,devData,sizeof(float)); printf(\u0026#34;Host: the value changed by the kernel to %f \\n\u0026#34;,value); cudaDeviceReset(); return EXIT_SUCCESS; } 运行结果\n这个唯一要注意的就是，这一句\ncudaMemcpyToSymbol(devData,\u0026amp;value,sizeof(float)); 函数原型说的是第一个应该是个void*，但是这里写了一个__device__ float devData;变量，这个说到底还是设备上的变量定义和主机变量定义的不同，设备变量在代码中定义的时候其实就是一个指针，这个指针指向何处，主机端是不知道的，指向的内容也不知道，想知道指向的内容，唯一的办法还是通过显式的办法传输过来：\ncudaMemcpyFromSymbol(\u0026amp;value,devData,sizeof(float)); 这里需要注意的只有这点：\n 在主机端，devData只是一个标识符，不是设备全局内存的变量地址 在核函数中，devData就是一个全局内存中的变量。 主机代码不能直接访问设备变量，设备也不能访问主机变量，这就是CUDA编程与CPU多核最大的不同之处  cudaMemcpy(\u0026amp;value,devData,sizeof(float)); 是不可以的！这个函数是无效的！就是你不能用动态copy的方法给静态变量赋值！ 如果你死活都要用cudaMemcpy，只能用下面的方式：\nfloat *dptr=NULL; cudaGetSymbolAddress((void**)\u0026amp;dptr,devData); cudaMemcpy(dptr,\u0026amp;value,sizeof(float),cudaMemcpyHostToDevice); 主机端不可以对设备变量进行取地址操作！这是非法的！ 想要得到devData的地址可以用下面方法：\nfloat *dptr=NULL; cudaGetSymbolAddress((void**)\u0026amp;dptr,devData); 当然也有一个例外，可以直接从主机引用GPU内存——CUDA固定内存。后面我们会研究这部分。 CUDA运行时API能访问主机和设备变量，但这取决于你给正确的函数是否提供了正确的参数，使用运行时API，如果参数填错，尤其是主机和设备上的指针，结果是无法预测的。\n总结 本文给出了CUDA内存模型的Big picture，以提纲方式，引出下面两章的内容。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-4-1-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA编程的内存模型个概述，主要讲解CUDA包含的几种内存，以及各种内存的主要特点和用途，这篇作为内存部分地图一样，指导我们后面的写作和学习。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA内存模型，CUDA内存层次结构，寄存器，共享内存，本地内存，常量内存，纹理内存，全局内存\u003c/p\u003e","title":"【CUDA 基础】4.1 内存模型概述"},{"content":"Abstract: 本文介绍一些关于概率的几点看法，主要来自伯努利的推测术。 Keywords: 主观概率，客观概率，《推测术》，先验，后验，因果律，道德确定性，小概率事件原理，贝叶斯\n关于概率的几点看法 今天我们继续介绍《推测术》中对我们如今仍有影响的观点，伯努利把概率分为“主观概率”和“客观概率”，其余的有些观点也来自前人，但值得注意的有下面几条：\n 客观概率分为两类：  先验概率（在试验之前就知道了，与贝叶斯公式中的先验概率不同，贝叶斯公式我们会在后面给出），可以先验地计算的概率，从现在角度来看就是古典概率，计算建立在对称性（早就知道的事实，不需要证明）基础上的等可能性。 后验概率，这种是统计概率的一种表示，比如，某地区，“出生男孩”这个事件的概率，通过大量观察来后验地计算。   伯努利采取一种机械决定论的观点，也就是说，世界上的一切事物都受到严格的因果律支配。他分析掷骰子这个例子，认为，若把一切所有相关的条件全部研究清楚，并且控制得当，投掷结果就不再是随机的，比如，我们控制了骰子的大小，质量分布，投掷力度，投掷方向，地球引力，甚至月亮的引力，等等。当一切都相同的话，结果不随机了，而是确定的，大科学家拉普拉斯也采取同样的观点，对掷骰子的随机结果是因为：对于某些影响投掷的结果的条件，我们还没有发现，所以不可预测，也就是随机现象的出现。 伯努利引进了所谓的“道德确定性”的概念（moral certainty），如果对于一个事件，我们不能确定其发生，但是它被认为有极大的可能性以至于几乎完全有可能发生，就称它有到的确定性。用简单的话来说，当事件发生的概率接近于1的时候，我们就说其是到的确定性的，但是问题是，如何才能判断是否接近 1，比如，我们可以说0.9接近1，但是我们当以0.99作为接近1与否的标准（ $p(x)\\geq 0.99$ 时算接近1），那么0.90不算接近1。与此同时，当事件的概率接近于0的时候，这个事件被称为“道德否定”的，这个概念对后世的数理统计有非常重大的影响，在统计推断的时候，我们一般无法保证推断结果100%没有出错的时候，于是，我们提出一个很小的数 $a \u0026gt; 0$ 使得出错的概率不大于 $a$ ，这就是说，错误的结果是“道德否定”发生的，推断可信，否则这个推断的错误不是“道德否定”的，推断结果不能接受。现在我们把“道德确定性”叫做“事实上的确定性”（practical certainty），把 “概率很小的事件，在一次试验中极不可能发生”的看法叫做“小概率事件原理”，这个原理每天都在使用，比如我们可以不用担心飞机回失事，或者你的股票能一年涨十倍这种，都是不需要抱太大希望的。 伯努利主张把古典概率“等可能性”的思想扩展到主观概率的场合，他认为，主观来推测一个试验两个结果的可能性的时候，如果我们不知道任何咸盐的情况下，我们应该给他们是等可能的主观概率。比如两个人比赛下棋，我们不知道他们之间的水平高低，所以给出每人 $\\frac{1}{2}$ 的获胜概率。或则在其他实验中，一个数字 $a$ 可能出自 $[b,c]$ 之间的任意一个数，我们不知道有什么分布和什么特点的时候，我们认为 $a$ 是 $[b,c]$ 间的均匀分布。 最后一点后世科学家把它称作 “同等无知原则”，在数理统计史上有极其重要的意义，英国学者贝叶斯在1763年发表的论文就基于这个思想，同时这篇论文建立了贝叶斯学派。不确定贝叶斯是否受到《推测术》。大数学家拉普拉斯也有提出——“不充分理由原则”，思想与此相同。  总结 本文用伯努利的几个基本观点，来介绍数理统计学的重要基础，看似平常的原理值得我们注意。\n","permalink":"https://go.face2ai.com/math/math-statistics-1-6-some-idea-of-probability.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍一些关于概率的几点看法，主要来自伯努利的推测术。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 主观概率，客观概率，《推测术》，先验，后验，因果律，道德确定性，小概率事件原理，贝叶斯\u003c/p\u003e","title":"【数理统计学简史】1.6 关于概率的几点看法"},{"content":"Abstract: 本文为学习CUDA编程的第四章的概要，主要介绍第四章研究的对象 Keywords: 全局内存，CUDA内存模型，CUDA内存管理，全局内存编程，全局内存访问模式，全局内存数据布局，统一内存编程，提高内存吞吐量。\n全局内存 上一章我们整个一章都在研究CUDA的执行模型，必须承认执行模型中，核的配置，决定了程序执行效率，但是程序的执行效率不只由线程束，线程块等执行结构决定，内存也严重的影响了性能。 举个例子，一个老例子，但是这个例子真的非常贴切，在别的书上也看过（如果一模一样，算我抄你）：工厂生产，我们可以通过优化工厂内部流水线，工人分配，工人质量，来提高生产速度，但是如果你把工厂开到珠穆朗玛峰顶，你的提供原料的车（我们目前关心产量不关心出货量，所以不关心如何将成品运出）一年来一辆，那整个工厂的生产效率也是非常低的，因为工人，流水线，都在等待，等待原料进来。这就是典型的一个GPU或者CPU的效率模型。内存带宽，速度，也是影响了吞吐量的重要因素。 本章我们将剖析核函数与全局内存的联系，性能影响。CUDA模型是主要研究内容，通过不同的内存访问模式来使得内核高效运行。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-4-0-%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文为学习CUDA编程的第四章的概要，主要介绍第四章研究的对象\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 全局内存，CUDA内存模型，CUDA内存管理，全局内存编程，全局内存访问模式，全局内存数据布局，统一内存编程，提高内存吞吐量。\u003c/p\u003e","title":"【CUDA 基础】4.0 全局内存"},{"content":"Abstract: 本文介绍《推测术》前三部分，主要是古典概率的系统化和深化 Keywords: 推测术，猜度术，排列，古典概率，全概率公式\n《推测术》前三部分内容提要 本文我们主要介绍伯努利，我们这里要说一下我们主要介绍的伯努利全名叫：雅各布·伯努利，说一下全名是因为他们家族很多人都是数学家，陈老师书中说至少有12人，其中5位在概率论方面有贡献，雅各布的弟弟约翰，和侄儿尼古拉斯都是概率论的贡献者。 雅各布的父亲本来想让他当一个神职人员，但是他爱好的是数学，似乎尼古拉特斯拉也有相同的经历，都是放着神父不去做，而是转向科学，雅各布还对微积分，微分方程，和变分法有贡献，变分法包含著名的悬链线问题。 雅各布时代的著名科学家数学家还有牛顿和莱布尼兹，雅各布和莱布尼兹保持有书信来往。对于微积分的发展，有人认为雅各布可以作为除牛顿莱布尼兹以外的第一任。 雅各布和惠更斯保持通信联系，并研究过其著作《机遇的规律》，并以此启发了自己对概率论的兴趣。 从他的和莱布尼兹通信中得出，他写《推测术》（也译作《猜度术》）是在声明的最后两年，1705年雅各布离世，此书尚未定稿，然而由于家族内问题，其遗孀不放心把书稿交给其弟约翰，后来在莱布尼兹的催促下，才将书稿教给侄儿尼古拉斯，当时还没有期刊会议，学者之间的通讯就是相当于学术交流。\n《推测术》全书239页，分四部分： 第一部分：对《机遇的规律》一书作了仔细的注解，总量相当于袁术的四倍。 第二部分：对排列组合进行了系统的论述。 第三部分：使用前面的知识，讨论了一些用骰子赌博问题 第四部分：是关于概率论在社会到的和经济领域的应用，其中包含本书精华，在概率史上不朽的地位，以其名字命名的大数定律——伯努利大数定律！ “大数定律”在本书中没有得到命名，而是 1837年 泊松的一篇著作中提到了“大数定律”这个名词。\n第四部分是使得本书名垂概率史的重要原因。本书最后一部分长达35页记录了与友人讨论网球比赛中的计分问题。\n《推测术》前三部分，是古典概率的系统化和深化，比前面概率论的进步在于脱离了赌博等具体问题，而是着重与计算的一般规律和数学证明。完全和现代教材一致，并且其中意识到一些关键条件，比如，在重复扔骰子的描述中，书中表示，每次扔骰子获得点数的概率是不变的，这在以前被默认而无人特别说明，而伯努利指出了这一点，所以复合这种条件的模型被称为伯努利模型。他指明了独立概率下的乘法定理的表达方式，并在此基础上严格的证明了二项概率公式。 伯努利还开创了用无穷级数求和去计算概率的方法，那时候无穷级数尚属新的领域，其在本领域也有贡献。 书中第二部分，引进了排列的概念，证明了 $n$ 个相异的物件不同的排列数 $n!$ ，他也证明了 $n$ 个不全相异的物件的排列公式，组合方面，他也救了组合系数的性质，可以重复的组合数，超几何分布，特别是正整数幂次和的表达式 $$ \\sum^{n}{i=1}i^{m}=\\frac{n^{m+1}}{m+1}+\\frac{n^m}{2}+\\sum^{\\frac{m}{2}}{i=1}\\frac{1}{2i}B_{2i}C^{m}{2i-1}n^{m-2i+1} $$ 其中 $B{2i}$ 叫做伯努利常数，最初几个值是： $$ B_2=\\frac{1}{6},B_4=-\\frac{1}{30},B_6=\\frac{1}{42},B_6=-\\frac{1}{30} $$ 得出一般值由下式归纳地定出： $$ \\frac{1}{2}=\\frac{1}{2k+1}+\\sum^{k}{i=1}\\frac{1}{2i}B{2i}C^{2k}_{2i-1},k=1,2,\\dots $$ 在伯努利之前也有人研究组合系数，包括莱布尼兹。《推测术》称为排列组合的教科书，对组合学也是一个重要的事件。 第三部分伯努利用前两部分的知识去研究帕斯卡，费马和惠更斯等人提出的赌博问题，共讨论了24个当时比较流行的赌博问题，今天看不算难，但是用了当时用了排列组合，加法，乘法定理，全概率公式，递推公式。这些都是现在我们依然在用的工具。 最后，伯努利在附录中给出了一些网球中取胜的概率问题，有些非常有难度，我们下面举一个简单的例子： A,B 两个人打网球，每局A胜的概率为 $p$ 对应的 B 获胜的概率 $q$ ，并且满足 $p\u0026gt;0,q\u0026gt;0,p+q=1$ 规定：当一方领先不少于2局，且领先一方至少胜4局时，该方取胜，求A取胜的概率。 证明方法如下： 以 $h(i,j)$ 记在A已经胜 i局，B已胜 j 局的情况下，若B 胜，情况变为 $(i,j+1)$ 故全概率公式，有： $$ h(i,j)=ph(i+1,j)+qh(i,j+1)\\ \\text{set: }r=\\frac{p}{q}=\\frac{p}{1-p} $$ 通过带入数字，得到 $$ h(3,3)=p^2h(5,3)+2pqh(4,4)+q^2h(3,5)\\ =p^2+2pqh(3,3) $$ 于是就能得到 $$ h(3,3)=\\frac{p^2}{p^2+q^2}=\\frac{r^2}{r^2+1} $$ 然后我们再来看 $h(2,3)$ 就能得到 $$ h(2,3)=ph(3,3)+qh(2,4)=\\frac{pr^2}{r^2+1} $$\n然后依次，计算 $h(3,2),h(2,2),h(3,1),h(1,3),\\dots$ 最后能得出结论： $$ \\frac{r^7+5r^6+11r^5+15r^4}{r^7+5r^6+11r^5+15r^4+15r^3+11r^2+5r+1} $$\n唯一需要解释的就是全概率公式那个部分 $h(i,j)=ph(i+1,j)+qh(i,j+1)$ 这个和我们常见的 $p(A)=\\sum_{\\text{All } i}p(A|B_i)p(B_i)$ 不太一样，我们这里的 $h$ 是最终A获胜的概率，而不是现在这场比赛就结束比赛，而是当前比赛不能分出胜负，而需要继续下去，此时，A可能获胜的概率是 $h(i,j)$ ，当比赛继续进行，这就和分赌本的问题有些类似，当比赛继续进行，有两种情况可能发生，一种是当A获得本轮胜利，那么A在赢得本轮后最终获胜的概率是 $h(i+1,j)$ 同理，相对的，如果B获得本轮胜利，那么A最终胜利的概率是 $h(i,j+1)$ ，那么由于结果不变，都是A获胜，那么本轮进行前后A获胜的概率应该是满足全概率关系的，也就是 $h(i,j)=ph(i+1,j)+qh(i,j+1)$ 这个可能不太好理解，因为我们研究概率的时候，都是从前往后推到的居多，这种从后往前的递推关系见得不多，所以需要注意。\n总结 本文介绍了《推测术》的前三章，伯努利作为祖师爷级别的任务，值得尊重，其理论值得深入研究。 一句废话，网站上了广告，大家看到了就帮我点一下，凑点钱买个云服务器，大家的访问速度能快一点🙄\n","permalink":"https://go.face2ai.com/math/math-statistics-1-5-ars-conjectandi-first-3-sections.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍《推测术》前三部分，主要是古典概率的系统化和深化\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 推测术，猜度术，排列，古典概率，全概率公式\u003c/p\u003e","title":"【数理统计学简史】1.5 《推测术》前三部分内容提要"},{"content":"Abstract: 本文介绍惠更斯的著作“机遇与规律” Keywords: 期望，全概率公式，概率概念的形成\n“机遇与规律” 惠更斯有多方面成就，其当时的名声与牛顿相当，比较熟悉的一个研究成果是单摆周期公式 $T=2\\pi\\sqrt{\\frac{l}{g}}$ ，他对概率早期的发展也有重要贡献，其中最主要的是 《机遇与规律》 一书，出版于1657年 ，出版后得到学术界的重视，在欧洲作为概率论的标准教材长达50年。 本书更像一篇论文，从一条公理出发——“公平赌博的值”，推出关于期望的三条定理，这也是期望的首次引进。 本书中解决了当时感兴趣的一些机遇博弈的问题，最后提出五个问题，三个给出了结论但是未加证明。 除了上面说他提出的五个问题，本书中有3条定理，11个问题，常被称为惠更斯的14个命题，前三条如下：\n 某人在赌博中以等概率 $\\frac{1}{2}$ 得到 $a$ 元和 $b$ 元，那么期望是 $\\frac{a+b}{2}$ 元 某人在赌博中以等概率 $\\frac{1}{3}$ 得到 $a,b,c$ 元，那么期望是 $\\frac{a+b+c}{3}$ 元 若某人在赌博中以概率 $p,q(p+q=1)$ 得到 $a,b$ 元 ，则期望是 $pa+qb$ 元  这些命题如今看来完全就是定义，但是当时由于思想问题，大家认为从尽可能少的公理出发，把其他内容推演出来。惠更斯就是通过一条公理，推理出了这些命题，推理过程也很别致，有兴趣可以去查查原文。\n上述3个命题就是期望的一般化，根据惠更斯给出的命题，进一步一般化就很容易得出现代概率论的期望公式 $E[a]=\\sum_{i=0}^{n}a_ip_i$\n但是惠更斯的局限性在于都是研究各种赌博问题，没有将赌博问题抽象化到纯数学问题，而惠更斯用了现代概率论中一些常见的方法，得出计算结果，与帕斯卡的做法基本相似，后来伯努利把这些称为“惠更斯的分析方法”。 机遇博弈在概率概念的产生和运算规则的建立中，起了非常大的作用，机遇处处存在，但是把这种东西精确到数字上还是很有难度的。随概率论这门科学建立后，即脱离了赌博，进入了更多的应用。 这也是一个有趣的现象，从一个没什么意义的活动中，形成了一个非常有价值的副产物。 1713年伯努利划时代著作《推测术》出版，在惠更斯的《机遇与规律》出版后的56年，伯努利不仅脱离了赌博，还提出了著名的——“大数定理” ，一个一直影响至今的结论。许多统计方法和理论都建立在大数定理上。 有概率史学家认为，《推测术》是概率论概念形成过程的终结，宣告数学概率论的开端。 假设一个事件A，根据某种理论，当然这个理论是啥我们可以先不考虑，我们只假设，根据这个理论，我们能得出这个事件的概率为 $P(A)=p$ 这个理论正确与否就是我们我们不知道，继续进行研究的方法有两种：\n 通过大量的试验，观察我们的理论，“ $P(A)=p$ ” 在实验中是否接近实际结果。 在不知道到 $P(A)$ 的时候根本不提出什么理论，而是直接根据实验去估计这个概率。  上面就是数理统计的两个最基本问题——检验与估计。 对于上面这个例子的最简单的实验，就是从一个盒子里面拿球（放回的随机拿），不同颜色的球，每个球等概率被拿出，然后统计实验结果，然后比对和古典概率给出的结论，进行检验，或者，统计结果，直接用结果作为拿出不同球的概率。但是估计会有误差，这个误差的直观感受是随着试验次数的增加而不断变小，这一点，伯努利认为“再笨的大傻瓜都能看出来”，然后我们就莫名的被嘲讽了，但是这个结论一直没有给出严格的证明。 然后伯努利决定自己搞定这些问题，然后就导致了以他的名字命名的大数定理，这个发现对概率论和数理统计的发展有极其重大的意义，他把这部分内容写在了《推测术》的第四部分，作为该著作的精华部分。后面我们主要介绍这本书。\n总结 我们终于从赌博中走出来了，再不出来，google虫子真的会屏蔽我的网页了。\n","permalink":"https://go.face2ai.com/math/math-statistics-1-4-huygens.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍惠更斯的著作“机遇与规律”\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 期望，全概率公式，概率概念的形成\u003c/p\u003e","title":"【数理统计学简史】1.4 惠更斯的“机遇与规律”"},{"content":"Abstract: 本文介绍帕斯卡和费马当时的通信中解决的一些概率问题，这些信件奠定了概率论的基础。 Keywords: 期望，分赌本问题，乘法定理，加法定理，全概率公式\n帕斯卡和费马的通信 费马就是费马大定理在大家学习数学的过程中应该比较容易接触到，在近年被证明，费马主要在数论方面的研究名声较大，其在概率中的恭喜有些偶然——他在 1654年7~10月 和帕斯卡有7封书信来往，其中3封是帕斯卡写给费马的。 这7封信都在讨论赌博问题，他们也是用计算等可能的有利与不利情况数，然后作为“机遇数”——概率的计算方法，当然那时候还没有确定概率这一名称，但是他们的方法的惊喜和复杂度，大大的前进了不少。 那时候他们已经使用了组合工具，递推公式，初等概率的一些基本规律也都用上了，他们还引入了一个叫做赌博值的概念，值等于赌注乘以胜利概率，这也就是概率论最重要的概念之一——数学期望的形成和命名过程，这个值并不是他们独创的，而是已经经过了一段时间的酝酿，。 这些信中讨论了分赌本问题，还有更复杂的输光问题：甲乙两个人各有赌本 $a$ 和 $b$ 元，每局输赢1元，要计算个人输光的概率。这个问题现在看起来都不是很好计算，所以这些信件中讨论的问题达到了非常高的水平。其在概率发展史上起到了重要作用。丹麦概率学者哈儿德认为这些通信奠定了概率论的基础。 这些信件的内容都是在讨论具体的问题，但是没有给出明确的陈述概率运算原则性的东西，比如他们使用了乘法，加法原理，但是没有将其作为一般原则凸现出来。 他们之间的的通信促成主要愿意可能是来自一个叫德梅尔的人，专业赌博家，他向帕斯卡请教了一些问题，但不知道为啥帕斯卡没有回答，而是写信给了费马，有些问题并不难，比如两个骰子扔24次，至少出现一对6点的机遇小于 $\\frac{1}{2}$ 因为 $1-(\\frac{35}{36})^{24}\\approx 0.4914$ 另一方面看骰子只有 36 种等可能结果，而24占了一半以上，这似乎是矛盾的，很明显后面那个期望求法根本不对，这个我们现在学过点概率论的都能看出来。 他们的通信中分赌本用了非常有思想和技巧，我们下面来描述一下： 继承上文所说， $r_1$ 和 $r_2$ 为 A和B获胜要取得胜利的场数，帕斯卡发现，合理的分配应该和 $r_1,r_2$ 有关，因为赌博继续消去，胜负只与 $r_1,r_2$ 有关，记概率为 $e(r_1,r_2)$ ,那么就有下面的边界： $$ e(0,r_2)=1,\\text{ when } r_2\u0026gt;0\\ e(r_1,0)=0,\\text{ when } r_1\u0026gt;0\\ e(a,a)=\\frac{1}{2} \\tag{1} $$ 并且递推公式： $$ e(r_1,r_2)=\\frac{[e(r_1-1,r_2)+e(r_1,r_2-1)]}{2}\\tag{2} $$ 成立。 然后帕斯卡用了比较基础的方法，就是让赌博继续，那么接下来就会是 $e(2,1),e(1,2),e(3,1),e(1,3),e(3,2),e(2,3),\\dots$ 他通过对这些数值进行观察，得到了通用解： $$ e(r_1,r_2)=\\sum^{r_2-1}{i=0}C^{r_1+r_2-1}{i}2^{-(r_1+r_2-1)}\\tag{3} $$\n证明的方法就是(3)要先验证(1) 中的边界，然后用归纳法证明(3) 对于(2) 的正确性。\n费马使用了不同的方法，费马列出了完备事件群，如果赌博结束，我们可以假设 $r_1\u0026lt;r_2$ 可能还需要 $r_1,r_1+1,\\dots,r_1+r_2-1$ 次赌博。 如果A获胜了，而此时 B 已经获胜了 $i(i=1,\\dots,r_2-1)$ 局，那么到A获胜结束，共需要 $r_1+i$ 局，那么如果从总数来看，$r_1+i$ 局中，前 $r_1+i-1$ 中，A获胜了 $r_1-1$ 局，然后第 $r_1+i$ 局 A获胜，比赛结束，那么这件事的概率就是： $$ C^{r_1+i-1}{r_1-1}2^{-(r_1+i-1)}\\cdot 2^{-1}=C^{r_1+i+1}{r_1-1}2^{-(r_1+i)} $$ 这个结果已经用到了二项式定理和概率乘法定理，对 $i=1,\\dots,r_2-1$ 相加，得到\n$$ e(r_1,r_2)=\\sum^{r_2-1}{i=0}C^{r_1-1+i}{r_1-1}2^{-(r_1+i)} $$\n这里运用了加法定理\n总结 这篇文章通过帕斯卡和费马的通信得出了概率论的相当多的基础，虽然当时没有总结成通用的定理，但是这两位高手已经在问题中使用了相关知识。 也许有人认为因为当时时间早，所以这些简单的问题被他们攻克了，所以他们青史留名，而我们现在可能比他们厉害，但是问题变得更难了。表面看起来是这样，但是忽略了时代背景，每一次进步都是从已知到未知的探索，受到时代背景和知识背景的局限，看似简单，却是非常困难，就像我们现在研究的困难的东西，百年后可能是幼儿园小朋友学的知识。\n","permalink":"https://go.face2ai.com/math/math-statistics-1-3-pascal-fermat.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍帕斯卡和费马当时的通信中解决的一些概率问题，这些信件奠定了概率论的基础。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 期望，分赌本问题，乘法定理，加法定理，全概率公式\u003c/p\u003e","title":"【数理统计学简史】1.3 帕斯卡和费马的通信"},{"content":"Abstract: 本文介绍CUDA动态并行——在设备上运行时的网格启动新的子网格 Keywords: 动态并行，嵌套执行\n动态并行 本文作为第三章CUDA执行模型的最后一篇介绍动态并行，书中关于动态并行有一部分嵌套归约的例子，但是我认为，这个例子应该对我们用途不大，首先它并不能降低代码复杂度，其次，其运行效率也没有提高，动态并行，相当于串行编程的中的递归调用，递归调用如果能转换成迭代循环，一般为了效率的时候是要转换成循环的，只有当效率不是那么重要，而更注重代码的简洁性的时候，我们才会使用，所以我们本文只介绍简单的一些基础知识，如果需要使用动态并行相关内容的同学，请查询文档或更专业的博客。 到目前为止，我们所有的内核都是在主机线程中调用的，那么我们肯定会想，是否我们可以在内核中调用内核，这个内核可以是别的内核，也可以是自己，那么我们就需要动态并行了，这个功能在早期的设备上是不支持的。 动态并行的好处之一就是能让复杂的内核变得有层次，坏处就是写出来的程序更复杂，因为并行行为本来就不好控制，去年我在没有系统的学习CUDA的时候写过一个400行左右的内核，用来训练人脸检测程序，确实比cpu块，但是从gpu的温度来判断，并没有很高的利用率（当时还不会使用性能检测工具这些，当时TensorFlow跑的时候GPU温度有80多，但是我写的就只有60多，所以我断定，gpu性能完全没发挥，但是那个程序还是运行了好久，可见磨刀不误砍柴工这句话是多么正确） 动态并行的另一个好处是等到执行的时候再配置创建多少个网格，多少个块，这样就可以动态的利用GPU硬件调度器和加载平衡器了，通过动态调整，来适应负载。并且在内核中启动内核可以减少一部分数据传输消耗。\n嵌套执行 前面我们大费周章的其实也就只学了，网格，块，和启动配置，以及一些线程束的知识，现在我们要做的是从内核中启动内核。 内核中启动内核，和cpu并行中有一个相似的概念，就是父线程和子线程。子线程由父线程启动，但是到了GPU，这类名词相对多了些，比如父网格，父线程块，父线程，对应的子网格，子线程块，子线程。子网格被父线程启动，且必须在对应的父线程，父线程块，父网格结束之前结束。所有的子网格结束后，父线程，父线程块，父网格才会结束。\n上图清晰地表明了父网格和子网格的使用情况，一种典型的执行方式：\n 主机启动一个网格（也就是一个内核）-\u0026gt; 此网格（父网格）在执行的过程中启动新的网格（子网格们）-\u0026gt;所有子网格们都运行结束后-\u0026gt; 父网格才能结束，否则要等待\n 如果调用的线程没有显示同步启动子网格，那么运行时保证，父网格和子网格隐式同步。 图中显式的同步了父网格和子网格，通过设置栅栏的方法。 父网格中的不同线程启动的不同子网格，这些子网格拥有相同的父线程块，他们之间是可以同步的。线程块中所有的线程创建的所有子网格完成之后，线程块执行才会完成。如果块中的所有线程在子网格完成前退出，那么子网格隐式同步会被触发。隐式同步就是虽然没用同步指令，但是父线程块中虽然所有线程都执行完毕，但是依旧要等待对应的所有子网格执行完毕，然后才能退出。 前面我们讲过隐式同步，比如cudaMemcpy就能起到隐式同步的作用，但是主机内启动的网格，如果没有显式同步，也没有隐式同步指令，那么cpu线程很有可能就真的退出了，而你的gpu程序可能还在运行，这样就非常尴尬了。父线程块启动子网格需要显示的同步，也就是说不通的线程束需要都执行到子网格调用那一句，这个线程块内的所有子网格才能依据所在线程束的执行，一次执行。 接着是最头疼的内存，内存竞争对于普通并行就很麻烦了，现在对于动态并行，更麻烦，主要的有下面几点：\n 父网格和子网格共享相同的全局和常量内存。 父网格子网格有不同的局部内存 有了子网格和父网格间的弱一致性作为保证，父网格和子网格可以对全局内存并发存取。 有两个时刻父网格和子网格所见内存一致：子网格启动的时候，子网格结束的时候 共享内存和局部内存分别对于线程块和线程来说是私有的 局部内存对线程私有，对外不可见。  在GPU上嵌套Hello World 为了研究初步动态并行，我们先来写个Hello World进行操作，代码如下：\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;__global__ void nesthelloworld(int iSize,int iDepth) { unsigned int tid=threadIdx.x; printf(\u0026#34;depth : %d blockIdx: %d,threadIdx: %d\\n\u0026#34;,iDepth,blockIdx.x,threadIdx.x); if (iSize==1) return; int nthread=(iSize\u0026gt;\u0026gt;1); if (tid==0 \u0026amp;\u0026amp; nthread\u0026gt;0) { nesthelloworld\u0026lt;\u0026lt;\u0026lt;1,nthread\u0026gt;\u0026gt;\u0026gt;(nthread,++iDepth); printf(\u0026#34;-----------\u0026gt; nested execution depth: %d\\n\u0026#34;,iDepth); } } int main(int argc,char* argv[]) { int size=64; int block_x=2; dim3 block(block_x,1); dim3 grid((size-1)/block.x+1,1); nesthelloworld\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(size,0); cudaGetLastError(); cudaDeviceReset(); return 0; } 这就是完成可执行代码，编译的命令与之前有些不同，工程中使用cmake管理，但是本程序没有纳入其中，而是使用了一个单独的makefile\nnvcc -arch=sm_35 nested_Hello_World.cu -o nested_Hello_World -lcudadevrt --relocatable-device-code true -lcudadevrt \u0026ndash;relocatable-device-code true 是前面没有的，这两个指令是动态并行需要的一个库，relocatable-device-code表示生成可重新定位的代码，第十章将会讲解更多重新定位设备代码的内容。 这个程序的功能如下 第一层： 有多个线程块，执行输出，然后在tid==0的线程，启动子网格，子网格的配置是当前的一半，包括线程数量，和输入参数 iSize。 第二层： 有很多不同的子网格，因为我们上面多个不同的线程块都启动了子网格，我们这里只分析一个子网格，执行输出，然后在tid==0的子线程，启动子网格，子网格的配置是当前的一半，包括线程数量，和输入参数 iSize。 第三层： 继续递归下去，直到iSize==0 结束。 执行结果如下，有点长，但是能看出一些问题。由于输出太多，我截取部分有意思的部分来给大家看一下，想自己运行的可以去github上clone\n可见，当多层调用子网格的时候，同一家的（就是用相同祖宗线程的子网）是隐式同步的，而不同宗的则是各跑各的。\n总结 本文简单介绍了动态并行的基础知识，非常基础，对于动态并行的归约这里就不再讲解了，需要使用的同学自行学习，下一篇开始进入下一章，我们开始研究内存。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-6-%E5%8A%A8%E6%80%81%E5%B9%B6%E8%A1%8C.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA动态并行——在设备上运行时的网格启动新的子网格\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 动态并行，嵌套执行\u003c/p\u003e","title":"【CUDA 基础】3.6 动态并行"},{"content":"Abstract: 本文介绍经典的分赌本问题的历史，和最后得出的解法，为期望的出现做了铺垫 Keywords: 分赌本问题，二项分布，期望，帕斯卡三角\n分赌本问题 问题描述：\n A,B二人赌博，各出赌金 a 元，他们拥有相同的获胜概率（也就是 $\\frac{1}{2}$ ），约定，谁先获得S场胜利，谁获得 2a 元的全部赌注金，但是由于某种原因，赌博无法进行，此时 A获得了 $S_1$ 场胜利，B获得了 $S_2$ 场胜利，（ $S_1,S_2$ 都小于 $S$ ），那么我们应该怎么把赌注分给两人才算是公平？ 本问题最早的记载出现在 1494年 帕西奥利的一本著作，其中 $S=6,S_1=5,S_2=2$ 的情况。\n 分析问题，首先最重要的一句话是问题中的问题，如何分配才算公平，这就是最关键的问题所在，当时每个人都对此问题的公平的理解有所不同，如何分配就变成了对公平的定义，比如下面这几种方案：\n 记录问题的帕西奥利给出的解法是：按照 $S_1:S_2$ 的比例分配 塔泰格利亚在 1556年 怀疑，这个问题没有数学解法，应该交给法官处理，但是他也给出了一个数学解 $S_1\u0026gt;S_2$ 时， A 取走自己的全部赌注 a 元，并拿走 B的赌注的 $\\frac{S_1-S_2}{S}$ ，也就是A拿走 $\\frac{S_1-S_2+S}{S}a$ 元，这时候的比例是 $S+S_1-S_2:S-S_1+S_2$ 1603年 法雷斯泰尼根据某种理由提出按照 $2S-1+S_1-S_2:2S-1-S_1+S_2$ 的比例分配。 1539年 卡丹诺，在其著作中通过比较深的推理提出解法 $r_1=S-S_1,r_2=S-S_2$ 赌注按照 $r_2(r_2+1):r_1(r_1+1)$ 的比例分给A和B。  虽然卡丹诺这个解法不知道根据啥得到的，但是他注意到了起决定性的是 $S_1,S_2$ 和 $S$ 之间的差距，而不是 $S_1,S_2$ 本身。 这个问题的根本原因在于人们当时对期望的认识不统一，这些数学家们都意识到这一点，但是没有人把期望和概率联系到一起 学概率，期望这个词是最不好理解的，如果从这个问题来看，期望的字面意思是继续进行下去的结果，或者是未来有可能发生的情况，这和这个题目很合适。 解决这个问题的思路是：假设赌博继续进行下去，个人最终获得胜利的概率，按照这个概率分赌本是公平的。 按照这个思路，赌博再进行至多 $r=r_1+r_2-1$ 局，就能得到最终结果，如果A获胜，至少要再赢 $r_1$ 场，按照二项分布 $n=r,p=\\frac{1}{2}$，那么A获胜的概率是： $$ p_A=\\sum^{r}{i=r_1}\\begin{pmatrix}r\\i\\end{pmatrix}(\\frac{1}{2})^i(\\frac{1}{2})^{r-i}=\\sum^{r}{i=r_1}\\begin{pmatrix}r\\i\\end{pmatrix}2^{-r}\\ p_B=1-p_A $$ 得到上面结果，我们按照 $p_A:p_B$ 分配赌注，，结果是 A获得 $2ap_A$ B获得 $2ap_B$ 这两个结果就是A、B在当时状态下的期望。 以上解法为 1654年 帕斯卡提出的，他用了两种方法，一种是地推公式，一种是帕斯卡三角，我们叫杨辉三角 1710年 蒙特姆特在一封信中给出了上述解法的通用形式，也就是两人获胜的概率不必相等，后来他又把这个推广到多个赌徒的情况下。 分赌本的最重要的作用是让概率和数学期望产生联系，而且这个过程中使用了：组合法，递推公式，条件概率和全概率公式等。这些工具至今我们仍然在使用，这个问题使得早起概率的简单计数，进入了更深入精细的阶段。\n总结 身边所有事都蕴含了有趣的数学，真正能发现其中乐趣的人改变了数学的面貌。\n","permalink":"https://go.face2ai.com/math/math-statistics-1-2-gambling-problem.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍经典的分赌本问题的历史，和最后得出的解法，为期望的出现做了铺垫\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 分赌本问题，二项分布，期望，帕斯卡三角\u003c/p\u003e","title":"【数理统计学简史】1.2 分赌本问题"},{"content":"Abstract: 本文介绍概率最早形成时的大致情况，年代久远，本文中所说的都是由记载的，更早的相关内容无处考证 Keywords: 《机遇博弈》，组合公式，卡丹诺\n卡丹诺的著作 文章短小不说废话，从赌博出来的学问却改变了后世的生活方式，以及思考看待事物的角度，不得不感叹大自然和人类思想的奇妙。\n卡丹诺的贡献 目前唯一有记载的概率相关的描述最早就是卡丹诺的著作《机遇博弈》 但是卡丹诺在数学上留名的另一个贡献是发现了一般的三次代数方程的解法。 《机遇博弈》成于1564年，于1663年 才出版，但是出版时已经有别人出版了概率相关著作，所以卡丹诺才不怎么为人所知。 这本书记载了卡丹诺对赌博的实践经验，如什么时候宜于赌博，如何判断赌博是否公正，如何防止赌博中比人作弊。他的著作中，对等可能进行了规定，他指出，骰子应该是诚实的（也就是等可能出现每一面的） 当时还没有排列组合的相关计算方法。他在书中给出了计算几个骰子的出现的所有结果的可能性，比如骰子有多少种等可能的结果：如果有三个骰子，结果包括(a)全部相同(b)二同一不同(c)全不同；上文我们说这些情况共56种，那是不区分三个骰子时候的结果，那个结果出现的概率是不同的，下面我们给出的是每种组合出现的可能性相同，按照我们已有知识，应该是 $6^3$ 种，可是当时没有组合的计算公式，所以，卡丹诺给出下面的计算结果 $$ 6\\times 1+30\\times 3 +20\\times 6=216 $$ 在书中，他记录了“n个相异物，至少取两个，不同的取法有 $2^n-n-1$ ”，还记录了他对组合数 $C_{k}^{n}$ 其中 $n\\leq 11$ 的结果的表格，并且他给出了组合的地推公式 $$ C^{n}{k}=C^{n}{k-1}\\frac{n-k+1}{k} $$ 通过这个递推公式，得出组合公式： $$ C^{n}_{k}=\\frac{n(n-1)\\dots(n-k+1)}{k!} $$ 但是书中没用这个结果取解释赌博中情况数的计算问题。 另一个比较遗憾的是作为一个赌博家，卡丹诺没有给出赌博中结果出现的频率记载，可能是的昂视大家对用频率逼近概率的想法还没有什么概念，不过后来伯努利却说，用频率逼近概率傻瓜都知道，卡丹诺莫名中枪。。哈哈 卡丹诺的著作整理和总结了在此之前的赌博中形成的一些概念，也就是古典概率的定义和计算。 后来1529年他的著作中提出了著名的问题——“分赌本问题”的一种解法，此解法对概率论的发展起到了重要作用，下面一篇全篇都会介绍卡丹诺的分赌本问题的解法。\n总结 这篇算是从概率论出现的开端开始介绍整个学科的开始，卡丹诺是目前记载中最早的一位提出相关概念的数学家，除此之外，他还是个医学家和赌博家，值得膜拜！\n","permalink":"https://go.face2ai.com/math/math-statistics-1-1-cardano.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍概率最早形成时的大致情况，年代久远，本文中所说的都是由记载的，更早的相关内容无处考证\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《机遇博弈》，组合公式，卡丹诺\u003c/p\u003e","title":"【数理统计学简史】1.1 卡丹诺的著作"},{"content":"Abstract: 作为第一章的开篇文章，本文从数理统计学的由来开始，大致的说了一下整体的数理统计学的发展，后面逐一展开，进行介绍。 Keywords: 《推测术》，早期概率论，赌博\n早期概率论——从萌芽到《推测术》 本文开始介绍数理统计学简史，信息基本全部来自陈希孺老师——《数理统计学简史》，这本书目前已经没有在售了，所以尽量将原始信息归纳总结，然后以博客的形式发到网上，后来之人可以阅读，也算是对知识的传承了。 本篇作为第一章的开篇，讲的非常大，都是框架性的东西，所以，追求细节是后面的事情，现在我们要做的是根据这些概念间的关系，逐步构建起整个知识结构，通过他们的发展过程，这无疑能帮助我们加深对知识的理解，本系列每篇都会配图，最后会用一个时间轴来总结全部知识点，希望大家多多访问，多多留言！ 本系列计划每日一篇，敬请期待。。\n概率到事件 概率是什么 概率是一件事发生，一种情况出现的可能性大小的数量描述，你可以用文字的可能，非常可能，肯定可能来描述一件事情的发生可能性，但是没办法进行更细致的比较，因为你用的这些描述语，说到底也是离散的，所以，我们想用一种连续的方法来描述，数量是一个很好的方法，把不同的可能性程度对应到不同的数字上去，而这些数字，我们规定从0到1，当然，也可以从0到100，但是前辈们没那么选择，所以我们还是从0到1吧 上面这种数字描述概念最早出现在 16世纪 与掷骰子密切相关，最早的概念已无法追溯。\n“可重复性” vs “一次性” 事件分“可重复性” 和 “一次性”之分。第一种是可以重复进行的，比如扔骰子，原则上在一定条件下，可以多次的重复该事件，但是永远不能做到无限次。一次性的，明显，这个事就一次，比如明天是2018年4月22日上午8点下雨，这事到明天8点整就可以确定发没发生了，而且不可重复，就一个2018年4月22日上午8点（除非时间逆转） 当然，第一种说的重复进行的事件是控制在一定条件下的，这个我其实是有疑问的，因为根本没办法控制，最简单的，我认为温度和湿度会影响扔骰子，首先你没有证据证明我这个怀疑是无效的，所以，因为温度湿度根本不可控制，所以没办法控制条件完全一致，只能说相对一致。 我们把可重复性的事件概率叫做客观的，一次性的事件成为主观的。客观概率有一定的依据，不会随人的意志转义，主观完全随人的意志转移。 哲学家有不同的观点，主观概率随人的意志转移的根本原因是人的知识不够全面，如果人对这件事的知识完全掌握，主观概率只有0，1两种情况，比如目前来看，我问你火星有生命么？你可能说有万分之一的可能性有，明显，这个回答目前来说是没什么问题的。但是如果有一天我们完全掌握了火星，那么这个概率就会变成0或者1，有就是有没有就是没有，只是我们现在没有弄清楚而已。 逻辑学家和神学家也在研究主观概率。\n客观概率 可重复的事件产生客观概率，而客观概率有开始分成了两种形式：\n统计概率 因为事件可重复，所以经过大量试验后，事件出现的频率，可以被估计成该事件的概率，这一点非常直观，但是估计就会有所不同，比如某地12岁以下的儿童有100万，其中5000人得了某种疾病，我们就可以说该地区12岁以下儿童发病概率是0.005，这是一种估计，但显然这个估计准不准，值得商榷，所以用这种方法来定义概率也有点说不通，因为定义要保证一致性，不能一个事物的定义会经常发生变化，这是不对的。但是种方法的客观性还是值得肯定的，毕竟不是某个大神自己评估出来的，比如我国有 $95%$ 的好人，这就是不客观的！ 上面的定义形式好处是得到的结果比较能得到人们的认同，没有主观意见在里面，但是作为定义似乎又不太合适。 这种概率的定义方式被叫做统计概率\n古典概率 试验结果数量有限，而且每种结果出现机会均等，不存在谁比谁高或者低的暗示，那么如果结果有 $N$ 个，那么这种结果出现 $M$ 次的概率就是 $\\frac{M}{N}$ 比如均匀的骰子，这个均匀就暗示每种结果出现等可能，所以，扔出3的倍数的情况只有两种 3，6，那么相当于结果出现了2次，一共6种结果，概率为 $\\frac{2}{6}$ 如果骰子不均匀，以上所有结论轰然倒塌。 这种概率的定义方式被叫做古典概率\nKolmogorov拯救概率 1933年 苏联大数学家柯尔莫哥洛夫提出了公理化的定义方式，这种定义方式不会和上面任何派系产生冲突，而是只定义了概率的几条规则，而根本不说什么是概率. 而客观主观概率的计算都遵守上面的公理，而本质是啥，他们会继续争下去。\n“贝叶斯学派” vs “频率学派” 主观概率和客观概率在数理统计中有非常重要的地位，不要以为主观概率完全是瞎说，主观概率产生了贝叶斯学派，后面会详细介绍，这个学派的很多思想我是非常赞同的，而另一派是频率学派，也就是客观概率的学派，客观学派目前占领了数理统计的大部分阵地。\n赌博到骰子 古典概率试用的典型场合是扔骰子，骰子出现在 公元960年左右，怀特尔德大主教，计算了三个骰子的不计顺序的组合数，56种情况，三颗相同的6种，两颗相同另一颗不同的30种，都不同的20种， 14世纪 骰子赌博成风，纸牌到1350年才出现记载，由于教会反对，和国家禁止，直到18世纪初纸牌才取代骰子，称为主要的赌具。 纸牌的情况更复杂，促进了概率论的发展。\n卡丹诺 公元960年左右文艺复兴前概率还不算数学概念。 16世纪初意大利数学家讨论掷骰子中各种情况出现的可能性。这些研究结晶出了古典概率定义：把要研究的情况分解成一些看似同等可能的简单情况，其数目与全部可能结果数之比，即取为该情况出现的概率。这个说法最初出自谁人已经无法考证，但是，现在为人所知的一位叫做，卡丹诺(G.Cardano 1501~1576)，把古典概率的发明归功于他的名下也许无人反对。\n总结 上面这个图是我大概本篇的大概归纳，如有不妥，还望各位指点。\n","permalink":"https://go.face2ai.com/math/math-statistics-1-0-early-probability-theory.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 作为第一章的开篇文章，本文从数理统计学的由来开始，大致的说了一下整体的数理统计学的发展，后面逐一展开，进行介绍。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《推测术》，早期概率论，赌博\u003c/p\u003e","title":"【数理统计学简史】1.0 早期概率论——从萌芽到《推测术》"},{"content":"Abstract: 本文是数理统计学的第一篇文章，从数理统计学的发展过程来了解我们要研究什么 Keywords: 数学统计学简史\n数理统计学简史 介绍 开篇废话 经济市场和国际形势天天都有新情况，没有谁对谁错，你的利益在哪边，哪边就是正义，但是别当傻子，更多的知识，才能有更读到有观点的见解 知识是两个词的组合知者，信息也，也就是你所获得的先验信息，识为判断能力，类似于predict过程，也就是你对一件未知事物的判断。 希望大家知识越来越多。\n序 “统计思想，包含世界观——他是看待世间万物的一种方法或技术”\n导言 本系列的博客的全部主要信息都来自陈希孺老师的《数理统计学简史》 本系列写历史，是想给大家讲述下学科内涵，毕竟研究机器学习就是在研究数理统计，从数理统计的逻辑里，也能找到不少机器学习的新思路，个人认为，机器学习人工智能的未来突破只能在数学和生理学中寻找，自然界给出的智慧并不是我们调参数改模型能达到的。所以我们更需要学习写能启发我们灵感，解放思想的东西，学习历史能使所有知识得到连贯，也能对整个学科有更深刻的认识。 《不列颠百科全书》的说法，统计学，也就是数理统计，“是搜集和分析数据的科学与艺术”。说是艺术，也不是让你唱歌跳舞，就像高德纳的那一摞计算机程序设计的艺术，我们并没有看到程序里有歌舞画作，而是算法本身的美感，这个仁者见仁智者见智，没有必要统一口径。 所以想要记住公式方法然后遇到问题套上去的人，可能就没法掌握精髓了。 我们的数理统计就是别的国家的统计，因为语言问题，我们的统计学是社会科学的学科，数学学科的只能改名叫数理统计了。西方的数理统计学的定义是概率数学理论基础的那一部分，也就是纯粹数学的部分。\n收集分析数据 收集和分析数据，通常是指有效的方法，我们平时做机器学习的时候数据总是最重要的，数据从哪来，误差是什么，是否全部有效，无效的数据占多少，这些我们其实很少去了解，多半都是上来就清理数据，数字化，然后做特征，做分类。。。其实这是很机械化的，获取数据的方法有时候能使得数据呈现不同的信息，而这些信息有时候要使用不同的模型，分析数据指的的是精度和准确度，当你预测一个人的年龄，精度是指你给出的范围，如果你说一个人的年龄是0到1000岁，那么准确性肯定是百分之百，这种分析结果没意义，因为精度太低了，如果你说这个人在10到12岁之间，这显然精度就提高很多很多，但是你又没办法保证，随意你说他在10到12岁之间的概率是90%，这就是一个有效的分析，精度是 $[10,12]$ 准确度是 $90%$ 显然这个比那个一千岁靠谱多了，并且一千岁可以理解为一个无效的分析。\n预测不确定性 有人认为数理统计是处理随机和非随机数据的学科，陈老师说他不敢苟同，原因是统计学的一些方法利用到非随机数据的时候，比如你记录了你一个月的开销，你用平均数来表示多少，这里有统计学的方法——均值，但是没有统计学的问题，统计学的问题是要预测的，这个必须要深深的刻在脑子里，我们研究数理统计的对象：就是为了预测！\n专业知识 我们收集和分析数据的目的是解决特定问题，必须得到一些结论，作为某些行动的依据和建议，但是必须指出，给出这些行动依据和建议完全只基于数据，数据来自的知识背景无关，比如新药试验，某种药A数据比B更漂亮，但是专业的医生说，不对啊，B中的某某成分更有效啊。。。这时候有两种选择，重新采集数据，做实验，或者再去研究你的专业知识，我们的数据并不能推到出那些成分有效，只能给出统计结果，A更有效，至于你接受与否，那是另一会儿事。 发生上述争端，做数理统计的人需要立刻离场，因为别事与你无关，你不走他就要跟你讨论，你又不懂别人的知识，所以还是离开，让他自己冷静冷静。\n两点意义  数理统计方法是中立的，不偏向任何一方的工具，只根据数据说话，并且给出可信程度，信不信你说了算。任何人可以把数理统计运用到任何学科或者任何工程任何事情中去，当然你也可以对其进行否定，那你大可不用，但是如果任何人决定相信和使用数理统计这个工具，那么，请必须遵守数理统计的规则，包括数据采集分析等全部过程，这样得出的结论才会被数理统计学的定理所支持。 数理统计只是数据表面的关系，不能证明事情的因果。著名的例子就是吸烟和肺癌的关系，数字层面只能说统计出来的结果是有关系，但实际有没有关系需要医学给出严格的理论，这个只能说是数字上的，因为我们无法排除其他所有的影响，因为每个人都是不同的，环境，基因等，这些数据是没有考虑到数理统计过程中的。  不想说什么了 数理统计一直被批判，西欧有人从道德层面批评，说他抹杀了事物的本质 我们的特殊时期，就是那段疯狂的日子，说数理统计学抹杀事物本质，美化资本主义和丑化社会主义。 我想说，这都哪跟哪啊，当时说这话的人脑子里的东西到底是什么。 用上面的两点意义可以回应这些（不过当时似乎没人听你说什么），使数理统计学者能够站在一个超然的立场，避免陷入无畏的争论。\n数理统计是数学 有人说统计不是数学，我刚开始学的时候也有这种感觉，数学证明，计算没有说给出哪个结论说这个可能是这样的，这明显就是没解决问题啊，但是数理统计比较例外，他就是这样给出结论的，机器学习和深度学习也是，给出个结果，还有可信度，但是从另一个角度，数理统计学不研究数据背后的知识，如医学，或者其他的，他只研究数字，抽象的数据。 数学研究数和形，本身就是虚无缥缈的问题，数学也不属于自然科学，如果说统计学不属于数学也行，不过数学和数理统计学共有的研究抽象性被否定，反正说数理统计学是数学的分支，问题不大，起码现在来看问题不大\n本书（博客）的简介 第一章 早期发展，伯努利的《推测术》，主要说概率的基本概念和产生，惠更斯，帕斯卡和费马的通信，然后介绍伯努利的大数定理，作为数理统计的基础，这段历史很有意思。\n第二章 依旧是概率论：二项分布正太逼近工作，但是引入了正态分布和中心极限定理！这个是数理统计的基石.怎么强调都不过分，所以我们来换个颜色。\n第三章 贝叶斯学派的故事\n第四章 最小二乘法的相关故事和重要作用\n第五章 正态分布的历史，偏态分布的历史\n第六章 社会统计，这个是国内的统计学习，在数理统计里面说这个，因为还是有些关系的。\n第七章 回归的历史\n第八章 小样本理论，和线性模型的发展\n第九章 假设检验的历史\n第十章 参数估计的历史\n卷尾语 陈老师关于过去和未来的思考\n总结 本系列以小短文的形式给出，字数会少很多，但是前后关联，应该很有趣。 巨人已逝，馈如此著作名篇，站在巨人之肩，我辈自当奋进。\n原文地址：https://www.face2ai.com/Math-Statistics-History-Introduction转载请标明出处\n","permalink":"https://go.face2ai.com/math/math-statistics-history-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是数理统计学的第一篇文章，从数理统计学的发展过程来了解我们要研究什么\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 数学统计学简史\u003c/p\u003e","title":"【数理统计学简史】 介绍"},{"content":"Abstract: 本文介绍循环展开技术，在归约的基础上继续加速。 Keywords: 展开归约，归约，模板函数\n展开循环 博客从CSDN那边截流了一些流量，现在网站访问突然增多到让我有点不适应，于是，想想还是别总盯着流量看吧，注意文章质量，同时保证一定的更新，从数学到算法，最后到实现，优化，这些都做好，估计访问量会更多了。 到时候我就可以挂广告了，然后挣了钱吃煎饼就可以加个鸡蛋了！去网吧也能叫饮料了。。想想还有点小激动，感觉自己都有点膨胀了。 今天我们来做循环展开，GPU喜欢确定的东西，像前面讲解执行模型和线程束的时候，明确的指出，GPU没有分支预测能力，所有每一个分支他都是执行的，所以在内核里尽量别写分支，分支包括啥，包括if当然还有for之类的循环语句。 如果你不知道到为啥for算分支语句我给你写个简单到不能运行的例子：\nfor (itn i=0;i\u0026lt;tid;i++) { // to do something  } 如果上面这段代码出现在内核中，就会有分支，因为一个线程束第一个线程和最后一个线程tid相差32（如果线程束大小是32的话） 那么每个线程执行的时候，for终止时完成的计算量都不同，这就有人要等待，这也就产生了分支。\n 循环展开是一个尝试通过减少分支出现的频率和循环维护指令来优化循环的技术。 上面这句属于书上的官方说法，我们来看看例子，不止并行算法可以展开，传统串行代码展开后效率也能一定程度的提高，因为省去了判断和分支预测失败所带来的迟滞。 先来个c++ 入门循环\n for (int i=0;i\u0026lt;100;i++) { a[i]=b[i]+c[i]; } 这个是最传统的写法，这个写法在各种c++教材上都能看到，不做解释，如果我们进行循环展开呢？\nfor (int i=0;i\u0026lt;100;i+=4) { a[i+0]=b[i+0]+c[i+0]; a[i+1]=b[i+1]+c[i+1]; a[i+2]=b[i+2]+c[i+2]; a[i+3]=b[i+3]+c[i+3]; } 没错，是不是很简单，修改循环体的内容，把本来循环自己搞定的东西，我们自己列出来了，这样做的好处，从串行较多来看是减少了条件判断的次数。 但是如果你把这段代码拿到机器上跑，其实看不出来啥效果，因为现代编译器把上述两个不同的写法，编译成了类似的机器语言，也就是，我们这不循环展开，编译器也会帮我们做。 不过值得注意的是：目前CUDA的编译器还不能帮我们做这种优化，人为的展开核函数内的循环，能够非常大的提升内核性能 在CUDA中展开循环的目的还是那两个：\n 减少指令消耗 增加更多的独立调度指令 来提高性能 如果这种指令  a[i+0]=b[i+0]+c[i+0]; a[i+1]=b[i+1]+c[i+1]; a[i+2]=b[i+2]+c[i+2]; a[i+3]=b[i+3]+c[i+3]; 被添加到CUDA流水线上，是非常受欢迎的，因为其能最大限度的提高指令和内存带宽。 下面我们就在前面归约的例子上继续挖掘性能，看看是否能得到更高的效率。\n展开的归约 前面在避免分支的博客中,我们的内核函数reduceInterleaved 核函数中每个线程块只处理对应那部分的数据，我们现在的一个想法是能不能用一个线程块处理多块数据，其实这是可以实现的，如果在对这块数据进行求和前（因为总是一个线程对应一个数据）使用每个线程进行一次加法，从别的块取数据，相当于先做一个向量加法，然后再归约，这样将会用一句指令，完成之前一般的计算量，这个性价比看起来太诱人了。 上代码\n__global__ void reduceUnroll2(int * g_idata,int * g_odata,unsigned int n) { //set thread ID \tunsigned int tid = threadIdx.x; unsigned int idx = blockDim.x*blockIdx.x*2+threadIdx.x; //boundary check \tif (tid \u0026gt;= n) return; //convert global data pointer to the \tint *idata = g_idata + blockIdx.x*blockDim.x*2; if(idx+blockDim.x\u0026lt;n) { g_idata[idx]+=g_idata[idx+blockDim.x]; } __syncthreads(); //in-place reduction in global memory \tfor (int stride = blockDim.x/2; stride\u0026gt;0 ; stride \u0026gt;\u0026gt;=1) { if (tid \u0026lt;stride) { idata[tid] += idata[tid + stride]; } //synchronize within block \t__syncthreads(); } //write result for this block to global mem \tif (tid == 0) g_odata[blockIdx.x] = idata[0]; } 这里面的第二句，第四句，在确定线程块对应的数据的位置的时候有个乘2的偏移量， 这就是第二句，第四句指令的意思，我们只处理红色的线程块，而旁边白色线程块我们用\nif(idx+blockDim.x\u0026lt;n) { g_idata[idx]+=g_idata[idx+blockDim.x]; } 处理掉了，注意我们这里用的是一维线程，也就是说，我们用原来的一半的块的数量，而每一句只添加一小句指令的情况下，完成了原来全部的计算量，这个效果应该是客观的，所以我们来看一下效果之前先看一下调用核函数的部分：\n//kernel 1:reduceUnrolling2 CHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); reduceUnroll2 \u0026lt;\u0026lt;\u0026lt;grid.x/2, block \u0026gt;\u0026gt;\u0026gt;(idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i \u0026lt; grid.x/2; i++) gpu_sum += odata_host[i]; printf(\u0026#34;reduceUnrolling2 elapsed %lf ms gpu_sum: %d\u0026lt;\u0026lt;\u0026lt;grid %d block %d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;, iElaps, gpu_sum, grid.x/2, block.x); 这里需要注意由于合并了一半的线程块，这里的网格个数都要对应的减少一半，来看效率 相比于上一篇中的效率，“高到不知道哪里去了”（总能引用名人名言），比最简单的归约算法快了三倍，warmup的代码，不需要理睬。 我们上面框里有2，4，8三种尺度的展开，分别是一个块计算2个块，4个块和8个块的数据，对应的调用代码也需要修改，在github上有库，可以自己去看\nGithub:https://github.com/Tony-Tan/CUDA_Freshman\n可见直接展开对效率影响非常大，从上一篇naive的归约算法的0.01降低到0.002，然后到0.0013可见其威力巨大。 这个不光是节省了多于的线程块的运行，而且更多的独立内存加载/存储操作会产生更好的性能，更好的隐藏延迟（忘了的看前面的博客，有相关知识介绍）下面我们看一下他们的吞吐量\nnvprof --metrics dram_read_throughput ./reduceUnrolling 可见执行效率是和内存吞吐量是呈正相关的\n完全展开的归约 接着我们的目标是最后那32个线程，因为归约运算是个倒金字塔，最后的结果是一个数，所以每个线程最后64个计算得到一个数字结果的过程，没执行一步，线程的利用率就降低一倍，因为从64到32，然后16。。这样到1的，我们现在像个办法，展开最后的6步迭代（64，32，16，8，4，2，1）使用下面的核函数来展开最后6步分支计算：\n__global__ void reduceUnrollWarp8(int * g_idata,int * g_odata,unsigned int n) { //set thread ID \tunsigned int tid = threadIdx.x; unsigned int idx = blockDim.x*blockIdx.x*8+threadIdx.x; //boundary check \tif (tid \u0026gt;= n) return; //convert global data pointer to the \tint *idata = g_idata + blockIdx.x*blockDim.x*8; //unrolling 8; \tif(idx+7 * blockDim.x\u0026lt;n) { int a1=g_idata[idx]; int a2=g_idata[idx+blockDim.x]; int a3=g_idata[idx+2*blockDim.x]; int a4=g_idata[idx+3*blockDim.x]; int a5=g_idata[idx+4*blockDim.x]; int a6=g_idata[idx+5*blockDim.x]; int a7=g_idata[idx+6*blockDim.x]; int a8=g_idata[idx+7*blockDim.x]; g_idata[idx]=a1+a2+a3+a4+a5+a6+a7+a8; } __syncthreads(); //in-place reduction in global memory \tfor (int stride = blockDim.x/2; stride\u0026gt;32; stride \u0026gt;\u0026gt;=1) { if (tid \u0026lt;stride) { idata[tid] += idata[tid + stride]; } //synchronize within block \t__syncthreads(); } //write result for this block to global mem \tif(tid\u0026lt;32) { volatile int *vmem = idata; vmem[tid]+=vmem[tid+32]; vmem[tid]+=vmem[tid+16]; vmem[tid]+=vmem[tid+8]; vmem[tid]+=vmem[tid+4]; vmem[tid]+=vmem[tid+2]; vmem[tid]+=vmem[tid+1]; } if (tid == 0) g_odata[blockIdx.x] = idata[0]; } 在unrolling8的基础上，我们对于tid在 $[0,32]$ 之间的线程用这个代码展开\nvolatile int *vmem = idata; vmem[tid]+=vmem[tid+32]; vmem[tid]+=vmem[tid+16]; vmem[tid]+=vmem[tid+8]; vmem[tid]+=vmem[tid+4]; vmem[tid]+=vmem[tid+2]; vmem[tid]+=vmem[tid+1]; 第一步定义 volatile int类型变量我们先不说，我们先把最后这个展开捋顺一下，当只剩下最后下面三角部分，从64个数合并到一个数，首先将前32个数，按照步长为32，进行并行加法，前32个tid得到64个数字的两两和，存在前32个数字中 接着，到了我们的关键技巧了 然后这32个数加上步长为16的变量，理论上，这样能得到16个数，这16个数的和就是最后这个块的归约结果，但是根据上面tid\u0026lt;32的判断条件线程tid 16到31的线程还在运行，但是结果已经没意义了，这一步很重要（这一步可能产生疑惑的另一个原因是既然是同步执行，会不会比如线程17加上了线程33后写入17号的内存了，这时候1号才来加17号的结果，这样结果就不对了，因为我们的CUDA内核从内存中读数据到寄存器，然后进行加法都是同步进行的，也就是17号线程和1号线程同时读33号和17号的内存，这样17号即便在下一步修改，也不影响1号线程寄存器里面的值了），虽然32以内的tid的线程都在跑，但是没进行一步，后面一半的线程结果将没有用途了， 这样继续计算，得到最后的一个有效的结果就是 tid[0]。 上面这个过程有点复杂，但是我们自己好好想一想，从硬件取数据，到计算，每一步都分析一下，就能得到实际的结果。\nvolatile int类型变量是控制变量结果写回到内存，而不是存在共享内存，或者缓存中，因为下一步的计算马上要用到它，如果写入缓存，可能造成下一步的读取会读到错误的数据 你可能不明白\nvmem[tid]+=vmem[tid+32]; vmem[tid]+=vmem[tid+16]; tid+16要用到tid+32的结果，会不会有其他的线程造成内存竞争，答案是不会的，因为一个线程束，执行的进度是完全相同的，当执行 tid+32的时候，这32个线程都在执行这步，而不会有任何本线程束内的线程会进行到下一句，详情请回忆CUDA执行模型（因为CUDA编译器是激进的，所以我们必须添加volatile，防止编译器优化数据传输而打乱执行顺序）。 然后我们就得到结果了，看看时间: 又往后退了一位，看起来还是很爽的。 这个展开还有一个节省时间的部分就是减少了5个线程束同步指令 __syncthreads(); 这个指令被我们减少了5次，这个也是非常有效果的。我们来看看阻塞减少了多少 使用命令\nnvprof --metrics stall_sync ./reduceUnrolling 哈哈哈，又搞笑了，书上的结果和运行结果又不一样，展开后的stall_sync 指标反而高了，也就是说之前有同步指令的效率更高，哈哈，无解。。可以把锅甩给CUDA编译器。\n模板函数的归约 根据上面展开最后64个数据，我们可以直接就展开最后128个，256个，512个，1024个， 废话不多说，直接上代码，我们这次的目的就是让循环上西天：\n__global__ void reduceCompleteUnrollWarp8(int * g_idata,int * g_odata,unsigned int n) { //set thread ID \tunsigned int tid = threadIdx.x; unsigned int idx = blockDim.x*blockIdx.x*8+threadIdx.x; //boundary check \tif (tid \u0026gt;= n) return; //convert global data pointer to the \tint *idata = g_idata + blockIdx.x*blockDim.x*8; if(idx+7 * blockDim.x\u0026lt;n) { int a1=g_idata[idx]; int a2=g_idata[idx+blockDim.x]; int a3=g_idata[idx+2*blockDim.x]; int a4=g_idata[idx+3*blockDim.x]; int a5=g_idata[idx+4*blockDim.x]; int a6=g_idata[idx+5*blockDim.x]; int a7=g_idata[idx+6*blockDim.x]; int a8=g_idata[idx+7*blockDim.x]; g_idata[idx]=a1+a2+a3+a4+a5+a6+a7+a8; } __syncthreads(); //in-place reduction in global memory \tif(blockDim.x\u0026gt;=1024 \u0026amp;\u0026amp; tid \u0026lt;512) idata[tid]+=idata[tid+512]; __syncthreads(); if(blockDim.x\u0026gt;=512 \u0026amp;\u0026amp; tid \u0026lt;256) idata[tid]+=idata[tid+256]; __syncthreads(); if(blockDim.x\u0026gt;=256 \u0026amp;\u0026amp; tid \u0026lt;128) idata[tid]+=idata[tid+128]; __syncthreads(); if(blockDim.x\u0026gt;=128 \u0026amp;\u0026amp; tid \u0026lt;64) idata[tid]+=idata[tid+64]; __syncthreads(); //write result for this block to global mem \tif(tid\u0026lt;32) { volatile int *vmem = idata; vmem[tid]+=vmem[tid+32]; vmem[tid]+=vmem[tid+16]; vmem[tid]+=vmem[tid+8]; vmem[tid]+=vmem[tid+4]; vmem[tid]+=vmem[tid+2]; vmem[tid]+=vmem[tid+1]; } if (tid == 0) g_odata[blockIdx.x] = idata[0]; } 内核代码如上，这里用到了tid的大小，和最后32个没用到tid不同的是，这些如果计算完整会有一半是浪费的，而最后32个已经是线程束最小的大小了，所以无论后面的数据有没有意义，那些进程都不会停。 每一步进行显示的同步，然后我们看结果，哈哈，又又又搞笑了：\n似乎速度根本没什么影响，所以我觉得是编译器的锅没错了！他已经帮我们优化这一步了。\n模板函数的归约 我们看上面这个完全展开的函数，\nif(blockDim.x\u0026gt;=1024 \u0026amp;\u0026amp; tid \u0026lt;512) idata[tid]+=idata[tid+512]; __syncthreads(); if(blockDim.x\u0026gt;=512 \u0026amp;\u0026amp; tid \u0026lt;256) idata[tid]+=idata[tid+256]; __syncthreads(); if(blockDim.x\u0026gt;=256 \u0026amp;\u0026amp; tid \u0026lt;128) idata[tid]+=idata[tid+128]; __syncthreads(); if(blockDim.x\u0026gt;=128 \u0026amp;\u0026amp; tid \u0026lt;64) idata[tid]+=idata[tid+64]; __syncthreads(); 这一步比较应该是多于的，因为blockDim.x 自内核启动时一旦确定，就不能更改了，所以模板函数帮我解决了这个问题，当编译时编译器会去检查，blockDim.x 是否固定，如果固定，直接可以删除掉内核中不可能的部分也就是上半部分，下半部分是要执行的，比如blockDim.x=512 ,代码最后生成机器码的就是如下部分：\n__syncthreads(); if(blockDim.x\u0026gt;=512 \u0026amp;\u0026amp; tid \u0026lt;256) idata[tid]+=idata[tid+256]; __syncthreads(); if(blockDim.x\u0026gt;=256 \u0026amp;\u0026amp; tid \u0026lt;128) idata[tid]+=idata[tid+128]; __syncthreads(); if(blockDim.x\u0026gt;=128 \u0026amp;\u0026amp; tid \u0026lt;64) idata[tid]+=idata[tid+64]; __syncthreads(); 删掉了不可能的部分。 我们来看下模板函数的效率：\n结果是，居然还慢了一些。。书上不是这么说的。。编译器的锅！\n小结 我们总结下本文四步优化的指标对比： 加载效率存储效率：\nnvprof --metrics gld_efficiency,gst_efficiency ./reduceUnrolling    算法 时间 加载效率 存储效率     相邻无分化（上一篇） 0.010491 25.01% 25.00%   相邻分化（上一篇） 0.005968 25.01% 25.00%   交错（上一篇） 0.004956 98.04% 97.71%   展开8 0.001294 99.60% 99.71%   展开8+最后的展开 0.001009 99.71% 99.68%   展开8+完全展开+最后的展开 0.001001 99.71% 99.68%   模板上一个算法 0.001008 99.71% 99.68%    虽然和书上结果不太一样，但是指标和效率关系还是很明显的，所以我们今天得出的结论是。。一步一步优化，如果改了代码没效果，那么锅是编译器的，谁让你帮老子优化了还不告诉我！\n总结 在开发之前最好了解一下编译器特性，以及最关键的一点，一个真理，没有人能一下写出最好的代码，一步一步优化，从0.0104优化到0.001001，近10倍的效率提升，一分钟的程序你可能觉得无所谓 但是一个运行一年的程序，有化了十倍，那可是大功一件啊。 下篇我们继续，锅都是编译器的！\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-5-%E5%B1%95%E5%BC%80%E5%BE%AA%E7%8E%AF.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍循环展开技术，在归约的基础上继续加速。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 展开归约，归约，模板函数\u003c/p\u003e","title":"【CUDA 基础】3.5 展开循环"},{"content":"Abstract: 介绍规约问题中的分支分化问题 Keywords: 规约问题，分支分化 此篇有些结果和参考书中结果相反，需要更深入的技术才能解决\n避免分支分化 我坚持写博客是因为我上次最困惑最难过的那段时间通过写博客改变了我的非常不好的情况，所以我认为写些东西梳理自己的思路能够改变我的生活，所以我会一直坚持，学习的内容是没有止境的，所以博客也可以写很多。 写博客为了收入我之前也想过，最后放弃了，因为如果你的目的就是挣钱，有写博客这大把时间还不如出去跑个滴滴或者送个外卖来得快，所以我把之前有的捐赠部分都取消掉了，我并不否定那些博客写的质量非常高的人因此有收入，做得好，帮助到人了就可以获得收入，但是为了收入去帮助人，那叫服务。所以我后面可能会挂一个小广告，但是绝对不会因为广告搞得博客非常凌乱，而且收入应该只用作服务器和域名费用，仅此而已，我个人对于做事非常看重目的，我的目的是分享知识，并不是收入，收入只是附加的。 本文介绍一个并行计算中最常见的典型情况，并行分化(线程束分化的等价问题)，以及规约问题，以及其初步优化。\n并行规约问题 在串行编程中，我们最最最常见的一个问题就是一组特别多数字通过计算变成一个数字，比如加法，也就是求这一组数据的和，或者乘法，这种计算当有如下特点的时候，我们可以用并行归约的方法处理他们：\n 结合性 交换性  对应的加法或者乘法就是交换律和结合律，在我们的数学分析系列已经详细的介绍了加法和乘法的结合律和交换律的证明。所以对于所有有这两个性质的计算，都可以使用归约式计算。 为什么叫归约，归约是一种常见的计算方式（串并行都可以），一开始我听到这个名字的时候应该是在两年前了，感觉很迷惑，后来发现，归约的归有递归的意思，约就是减少，这样就很明显了，每次迭代计算方式都是相同的（归），从一组多个数据最后得到一个数（约）。 归约的方式基本包括如下几个步骤：\n 将输入向量划分到更小的数据块中 用一个线程计算一个数据块的部分和 对每个数据块的部分和再求和得到最终的结果。  数据分块保证我们可以用一个线程块来处理一个数据块。 一个线程处理更小的块，所以一个线程块可以处理一个较大的块，然后多个块完成整个数据集的处理。 最后将所有线程块得到的结果相加，就是结果，这一步一般在cpu上完成。\n归约问题最常见的加法计算是把向量的数据分成对，然后用不同线程计算每一对元素，得到的结果作为输入继续分成对，迭代的进行，直到最后一个元素。 成对的划分常见的方法有以下两种：\n 相邻配对：元素与他们相邻的元素配对  交错配对：元素与一定距离的元素配对   图中将两种方式表现的很清楚了，我们可以用代码实现以下。 首先是cpu版本实现交错配对归约计算的代码：\nint recursiveReduce(int *data, int const size) { // terminate check \tif (size == 1) return data[0]; // renew the stride \tint const stride = size / 2; if (size % 2 == 1) { for (int i = 0; i \u0026lt; stride; i++) { data[i] += data[i + stride]; } data[0] += data[size - 1]; } else { for (int i = 0; i \u0026lt; stride; i++) { data[i] += data[i + stride]; } } // call \treturn recursiveReduce(data, stride); } 和书上的代码有些不同，因为书上的代码没有考虑数组长度非2的整数幂次的结果。所以我加了一个处理奇数数组最后一个无人配对的元素的处理。 这个加法运算可以改成任何满足结合律和交换律的计算。比如乘法，求最大值等。 下面我们就来通过不同的配对方式，不同的数据组织来看CUDA的执行效率。\n并行规约中的分化 线程束分化已经明确说明了，有判断条件的地方就会产生分支，比如if 和 for这类关键词。 如下图所表示的那样，我们对相邻元素配对进行内核实现的流程描述：\n根据上一小节介绍： 第一步：是把这个一个数组分块，每一块只包含部分数据，如上图那样（图中数据较少，但是我们假设一块上只有这么多。），我们假定这是线程块的全部数据 第二步：就是每个线程要做的事，橙色圆圈就是每个线程做的操作，可见线程threadIdx.x=0 的线程进行了三次计算，奇数线程一致在陪跑，没做过任何计算，但是根据3.2中介绍，这些线程虽然什么都不干，但是不可以执行别的指令，4号线程做了两步计算，2号和6号只做了一次计算。 第三步：将所有块得到的结果相加，就是最终结果 这个计算划分就是最简单的并行规约算法，完全符合上面我们提到的三步走的套路 值得注意的是，我们每次进行一轮计算（黄色框，这些操作同时并行）的时候，部分全局内存要进行一次修改，但只有部分被替换，而不被替换的，也不会在后面被使用到，如蓝色框里标注的内存，就被读了一次，后面就完全没有人管了。 我们现在把我们的内核代码贴出来\n__global__ void reduceNeighbored(int * g_idata,int * g_odata,unsigned int n) { //set thread ID \tunsigned int tid = threadIdx.x; //boundary check \tif (tid \u0026gt;= n) return; //convert global data pointer to the \tint *idata = g_idata + blockIdx.x*blockDim.x; //in-place reduction in global memory \tfor (int stride = 1; stride \u0026lt; blockDim.x; stride *= 2) { if ((tid % (2 * stride)) == 0) { idata[tid] += idata[tid + stride]; } //synchronize within block \t__syncthreads(); } //write result for this block to global mem \tif (tid == 0) g_odata[blockIdx.x] = idata[0]; } 这里面唯一要注意的地方就是同步指令\n__syncthreads(); 原因还是能从图上找到，我们的每一轮操作都是并行的，但是不保证所有线程能同时执行完毕，所以需要等待，执行的快的等待慢的，这样就能避免块内的线程竞争内存了。 被操作的两个对象之间的距离叫做跨度，也就是变量stride， 完整的执行逻辑如下， 注意主机端和设备端的分界，注意设备端的数据分块。 完整的可执行代码Github:https://github.com/Tony-Tan/CUDA_Freshman\n这里把主函数贴出来，但注意里面包含后面的核函数执行部分，所以想要运行还是去github上拉一下吧，顺便点个star\nint main(int argc,char** argv) { ......... int size = 1 \u0026lt;\u0026lt; 24; ......... dim3 block(blocksize, 1); dim3 grid((size - 1) / block.x + 1, 1); ......... //cpu reduction \tint cpu_sum = 0; iStart = cpuSecond(); for (int i = 0; i \u0026lt; size; i++) cpu_sum += tmp[i]; printf(\u0026#34;cpu sum:%d \\n\u0026#34;, cpu_sum); iElaps = cpuSecond() - iStart; printf(\u0026#34;cpu reduce elapsed %lf ms cpu_sum: %d\\n\u0026#34;, iElaps, cpu_sum); //kernel 1:reduceNeighbored  CHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); warmup \u0026lt;\u0026lt;\u0026lt;grid, block \u0026gt;\u0026gt;\u0026gt;(idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i \u0026lt; grid.x; i++) gpu_sum += odata_host[i]; printf(\u0026#34;gpu warmup elapsed %lf ms gpu_sum: %d\u0026lt;\u0026lt;\u0026lt;grid %d block %d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;, iElaps, gpu_sum, grid.x, block.x); //kernel 1:reduceNeighbored  CHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); reduceNeighbored \u0026lt;\u0026lt; \u0026lt;grid, block \u0026gt;\u0026gt; \u0026gt;(idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i \u0026lt; grid.x; i++) gpu_sum += odata_host[i]; printf(\u0026#34;gpu reduceNeighbored elapsed %lf ms gpu_sum: %d\u0026lt;\u0026lt;\u0026lt;grid %d block %d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;, iElaps, gpu_sum, grid.x, block.x); //kernel 2:reduceNeighboredLess  CHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); reduceNeighboredLess \u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i \u0026lt; grid.x; i++) gpu_sum += odata_host[i]; printf(\u0026#34;gpu reduceNeighboredLess elapsed %lf ms gpu_sum: %d\u0026lt;\u0026lt;\u0026lt;grid %d block %d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;, iElaps, gpu_sum, grid.x, block.x); //kernel 3:reduceInterleaved \tCHECK(cudaMemcpy(idata_dev, idata_host, bytes, cudaMemcpyHostToDevice)); CHECK(cudaDeviceSynchronize()); iStart = cpuSecond(); reduceInterleaved \u0026lt;\u0026lt; \u0026lt;grid, block \u0026gt;\u0026gt; \u0026gt;(idata_dev, odata_dev, size); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; cudaMemcpy(odata_host, odata_dev, grid.x * sizeof(int), cudaMemcpyDeviceToHost); gpu_sum = 0; for (int i = 0; i \u0026lt; grid.x; i++) gpu_sum += odata_host[i]; printf(\u0026#34;gpu reduceInterleaved elapsed %lf ms gpu_sum: %d\u0026lt;\u0026lt;\u0026lt;grid %d block %d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;, iElaps, gpu_sum, grid.x, block.x); // free host memory  ..... } 代码太长不美观，删减一下，只留下了内核执行部分，可见，主函数只有最后一个循环求和的过程是要注意别忘了的，其他都是常规操作 还有一点，需要注意实际任务中数组不可能每次都是2的整数幂，如果不是2的整数幂需要确定数组边界。\n上图就是执行结果，为啥有那么多，因为我把下面两个经过优化的也装进去了，黄色框框里是我们上面这段代码执行结果和时间，warmup 是为了启动gpu防止首次启动计算时gpu的启动过程耽误时间，影响效率测试，warmup的代码就是reducneighbored的代码，可见还是有微弱的差别的。\n改善并行规约的分化 上面归约显然是最原始的，未经过优化的东西是不能拿出去使用的，或者说一个真理是，不可能一下子就写出来满意的代码。\nif ((tid % (2 * stride)) == 0) 这个条件判断给内核造成了极大的分支，如图所示： 第一轮 有 $\\frac{1}{2}$ 的线程没用 第二轮 有 $\\frac{3}{4}$ 的线程没用 第三轮 有 $\\frac{7}{8}$ 的线程没用\n可见线程利用率是非常低的，因为这些线程在一个线程束，所以，只能等待，不能执行别的指令。 对于上面的低利用率，我们想到了下面这个方案来解决： 注意橙色圆形内的标号是线程符号，这样的计算线程的利用率是高于原始版本的，核函数如下：\n__global__ void reduceNeighboredLess(int * g_idata,int *g_odata,unsigned int n) { unsigned int tid = threadIdx.x; unsigned idx = blockIdx.x*blockDim.x + threadIdx.x; // convert global data pointer to the local point of this block \tint *idata = g_idata + blockIdx.x*blockDim.x; if (idx \u0026gt; n) return; //in-place reduction in global memory \tfor (int stride = 1; stride \u0026lt; blockDim.x; stride *= 2) { //convert tid into local array index \tint index = 2 * stride *tid; if (index \u0026lt; blockDim.x) { idata[index] += idata[index + stride]; } __syncthreads(); } //write result for this block to global men \tif (tid == 0) g_odata[blockIdx.x] = idata[0]; } 最关键的一步就是\nint index = 2 * stride *tid; 这一步保证index能够向后移动到有数据要处理的内存位置，而不是简单的用tid对应内存地址，导致大量线程空闲。 那么这样做的效率高在哪？ 首先我们保证在一个块中前几个执行的线程束是在接近满跑的，而后半部分线程束基本是不需要执行的，当一个线程束内存在分支，而分支都不需要执行的时候，硬件会停止他们调用别人，这样就节省了资源，所以高效体现在这，如果还是所有分支不满足的也要执行，即便整个线程束都不需要执行的时候，那这种方案就无效了，还好现在的硬件比较只能，于是，我们执行后得到如下结果 这个效率提升惊人，有木有，直接降了一位！大约差了一半。 我们前面一直在介绍一个叫做nvprof的工具，那么我们现在就来看看，每个线程束上执行指令的平均数量，同样我会给出四个内核的，但我们之关心黄框内的 使用如下指令\nnvprof --metrics inst_per_warp ./reduceInteger 指标结果是原始内核444.9 比新内核 150.5 可见原始内核中有很多分支指令被执行，而这些分支指令是没有意义的。 分化程度越高，inst_per_warp这个指标会相对越高。这个大家要记一下，以后我们测试效率的时候会经常使用。 接着我们看一下内存加载吞吐：\nnvprof --metrics gld_throughput ./reduceInteger 依旧只看黄色框框，我们的新内核，内存效率要高很多，也接近一倍了，原因还是我们上面分析的，一个线程块，前面的几个线程束都在干活，而后面几个根本不干活，不干活的不会被执行，而干活的内存请求肯定很集中，最大效率的利用带宽，而最naive的内核，不干活的线程也在线程束内跟着跑，又不请求内存，所以内存访问被打碎，理论上是只有一半的内存效率，测试来看非常接近。\n交错配对的规约 上面的套路是修改线程处理的数据，使部分线程束最大程度利用数据，接下来采用同样的思想，但是方法不同，接下来我们使用的方法是调整跨度，也就是我们每个线程还是处理对应的内存的位置，但内存对不是相邻的了，而是隔了一定距离的： 示意图是最好的描述方法： 我们依然把上图当做一个完整的线程块，那么前半部分的线程束依然是最大负载在跑，后半部分的线程束也是啥活不干\n__global__ void reduceInterleaved(int * g_idata, int *g_odata, unsigned int n) { unsigned int tid = threadIdx.x; unsigned idx = blockIdx.x*blockDim.x + threadIdx.x; // convert global data pointer to the local point of this block \tint *idata = g_idata + blockIdx.x*blockDim.x; if (idx \u0026gt;= n) return; //in-place reduction in global memory \tfor (int stride = blockDim.x/2; stride \u0026gt;0; stride \u0026gt;\u0026gt;=1) { if (tid \u0026lt;stride) { idata[tid] += idata[tid + stride]; } __syncthreads(); } //write result for this block to global men \tif (tid == 0) g_odata[blockIdx.x] = idata[0]; } 执行结果 如果单从优化原理的角度，这个内核和前面的内核应该是相同效率的，但是测试结果是，这个新内核比前面的内核速度快了不少，所以我们还是考察一下指标吧：\nnvprof --metrics inst_per_warp ./reduceInteger nvprof --metrics gld_throughput ./reduceInteger reduceInterleaved内存效率居然是最低的，但是线程束内分化却是最小的。 而书中说reduceInterleaved 的优势在内存读取，而非线程束分化，我们实际操作却得出了完全不同结论，到底是内存的无情，还是编译器的绝望，请看我们下个系列，到时候我们会直接研究机器码，来确定到底是什么影响了看似类似，却有结果悬殊的两个内核\n此处需要查看机器码，确定两个内核的实际不同\n总结 本文比较尴尬最后得出来一个跟书中相反的结论，但是整篇的思路还是很流畅的，所以我还是把它发出来了，按照我的性格这种有疑问没解决的（无法确定是否是已有技术无法解决的）一般不会发出来，但是考虑到可能是编译器进化导致的结果，所以还是先发出来，后面如果研究明白了，在CUDA进阶系列作为案例讨论。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 介绍规约问题中的分支分化问题\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 规约问题，分支分化 \u003cfont color=\"ff0000\"\u003e此篇有些结果和参考书中结果相反，需要更深入的技术才能解决\u003c/font\u003e\u003c/p\u003e","title":"【CUDA 基础】3.4 避免分支分化"},{"content":"Abstract: 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率） Keywords: nvprof\n并行性表现 继续更新CUDA，前面为了加速概率论的学习停了一段CUDA，从今天开始继续CUDA和数学分析的更新，每一篇都写一点废话就相当于自己的日记了，之前很佩服那些写日记的人，因为根本不知道日记可以写些什么，但是现在看看，如果写一些文字记录自己，首先可以反思当下，其次是过一段时间以后可以看看自己到底有没有进步，这些都是有用的，所以大家可以略过我的废话，直接看正文。\n本文的主要内容就是进一步理解线程束在硬件上执行的本质过程，结合上几篇关于执行模型的学习，本文相对简单，通过修改核函数的配置，来观察核函数的执行速度，以及分析硬件利用数据，分析性能，调整核函数配置是CUDA开发人员必须掌握的技能，本篇只研究对核函数的配置是如何影响效率的（也就是通过网格，块的配置来获得不同的执行效率。） 本文全文只用到下面的核函数\n__global__ void sumMatrix(float * MatA,float * MatB,float * MatC,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx=ix+iy*ny; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatC[idx]=MatA[idx]+MatB[idx]; } } 没有任何优化的最简单的二维矩阵加法。 全部代码：\nint main(int argc,char** argv) { //printf(\u0026#34;strating...\\n\u0026#34;);  //initDevice(0);  int nx=1\u0026lt;\u0026lt;13; int ny=1\u0026lt;\u0026lt;13; int nxy=nx*ny; int nBytes=nxy*sizeof(float); //Malloc  float* A_host=(float*)malloc(nBytes); float* B_host=(float*)malloc(nBytes); float* C_host=(float*)malloc(nBytes); float* C_from_gpu=(float*)malloc(nBytes); initialData(A_host,nxy); initialData(B_host,nxy); //cudaMalloc  float *A_dev=NULL; float *B_dev=NULL; float *C_dev=NULL; CHECK(cudaMalloc((void**)\u0026amp;A_dev,nBytes)); CHECK(cudaMalloc((void**)\u0026amp;B_dev,nBytes)); CHECK(cudaMalloc((void**)\u0026amp;C_dev,nBytes)); CHECK(cudaMemcpy(A_dev,A_host,nBytes,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(B_dev,B_host,nBytes,cudaMemcpyHostToDevice)); int dimx=argc\u0026gt;2?atoi(argv[1]):32; int dimy=argc\u0026gt;2?atoi(argv[2]):32; double iStart,iElaps; // 2d block and 2d grid  dim3 block(dimx,dimy); dim3 grid((nx-1)/block.x+1,(ny-1)/block.y+1); iStart=cpuSecond(); sumMatrix\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,C_dev,nx,ny); CHECK(cudaDeviceSynchronize()); iElaps=cpuSecond()-iStart; printf(\u0026#34;GPU Execution configuration\u0026lt;\u0026lt;\u0026lt;(%d,%d),(%d,%d)|%f sec\\n\u0026#34;, grid.x,grid.y,block.x,block.y,iElaps); CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost)); cudaFree(A_dev); cudaFree(B_dev); cudaFree(C_dev); free(A_host); free(B_host); free(C_host); free(C_from_gpu); cudaDeviceReset(); return 0; } 可见我们用两个 $8192\\times 8192$ 的矩阵相加来测试我们效率。 注意一下这里的GPU内存，一个矩阵是 $2^{13}\\times 2^{13}\\times 2^2=2^{28}$ 字节 也就是 256M，三个矩阵就是 768M 因为我们的GPU内存就是 2G 的，所以我们没办法进行更大的矩阵计算了（无法使用原文使用的是 $2^{14}$ 的方矩阵）。\n用 nvprof 检测活跃的线程束 对比性能要控制变量，上面的代码只用两个变量，也就是块的x和y的大小，所以，调整x和y的大小来产生不同的效率，我们先来看看结果： 图片看不清，数据结果如下\n   gridDim blockDim time(s)     256,256 32,32 0.008304   256,512 32,16 0.008332   512,256 16,32 0.008341   512,512 16,16 0.008347   512,1024 16,8 0.008351   1024，512 8,16 0.008401    当块大小超过硬件的极限，并没有报错，而是返回了错误值，这个值得大家注意 另外，每个机器执行此代码效果可能定不一样，所以大家要根据自己的硬件分析数据。 书上给出的 M2070 就和我们的结果不同，2070的 (32,16) 效率最高，而我们的 (32,32) 效率最高，毕竟架构不同，而且CUDA版本不同导致了优化后的机器码差异很大，所以我们还是来看看活跃线程束的情况，使用\nnvprof --metrics achieved_occupancy ./simple_sum_matrix 得出结果    gridDim blockDim time(s) Achieved Occupancy     256,256 32,32 0.008304 0.813609   256,512 32,16 0.008332 0.841264   512,256 16,32 0.008341 0.855385   512,512 16,16 0.008347 0.876081   512,1024 16,8 0.008351 0.875807   1024，512 8,16 0.008401 0.857242    可见活跃线程束比例高的未必执行速度快，但实际上从原理出发，应该是利用率越高效率越高，但是还受到其他因素制约。 活跃线程束比例的定义是：每个周期活跃的线程束的平均值与一个sm支持的线程束最大值的比。\n用 nvprof 检测内存操作 下面我们继续用nvprof来看看内存利用率如何。 首先使用:\nnvprof --metrics gld_throughput ./simple_sum_matrix 来看一下内核的内存读取效率：    gridDim blockDim time(s) Achieved Occupancy GLD Throughput (GB/s)     256,256 32,32 0.008304 0.813609 60.270   256,512 32,16 0.008332 0.841264 60.042   512,256 16,32 0.008341 0.855385 59.996   512,512 16,16 0.008347 0.876081 59.967   512,1024 16,8 0.008351 0.875807 59.976   1024，512 8,16 0.008401 0.857242 59.440    可以看出虽然第一种配置的线程束活跃比例不高，但是吞吐量最大所以可见吞吐量和线程束活跃比例一起都对最终的效率有影响。 接着我们看看全局加载效率，全局效率的定义是：被请求的全局加载吞吐量占所需的全局加载吞吐量的比值（全局加载吞吐量），也就是说应用程序的加载操作利用了设备内存带宽的程度；注意区别吞吐量和全局加载效率的区别，这个在前面我们已经解释过吞吐量了，忘了的同学回去看看。\nnvprof --metrics gld_efficiency ./simple_sum_matrix 获得如下运行结果 很遗憾，在当前机器上进行测试所有的利用率都是 100% ，可见CUDA对核函数进行了优化，在 M2070上 使用以前的CUDA版本，并没有如此高的加载效率，有效加载效率是指在全部的内存请求中（当前在总线上传递的数据）有多少是我们要用于计算的。\n书上说如果线程块中内层的维度（blockDim.x）过小，小于线程束会影响加载效率，但是目前来看，不存在这个问题了。 随着硬件的升级，以前的一些问题，可能就不是问题了，当然对付老的设备，这些技巧还是很有用的。\n增大并行性 上面说 \u0026ldquo;线程块中内层的维度（blockDim.x）过小\u0026rdquo; 是否对现在的设备还有影响，我们来看一下下面的试验： 用表格列举一下数据\n   gridDim blockDim time(s)     (128,4096) (64,2) 0.008391   (128,2048) (64,4) 0.008411   (128,1024) (64,8) 0.008405   (64,4096) (128,2) 0.008454   (64,2048) (128,4) 0.008430   (64,1024) (128,8) 0.008418   (32,4096) (256,2) 0.008468   (32,2048) (256,4) 0.008439   (32,1024) (256,8) fail    通过这个表我们发现，最快的还是第一个，块最小的反而获得最高的效率，这里与书上的结果又不同了，我再想书上的数据量大可能会影响结果，当数据量大的时候有可能决定时间的因素会发生变化，但是一些结果是可以观察到\n 尽管（64，4） 和 （128，2） 有同样大小的块，但是执行效率不同，说明内层线程块尺寸影响效率。 最后的块参数无效 第一种方案速度最快  我们调整块的尺寸，还是为了增加并行性，或者说增加活跃的线程束，我们来看看线程束的活跃比例： 得到如下数据\n   gridDim blockDim time(s) Achieved Occupancy     (128,4096) (64,2) 0.008391 0.888596   (128,2048) (64,4) 0.008411 0.866298   (128,1024) (64,8) 0.008405 0.831536   (64,4096) (128,2) 0.008454 0.893161   (64,2048) (128,4) 0.008430 0.862629   (64,1024) (128,8) 0.008418 0.833540   (32,4096) (256,2) 0.008468 0.859110   (32,2048) (256,4) 0.008439 0.825036   (32,1024) (256,8) fail Nan    可见最高的利用率没有最高的效率。 没有任何一个因素可以直接左右最后的效率，一定是大家一起作用得到最终的结果，多因一效的典型例子，于是在优化的时候，我们应该首先保证测试时间的准确性，客观性，以及稳定性，说实话，我们上面的时间测试方法并不那么稳定，更稳定方法应该是测几次的平均时间，来降低人为误差。\n总结 指标与性能\n 大部分情况，单一指标不能优化出最优性能 总体性能直接相关的是内核的代码本质（内核才是关键） 指标与性能之间选择平衡点 从不同的角度寻求指标平衡，最大化效率 网格和块的尺寸为调节性能提供了一个不错的起点  从这个起点开始，我们后面逐渐的深入到各项指标，总之，用CUDA就是为了高效，而研究这些指标是提高效率最快的途径（当然内核算法提升空间更大），再强调一下，本文的所有数据只针对我使用的设备，对于任何其他的设备这些数据会完全不同，大家主要学习这几个测试指标和其间的相互关系。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要通过nvprof工具来分析核函数的执行效率（资源利用率）\n\u003cstrong\u003eKeywords:\u003c/strong\u003e nvprof\u003c/p\u003e","title":"【CUDA 基础】3.3 并行性表现"},{"content":"Abstract: 本文介绍如何使用中心极限定理，将某区间上离散的随机变量，用一段连续的正态分布来近似 Keywords: The Central Limit Theorem，Approximation\n分布连续性修正 本篇应该是初等概率论的最后一篇博客，一路写下来，激动过，怀疑过，痛苦过，沮丧过，但是看着满屏幕的文字，和一些以前不知道的知识，感觉还是有收获的，虽然有些知识不能变现，但是，收益终生。我怎么知道会收益终生？如果想确定这个观点，基本要去问死了的人，因为你有死前的一瞬间，才知道什么东西在你一生中有用。但是我这么说是基于古人的观点，因为那么多古人都死过，而且他们都说读书学习非常有用，所以我选择相信那些死了的人说的话，而不是活着的——那些看起来活的还不错的人（读书无用论的那些）。\n本文使用中心极限定理，通过这段区间 $[a,b]$ 上的某个正态分布的随机变量的概率，近似某区间上的离散随机变量，并且可以通过简单的调整 $Pr(X=a),Pr(X=b)$ 的近似程度，来提高整体近似度\n连续分布近似离散分布 Approximating a Discrete Distribution by a Continuous Distribution  🌰 ： 对于一个大的样本，6.3中我们讲了正态分布 $\\mu=50,\\sigma^2=25$ 可以用来近似 $n=100,p=0.5$ 的二项分布随机变量 $X$ 。特别的，如果 $Y$ 有 $\\mu=50,\\sigma^2=25$ 的正态分布，我们知道 $Pr(Y\\leq X)$ 对于所有 $x$ 近似于 $Pr(X\\leq x)$ 但是有对称的误差，如图 可以看出，离散随机变量X的c.d.f.在图中是阶梯状的，因为其变量对应的为整数，所以每个阶梯左右端点对应的是整数，那么在 $[30,70]$ 区间上，可见连续随机变量的c.d.f穿过所有的离散阶梯的中心部分，也就是 $n+0.5$ 这里对于两个分布是相等，中间左半部分 $[n,n+0.5)$ 离散的c.d.f较大，反之，右半部分连续的c.d.f.较大。 我们应该可以利用这个特点对近似做一点优化。因为我们想要个一致的近似，比如总是大于总是小于的近似，而不是一个一会儿大一会小的近似。\n 接下来我们讨论针对上面例子这种情况下的一种标准做法，来提高近似的质量，主要的突破点就是那个对称的误差。 让 $f(x)$ 为离散随机变量 $X$ 的p.f. ,然后用连续的随机变量的p.d.f. $g(x)$ 来近似 $f(x)$ 我们可以设连续随机变量 $Y$ 的 p.d.f. 是 $g$ ，我们设所有可能的 $X$ 都是整数，这个条件适用于我们前面介绍过的所有离散分布，二项分布，泊松分布，超几何分布等。我们可以通过以下这两个关系之间相等进行近似： $$ Pr(a\\leq X\\leq b)=\\sum^{b}{x=a}f(x) $$ 以及 $$ Pr(a\\leq Y\\leq b)=\\int^{b}{a}g(x)dx\\tag{6.4.2} $$\n只要让上面这两个概率相等就能得到一个高质量的近似，根据上一篇关于中心极限定理也可以得出 $g$ 是一个正态分布的p.d.f.\n但是这个简单近似有很多不足，比如说对于离散分布经常会有 $Pr(X\\geq a)\\neq Pr(X\u0026gt;a)$ 而对于连续随机变量则有 $Pr(Y\\geq a) = Pr(Y\u0026gt;a)$\n上面的近似有点黎曼积分的意思在里面，大家可以参考数学分析书籍进行理解，因为不能取极限，所以分析误差就变成了重要的一个环节。\n近似直方图 Approximating a Bar Chart 接着我们来看如何近似一个直方图，直方图的理论依据在大数定理中已经进行了证明，今天我们来看如何用连续分布的p.d.f.来近似一个直方图。 同样，直方图的面积对应的就是概率（高度和面积一样，因为宽度是1）但是我们和上面的处理方法不同，前面的处理方法是从整数到下一个整数，对应一个概率，这里改成从负半个整数到正半个整数作为一个概率，所以根据坐标来求和，区间 $[a-\\frac{1}{2},b+\\frac{1}{2}]$ 上条形图的面积近似于积分结果： $$ Pr(a-\\frac{1}{2} \u0026lt; Y \u0026lt; b+\\frac{1}{2})=\\int^{b+\\frac{1}{2}}_{a-\\frac{1}{2}}g(x)dx $$ 这个相比于 6.4.2 叫做连续性修正。当然修正后的更准确一些，从图像也能看出来，或者模拟实验也能得出结果。\n 有了上面的修正结论，我们就可以对第一个例子进行优化了。 比如我们用 $Pr(Y\\leq x+0.5)$ 来替代 $Pr(Y\\leq x)$ 来近似 $Pr(X\\leq x)$ ， 或者用 $Pr(Y\\leq x-0.5)$ 来替代 $Pr(Y\\leq x)$ 来近似 $Pr(X\\leq x)$ 都能得出优于前面的结论，但是图像会变成这样： 移动后的连续分布更能近似不移动的分布\n 总结 一个只能取整数的离散随机变量，用正态分布来近似某个区间，如果按照整数进行分段（如 $[2,3)$），近似效果不如半个整数分段（比如 $[1.5,2.5)$ ）.\n本文为概率论初级博客的收官之作，主要讲如何用连续分布来近似离散分布，这个也是要在统计中使用的技术，或者直接叫做拟合或者回归也可以，所以可见我们后面这些课都是为了为数理统计铺路的，至此概率论讲解完毕，我们继续我们的学习路线！加油。\n","permalink":"https://go.face2ai.com/math/math-probability-6-4-the-correction-for-continuity.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍如何使用中心极限定理，将某区间上离散的随机变量，用一段连续的正态分布来近似\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Central Limit Theorem，Approximation\u003c/p\u003e","title":"【概率论】6-4:分布连续性修正(The Correction for Continuity)"},{"content":"Abstract: 本文介绍中心极限定理 Keywords: The Central Limit Theorem，The Normal distribution，The Delta Method\n中心极限定理 读书的一个重要用途就是建立自己对事情的理解方法，在数学领域，尤其是概率和数理统计，学习这两门课程，可以让你对世界上所有的事情的理解改变一个角度，甚至统计最后可以解释哲学，那么这样解释自然的三种方法——神学，哲学，科学，就会被改成“神学”和“科学”了，如果哪天神学也被建模了，哈哈哈，世界大一统，这里并不是对宗教或者哲学家的任何不尊重，只是谈一个小想法😆\n概率论快接近尾声了，本文讲完基本也就剩下一篇了，学了微积分，学了线性代数，学了数学分析，学了概率论，但是感觉自己还是没什么长进，这就是数学基础的困难，变现速度慢，但是作为长久投资，我相信是值得的。\n本文我们介绍中心极限定理，上一篇的大数定理，围绕的一个核心观点就是，样本均值概率极限是分布均值。而今天的中心极限定理是描述样本均值的分布的。一个数量足够多的随机变量样本的样本期望（Sample Mean,一个随机变量）的期望是 $\\mu$ 以及有限的方差 $\\sigma^2$ 那么他的分布近似于均值为 $\\mu$ 方差为 $\\sigma^2/n$ 的正态分布。这个结论可以帮助证明正态分布可以用来建模一些随机变量，当然这些随机变量是多个独立的部分组成的。\n我们本文会提出Delta方法来帮助我们一组随机变量样本的期望的函数变化后产生的新随机变量。 中心极限定理和大数定理放在后面是有目的的，因为我们马上要过渡到数理统计，而前面我们说了，我们的主要应用时根据数据（已知样本）找模型，也就是我们在数理统计里面要研究的，而中心极限定理和大数定理给出的就是样本和模型之间的理论关系。\n中心极限定理 Statement of the Theorem  先举个🌰 ： 临床试验，100个患者将接受治疗，患者有 0.5的可能性接受治疗，而其本人不知道自己是否接受治疗，我们这里假设所有患者之间相互独立，本实验的目的是观察新的治疗能否提高患者的治愈率，随机变量 $X$ 表示 100 个人中治愈的人数，如果接受治疗的治愈率是0.5和没有接受治疗治愈率的一样，那么 $X$ 就是 $n=100,p=0.5$ 的二项分布，那么根据二项分布的随机变量的数值，可以描绘出如下的图片 如果我们绘制一个均值为 $\\mu=50$ 方差为 $\\sigma^2=25$ 的正态分布，刚好可以把这个二项分布包裹起来\n 在Poisson分布的学习中，当我们遇到 $n$ 很大 $p$ 很小，而$np$ 有不大不小（这个标准有点含糊） 的时候可以用泊松分布来近似二项分布，上面的例子我们看出，当 $n$ 很大而 $p$ 又不太小的时候，正态分布可以用来近似二项分布。 中心极限定理就是一个形式化的描述，来描述正态分布是如何近似i.i.d的随机变量的一般和（随机变量）或者均值（样本均值，随机变量）的分布的。 在正态分布的那一课证明了 $n$ 个独立同正态分布的的随机变量,如果其分布均值是 $\\mu$ 分布方差是 $\\sigma^2$ 那么样本均值 $\\bar{X}_n$ 也是正态分布，均值是 $\\mu$ 方差是 $\\sigma^2/n$ 而我们本篇就会介绍根据中心极限定理，无论原始分布是否是正态分布，其样本均值分布都可以用正态分布来近似，样本均值。 而对于样本是独立的不同分布的随机变量的话，我们也有一套中心极限定理能够建模这些样本的和。 注意，中心极限定理，有两个版本，就像微积分基本定理有两个一样，中心极限定理，一个针对独立同分布的样本，一个针对独立不同分布的样本，大家要注意区分。 首先来看同分布的，这个定理由 Lindeberg 和Levy 在1920s初期证明：\n Theorem Central Limit Theorem(Lindeberg and Levy) If the random variables $X_1,\\dots,X_n$ form a random sample of size $n$ from a given distribution with mean $\\mu$ and variance $\\sigma^2$ ( $0\u0026lt;\\sigma^2\u0026lt;\\infty$ ) ,then for each fixed number $x$ , $$ lim_{n\\to \\infty}Pr[\\frac{\\bar{X}_n-\\mu}{\\frac{\\sigma}{n^{1/2}}}\\leq x]=\\Phi(x) $$ where $\\Phi$ denotes the c.d.f. of the standard normal distribution.\n 书上并没有给出上述定理的证明，而是给出了定义的解释说明（我觉得还是证明了的定理用的踏实，但是有些定理证明过于复杂，所以理解定理含义，也算是站在巨人的肩膀上，但还是证明一下用的踏实。），定理说明是，从任何一个分布中选取大量的随机变量，这个分布的均值是 $\\mu$ 方差是 $\\sigma^2$ 不需要考虑这个分布式离散的还是连续的，则 $n^{1/2}(\\bar{X}n-\\mu)/\\sigma$ 近似于标准正太分布；那么根据正态分布的性质，样本均值 $\\bar{X}$ 近似于 $\\mu{\\bar{X}}=\\mu,\\sigma^2_{\\bar{X}}=\\sigma^2/n$ 的正态分布； $\\sum^{n}{i=1}X_i$ 的分布近似于 $\\mu{\\Sigma}=n\\mu,\\sigma^2_{\\Sigma}=n\\sigma^2$ 的正态分布\n关于中心极限定理的证明，我们在后面的高级课程中给出，这里我们需要做到的是理解定理内容。\n 又一个🌰 ： 从一个均匀分布中采样，假设从 $[0,1]$ 的均匀分布重采样得到 $n=12$ 个样本，求 $Pr(|\\bar{X}_n-\\frac{1}{2}|\\leq 0.1)$ 的大概值。 解：\n $[0,1]$ 之间的均匀分布的期望是 $\\frac{1}{2}$ 方差是 $\\sigma^2=\\frac{1}{12}$ 这里 $n=12$ $\\bar{X}n$ 根据中心极限定理，其分布近似为 $\\mu{\\bar{X}n}=\\frac{1}{2},\\sigma{\\bar{X}_n}^2=\\frac{1}{144}$ 根据中心极限定理，设随机变量 $Z=12(\\bar{X}_n-\\frac{1}{2})$ 近似于标准正太分布，于是有： $$ \\begin{aligned} Pr(|\\bar{X}_n-\\frac{1}{2}|\\leq 0.1)\u0026amp;=Pr[12|\\bar{X}_n-\\frac{1}{2}|\\leq 1.2]\\ \u0026amp;=Pr(|Z|\\leq 1.2)\\ \u0026amp;\\approx 2\\Phi(1.2)-1\\ \u0026amp;=0.7698 \\end{aligned} $$  上面是书上的答案，但是我看有点问题就是第5步，根据中心极限定理 $Z=\\frac{12^{1/2}(\\bar{X}_n-\\frac{1}{2})}{\\frac{1}{12}}=24\\sqrt{3}(\\bar{X}_n-\\frac{1}{2})$ 但不知道为啥直接被变成12了。\n 下面我们来介绍在中心极限定理里面出现的收敛在其他过程中的应用，以及他有自己的名字。\n Definition Convergence in Distribution/Asymptotic Distribution.Let $X_1,X_2,\\dots$ be a sequence of random variables,and for $n=1,2,\\dots$ ,let $F_n$ denote the c.d.f. of $X_n$ .Also,let $F\u0026rsquo;$ be a c.d.f. Then it is said that the sequence $X_1,X_2,\\dots$ converges in distribution to $F\u0026rsquo;$ if $$ lim_{n\\to \\infty}F_n(x)=F\u0026rsquo;(x) $$ for all x at which $F\u0026rsquo;(x)$ is continuous.Sometimes,it is simply said that $X_n$ converges in distribution to $F\u0026rsquo;$ ,and $F\u0026rsquo;$ is called the asymptotic distribution of X_n,If $F\u0026rsquo;$ has a name ,then we say that X_n converges in distribution to that name\n 定义收敛到分布，或者渐进分布：一个随机变量序列 $X_1,X_2,\\dots$ 对应的c.d.f. 为$F_n$ 其中 $n=1,2\\dots$ 并且 $F\u0026rsquo;$ 是一个c.d.f. 那么当 $lim_{n\\to \\infty}F_n(x)=F\u0026rsquo;(x)$ 时，我们说随机变量序列 $X_1,X_2,\\dots$ 的分布收敛到 $F\u0026rsquo;$ 。 $x$ 对于所有 $F\u0026rsquo;(x)$ 都是连续的，有时候简单的说 $X_n$ 收敛到 $F\u0026rsquo;$ ，如果这个 $F\u0026rsquo;$ 有自己的名字，比如正态分布，我们就说 $X_n$ 收敛到正态分布。也可以说 $X_n$ 的渐进分布是正态分布。\n根据上面的定义，我们可以说 $\\frac{n^{1/2}(\\bar{X}_n)-\\mu}{\\sigma}$ 收敛到标准正态分布。\n中心极限定理也解释了为什么自然界中的大量数据都收敛到正态分布，比如人的身高，体重，收集足够的数据后会发现其收敛到正态分布。\nDelta 方法 The Delta Method  🌰 ： 服务时间问题，顾客们在排队，假设第 $i$ 个顾客到达柜台至完成服务用时 $X_i$ ，我们会得到 $X_1,X_2,\\dots$ 共 $n$ 个随机变量，我们假设这些随机变量独立同分布，并且分布均值为 $\\mu$ 方差为 $\\sigma^2$ 那么我们可以根据中心极限定理给出 $\\bar{X}_n$ 的分布，但是如果我们关心服务速度，也就是 $\\frac{1}{\\bar{X}_n}$ 那么我们改如何建模呢？\n 例子引出的问题，独立同分布随机变量样本期望可以用中心极限定理建模，如果这个期望经过某个函数变换呢？比如上面例子中的取倒数，如何对这个新的随机变量建模呢？\n统计中一个已知的方法叫做 Delta方法，能够帮我们解决这个问题。\n Theorem Delta Method. Let $Y_1,Y_2,\\dots$ be a sequence of random variables,and let $F\u0026rsquo;$ be a continuous c.d.f. Let $\\theta$ be a real number,and let $\\alpha_1,\\alpha_2,\\dots$ be a sequence of positive numbers that increase to $\\infty$ .Suppose that $a_n(Y_n-\\theta)$ converges in distribution to $F\u0026rsquo;$ . Let $\\alpha$ be a function with contimuous derivative such that $\\alpha\u0026rsquo;(\\theta)\\neq 0$ .Then $a_n[\\alpha(Y_n)-\\alpha(\\theta)]/\\alpha\u0026rsquo;(\\theta)$ converges in distribution to $F'$\n 一个随机变量序列的线性变换收敛到某分布，那么这个随机变量序列的某函数映射后的随机变量的线性变换依旧收敛到这个分布。\n证明： 这个过程相对粗糙相当于给出了证明框架。\n  $a_n\\to\\infty$ 的时候 $Y_n$ 会概率收敛到 $\\theta$ 否则 $|a_n(Y_n-\\theta)|\\to \\infty$ ，也就是不收敛。\n  因为 $\\alpha$ 是连续函数, $\\alpha(Y_n)$ 将会非常接近 $\\alpha(\\theta)$\n  使用泰勒级数(忽略更高阶项)： $$ \\alpha(Y_n)\\approx \\alpha(\\theta)+\\alpha\u0026rsquo;(\\theta)(Y_n-\\theta) $$\n  两边同时减去 $\\alpha(\\theta)$ 后乘以 $\\frac{a_n}{\\alpha\u0026rsquo;(\\theta)}$ 得到： $$ \\frac{a_n}{\\alpha\u0026rsquo;(\\theta)}(Y_n-\\theta)\\approx a_n(Y_n-\\theta) $$\n  4中结论左右两边都收敛到 $F'$\n  这个定理给出的方法最常见的一个就是 $Y_n$ 是随机样本的平均数，随机样本的方差有限，可以得出以下推论。\n Corollary Delta Method for Average of a Ramdom Sample.Let $X_1,X_2\\dots$ be a sequence of i.i.d. random variables from a distribution with mean $\\mu$ and finite variance $\\sigma^2$ .Let $\\alpha$ be a function with continuous derivative such that $\\alpha\u0026rsquo;(\\mu)\\neq 0$ .Then the asymptotic distribution of $$ \\frac{n^{1/2}}{\\sigma\\alpha\u0026rsquo;(\\mu)}[\\alpha(\\bar{X}_n)-\\alpha(\\mu)] $$ is the standard normal distribution.\n 证明方法就是使用Delta法则，就能得到结果。\n总结 本文介绍了中心极限定理的基础知识，因为本定理在概率论和数理统计中的超级低位，希望大家能仔细阅读文章内容。 下一篇我们对本章进行收尾，同时结束概率论基础部分。\n","permalink":"https://go.face2ai.com/math/math-probability-6-3-the-central-limit-theorem.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍中心极限定理\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Central Limit Theorem，The Normal distribution，The Delta Method\u003c/p\u003e","title":"【概率论】6-3:中心极限定理(The Central Limit Theorem)"},{"content":"Abstract: 本文介绍马尔科夫不等式，切比雪夫不等式，样本均值，和大数定理的知识内容 Keywords: Markov Inequality，Chebyshev Inequality，Sample Mean，The Law of Large Numbers\n大数定理 最早做图像处理的时候建了一个QQ群，后来在里面认识了图像处理第一份工作的老板，后来离开了群，因为里面很多人基本都是来凑热闹的，所以质量堪忧，今天我又建了一个本博客的微信群，希望群内的同学们，能找到自己喜欢的方向，深入到自己热爱的领域，其实如果我的这些文字能帮助三五十个人，说实话，我自己感觉比那些小作坊身价过亿的小老板对社会的贡献更大一些。所以继续努力，戒骄戒躁。 想加入我们的同学，可以看目录页里面有进群的方法。\n若干个拥有相同分布的独立随机变量的均值，被称为样本均值(“样本期望”等表述同一概念：Sample Mean)，这些被选取出来的随机变量被称为样本。样本均值对于样本的信息描述，类似于一个分布的期望对这个分布的描述。注意这句话有两个信息：\n 我们前面介绍的均值，期望都是针对分布的。 样本的均值不同于分布的均值，但是有很多相似之处。  本节我们就会介绍一些结果来表明，“样本均值”和“组成随机样本的单个随机变量”之间的关系。\n马尔科夫不等式和切比雪夫不等式 The Markov and Chebyshev Inequalities 在学习均值的时候讲到过有关重心类似的概念，也就是说当我们改变分布，让小概率对应一个大的值的时候，比如离散情况下随机变量值 ${1,100,0.1}$ 对应于概率 ${0.1,0.01,0.89}$ 这时的期望是 $1\\times 0.1+100\\times 0.01 + 0.1\\times 0.89=1.189$ 也可以说重心在1.189这个位置，如果我们调整下，让大的随机变量值对应到大概率 ${1,0.1,100}$ 对应于概率 ${0.1,0.01,0.89}$ 这时的期望是 $1\\times 0.1+0.1\\times 0.01 + 100\\times 0.89=89.101$ 显然这个重心发生了明显的偏移，但是我们有个新想法，如果我们有很多个离散随机变量值，或者是连续分布的随机变量，我们在固定分布均值的情况下，有多少随机变量值可以调整位置呢？\n马尔科夫不等式 Markov Inequality  Theorem Markov Inequality.Suppose that $X$ is a random variable such that $Pr(X\\geq 0)=1$ .Then for every real number $t\u0026gt;0$ , $$ Pr(X\\geq t)\\leq \\frac{E(X)}{t} $$\n 证明思路的话我们就用一个离散分布来证明上面这个不等式的正确性然后延伸到连续情况。 证明：\n 假设 $X$ 有一个离散分布，其p.f.是 $f$ 那么 $X$ 的期望是： $$ E(X)=\\sum_{x}xf(x)=\\sum_{x\u0026lt;t}xf(x)+\\sum_{x\\geq t}xf(x) $$ 因为我们在条件中规定 $X\\geq 0$ 那么，上面的求和部分都是大于等于0的。 所以我们有： $$ E(X)=\\sum_{x\\geq t}xf(x)\\geq \\sum_{x\\geq t}tf(x)=tPr(X\\geq t) $$ 根据 $t\u0026gt;0$ 得出我们要的结论： $$ E(X)\\geq t Pr(X\\geq t)\\Rightarrow Pr(X\\geq t)\\leq\\frac{E(X)}{t} $$ 证毕  也就是说，Markov不等式回答了我们上面的问题，有多少随机变量值可以调整概率？对于某个正数 $t$ 调整前后必须满足大于 $t$ 的随机变量的全部概率要小于等于随机变量的期望除以 $t$ 这就是马尔科夫不等式给出我们的表面含义。 观察马尔科夫不等式，我们可以看出一个有趣的现象，如果 $t$ 是小于期望 $E(X)$ 的话，这个不等式就没啥意义了，因为概率必然小于1，所以多半情况下我们研究的是 $t \u0026gt; E(X)$ 的情况。 一个有趣的现象，如果一个随机变量的均值是1，那么其取到大于等于100的概率是 $Pr(X\\geq 100)\\leq \\frac{1}{100}=0.01$ 马尔科夫不等式得到概率和期望之间的关系，接下来我们想要看到的肯定是方差和概率之间的关系。\n切比雪夫不等式 Chebyshev Inequality 方差是反应随机变量分布的离散情况（spread out）的描述。不等式表明，随机变量值与其均值之间的距离的概率受到其方差的制约，注意分析这句话的主语和定语。注意，今日主角大数定理的证明需要用到切比雪夫不等式。\n Theorem Chebyshev Inequality.Let $X$ be a random variable for which $Var(X)$ exists.Then for every number $t\u0026gt;0$ , $$ Pr(|X-E(X)|\\geq t)\\leq \\frac{Var(X)}{t^2} $$\n 看形势跟马尔科夫不等式很相似，所以证明也会用到马尔科夫不等式。 证明：\n 设 $Y=[X-E(X)]^2$ 因为 $Y$ 是一个数的平方，那么我们有 $Pr(Y\\geq 0)=1$ 根据方差的定义，有 $E[Y]=Var(X)$ 根据马尔科夫不等式，我们有 $Pr(Y\\geq m)\\leq \\frac{E(Y)}{m}$ 其中 $m\u0026gt;0$ 因为想证明 $|X-E(X)|\\geq t$ 形式下的关系，所以我们令 $t^2=m$ 所以 $Pr(Y\\geq m)=Pr(|X-E(X)|\\geq t)\\leq \\frac{E(Y)}{m}=\\frac{Var(X^2)}{t^2}$ 所以得到最后结论 $Pr(|X-E(X)|\\geq t)\\leq \\frac{Var(X)}{t^2}$ 证毕。  根据马尔科夫不等式给出的一些结论，在切比雪夫不等式中同样也是有响应的推论，比如我们令 $Var(X)=\\sigma^2$ 以及 $t=3\\sigma$ 那么我们的切比雪夫不等式就有： $$ Pr(|X-E(X)|\\geq 3\\sigma)\\leq \\frac{\\sigma^2}{(3 \\sigma)^2}=\\frac{1}{9} $$\n这个结论很有用，因为他刻画了一个分布要遵守的规则，就是超过 $3\\sigma$ 的部分概率小于 $\\frac{1}{9}$\n样本均值的性质 Properties of the Sample Mean 样本均值的性质，首先我们还是来看样本均值的定义，样本均值还是一个随机变量，他是其他随机变量的线性组合，而组成他的这些变量必须是独立同分布的。 $$ \\bar{X_n}=X_1+\\dots+X_n $$\n注意，这里都是大写字母表示的是随机变量，而不是某个值，所以这个均值也是随机变量，接下来我们就要研究这个随机变量的性质了。\n Theorem Mean and Variance of the Sample Mean.Let $X_1,\\dots,X_n$ be a random sample from a distribution with mean $\\mu$ and variance $\\sigma^2$ .Let $\\bar{X_n}$ be the sample mean.Then $E(\\bar{X_n})=\\mu$ and $Var(\\bar{X_n})=\\frac{\\sigma^2}{n}$\n 证明虽然比较简单，我们还是证明一下比较好： 证明：\n 根据4.2 中的定理 $$ E(\\bar{X_n})=\\frac{1}{n}\\sum^{n}_{i=1}E(X_i)=\\frac{1}{n}n\\mu=\\mu $$ 根据4.3 中方差的定理 $$ \\begin{aligned} Var(\\bar{X_n})\u0026amp;=\\frac{1}{n^2}Var(\\sum^n_{i=1}X_i)\\ \u0026amp;=\\frac{1}{n^2}\\sum^{n}_{i=1}Var(X_i)\\ \u0026amp;=\\frac{1}{n^2}\\cdot n\\sigma^2\\ \u0026amp;=\\frac{\\sigma^2}{n} \\end{aligned} $$  换句话说，就是样本均值的期望会接近原始分布的期望，方差会变小，变小的比例是取样的数量，这个可以理解，因为你取的越少，离散程度自然越小。可以利用切比雪夫不等式来使得定理更加精确 因为$E(\\bar{X_n})=\\mu,Var(\\bar{X_n})=\\frac{\\sigma^2}{n}$ $$ Pr(|\\bar{X}-\\mu|\\geq t)\\leq \\frac{\\sigma^2}{nt^2} $$\n 举个🌰 ： 假设从一个分布中选取一个随机样本，均值 $\\mu$ 未知，但是知道标准偏移 $\\sigma$ 小于等于两个单位（2 units），那么我们需要至少多大的样本才能使得 $|\\bar{X}-\\mu|$ 小于1个单位的概率大于等于 0.99 。\n注意，这里的2个单位，1个单位的表述方式是因为 方差没有单位，所以没办法说出量词 也就是说 $\\sigma\\leq 2\\Rightarrow \\sigma^2\\leq 4$ 根据切比雪夫不等式： $$ Pr(|\\bar{X_n}-\\mu|\\geq 1)\\leq \\frac{\\sigma^2}{n}\\leq frac{4}{n} $$ 为了让 $Pr(|\\bar{X}-\\mu|\u0026lt;1)\\geq 0.99$ 必须使得 $\\frac{\\sigma^2}{n}\\leq \\frac{4}{n}\\leq 0.01$ 这样就要求 $n$ 至少是 400 这个过程有问题的地方就是上面0.01和 $\\frac{4}{n}$ 的关系这里，需要理清思路，切比雪夫不等式给出的不是最好的界限，因为其可能比最好的界限更严格，比如可能100个样本能解决，但是用切比雪夫算出来需要至少200个。\n 大数定理 The Law of Large Numbers 大数定理，大名鼎鼎，但是之前一直不知道他是干什么的，今天我们就来一起探索大数定理要表述什么，以及其背后的真相。 上面的例子用切比雪夫不等式来确定取样规模的大小，但是有些情况下确实有些虚报，比如其实只需要1000个样本，但是切比雪夫不等式得出的是10000个，这种情况经常发生，但是切比雪夫不等式绝对是个有用的工具，帮助我们证明下面的定理——“大数定理” ，大数不是指很大的数字，而是指很多的数字。 首先说说序列的收敛，这个在数学分析系列中会有严格的证明和解释，这里随机序列的收敛可以简单的理解为一个序列中的元素 $Z_n$ 在 $n$ 逐渐增加，$Z_n$ 逐渐接近某个实数 $b$ 的概率逐渐增大 ， 极限情况下 $lim_{n\\to \\infty}Z_n=b$ 发生的概率为1.\n Definition Convergence in Probability.A sequence $Z_1,Z_2,\\dots$ of random variables converges to $b$ in probability if for every number $\\varepsilon \u0026gt;0$ , $$ lim_{n\\to\\infty}Pr(|Z_n-b|\u0026lt;\\varepsilon)=1 $$ This property is denoted by $$ Z_n\\xrightarrow{p} b $$ and sometimes stated simply as $Z_n$ converges to $b$ in probability.\n 上面是极限的一种典型的定义方法，无论 $\\varepsilon$ 多么小，都存在 $n$ 使得 $|Z_n-b|\u0026lt;\\varepsilon$ 成立（概率的方法就是概率趋近于 1）。 换句话说，无论在 $b$ 附近多么小的区间，都存在当$n\\to \\infty$ 时 $Z_n$ 在此区间的概率趋近于1。\n然后我们结合这个定义以及前面关于样本均值的定义，请出赫赫有名的“大数定理”\n Theorem Law of Large Numbers.Suppose that $X_1,\\dots,X_n$ from a random sample from a distribution for which the mean is $\\mu$ and for which the variance is finite.Let $\\bar{X}_n$ denote the sample mean.Then $$ \\bar{X}_n\\xrightarrow{p}\\mu\\tag{6.2.5} $$\n 证明：\n 对于每一个随机变量 $X_i$ ，我们设其方差为 $\\sigma^2$ 根据切比雪夫不等式(变形)对于每一个 $\\varepsilon\u0026gt;0$ 有： $$ Pr(|\\bar{X}_n-\\mu|\u0026lt;\\varepsilon)\\geq 1-\\frac{\\sigma^2}{\\varepsilon^2} $$ 再根据样本的方差的性质 $Var(\\bar{X}_n)=\\frac{\\sigma^2}{n}$ 那么因此有： $$ Pr(|\\bar{X}_n-\\mu|\u0026lt;\\varepsilon)\\geq 1-\\frac{\\sigma^2}{n\\varepsilon^2} $$ 于是得到大数定理的结论： $$ lim_{n\\to \\infty}Pr(|\\bar{X}_n-\\mu|\u0026lt;\\varepsilon)=1\\ \\bar{X}_n\\xrightarrow{p}\\mu $$ 证毕  上述证明是在均值有限，方差有限的分布上进行证明的，当均值有限，方差无限的时候，需要用到更高深的方法进行证明。 大数定理有两种用法：\n 当样本足够多的时候，样本均值和分布均值足够接近的概率非常大 当样本足够多的时候，可以用样本均值来近似分布均值。  2中的用法将在下一篇继续讨论，进而引出中心极限定理，并且能够提出描述这两个均值之间的差。\n Theorem Continuous Functions of Random Variables.If $Z_n\\xrightarrow{p}b$ ,and if $g(z)$ is a function that is continuous at $z=b$ ,then $g(Z_n)\\xrightarrow{p}g(b)$\n 这个定理可以被扩展成多维情况：\n Corollary Continuous Functions of Random Variables.If $\\vec{Z_n}\\xrightarrow{p}\\vec{b}$ ,and if $g(vec{z})$ is a function that is continuous at $\\vec{z}=\\vec{b}$ ,then $g(\\vec{Z_n})\\xrightarrow{p}g(\\vec{b})$\n 这个定理的证明用到了随机变量的函数，和极限的函数两个知识点，这里不进行证明，具体可以参考3.8\n Theorem Histogram.Let $X_1,X_2,\\dots$ be a sequence of i.i.d. random variables.Let $c_1 \u0026lt; c_2$ be two constants.Define $Y_i=1$ if $c_1\\leq X_i\u0026lt;c_2$ and $Y_i=0$ if not .Then $\\bar{Y}n=\\frac{1}{n}\\sum^{n}{i=1}Y_i$ is the proportion of $X_1,\\dots,X_n$ that lie in the interval $[c_1,c_2)$ ,and $\\bar{Y}_n \\xrightarrow{p}Pr(c_1\\leq X_1\u0026lt;c_2)$\n 在证明这个定理之前解释一下，这个定理是关于直方图的，对于一个随机样本（内包含多个i.i.d.的随机变量）其中落在某个固定区间上的个数与全部样本的个数的比例，概率趋近于原始分布在此区间的概率。这个定理存在的根本意义是证明了直方图可以反映原始分布形态！\n证明：\n 对于独立同分布的随机变量 $Y_1,Y_2,\\dots$ 是独立同分布的伯努利随机变量 根据伯努利分布的定义，其参数 $p=Pr(c_1\\leq X_1\u0026lt; c_2)$ 根据大数定理，$\\bar{Y}_n\\xrightarrow{p}p$  这个定理直观的应用就是，一个随机样本中的随机变量值，落在范围 $[c_1,c_2)$ 内的比例概率趋近于这个分布在此区间的概率，这也就证明了，我们用落在某个范围内的随机变量的个数来刻画产生这些随机变量的分布在此区间的概率是受到大数定理的支持的。 上面这句话不好理解，可以多读几遍，或者定理的语言更直观！因为连续分布的p.d.f下的面积是对应区间的概率，所以我们经常用直方图的面积来表示这个区间的概率就是这么来的（有黎曼积分的思想在里面），作为进一步的近似，样本越多，直方图越接近原始分布。 以上全部定理，在条件分布下同样适用！\n总结 本文介绍了大数定理，为了介绍大数定理，还介绍了马尔科夫不等式，切比雪夫不等式，样本均值方差，最后才引出大数定理，整个过程非常流畅，学完定理后需要多做练习。 明天继续中心极限定理。\n","permalink":"https://go.face2ai.com/math/math-probability-6-2-the-law-of-large-numbers.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍马尔科夫不等式，切比雪夫不等式，样本均值，和大数定理的知识内容\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Markov Inequality，Chebyshev Inequality，Sample Mean，The Law of Large Numbers\u003c/p\u003e","title":"【概率论】6-2:大数定理(The Law of Large Numbers)"},{"content":"Abstract: 本文作为第六章的开篇主要介绍第六章我们要研究的内容 Keywords: Large Random Samples\n大样本介绍 身着简陋而举止优雅，身着华丽而举止粗俗，比选其一的话，我更愿意尊重第一种类型。 本章我们介绍一些近似的结果，简化大量随机样本的分析。\n大样本介绍 Introduction 本文通过两个例子来举例两个不同的分析方向，并有不同的分析工具。\n 🌰 ： 扔一个硬币，你可能感觉出现正反面的概率基本相同，也就是出现正面的概率大概是 $\\frac{1}{2}$ ,然而，当你扔10次，出现五次正面的可能性不一定很大。如果你扔100次，也不一定出现正好的50次正面。多次扔硬币的过程可以通过我们前面介绍的二项分布来建模，参数是扔硬币的次数 $n$ 和正面出现的概率 $\\frac{1}{2}$ 。那么上述两种情况的概率： $$ Pr(X=5)=\\begin{pmatrix}10\\5\\end{pmatrix}(\\frac{1}{2})^{5}(1-\\frac{1}{2})^{5}=0.2461\\tag{1} $$ 100次其中50次的概率 $$ Pr(Y=50)=\\begin{pmatrix}100\\50\\end{pmatrix}(\\frac{1}{2})^{50}(1-\\frac{1}{2})^{50}=0.0796\\tag{2} $$\n可见在一定次数 $n$ 的独立实验中，出现 $n/2$ 的次数的概率并不大，并且试验次数越多，这个概率越小。 但是如果我们把这个概率稍微移动一下，产生一个区间，那么这个概率会急剧上升。 $$ Pr(0.4\\leq \\frac{Y}{100}\\leq 0.6)=Pr(40\\leq Y\\leq 60)=\\sum^{60}{i=40}\\begin{pmatrix}100\\i\\end{pmatrix}(\\frac{1}{2})^{i}(1-\\frac{1}{2})^{100-i}=0.9648 $$ 即使n不大的到时候 $$ Pr(0.4\\leq \\frac{X}{10}\\leq 0.6)=Pr(4\\leq X\\leq 6)=\\sum^{6}{i=4}\\begin{pmatrix}10\\i\\end{pmatrix}(\\frac{1}{2})^{i}(1-\\frac{1}{2})^{10-i}=0.6563 $$\n可见，同样的独立试验次数n越大的时候，在 $\\frac{1}{2}$ 附近（比如 $[0.4,0.6]$） 的概率越大。\n 上述例子简单就简单在每次试验都是独立的伯努利分布，且概率固定。接下来这个例子稍微复杂一点。\n 🌰 ： 一个队列的客户，第 $i$ 个客户在队列中等待 $X_i$ 其是随机变量，假设 $X_1,X_2,\\dots$ 是i.i.d的，其是 $[0,1]$ 上的均匀分布，等待的期望是 0.5，所以当用户样本数量足够大的时候这些样本的均值越接近0.5 。但是多个样本的均值的分布其实是很复杂的，可能没办法准确的描述多个样本的均值与0.5的接近程度。\n 大数定理会给出数学基础来证明一些随机变量的大量样本的均值，接近于他们的期望。 中心极限定理来给出样本的均值来近似期望的概率。\n总结 本文给出本章的研究方向，和基本研究背景。\n","permalink":"https://go.face2ai.com/math/math-probability-6-1-large-random-samples-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文作为第六章的开篇主要介绍第六章我们要研究的内容\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Large Random Samples\u003c/p\u003e","title":"【概率论】6-1:大样本介绍(Large Random Samples Introduction)"},{"content":"Abstract: 本文介绍第一个多变量连续分布——双变量正态分布(本篇内有未证明定理，需要后续要补充 ) Keywords: The Bivariate Normal Distributions\n二维正态分布 今天的废话想说说我们周围会有各种各样的事，各种各样的诱惑，各种各样的理由来告诉我们读书学习很苦而不学习也可以活的很好，但是坚持还是放弃只能选择一次，所以要慎重，开弓没有回头箭，放弃学习，就相当于放弃了一条抗争的路。\n 万般皆下品惟有读书高\n 今天我们来研究双变量的正态分布，多变量，连续分布。 对于某些研究者，可能用正态分布来非常好的描述某个随机变量，那么如果我们有两个随机变量，都可以用正态分布描述，而且他们之间存在关系，这时候我们就可以用一个双变量正态分布来描述了这两个变量之间的关系，并且这个二维分布的边缘分布，还是这两个随机变量单变量的分布。5.6中 我们介绍了某些有正态分布的独立随机变量的线性组合还是正态分布。但是双变量正态分布（联合分布）可以是相关的。\n二维正态分布的定义和来源 Definition and Derivation of Bivariate Normal Distributions  Theorem Suppose that $Z_1$ and $Z_2$ are independent random variables,each of which has the standard normal distribution.Let $\\mu_1,\\mu_2,\\sigma_1,\\sigma_2$ ,and $\\rho$ be constants such that $-\\infty\u0026lt;\\mu_i\u0026lt;\\infty(i=1,2)$ , $\\sigma_i\u0026gt;0(i=1,2)$ ,and $-1\u0026lt;\\rho\u0026lt;1$ . Define two new random variables $X_1$ and $X_2$ as follows: $$ X_1=\\sigma_1Z_1+\\mu_1\\ X_2=\\sigma_2[\\rho Z_1+(1-\\rho^2)^{\\frac{1}{2}}Z_2]+\\mu_2 \\tag{5.10.1} $$ The joint p.d.f. of $X_1$ and $X_2$ is $$ f(x_1,x_2)=\\frac{1}{2\\pi(1-\\rho^2)^{\\frac{1}{2}}\\sigma_1\\sigma_2}e^{-\\frac{1}{2(1-\\rho^2)}[(\\frac{x_1-\\mu_1}{\\sigma_1})^2-2\\rho(\\frac{x_1-\\mu_1}{\\sigma_1})(\\frac{x_2-\\mu_2}{\\sigma_2})+(\\frac{x_2-\\mu_2}{\\sigma_2})^2]} \\tag{5.10.2} $$\n 上面这个定理的证明需要定理3.9.5 ，而定理3.9.5是个选证题，也就是说会在我们后面的高级课程中进行证明，所以这个定理也就没法证明了，在证明了3.9.5 以后，我们会对此定理进行证明。\n Theorem Suppose that $X_1$ and $X_2$ have the joint distribution whose p.d.f. is given by Eq.(5.10.2) Then there exist independent standard normal random variables $Z_1$ and $Z_2$ such that Eqs (5.10.1) hold .Also,the mean of $X_i$ is $\\mu_i$ and the variance of $X_i$ is $\\sigma_i^2$ for $i=1,2$ .Furthermore the correlation between $X_1$ and $X_2$ is $\\rho$ .Finally,the marginal distribution of $X_i$ is the normal distribution with mean $\\mu_i$ and variance $\\sigma_i^2$ for $i=1,2$\n 此定理的证明也需要 3.9.5 的结论，所以我们目前只做不严谨的推理，两个联合分布如5.10.2，那么他们中的一个随机变量的分布（也就是联合变量的边缘分布）就是一个正态分布。均值和方差可求。\n Definition Bivariate Normal Distributions.When the joint p.d.f. of two random variables $X_1$ and $X_2$ is of the form in Eq(5.10.2),it is said that $X_1$ and $X_2$ have the bivariate normal distribution with mean $\\mu_1$ and $\\mu_2$ variance $\\sigma_1^2$ and $\\sigma_2^2$ ,and correlation $\\rho$\n 以上就是第一部分要讲的内容，两个没证明的定理，和一个定义，这篇文章看起来有点水，确实是这样，但是如果没有知识又不完全，算是个占位符，但是双变量正态分布这个用途确实太多了，举个最简单的例子，我们的身高体重，就经常用双变量的正态分布来建模。\n二维正态分布的性质 Properties of Bivariate Normal Distributions 接下来我们来研究一下双变量正态分布的性质\n Theorem Independence and Correlation.Two random variables $X_1$ and $X_2$ that have a bivariate normal distribution are independent if and only if they are uncorrelated.\n 两个随机变量有一个双变量正态分布，那么他们独立的充分必要条件是他们不相关。 来回忆一下独立性和相关性，独立性是两个随机变量分布之间满足 $f(x,y)=f_1(x)f_2(y)$ 这时 $X,Y$ 独立，不相关是说 $\\rho(X,Y)=\\frac{Cov(X,Y)}{\\sigma_X^2\\sigma_Y^2}=0$ 的时候两个变量不相关，相关性(点击传送)的介绍中对于任何随机变量的分布，独立性都能推出不相关，但是不相关不能推出独立，所以为了证明本定理我们可以只证明，if过程，也就是不相关来推到独立，另一部分在相关性的文章中已经证明了。 证明 if 过程： 假设两个变量不相关，所以当 $\\rho=0$ 从公式 5.10.2 中可以看出 $f(x_1,x_2)$ 可以被分解成两个分布相乘的形式，所以，我们可以得到这两个边缘分布独立。 证毕。 双变量的正态分布，不相关就独立，独立就不相关，在别的分布下不一定成立！ 当相关性不为0的时候，我们会得到下面这个定理，就是一个变量再另一个变量给定情况下的分布。\n Theorem Conditional Distribution.Let $X_1$ and $X_2$ have the bivariate normal distribution whose p.d.f. is Eq.(5.10.2) .The conditional distribution of $X_2$ given that $X_1=x_1$ is the normal distribution with mean and variance given by $$ \\begin{aligned} E(X_2|x_1)\u0026amp;=\\mu_2+\\rho\\sigma_2(\\frac{x_1-\\mu_1}{\\sigma_1})\\ Var(X_2|x_1)\u0026amp;=(1-\\rho^2)\\sigma_2^2 \\end{aligned} $$\n 当两个变量的联合分布为双变量正态分布，并且他们相关的时候，已知一个变量怎么来计算条件分布呢？ 证明：\n 给定条件 $X_1=x_1$ 等价于给定 $Z_1=\\frac{x_1-\\mu_1}{\\sigma_1}$ 那么我们只需要证明给定条件 $Z_1=\\frac{x_1-\\mu_1}{\\sigma_1}$ 下的 $X_2$ 的分布。 那么把 $Z_1=\\frac{x_1-\\mu_1}{\\sigma_1}$ 带入到式子 5.10.1中的第二个公式。 那么给定条件 $X_1=x_1$ 下的 $X_2$ 的分布，等价于给定条件 $Z_1=\\frac{(x_1-\\mu_1)}{\\sigma_1}$ 下，以下关系式的分布： $$ (1-\\rho^2)^{1/2}\\sigma_2Z_2+\\mu_2+\\rho\\sigma_2(\\frac{x_1-\\mu_1}{\\sigma_1})\\tag{5.10.7} $$ 上式可见 $Z_2$ 是唯一的随机变量，并且 $Z_1$ 和 $Z_2$ 是独立的，所以 $X_2$ 在给定 $X_1=x_1$ 的条件下是 5.10.7 的边缘分布 所以条件期望和条件方差如 5.10.6 所写。 证毕  同理可以证明， 给定条件 $X_2=x_2$ 时 $X_1$ 的分布也是正态分布，并且其期望和方差如下 $$ E(X_1|x_2)=\\mu_1+\\rho\\sigma_1(\\frac{x_2-\\mu_2}{\\sigma_2})\\ Var(X_1|x_2)=(1-\\rho^2)\\sigma_1^2 $$\n上面这两个结论可以看出，当两个变量相关的 $\\rho\\neq 0$ 时， $E(X_2|x_1)$ 是 $x_1$ 的线性函数，并且 $\\rho$ 是斜率。并且此时条件方差是不依赖于条件的。条件方差比边缘概率的方差小，也就是说 $Var(X_1|X_2=x_2)\u0026lt; \\sigma_2$ .\n 这里可以简单的给个🌰 的大概描述： 在一群人中进行建模，得到一个身高和体重的二维联合分布，是正态分布。已知一个人的身高预测他的体重，和不知道身高预测体重，是完全不同的两个过程，一个是条件期望，一个是边缘分布的期望，因为两个随机变量相关，所以必然用条件期望更准确一些（误差更小一点）。\n Linear Combination 线性组合我们只证明一个定理：\n Theorem Linear Combination of Bivariate Normals.Suppose that two random variables $X_1$ and $X_2$ have a bivariate normal distribution ,for which the p.d.f is specified by Eq.(5.10.2).Let $Y=a_1X_1+a_2X_2+b$ ,where $a_1,a_2$ and $b$ are arbitrary given constants .Then $Y$ has the normal distribution with mean $a_1\\mu_1+a_2\\mu_2+b$ and variance $$ a_1^2\\sigma_1^2+a_2^2\\sigma_2^2+2a_1a_2\\rho\\sigma_1\\sigma_2 $$\n 当两个变量的联合分布是双变量正态分布的时，其和是一个正态分布，并且期望和方差满足上述关系。\n证明：\n 依据双变量正态分布的定义，我们可以用 $Z_1$ 和 $Z_2$ 的线性组合来表示 $X_1$ 和 $X_2$ 的线性组合。 $Z_1$ 和 $Z_2$ 独立（已知条件） $Y$ 可以表示成$Z_1$ 和 $Z_2$ 的线性组合 根据5.6中推论 Y还是正态分布，并且期望为： $$ \\begin{aligned} E(Y)\u0026amp;=a_1E(X_1)+a_2E(X_2)+b\\ \u0026amp;=a_1\\mu_1+a_2\\mu_2+b \\end{aligned} $$ 根据4.6 中的推论： $Var(Y)=a_1^2 Var(X_1)+a_2^2 Var(X_2)+2a_1a_2 Cov(X_1,X_2)$ 证毕  总结 给出了两个变量的正态分布的定义（这个定理中给出了双变量正态分布的所有有用性质的根本），双变量正态分布的性质的证明主要用到这个定理（本篇第一个定理），所以本篇第一个定理是关键中的关键。\n","permalink":"https://go.face2ai.com/math/math-probability-5-10-the-bivariate-normal-distributions.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍第一个多变量连续分布——双变量正态分布(\u003cfont color=\"ff0000\"\u003e本篇内有未证明定理，需要后续要补充\u003c/font\u003e )\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Bivariate Normal Distributions\u003c/p\u003e","title":"【概率论】5-10:二维正态分布(The Bivariate Normal Distributions)"},{"content":"Abstract: 本文介绍多项式分布的相关知识 Keywords: The Multinomial Distributions\n多项式分布 生病的时候才会体会到人生的短暂和生命的含义，你可以选择自己的生活，也可以选择自己的快乐，一切都是正确的。 本文开始介绍多于一个变量的分布，其实分布我们已经学了不少了后面再讲一个双变量的正态分布本章就算结束了，主要学的就是如何使用前面学到的工具来对新的随机变量的性质进行分析。今天我们来分析多项式分布。 多项式是二项分布的一个扩展。\n多项式分布的定义和导出 Definition and Derivation of Multinomial Distribution 把二项分布中的两个变量扩展成多个变量，就能得到我们我们今天要介绍的多项式分布，而且遵守和二项式分布一样的放回的采样方式（with replacement），在计数方法中我们也学过多项式系数这个知识，与我们今天要说的多项式分布是紧密相关的，比如我们举个例子： 人类的血型可以分为 A，B，o，AB 四种类型，每种类型都有相应的比例（这个比例是从所有人的类型中统计计算出来的）现在才去放回式的抽样，假设我们抽取了若干个样本，得到随机变量的向量为： $\\vec{x}=(X_A,X_B,X_o,X_{AB})$ 对应的概率为 $\\vec{p}=(p_A,p_B,p_o,p_{AB})$ 那么我们可以根据多项式系数的相关知识得到其分布： $$ f(\\vec{x}|4,\\vec{p})=Pr(X_A=x_1,X_B=x_2,X_o=x_3,X_{AB}=x_4)\\ =\\begin{cases} \\begin{pmatrix} \u0026amp;n\u0026amp;\\ x_1\u0026amp;x_2\u0026amp;x_3\u0026amp;x_4 \\end{pmatrix}p_A^{x_1}p_B^{x_2}p_o^{x_3}p_{AB}^{x_4}\u0026amp;\\text{if } x_1+x_2+x_3+x_4=n\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n这就是多项式系数的扩展，称为多项式分布的的样子，对应于多个随机变量，随机变量的个数为固定值。可以写成一下形式： $$ f(\\vec{x}|n,\\vec{p})= \\begin{cases} \\begin{pmatrix} \u0026amp;n\u0026amp;\\ x_1\u0026amp;\\dots\u0026amp;x_k \\end{pmatrix}p_1^{x_1}\\dots p_{k}^{x_k}\u0026amp;\\text{if } x_1+\\dots+x_k=n\\ 0\u0026amp;\\text{otherwise} \\end{cases}\\tag{5.9.1} $$\n Definition Multinomial Distributions.A discrete random vector $\\vec{X}=(X_1,\\dots,X_k)$ whose p.f. is given Eq(5.9.1) has the multinomial distribution with parameters $n$ and $\\vec{p}=(p_1,\\dots,p_k)$ .\n 这个定义看起来没什么，而且上面的例子也给出了多项式分布的一般用法，接下来我们就说说多项式分布和二项分布的关系。\n多项式分布和二项分布的关系 Relation between the Multinomial and Binomial Distributions  Theorem Suppose that the random vector $\\vec{X}=(X_1,X_2)$ has the multinomial distribution with parameters $n$ and $\\vec{p}=(p_1,p_2)$ .Then $X_1$ has the binomial distribution with parameters $n$ and $p_1$ ,and $X_2=n-X_1$\n 这个定理应该不需要证明了，因为多项式分布无论从定义来看还是原理来看，二项式是多项式的退化，多项式更加宽泛。 上面的定理可以轻易的推导出下面两个推论：\n Corollary Suppose that the random vector $\\vec{X}=(X_1,\\dots,X_k)$ has the multinomial distribution with parameters $n$ and $\\vec{p}=(p_1,\\dots,p_k)$ .The marginal distribution of each variable $X_i(i=1,\\dots,k)$ is the binomial distribution with parameters $n$ and $p$\n 这个推论比较好理解，一个多项式分布的边缘分布是其他变量所有可能值求和的结果，比如一个三个随机变量的多项式分布： $$ f_3(x_3)=\\sum_{\\text{all }x_1}\\sum_{\\text{all }x_2}f(x_1,x_2,x_3)\\text{ for }x_1+x_2+x_3=n $$ 那么可见 $x_3$ 的范围是从0到 $n$ 的其概率是 $p_3=1-p_1-p_2$ 明显的这是一个二项分布，$n=n$ 以及 $p=p_3$\n Corollary Suppose that the random vector $\\vec{X}=(X_1,\\dots,X_k)$ has the multinomial distribution with parameters $n$ and $\\vec{p}=(p_1,\\dots,p_k)$ with $k \u0026gt; 2$ .Let $\\ell\u0026lt;k$ ,and let $i_1,\\dots,i_{\\ell}$ be distinct elements of the set ${1,\\dots,k}$ .The distribution of $Y=X_{i_1}+\\dots+X_{i_{\\ell}}$ is the binomial distribution with parameters $n$ and $p_{i_1}+\\dots+p_{i_{\\ell}}$\n 这个推论的证明办法还是从二项分布出发，把本来分成多类的现在分成两类，比如一个分类可以把十个样本分为5类 ${1,2,3,4,5}$ 现在我们重新分，$A={1,2,3}$ 以及 $B={4,5}$ 那么新的分布就是二项分布 $p_A=p_1+p_2+p_3$ 以及 $p_B=p_4+p_5$\n多项分布也好，二项分布也好，其根本都是伯努利分布，所以我们在考虑这类分布之间的关系的时候可以退回到伯努利分布然后分析不同的试验过程，找出其不同点。\n均值，方差，协方差 Means,Variances and Covariances 接着就是多项式的数字特征了，因为其是多随机变量的分布，比前面讲的分布会多一个协方差(Covariances)\n Theorem Means,Variances,and Covariances.Let the random vector $X$ have the multinomial distribution with parameters $n$ and $p$ .The means and variances of the coordinates of $X$ are $$ E(X_i)=np_i\\text{ and } Var(X_i)=np_i(1-p_i)\\text{ for }i=1,\\dots,k $$ the covariances between the coordinates are $$ Cov(X_i,X_j)=-np_ip_j $$\n 证明过程我们只写最后一个协方差的计算，因为期望和方差我们可以通过前面的定理理解（其边缘分布就是二项分布），其期望和方差形式与二项分布相同。 协方差计算如下 $X_i+X_j$ 为一个随机变量，其他随机变量相加为另一个随机变量，那么新的分布是一个二项分布$p=p_i+p_j$ 以及$n=n$ 那么其分布是: $$ Var(X_i+X_j)=n(p_i+p_j)(1-p_i-p_j) $$ 根据前面学的有关方差的知识，有： $$ Var(X_i+X_j)=Var(X_i)+Var(X_j)-Cov(X_i,X_j)\\ =np_i(1-p_i)+np_j(1-p_j)-Cov(X_i,X_j) $$ 所以 $$ \\begin{aligned} n(p_i+p_j)(1-p_i-p_j)\u0026amp;=np_i(1-p_i)+np_j(1-p_j)-Cov(X_i,X_j)\\ Cov(X_i,X_j)\u0026amp;=-np_ip_j \\end{aligned} $$\n多项分布的协方差永远是负的，这是有道理的，因为多项分布的总数有限，所以当一个多了别人肯定就会少，这样的协方差肯定是负的。\n总结 今天我们研究了多项分布，第一个多随机变量的分布，明天我们将会学习双变量的正态分布。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-9-multinomial-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍多项式分布的相关知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Multinomial Distributions\u003c/p\u003e","title":"【概率论】5-9:多项式分布(The Multinomial Distributions)"},{"content":"Abstract: 本文介绍如何解决畅言在Hexo下无法同步移动端和PC端评论的问题 Keywords: Hexo，next，畅言\n解决畅言评论在移动端和PC端同步的问题 移动端的兴起使得互联网信息上的信息被告诉交换，随着博客的不断增多，评论也有若干个了，之前有个问题一直没解决就是pc端的评论移动端看不到，移动端的评论PC端也看不到，今天到了解决问题的时刻\n思路 首先我的文章生成的地址都是这个样子的： http://www.face2ai.com/Math-Set-Theory-1-Sample-Sets/ 默认情况下，next是吧url当做文章的标识，但是在移动端，或者有些情况下，url后面会增加某些参数，这样畅言后台就把他当做两篇文章了，所以我们根据畅言的帮助文档： 帮助中心SourceID的提示下，搜到了这篇仁兄的博文： https://segmentfault.com/a/1190000008091729 不过由于他的博客比较早，不适用于我们现在的next，我们进行如下修改\n/hexo/themes/next/layout/_partials/comments.swig 其中\n\u0026lt;div class=\u0026#34;comments\u0026#34; id=\u0026#34;comments\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;SOHUCS\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 修改为:\n\u0026lt;div class=\u0026#34;comments\u0026#34; id=\u0026#34;comments\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;SOHUCS\u0026#34; sid=\u0026#34;{{ page.title }}\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 这样畅言就会根据文章标题判断是不是一篇文章了，这样我们只要确保网站内没有同名文章就好了。\n总结 这样我们就能看到来自不同平台的评论了。\n原文地址：https://www.face2ai.com/other-Hexo-next-changyan转载请标明出处\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-hexo-next-changyan.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍如何解决畅言在Hexo下无法同步移动端和PC端评论的问题\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Hexo，next，畅言\u003c/p\u003e","title":"【Hexo】Hexo下next主题畅言评论同步问题的解决方案"},{"content":"Abstract: 本文介绍Beta分布的相关知识内容 Keywords: The Beta Distribution\nBeta分布 我们预测未来某件事情是否发生的主要依据是先验知识，于是我相信，自古流传至今的那些道理应该是值得信任的，人无信不立，立壁千仞无欲则刚，生于忧患死于安乐，这些所谓的被我曾经鄙视的大道理，现在看看，真的是值得我自己坚持的，我大中华文化几千年，流传出来的一定是被很多人验证过的先验知识，而现在这些有钱的爸爸总结出来的可能只适用于这个时代，想要追求真理，安全起见还是要多读古人的智慧。 本文继续在连续随机变量上进行探索，Gamma分布的随机变量是满足某些条件下的所有正实数，而我们今天要研究的beta族分布是分布在 \\([0,1]\\) 区间上的一种类型的连续分布。一个最常见的例子，是Bernoulli过程中对每次试验的成功概率的建模。 Bernoulli过程就是多次的独立的试验形成的一个结果序列，这个系列中每个随机变量的成功概率就可以用Beta分布来建模。 ## 贝塔函数 The Beta Function 和Gamma分布一样，Beta分布也是先有的Beta函数，先来看个例子，这个例子可以引出我们的Beta函数。 🌰 ： 一个机器制造零件，只有两种情况就是合格和不合格，不会出现第三种情况，我们让 \\(P\\) 表示不合格的零件占总零件的比例，假设我们得到了n个零件，其中X个不合格，我们假设在给定条件P下每个零件的合格与否条件独立，那么我们就能得出在3.6中的例子，对应这个例子，当给定 \\(X=x\\) 的条件下 \\(P\\) 的分布： \\[ g_2(p|x)=\\frac{p^x(1-p)^{n-x}}{\\int^{1}_{0}q^x(1-q)^{n-x}dx} \\text{ for }0\u0026lt;p\u0026lt;1 \\]\n这个p.d.f.就是我们今天要研究的主角，在研究完整分布之前，我们先来研究这个分母\n Definition The Beta Function .For each positive \\(\\alpha\\) and \\(\\beta\\) ,define: \\[ B(\\alpha,\\beta)=\\int^{1}_{0}x^{\\alpha-1}(1-x)^{\\beta-1}dx \\] the function B is called the beta function\n 所以上述就是beta函数的定义，也是上面例子中的分母的形式，可以看出beta函数中的 \\(\\alpha,\\beta \u0026gt; 0\\) 本文后面用到了3.9的一部分知识未在博客中体现，预计作为补充内容在下一部分给出，所以这个地方有些可以跳过。或者通过书本学习相关内容。\n Theorem For all \\(\\alpha,\\beta \u0026gt;0\\) , \\[ B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\]\n 这个命题的证明就用到了上面说的3.9的一部分选学内容，我们后面会给出相关证明，但是目前我们就当做此定理已经证明。\n贝塔分布的定义 Definition of the Beta Distributions 那么我们接下来就要定义Beta分布了。 \u0026gt;Definition Beta Distributions.Let \\(\\alpha ,\\beta \u0026gt;0\\) and let X be a random variable with p.d.f. \\[ f(x|\\alpha,\\beta)= \\begin{cases} \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\u0026amp;\\text{ for }0\u0026lt;x\u0026lt;1\\\\ 0\u0026amp;\\text{otherwise} \\end{cases}\\tag{5.8.3} \\]\n观察可以发现，如果 \\(\\alpha=1,\\beta=1\\) 那么5.8.3就是 \\([0,1]\\) 的均匀分布。\n举个🌰 ： 这个例子在西方社会可能比较常见，在我们这不流行这么落后的方法，资本主义国家迷路都是看指南针，看地图，我们是直接扔鞋，高效有特色！一天天选个举还要用模型预测，我口算都能算出来我们的选举结果。 从一个有79.1%墨西哥裔美国人的地区中选择220个陪审员，但是只有一百个陪审员是墨西哥裔，根据二项随机变量X的期望值是 \\(E(X)=220\\times 0.791=174.02\\) 。显然这比100多了不少。当然出现174个墨西哥裔的陪审员并不是必须的，也是概率的，因为 X可以为 [0,220] 中的任意数字。我们令 P 为墨西哥裔陪审员的比例。法庭假设X 在条件 \\(P=p\\) 上一个二项分布，参数 n=220 和 p ，我们比较感兴趣是否P小于0.791，我们现在假设存在种族歧视（墨西哥裔陪审员比例小于0.791）比如我们认为选择系统存在一个0.8的偏移，也就是 \\(P\u0026lt;0.8\\times0.791=0.6328\\) 那么我们要计算的就是当给定 \\(X=100\\) 时 \\(P\\leq 0.6328\\) 的条件概率\n解： 假设P的分布在得到X前已经被确定（比如选举系统被人做了手脚），那么我们把它假设成一个beta分布，参数为 \\(\\alpha,\\beta\\) ,那么 \\(P\\) 的p.d.f.是： \\[ f_2(p)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\text{ , for }0\u0026lt;x\u0026lt;1 \\] X在给定P=p条件下的概率函数： \\[ g_1(x|p)=\\begin{pmatrix}200\\\\x\\end{pmatrix}p^x(1-p)^{220-x}\\text{, for }x=0,\\dots,220 \\]\n然后我们用伟大的贝叶斯公式来X=100 条件下的P的概率： \\[ \\begin{aligned} g_2(p|100)\u0026amp;=\\frac{\\begin{pmatrix}220\\\\100\\end{pmatrix}p^{100}(1-p)^{120} \\cdot \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}}{f_1(100)}\\\\ \u0026amp;=\\frac{\\begin{pmatrix}220\\\\100\\end{pmatrix}\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)f_1(100)}p^{\\alpha+100-1}(1-p)^{\\beta+120-1} \\end{aligned} \\]\n上面结果可以看出来左半部分就是个数，右半部分才含有变量，并且这个形状，很明显，还是一个beta分布，然后我们选择参数值就可以知道这个 \\(Pr(P\\leq 0.6328|X=100)\\) 的分布了，而这个参数选择要在我们徐汇了beta分布的期望求法以后才能知道怎么选择参数。\n Theorem Suppose that \\(P\\) has the beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\) ,and the conditional distribution of \\(X\\) given \\(P=p\\) is the binomial distribution with parameters \\(n\\) and \\(p\\) .Then the conditional distribution of \\(P\\) given \\(X=x\\) is the beta distribution with parameters \\(\\alpha+x\\) and \\(\\beta+n-x\\)\n 这个定理上面我们的例子中已经用事实证明了可行，但是并没有严谨的证明，所以这个定理是未严格证明的定理。 ## 贝塔分布的距 Moments of Beta Distributions \u0026gt;Theorem Moments.Suppose that X has the beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\) .Then for each positive integer k, \\[ E(X^k)=\\frac{\\alpha(\\alpha+1)\\dots(\\alpha+k-1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)\\dots(\\alpha+\\beta+k-1)} \\] In particular, \\[ E(X)=\\frac{\\alpha}{\\alpha+\\beta},\\\\ Var(X)=\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\]\n证明： For \\(k=1,2,\\dots\\) \\[ \\begin{aligned} E(X^k)\u0026amp;=\\int^{1}_{0}x^kf(x|\\alpha,\\beta)dx\\\\ \u0026amp;=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\int^{1}_{0}x^{\\alpha+k-1}(1-x)^{\\beta-1}dx \\end{aligned} \\] 根据公式 5.8.2 \\[ E(X^k)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\cdot\\frac{\\Gamma(\\alpha+k)\\Gamma(\\beta)}{\\Gamma(\\alpha+k+\\beta)} \\] 简化之后就是定理中形状了，证毕。\nbeta分布有很多不同参数组合形式，计算器均值和c.d.f.是非常有用技能。\n在选择参数之前，我们必须明确Beta分布一般来建模概率的分布，0到1之间的分布，如果其中某个概率出现的比较大，那么分布在图像上会给出一个峰值，并且Beta分布的图像大致如下: 均值就是峰值的位置。 接着我们把参数改一下，看看有什么变化 可见，在均值不变的情况，增大 \\(\\alpha\\) 和 \\(\\beta\\) 的值，分布变高变瘦了。\n还要继续上面的例子，简单的概括一下上面的例子，首先，我们感兴趣的是一个概率的概率，而研究概率的办法是研究分布，也就是概率的分布，我们用beta 分布来建模这个概率，然后我们做试验来验证我们之前猜测概率也好，希望的概率也好，验证他们是否合理，根据上面选陪审员的例子，我们的目的就是为了验证有没有种族歧视，因为墨西哥裔占总人口数为 \\(79.1%\\) ，而只选择出了100人，理论上应该选择出174.02 人，我们想知道当我们选择出100人的条件下，是否还是公平的，用概率为\\(79.1%\\) 的参数去抽取了，还是用 \\(79.1%\\times 0.8\\) 或者更夸张的参数选取的。根据上面例子中我们已经求出了条件概率，接下来我们研究选择什么样的 \\(\\alpha\\) 和 \\(\\beta\\) 来准确的计算这个概率。 首先我们先来看原始分布（不是 \\(g_2(p|100)\\) 条件分布）原始分布我们希望的是对所有人平等的，所以根据期望来计算，这个分布的期望必然是 0.791 ，这样才是公平的，根据beta分布的数字特征，我们能计算出： \\[ E(X)=\\frac{\\alpha}{\\alpha+\\beta}=0.791\\Rightarrow \\alpha=3.785\\beta \\] 这个关系是我们最基本的性质，所以在条件情况 \\(g_2(p|100)\\) 下的参数也应该满足这个关系，\\(g_2\\) 参数为 \\(\\alpha+100\\) 和 \\(\\beta+120\\) 那么我们就能得出一个系列的不同参数的 \\(g_2\\) 但是这不好研究，因为 \\(p\\) 是自变量，还有 \\(\\beta\\) (或者 \\(\\alpha\\) ) 两个变量，所以我们来看当 \\(p\u0026lt; 0.791\\times 0.8 = 0.6328\\) 的时候各 \\(\\beta\\) 对这个条件分布的相互关系： 因为当 \\(p\u0026lt;0.6328\\) 就相当于非常歧视了，所以我们必须让这个概率低，怎么也要低于0.5 那么对应的 \\(\\beta\\) 就要选至少 51.5 ，此时 \\(\\alpha\\) 为 194.9 这个时候如果我们把 \\(\\beta=51.5,\\alpha=194.9\\) 作为参数带回到原始我们假设的 \\(p\\) 的分布，得到 \\(P(X=100)=3.28\\times 10^{-8}\\) 这也就意味着，我们原始的关于均值是0.791的beta分布，发生220个陪审员中有100个墨西哥裔的概率是 \\(3.28\\times 10^{-8}\\) 基本为0，所以这里面肯定有不公平！ 总结 这篇文章写了三天，原因是昨天胃肠炎发烧了，所以如果有点不连贯，请大家谅解，重点是例子，注意，重点是例子。\n","permalink":"https://go.face2ai.com/math/math-probability-5-8-the-beta-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍Beta分布的相关知识内容 \u003cstrong\u003eKeywords:\u003c/strong\u003e The Beta Distribution\u003c/p\u003e","title":"【概率论】5-8:Beta分布(The Beta Distributions)"},{"content":"Abstract: 本文介绍Gamma分布相关知识的第二部分指数分布 Keywords: The Exponential Distributions\nGama分布 怀疑是我们学习路上最大的绊脚石，因为学习完很多东西都不是立刻就能变现的，或者能直接在生活中能体现出变化的。所以能看长线的人，才适合进行长期学习，量变到质变的过程，而我们最重要的就是排除万难，坚定信念，至于结果如何，我相信古人的总结。 本文介绍的Gamma分布知识的第二部分有一个自己的名字，叫做指数分布，Gamma分布之所以叫Gamma分布是因为其中包含Gamma函数，而其中某个参数的特殊化产生的新分布，就是我们今天要学习的指数分布（The Exponential Distribution）。 指数分布一般用来建模等待时间等情况下的概率模型。 ## 指数分布 The Exponential Distribution 我们上面一篇大讲特讲的服务时间例子，就是一个典型的等待时间的情况，所以，本文介绍的分布族可以用来进行建模 \u0026gt;Definition Exponential Distributions.Let \\(\\beta \u0026gt;0\\) .A random variable \\(X\\) has the exponential distribution with parameter \\(\\beta\\) if \\(X\\) has a continuous distribution with the p.d.f. \\[ f(x\\beta)= \\begin{cases} \\beta e^{-\\beta x}\u0026amp; \\text{ for }x\u0026gt;0\\\\ 0\u0026amp;\\text{for} x\\leq 0 \\end{cases} \\]\n这就是指数分布的定义，还是想之前说的，定义就是对这个在什么情况下分布是什么样子的进行定义，然后我们接下来要做的是通过这个定义能产生多少相关的定理结论。 作为对比，我们来列出Gamma分布，进行对比就能清楚地知道一些性质了，Gamma分布\n Gamma Distributions.Let \\(\\alpha\\) and \\(\\beta\\) be positive numbers.A random variable \\(X\\) has the gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if \\(X\\) has a continuous distribution for which the p.d.f. is \\[ f(x|\\alpha,\\beta)= \\begin{cases} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\u0026amp;\\text{ for } x\u0026gt;0\\\\ 0\u0026amp;\\text{otherwise} \\end{cases} \\] \n因为Gamma函数在 \\(\\Gamma(1)=1\\) 所以，指数分布就是Gamma分布 \\(\\alpha=1\\) 的时候产生的分布。\n Theorem The exponential distribution with parameter \\(\\beta\\) is the same as the gamma distribution with parameters \\(\\alpha=1\\) and \\(\\beta\\) .If \\(X\\) has the exponential distribution with parameter \\(\\beta\\) ,then \\[ E(X)=\\frac{1}{\\beta}\\text{ and } Var(X)=\\frac{1}{\\beta^2} \\] and m.g.f. of \\(X\\) is \\[ \\Psi(t)=\\frac{\\beta}{\\beta-t}\\text{ for } t\u0026lt;\\beta \\]\n 这个证明就不需要了，因为完全就是Gamma分布的代数结果，所以我们还是看点有用的性质吧，比如说，无记忆性。\n Theorem Memoryless Property of Exponential Distributions.Let \\(X\\) have the exponential distribution with parameters \\(\\beta\\) ,and let \\(t\u0026gt;0\\) .Then for every number \\(h\u0026gt;0\\) , \\[ Pr(X\\geq t+h|X\\geq t)=Pr(X\\geq h) \\]\n 证明: 对于每一个 \\(t\u0026gt;0\\) , \\[ Pr(X\\geq t)=\\int^{\\infty}_{t}\\beta e^{-\\beta x}dx=e^{-\\beta t}\\tag{5.7.19} \\] 那么对于每一个 \\(t \u0026gt; 0\\) 以及 \\(h \u0026gt; 0\\) 我们有 \\[ \\begin{aligned} Pr(X\\geq t+h|X\\geq t)\u0026amp;=\\frac{Pr(X\\geq t+h)}{Pr(X\\geq t)}\\\\ \u0026amp;=\\frac{e^{-\\beta(t+h)}}{e^{-\\beta t}}=e^{-\\beta h}=Pr(X\\geq h) \\end{aligned}\\tag{5.7.20} \\]\n证明过程只用到了条件分布的求法，其他基本没有任何难度，这个结论是显然成立的。\n值得说明的是，指数分布是在连续随机变量里面唯一一个就有无记忆性的分布；为了说明无记忆性，我们来看看思路：假设 \\(X\\) 表示某个任务触发前的等待时间，根据式5.7.20 如果从时间0开始计时前t个时间单位没有任务触发，那么在接下来 \\(h\\) 个时间单位触发任务的概率为 \\(e^{-\\beta h}\\) 这个概率和从时间 \\(0\\) 开始计时前 \\(h\\) 个时间周期触发任务的概率一致.\n但是必须要说明的是，无记忆性并不只是适合所有场景，比如假设 \\(X\\) 是 一个 灯泡的的寿命，就是他坏掉之前持续点亮的时间，这个灯泡以后能点亮的时间完全取决于已经点亮了多久,因此单一灯泡不具有无记忆性，但是指数分布可以很好的近似产品或者零件的寿命。 下面我们就看看如何使用指数分布来建模使用寿命。\n使用寿命测试 Life Tests 🌰 ： 假设有 \\(n\\) 个灯泡点亮，来模拟检测他们的使用寿命，我们假设他们每个的使用寿命相互独立，并且有相同的分布，参数为 \\(\\beta\\) 的指数分布，换句话说，如果 \\(X_i\\) 定义第 \\(i\\) 个灯泡的寿命 \\(i=1,2,\\dots,n\\) 然后假设他们是i.i.d的，那么第一个问题，就是我们假设 \\(Y_1\\) 是 \\(n\\) 个灯泡中第一个坏掉的灯泡点亮的时间，那么其分布是什么样的？那么第二个坏掉的灯泡点亮的时间 \\(Y_2\\) 又是怎么分布的呢？ 分析：例子中 \\(Y_1\\) 的分布式 \\(n\\) 个指数分布随机变量中最小的，那么其分布应该比较容易计算。\n Theorem Suppose that the variables \\(X_1,\\dots,X_n\\) form a random sample from the exponential distribution with parameter \\(\\beta\\) .Then the distribution of \\(Y_1=min{X_1,\\dots,X_n}\\) will be the exponential distribution with parameter \\(n\\beta\\)\n 定理说，n个独立同分布的指数分布的随机变量中最小的那个的新随机变量的分布是参数是 \\(n\\beta\\) 的指数分布。\n证明： 对于每一个 \\(t\u0026gt;0\\) 那么： \\[ \\begin{aligned} Pr(Y_1\u0026gt;t)\u0026amp;=Pr(X_1\u0026gt;t,\\dots,X_n\u0026gt;t)\\\\ \u0026amp;=Pr(X_1\u0026gt;t)\\dots Pr(X_n\u0026gt;t)\\\\ \u0026amp;=e^{-\\beta t}\\dots e^{-\\beta t}=e^{-n\\beta t} \\end{aligned} \\] 根据计算过程5.7.19可以比较轻松地得到上述结论。\n根据无记忆性，对于例子中 \\(Y_2\\) 的求法相当于从n个灯泡中有一个已经坏了的情况下，从新开始进行指数分布，也就是说当第一个灯泡坏了以后，我们重新开始进行试验，此时的时间归为0，那么参数变成了 \\(n-1\\) 个灯泡，我们假设其中 第 \\(j\\) 个灯泡先坏掉( \\(1\u0026lt;j\u0026lt;n\\) ) 那么第二个坏掉的分布就是参数为 \\((n-1)\\beta\\) 的指数分布。\n那么我们接下来就有研究每次灯泡熄灭之间的时间间隔了。 \u0026gt;Theorem Suppose that the variables \\(X_1,\\dots,X_n\\) form a random sample from the exponential distribution with parameters \\(\\beta\\) .Let \\(Z_1\\leq Z_2\\dots \\leq Z_n\\) be the random variables \\(X_1,\\dots,X_n\\) sorted from smallest to largest.For each \\(k=2,\\dots,n\\) ,let \\(Y_k=Z_k-Z_{k-1}\\) ,Then the distribution of \\(Y_k\\) is the exponential distribution with parameter \\((n+1-k)\\beta\\)\n上述定理说明指数分布的随机变量 \\(X_1,\\dots,X_n\\) 有参数 \\(\\beta\\) 那么假设 \\(Z_1\\leq Z_2\\dots \\leq Z_n\\) 是 \\(X_1,\\dots,X_n\\) 从小到大的排列 ，对于每一个 \\(Y_k=Z_k-Z_{k-1}\\) 其中\\(k=2,\\dots,n\\) 那么 \\(Y_k\\)也是指数分布，并且其参数是 \\((n+1-k)\\beta\\) 。\n这个看起来就有点神奇了，但是证明过后发现，确实如此。 证明： 在时间 \\(Z_{k-1}\\) ，有 \\(k-1\\) 个寿命已经结束了，有 \\(n+1-k\\) 个还没坏，那么根据上一个例子，剩下的活着的的依然遵守无记忆性性质，依然服从指数分布，其参数为 \\(\\beta\\) ，所以 \\(Y_k=Z_k- Z_{k-1}\\) 也是活着的最小寿命时间分布，还是参数为 \\(\\beta\\) 的指数分布，只不过试验总数变成了 \\(n+1-k\\) 个，所以根据定理 5.7.10 指数分布参数为 \\((n+1-k)\\beta\\) 接下来我们研究一下，泊松过程和指数分布之间的关系。 ## 指数分布和泊松过程的关系 Relation to Poisson Process 回顾下泊松分布的提出，当时提出泊松分布的情况是为了计算某段时间内到达商店的顾客数量，当时假定平均每个小时有多少人，然后按照伯努利过程抽象最小时间单位比如秒内是否有人来，然后得出一段时的二项分布，发现二项分布太麻烦，于是用泊松分布来近似此情况下的二项分布，用于建模某段时间内的客户数量，但是，当我们想要知道每两个相邻进店的客户之间间隔了多少时间，这时候就可以用指数分布来进行建模了\n Theorem 5.7.12 Times between Arrivales in a Poisson Process.Suppose that arrivals occur according a Poisson process with rate \\(\\beta\\) .Let \\(Z_k\\) be the time until the \\(k\\) th arrival for \\(k=1,2,\\dots\\) .Define \\(Y_1=Z_1\\) and \\(Y_k=Z_k-Z_{k-1}\\) for \\(k\\geq 2\\) Then \\(Y_1,Y_2,\\dots\\) are i.i.d. and they each have the exponential distribution with parameter \\(\\beta\\)\n 证明，假设泊松分布的变量为 X ，那么当 \\(Y_1\\leq t\\) 时对应的 \\(X\\geq 1\\) 应该等于 \\(1-Pr(X=0)=1-e^{-\\beta t}\\) 可以看出上式是指数分布的c.d.f。其参数是 \\(\\beta\\)\n泊松过程中，两个到达之间的时间可以用指数分布来建模。\n根据定理5.7.7 和 5.7.12 可以得出以下推论。 \u0026gt;Corollary Time until \\(k\\) th Arrival. In the situation of Theorem 5.7.12,the distribution of \\(Z_k\\) is the gamma distribution with paramters \\(k\\) and \\(\\beta\\)\n总结 本文在上文Gamma分布上进行特例，得到一中经常由于建模使用寿命，或者泊松过程时间间隔的分布——指数分布。 下一篇我们继续啊在Gamma分布的基础上构建其他类型的分布。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-7-the-gamma-distributions-p2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍Gamma分布相关知识的第二部分指数分布 \u003cstrong\u003eKeywords:\u003c/strong\u003e The Exponential Distributions\u003c/p\u003e","title":"【概率论】5-7:Gama分布(The Gamma Distributions Part II)"},{"content":"Abstract: 本文介绍Gamma函数和Gamma分布，本课第二部分介绍指数分布 Keywords: The Gamma Distributions\nGama分布 今天的废话就是如果看书的时候没看透彻，写博客的时候就会不知所言，所以一定要学透了再总结，没学好就总结，会逻辑混乱 本文介绍了另一个非常有用的连续随机变量的分布族——Gamma分布，学习Gamma分布的适用场景和部分性质，以及一个贯穿始终的例子，排队时间，排队不只是人的排队，在计算机高性能计算，比如CUDA中，任务的排队也是有的，所以这个模型适用场景还是比较多的，虽然可能不如正态分布在自然界中那么普遍，但是在正随机变量中，Gamma分布族在连续分布中举足轻重。 ## 伽马函数 The Gamma Function 在提出Gamma分布之前，我们先来认识一个非常有趣的函数，这个函数叫做Gamma函数。 首先来看个例子： 我们在给一灯泡的寿命建模，根据我们经验，这个灯泡的寿命越长，发生的概率越小，时间越短，则概率越高，寿命是0的我们不考虑，我们只考虑从0开始但不包括零，我们用下面的这个模型建模是合理的，之所以说是合理的而不是唯一的，是因为这个模型不具备唯一性：\n\\[ f(x)= \\begin{cases} e^{-x}\u0026amp;\\text{for} x\u0026gt;0\\\\ 0\u0026amp;\\text{otherwise} \\end{cases} \\]\n我们在没有大量数据或者试验情况下无法验证模型正确性，但是从目前来看好像和我们知道的先验知识吻合，所以我们就假定其是合理的，然后我们求这个灯泡的均值和方差：\n\\[ E(X)=\\int^{\\infty}_{0}xe^{-x}dx\\\\ Var(X)=\\int^{\\infty}_{0}x^2e^{-x}dx \\]\n注意，第二个方差的计算我觉都有点问题，因为按照这个积分，是把均值当做 \\(\\mu=0\\) 来计算的，但是均值是从0到正无穷的积分，所以均值不会是0，所以这个方差公式我们留意一下（如果有人知道我哪错了，可以给我留言，谢谢） 这个均值的计算是一个有趣的函数。 我们来回忆一下，我们的函数都是什么样子的，我们目前学过的函数大多数都是由初等函数经过计算得到的，比如 \\(e^{x^2+\\alpha sin(y)}\\) 是指数计算组合了多项式和三角函数得到的一个新函数，当然，我们学了积分，微分运算后，我们可以用积分来生成新的函数，比如，我们把上面求均值的积分，定义为一个新函数，这个函数叫做Gamma函数 \u0026gt;Definition 5.7.1 The Gamma Function.For each positive number \\(\\alpha\\) ,let the value \\(\\Gamma(\\alpha)\\) be defined by the following integral: \\[ \\Gamma(\\alpha)=\\int^{\\infty}_{0}x^{\\alpha-1}e^{-x}dx=1 \\] The function \\(\\Gamma\\) defined by Eq.(5.7.1) for \\(\\alpha\u0026gt;0\\) is called the gamma function.\n这就是Gamma函数的定义，这个希腊字母 \\(\\Gamma\\) 读作 “Gamma” 注意，这个函数的自变量是 \\(\\alpha\\) 而 \\(x\\) 只是积分中的一个哑变量，没作用，可以写作任何变量。 在举个🌰 ： \\[ \\Gamma(1)=\\int^{\\infty}_{0}x^{1-1}e^{-x}dx=1 \\]\n上面的例子和接下来内容都说明了 \\(\\Gamma(\\alpha)\\) 对于所有 \\(\\alpha\u0026gt;0\\) 都是有限的。\n Theorem 5.7.1 If \\(\\alpha\u0026gt;1\\) then \\[ \\Gamma(\\alpha)=(\\alpha-1)\\Gamma(\\alpha-1) \\]\n 证明一下这个性质： 因为 \\(\\Gamma\\) 是用积分定义的，所以我们现在用分部积分来计算产生上面的定理： 我们令 \\(u=x^{\\alpha-1}\\) 以及 \\(v=-e^{-x}\\) 那么我们就会有 \\(du=(\\alpha-1)x^{\\alpha-2}dx\\) 以及 \\(dv=e^{-x}dx\\) 那么就有 \\[ \\begin{aligned} \\Gamma(\\alpha)\u0026amp;=\\int^{\\infty}_{0}udv=[uv]^{\\infty}_{0}-\\int^{\\infty}_{0}vdu\\\\ \u0026amp;=[-x^{\\alpha-1}e^{-x}]^{\\infty}_{x=0}+(\\alpha-1)\\int^{\\infty}_{0}x^{\\alpha-2}e^{-x}dx\\\\ \u0026amp;=0+(\\alpha-1)\\Gamma(\\alpha-1) \\end{aligned} \\] 使用分部积分法就把原始表达式转化成了一个乘法的递归式。从而证明了结论。进一步根据此性质能推导出下面这条关键性质。\n Theorem 5.7.2 For every positive integer \\(n\\) , \\[ \\Gamma(n)=(n-1)! \\]\n 证明： 对于\\(n\\geq 2\\) 我们有： \\[ \\begin{aligned} \\Gamma(n)\u0026amp;=(n-1)\\Gamma(n-1)=(n-1)(n-2)\\Gamma(n-2)\\\\ \u0026amp;=(n-1)(n-2)\\dots1\\cdots \\Gamma(1)\\\\ \u0026amp;=(n-1)! \\end{aligned} \\] 因为我们在上面的例子中知道 \\(\\Gamma(1)=1=0!\\) 所以结论正确。 证毕\n在实际的统计应用中 \\(\\alpha\\) 是正整数，或者 \\(\\alpha=n+\\frac{1}{2}\\) 其中 \\(n\\) 是正整数。 我们来计算下 \\(\\Gamma(n+\\frac{1}{2})=(n-\\frac{1}{2})(n-\\frac{3}{2})\\dots\\frac{1}{2}\\Gamma(\\frac{1}{2})\\) 这个式子的关键是最后那个积分的部分，令 \\(x=(1/2)y^2\\) 那么 \\(dx=ydy\\)： \\[ \\Gamma(\\frac{1}{2})=2^{\\frac{1}{2}}\\int^{\\infty}_{0}e^{(-\\frac{1}{2})y^2}dy \\] 代换后里面积分的形状和正态分布一致，但是积分范围不同，正态分布的积分形式是 \\(\\int^{\\infty}_{-\\infty}e^{(-\\frac{1}{2})y^2}dy=(2\\pi)^\\frac{1}{2}\\)，所以积分里面的结果是正态分布的一半 \\((\\frac{\\pi}{2})^{\\frac{1}{2}}\\) 然后有根据正态分布的性质，上面的最终结果是 \\[ \\Gamma(\\frac{1}{2})=\\pi^{\\frac{1}{2}} \\]\n这就是Gamma函数的一些典型用法和数学特性，接下来的定理帮我们提出后面的Gamma分布。 \u0026gt;Theorem 5.7.3 For each \\(\\alpha\u0026gt;0\\) and each \\(\\beta\u0026gt;0\\) , \\[ \\int^{\\infty}_{0}x^{\\alpha-1}e^{-\\beta x}dx=\\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}} \\]\n为了证明上面的定理成立，我们还是使用变量代换的方法 \\(-y=-\\beta x\\) 于是 \\(x=\\frac{y}{\\beta}\\) 以及 \\(dx=\\frac{dy}{\\beta}\\) 然后根据定义5.7.1 \\(\\Gamma(\\alpha)=\\int^{\\infty}_{0}x^{\\alpha-1}e^{-x}dx\\) 得到结论 \\[ \\begin{aligned} \u0026amp;\\int^{\\infty}_{0}x^{\\alpha-1}e^{-\\beta x}dx\\\\ \u0026amp;=\\frac{1}{\\beta}\\int^{\\infty}_{0}(\\frac{y}{\\beta})^{\\alpha-1}e^{-y}dy\\\\ \u0026amp;=\\frac{1}{\\beta^\\alpha}\\int^{\\infty}_{0}y^{\\alpha-1}e^{-y}dy\\\\ \u0026amp;=\\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}} \\end{aligned} \\] 本定理是对定义5.7.1的一个扩展，把 \\(e\\) 的指数从 \\(-x\\) 扩展到了 \\(\\beta x\\) 原书上给出的证明过程有些问题，从第二步到第三步似乎缺少一个负号，或者定理写的就有有问题，缺少一个负号，因为积分中 \\(x^{\\alpha-1}\\) 积分结果是 \\(\\infty\\) 然后 \\(e^{\\beta}x\\) 因为 \\(\\beta\u0026gt;0\\) 所以这个积分结果也是 \\(\\infty\\) 所以我认为这个公式应该是少了一个负号。(个人认为《Probability and Statistics 4th》影印版 P318 存在错误，所以我给他补上了一个负号，如果我错了，请及时指出)\n然后我们继续拿出前面提到的Stirling 公式的另一个版本。 \u0026gt;Theorem 5.7.4 Stirling’s formula: \\[ lim_{x\\to \\infty}\\frac{(2\\pi)^{1/2}x^{x-1/2}e^{-x}}{\\Gamma(x)}=1 \\]\n这个证明没有给出，我尝试的证明了一下，确实有点困难，所以我们就假装我们会证明就行了。\n接下来这个例子非常给力了，会给出Gamma分布的实际用途： 假设一个队列里有 \\(i=1,2,\\dots\\) 个顾客，当这个顾客开始被服务时，需要 \\(X_i\\) 个单位的时间，我们假设 \\(Z\\) 是这些顾客的平均速度，我们要建立个模型是在 \\(Z=z\\) 的条件下，\\(X_1,\\dots,X_n\\) 的分布(我们的目标)，其中\\(X_1,\\dots,X_n\\) 是在 \\(Z=z\\) 条件下的i.i.d（也就是独立同分布）.每个 \\(X_i\\) 的p.d.f. 为 \\(g_1(x_i|z)=z e^{-zx_i}\\) 其中 \\(x_i\u0026gt;0\\) 假设 \\(Z\\) 同样是一个随机变量有分布 \\(f_2(z)=2e^{-2z}\\) 其中 \\(z\u0026gt;0\\) 那么 \\(X_1,\\dots,X_n,Z\\) 满足以下关系（i.i.d.的联合分布等于其边缘分布的乘积，条件情况下依旧成立,前面学过的） \\[ \\begin{aligned} f(x_1,\\dots,x_n,z)\u0026amp;=[\\Pi^{n}_{i=1}g_1(x_i|z)]f_2(z)\\\\ \u0026amp;=2z^ne^{-z[2+x_1+\\dots+x_n]} \\end{aligned} \\] 上式为5.7.11\n我们关心的是\\(X_1,\\dots,X_n\\) 的分布，\\(Z\\) 在这里只是个条件，所以我们要对 \\(z\\) 进行积分，结合定理5.7.3 \\(\\int^{\\infty}_{0}x^{\\alpha-1}e^{-\\beta x}dx=\\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}}\\) 让 \\(\\alpha=n+1\\) 以及 \\(\\beta=2+x_1+\\dots+x_n\\) 。再加上定理 5.7.2: \\(\\Gamma(n)=(n-1)!\\) 整合 \\(2z^ne^{-z[2+x_1+\\dots+x_n]}\\) 。就能得到： \\[ \\begin{aligned} f_n(x_1,\\dots,x_n|z)\u0026amp;=\\int^{\\infty}_{0}f(x_1,\\dots,x_n,z)dz\\\\ \u0026amp;=\\frac{2(n!)}{(2+\\sum^{n}_{i=1}x_i)^{n+1}} \\end{aligned} \\]\n上式为5.7.12 当 \\(x_i\u0026gt;0\\) 时为上面的情况，其他情况为0（我们不要纠结 \\(g_1(x_i|z)=z e^{-zx_i}\\) 和 \\(f_2(z)=2e^{-2z}\\) 是哪里来的）\n接下来我们开始Gamma分布，在Gamma分布之前请务必把上面的例子看明白。 ## 伽马分布 The Gamma Distributions 继续上面的例子，我们假设我们得到了n个顾客的服务时间，我们可以很轻松的计算出每个客户的服务速度 \\(Z\\) 我们可以得到 \\(X_1,\\dots,X_n\\) 条件下 \\(Z\\) 的条件分布 \\(g_2(z|x_1,\\dots,x_n)\\) 我们用5.7.11除以5.7.12 并且其中令 \\(y=2+\\sum^{n}_{i=1}x_i\\) 我们就能得到： \\[ g_2(z|x_1,\\dots,x_n)= \\begin{cases} \\frac{y^{n+1}}{n!}e^{-yz}\u0026amp;\\text{for }z\u0026gt;0\\\\ 0\u0026amp;\\text{otherwise} \\end{cases} \\]\n这个例子中的p.d.f.就是我们今天要研究的对象，Gamma分布。\n Definition 5.7.2 Gamma Distributions.Let \\(\\alpha\\) and \\(\\beta\\) be positive numbers.A random variable \\(X\\) has the gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if \\(X\\) has a continuous distribution for which the p.d.f. is \\[ f(x|\\alpha,\\beta)= \\begin{cases} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\u0026amp;\\text{ for } x\u0026gt;0\\\\ 0\u0026amp;\\text{otherwise} \\end{cases} \\]\n 根据定理5.7.3 结果是1. Gamma分布长这个样子 接着我们研究一下gamma分布的距，进而能得到均值和方差。\n Theorem 5.7.5 Moments.Let \\(X\\) have the gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\) For \\(k=1,2,\\dots\\) \\[ E(X^k)=\\frac{\\Gamma(\\alpha+k)}{\\beta^k\\Gamma(\\alpha)}=\\frac{\\alpha(\\alpha+1)\\dots(\\alpha+k-1)}{\\beta^k} \\] In particular \\(E(X)=\\frac{\\alpha}{\\beta}\\) , and \\(Var(X)=\\frac{\\alpha}{\\beta^2}\\)\n 证明： \\(k=1,2,\\dots\\) 然后我们有 \\[ \\begin{aligned} E(X^k)\u0026amp;=\\int^{\\infty}_{0}x^kf(x|\\alpha,\\beta)dx\\\\ \u0026amp;=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int^{\\infty}_{0}x^{\\alpha+k-1}e^{-\\beta x}dx\\\\ \u0026amp;=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\cdot \\frac{\\Gamma(\\alpha+k)}{\\beta^{\\alpha+k}}\\\\ \u0026amp;=\\frac{\\Gamma(\\alpha+k)}{\\beta^k\\Gamma(\\alpha)} \\end{aligned} \\] 然后就能得到结论了，包括均值和方差。\n Theorem 5.7.6 Moment Generating Function.Let \\(X\\) have the gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\) .The m.g.f. of \\(X\\) is \\[ \\psi(t)=(\\frac{\\beta}{\\beta-t})^\\alpha \\text{ for }t\u0026lt;\\beta \\]\n 证明如下： \\[ \\psi(t)=\\int^{\\infty}_{0}e^{tx}f(x|\\alpha,\\beta)dx=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int^{\\infty}_{0}x^{\\alpha-1}e^{-(\\beta-t)x}dx \\] 上述积分在 \\(t\u0026lt;\\beta\\) 时是有限的，那么当 \\(t\u0026lt;\\beta\\) 时： \\[ \\psi(t)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\cdot\\frac{\\Gamma(\\alpha)}{(\\beta-t)^{\\alpha}}=(\\frac{\\beta}{\\beta-t})^\\alpha \\]\n然后就能得到结论，有同样参数 \\(\\beta\\) 的gamma分布的独立的随机变量，他们的和还是gamma分布。\n Theorem 5.7.7 If the random varibale \\(X_1,\\dots,X_n\\) are independent,and if \\(X_i\\) has the gamma distribution with parameters \\(\\alpha_i\\) and \\(\\beta(i=1,\\dots,k)\\) ,then the sum \\(X_1+\\dots+X_k\\) has the gamma distribution with parameters \\(\\alpha_1+\\dots+\\alpha_k\\) and \\(\\beta\\)\n 根据m.g.f.的性质，独立的随机变量的和的m.g.f.是其m.g.f.的积，那么我们证明如下： \\[ \\psi_i(t)=(\\frac{\\beta}{\\beta-t})^{\\alpha_i} \\text{ for }t\u0026lt;\\beta\\\\ \\psi(t)=\\Pi^{k}_{i=1}\\psi_i(t)=(\\frac{\\beta}{\\beta-t})^{\\alpha_1+\\dots+\\alpha_k} \\text{ for }t\u0026lt;\\beta \\] 那么我们就可以看出这个 \\(\\psi\\) 对应的也是gamma分布，其中 \\(\\alpha=\\alpha_1+\\dots+\\alpha_k\\) 并且 \\(\\beta\\) 不变。\n总结 至此我们大概的学习了一下 Gamma分布，下一篇我们继续从Gamma上扩展出指数分布。 待续。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-7-the-gamma-distributions-p1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍Gamma函数和Gamma分布，本课第二部分介绍指数分布 \u003cstrong\u003eKeywords:\u003c/strong\u003e The Gamma Distributions\u003c/p\u003e","title":"【概率论】5-7:Gama分布(The Gamma Distributions Part I)"},{"content":"Abstract: 本文介绍正态分布第三部分，标准正态分布，正态分布的线性组合，对数正态分布以及对数正态分布 Keywords: The Normal Distributions，The Standard Normal Distribution\n正态分布 废话就是概率论基础知识部分快要结束了，接下来的关于数理统计部分的内容很多都是依赖概率论的知识的，所以打好基础才好继续深入。 本文继续介绍标准正态分布，以及正态分布不同参数的比较。\n标准正态分布 The Standard Normal Distribution  Definition Standard Normal Distribution.The normal distribution with mean 0 and variance 1 is called the standard normal distribution.The p.d.f. of the standard nromal distribution is usually denoted by the symbol $\\phi$ ,and the c.d.f. is denoted by the symbol $\\Phi$ .Thus, $$ \\phi(x)=f(x|0,1)=\\frac{1}{(2\\pi)^{1/2}}e^{-\\frac{1}{2}x^2} \\text{ for }-\\infty\u0026lt;x\u0026lt;\\infty $$ and $$ \\Psi(x)=\\int^{x}_{-\\infty}\\Psi(\\mu)d\\mu \\text{ for }-\\infty\u0026lt;x\u0026lt;\\infty $$\n 第二个公式中 $\\mu$ 是个哑变量，根据微积分基本定理可以知道上面写的 c.d.f.的导数就是p.d.f. 正则化的本质就是均值为0，方差为1的正态分布被称为正态分布家族中的标准。 c.d.f是使用初等函数是无法求得的，也就是没有一个封闭的形式，就像本本节开始时说的，只能用查表或者数值法来求p.d.f的某段积分，或者查询c.d.f的结果做差得到对应段的p.d.f.\n Theorem Consequences of Symmetry.For all x and all $0 \u0026lt; p \u0026lt; 1$ $$ \\begin{aligned} \\Psi(-x)=1-\\Psi(x) \\text{ and } \\Psi^{-1}(p)=-\\Psi^{-1}(1-p) \\end{aligned} $$\n 这个证明相对简单，其实主要考察的是上一篇关于正态分布的形状问题，正态分布p.d.f.的根本性质是对称性，关于均值对称，这个性质就可以衍生出上面定理的结论，比如 $Pr(X\\leq -x)=Pr(X\\geq x)$ 就是对称性质的体现，然后是c.d.f.的反函数重新改写前面这个对称性质，等是左边为 $\\Psi^{-1}(\\Psi(-x))=\\Psi^{-1}(p)$ 以及 等式右边 $\\Psi^{-1}(1-\\Psi(x))=-\\Psi^{-1}(1-p)$\n Theorem Converting Normal Distributions to Standard.Let $X$ have the normal distribution with mean $\\mu$ and variance $\\sigma^2$ .Let $F$ be the c.d.f. of $X$ .Then $Z=(X-\\mu)/\\sigma$ has the standard normal distribution, and ,for all x and all $0 \u0026lt; p \u0026lt; 1$ $$ F(x)=\\Phi(\\frac{x-\\mu}{\\sigma})\\ F^{-1}(p)=\\mu+\\sigma\\Phi^{-1}(p) $$\n 这个定理要完成的一个任务是把一个一般的正态分布，通过随机变量的函数将原正态分布转换成标准正态分布，方法是目标随机变量减去均值后的差再除以标准差。 证明 $$ Pr(X\\leq x)=Pr(Z\\leq \\frac{x-\\mu}{\\sigma}) $$ 这就能得到结论了，令 $p=F(x)$ 能得到 $F^{-1}(p)=\\mu+\\sigma\\Phi^{-1}(p)$ 的结论。\n 我们来举个计算的例子，我们来计算一个正态分布中的概率，假设X有一个正态分布，均值是5方差是2，我们来计算 $Pr(1\u0026lt;X\u0026lt;8)$ 如果我们令 $Z=(X-5)/2$ 那么Z会有一个标准的正态分布并且： $$ Pr(1\u0026lt;X\u0026lt;8)=Pr(\\frac{1-5}{2}\u0026lt;\\frac{X-5}{2}\u0026lt;\\frac{8-5}{2})=Pr(-2\u0026lt;Z\u0026lt;1.5)\\ \\text{futhermore:}\\ \\begin{aligned} Pr(-1\u0026lt;Z\u0026lt;1.5)\u0026amp;=Pr(Z\u0026lt;1.5)-Pr(Z\\leq -2)\\ \u0026amp;=\\Phi(1.5)-\\Phi(-2)\\ \u0026amp;=\\Phi(1.5)-[1-\\Phi(2)] \\end{aligned} $$ 从书后标准正态分布的表格中可以查到c.d.f.为 $\\Phi(1.5)=0.9332$ 并且 $\\Phi(2)=0.9773$ 所以 $$ Pr(1\u0026lt;X\u0026lt;8)=0.9105 $$\n 本section的精髓是，首先我们没办法计算正态分布的不定积分，所以想求值要查表，查表你有不能对每一个分布参数都建表，所以要制造一个标准，其他的不同参数和标准有数字关系，于是定义一个标准正态分布，然后所有正态分布和标准正态分布产生数字联系，就能用一张表解决问题了。\n正态分布比较 Comparisons of Normal Distributions 接着我们来比较一下，不同参数的正态分布。得到的结论是均值决定了位置，方差决定了胖瘦。 这里有一个非常重要的性质，以均值位置为参考物，一个标准偏移是一个标准差，这个偏移之内的概率是相等的，并且任意个相同的标准偏移都相等，对于两个正态分布有$f_1(X|\\mu_X,\\sigma_X^2),f_2(Y|\\mu_Y,\\sigma_Y^2)$ 有正数 $k$ ： $$ Pr(\\mu_X-k\\sigma_X\u0026lt;X\u0026lt;\\mu_X+k\\sigma_X)=Pr(\\mu_Y-k\\sigma_Y\u0026lt;Y\u0026lt;\\mu_Y+k\\sigma_Y) $$\n另一种直观的方式是所有正态分布和标准正态分布进行比较，那么就有 $$ p_k=Pr(|X-\\mu|\\leq k\\sigma)=Pr(|Z|\\leq k) $$\n如图： 表格中k表示几个标准偏移，也就是k的具体值，可以看出，当$k=3$ 的时候，正态分布内 $Pr(\\mu-3\\sigma,\\mu+3\\sigma)\u0026gt;0.99$ 了而且我记得我大学书上有个 $3-\\sigma$ 原则，好像是当检测结果满足这个要求的时候就算合格了，这个具体的我们在数理统计部分再说，我们要知道的是正态分布的形状，和偏移性质。\n正太分布随机变量的线性组合 Linear Combinatios of Normally Distributed Variables 在经过标准化变换后我们思考的另一个问题，就是多个正态分布的随机变量的线性组合会是什么样子呢？\n Theorem If the random variables $X_1,\\dots,X_k$ are independent and if $X_i$ has the normal distribution with mean $\\mu_i$ and variance $\\sigma^2_i(i=1,\\dots,k)$ ,then the sum $X_1+\\dots+X_k$ has the normal distribution with mean $\\mu_1+\\dots+\\mu_k$ and variance $\\sigma^2_1+\\dots+\\sigma^2_k$\n 定理是说，如果把几个正态分布的独立随机变量加起来得到的随机变量也是正态分布的，并且均值是原来随机变量的均值的和，方差也是原随机变量的方差的和。\n证明： 使用强有力的工具m.g.f. 用 $\\Psi_i(t)$ 表示 第 $X_i$ 的m.g.f. $(i=1,2,\\dots)$ 然后用 $\\Psi(t)$ 表示 $X_1+\\dots+X_k$ 的m.g.f. $$ \\Psi(t)=\\Pi^{k}{i=1}\\Psi_i(t)=\\Pi^k{i=1}exp[\\mu_it+\\frac{1}{2}\\sigma^2_it^2]\\ =exp{[(\\sum^k_{i=1}\\mu_i)t+\\frac{1}{2}(\\sum^{k}_{i=1}\\sigma^2_i)t^2]} $$ 直接就能得到我们想要的结论，可见m.g.f.大法好。\n Corollary If the random variables $X_1,\\dots,X_k$ are independent,if $X_i$ has the normal distribution with mean $\\mu_i$ and variance $\\sigma^2_i (i=1,\\dots,k)$ ,and if $a_1,\\dots,a_k$ and $b$ are constants for which at least one of the values $a_1,\\dots,a_k$ is different from 0,then the variable $a_1X_1+\\dots+a_kX_k+b$ has the normal distribution with mean $a_1\\mu_1+\\dots+a_k\\mu_k+b$ and variance $a_1^2\\sigma_1^2+\\dots+a^2_k\\sigma_k^2$\n 线性组合的推论，证明方法和上面定理一致，使用m.g.f.可以很轻松的得到。\n Definition Sample Mean.Let $X_1,\\dots,X_n$ be random variables,The average of these $n$ random variables $\\frac{1}{n}\\sum^{n}_{i=1}X_i$ ,is called their sample mean and is commonly denoted $\\bar{X}_n$\n 有n个随机变量，他们的均值，被称为样本均值，记做 $\\bar{X}_n$ 注意，这里并没有说 $X_i$ 的分布和独立性关系。也就是说是任意的的分布都可以。\n Corollary Suppose that the random variables $X_1,\\dots,X_n$ form a random sample from the normal distribution with mean $\\mu$ and variance $\\sigma^2$ ,and let $\\bar{X}_n$ denote their sample mean .Then $\\bar{X}_n$ has the normal distribution with mean $\\mu$ and variance $\\sigma^2/n$\n 上面定义的一个推论，是否成立呢？我们来证明一下，用什么证明呢？我们的线性性质就能证明这一点： $$ \\bar{X}n =\\sum^{n}{i=1}(1/n)X_i $$ 这就是一个线性组合，而且根据条件，$X_i$ 的均值方差一致，那么均值最后不变 $\\sum^n_{i=1}\\frac{1}{n}\\mu_i=\\mu_i$ 而对应的方差应该是 $\\sum^{n}_{i=0}\\frac{1}{n^2}\\sigma^2=\\sigma^2/n$ 证毕\n正态分布的对数 The Lognormal Distributions 对数正态分布，就是对随机变量进行一个对数变换后，这个新的分布是正态分布。\n Definition Lognormal Distribution.If $log(X)$ has the normal distribution with mean $\\mu$ and variance $\\sigma^2$ ,we say that $X$ has the lognormal distribution with parameters $\\mu$ and $\\sigma^2$\n 我们之前研究的都是一个某分布的随机变量经过一个函数变换后产生新的随机变量对应的分布是什么样的，今天是反过来，一个随机变量经过函数变换后是正态分布，那么这个分布是怎么样的。\n总结 本文主要介绍了正态分布的形状，和不同正态分布之间的相关性一致性，以及正态分布随机变量的线性组合结果，以及扩展的对数正太分布，可以看出正态分布有很多非常简单有用的性质。 下一篇我们进入gamma分布。。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-6-the-normal-distributions-p3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍正态分布第三部分，标准正态分布，正态分布的线性组合，对数正态分布以及对数正态分布\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Normal Distributions，The Standard Normal Distribution\u003c/p\u003e","title":"【概率论】5-6:正态分布(The Normal Distributions Part III)"},{"content":"Abstract: 本文介绍正态分布的数学性质 Keywords: The Normal Distributions\n正态分布 一共要写四篇，哪来那么多废话。 首先我们要从最基础的原始的正态分布的数学原理说起\n正态分布的性质 Properties of Normal Distributions 正态分布的定义 Definition 到目前为止，我们还没看到正态分布长什么样。\n Definition and p.d.f. A random X has the normal distribution with mean $\\mu$ and variance $\\sigma^2$ ($-\\infty\u0026lt;\\mu\u0026lt;\\infty$ and $\\sigma \u0026gt; 0$) if X has a contimuous distribution with the following p.d.f. $$ f(x|\\mu,\\sigma^2)=\\frac{1}{(2\\pi)^{\\frac{1}{2}}\\sigma}e^{-\\frac{1}{2}(\\frac{(x-\\mu)}{\\sigma})^2}\\text{for} -\\infty\u0026lt;x\u0026lt;\\infty $$ 定义对于我们来说就是个准确的命名过程。那么我们接下来要证明的是定义里说的对不对？ Theorem $f(x|\\mu,\\sigma^2)=\\frac{1}{(2\\pi)^{\\frac{1}{2}}\\sigma}e^{-\\frac{1}{2}(\\frac{(x-\\mu)}{\\sigma})^2}\\text{for} -\\infty \u0026lt; x\u0026lt; \\infty$ is a p.d.f.\n 思路：证明一个表达式是不是，p.d.f.，肯定要根据p.d.f.的定义，①不能出现负数，②积分结果是1。 首先观察函数，发现其不可能出现负数，所以性质1符合p.d.f.的性质 那么接下来是求积分，并确保是1，不是说不能积分么，这里怎么做呢？ 首先我们令 $y=\\frac{x-\\mu}{\\sigma}$ 那么 $$ \\int^{\\infty}{-\\infty}f(x|\\mu,\\sigma^2)dx=\\int^{\\infty}{-\\infty}\\frac{1}{(2\\pi)^{1/2}}e^{-\\frac{1}{2}y^2}dy\\ \\text{we shall now let:}\\ I=\\int^{\\infty}{-\\infty}e^{-\\frac{1}{2}y^2}dy $$ 所以我们只要证明 $I=(2\\pi)^{1/2}$ 就算是得到结论了，但是怎么证明呢？我们用用1的特点吧，1和1相乘还是1所以我们让两个积分相乘，我们来到了二重积分的世界解决这个问题： $$ \\begin {aligned} I^2\u0026amp;=I\\times I=\\int^{\\infty}{-\\infty}e^{-\\frac{1}{2}y^2}dy \\cdot \\int^{\\infty}{-\\infty}e^{-\\frac{1}{2}z^2}dz\\ \u0026amp;=\\int^{\\infty}{-\\infty} \\int^{\\infty}{-\\infty}e^{-\\frac{1}{2}(y^2+z^2)}dydz\\ \\text{to the polar coordinates } r \\text{ and } \\theta :\\ I^2\u0026amp;=\\int^{2\\pi}{0} \\int^{\\infty}{0}e^{-\\frac{1}{2}(r^2)}rdrd\\theta \\ \\text{substitute }v=r^2/2\\ \u0026amp;\\int^{\\infty}{0}e^{-v}dv=1 \\end{aligned} $$\n证毕。 也就证明了两个这个积分相乘的结果是1，但是我们并没有求出他的反函数。\n正态分布的距生成函数 m.g.f. m.g.f. 一旦得到相应的均值和方差就非常简单了。\n Theorem Moment Generating Function.The m.g.f. of the distribution with p.d.f. given by upside is $$ \\begin{aligned} \\psi(t)\u0026amp;=e^{\\mu t+\\frac{1}{2}\\sigma^2t^2}\u0026amp;\\text{ for }-\\infty\u0026lt;t\u0026lt;\\infty \\end{aligned} $$\n 证明上面定理的唯一办法就是我们求一下正态分布定义中那个p.d.f.的m.g.f.看结果是否一致。 $$ \\begin{aligned} \\psi(t)\u0026amp;=E(e^{tX})=\\int^{\\infty}{-\\infty}\\frac{1}{(2\\pi)^{1/2}}e^{tx-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx\\ \\text{square inside the brackets:}\\ tx-\\frac{(x-\\mu)^2}{2\\sigma^2}\u0026amp;=\\mu t+\\frac{1}{2}\\sigma^2t^2-\\frac{[x-(\\mu+\\sigma^2t)]^2}{2\\sigma^2}\\ \\text{Therefore:}\\ \\psi(t)\u0026amp;=Ce^{\\mu t+\\frac{1}{2}\\sigma^2t^2}\\ \\text{where: }\\ C\u0026amp;=\\int^{\\infty}{-\\infty}\\frac{1}{(2\\pi)^{1/2}\\sigma}e^{-\\frac{[x-(\\mu+\\sigma^2t)]^2}{2\\sigma^2}}dx \\end{aligned} $$ 然后我们用 $\\mu+\\sigma^2t$ 替换掉 $\\mu$ 并且 $C=1$ 因此证明了结论的正确性 证毕。\n思路是按照m.g.f.的定义，然后把里面的凑成正态分布的样子利用积分为1，化简表达式\n正态分布的均值和方差 Mean and Variance  Theorem Mean and Variance.The mean and variance of the distribution with p.d.f. given by definition upside are $\\mu$ and $\\sigma^2$ ,repectively.\n 证明方法就是直接用m.g.f.求导就可以了： $$ \\begin{aligned} \\psi\u0026rsquo;(t)\u0026amp;=(\\mu+\\sigma^2t)e^{\\mu t+\\frac{1}{2}\\sigma^2t^2}\\ \\psi\u0026rsquo;\u0026rsquo;(t)\u0026amp;=([\\mu+\\sigma^2t]^2+\\sigma^2)e^{\\mu t +\\frac{1}{2}\\sigma^2t^2}\\ \\text{Plugging } t=0\\ E(X)\u0026amp;=\\psi\u0026rsquo;(0)=\\mu \\ Var(X)\u0026amp;=\\psi\u0026rsquo;\u0026rsquo;(0)-[\\psi\u0026rsquo;(0)]^2=\\sigma^2 \\end{aligned} $$ 注意m.g.f.对于所有 $t$ 都有限，所以所有正态分布的距都存在。\n正态分布的形状 the Shapes of Normal Distribution 我们上面稀稀拉拉写了一些常见的性质，但是到现在我们还不知道正态分布长什么样呢？ 分析p.d.f和我们已经计算出来的数字特征，我们可以总结出下面这些基本信息：\n 均值和中值都是 $\\mu$ $f(x|\\mu,\\sigma^2)$ 在 $x=\\mu$ 时得到最大值。 二次求导后在 $\\mu \\pm \\sigma$ 处为0，为曲线拐点。  于是我们会得到钟形曲线：\n并不是所有钟形曲线都是正态分布家族的，比如前面介绍的不存在期望的柯西分布，他的尾巴跟我们的正态分布不太一致。\n线性变换 Linear Transformations 我们接着研究研究正态分布的线性变换。\n Theorem If $X$ has the normal distribution with mean $\\mu$ and variance $\\sigma^2$ and if $Y =aX+b$ ,where $a$ and $b$ are given constants and $a \\neq 0$ ,then $Y$ has the normal distribution with mean $a\\mu+b$ and variance $a^2\\sigma^2$ .\n 定理给出了正态分布对应的随机变量经过线性变换后的结果，我们来计算下，证明定理的正确性。 证明： 使用m.g.f ，我们来计算 $\\psi_Y$ $$ \\psi_Y(t)=e^{bt}\\psi(at)=e^{(a\\mu+b)t+\\frac{1}{2}a^2\\sigma^2t^2} \\text{ for }-\\infty \u0026lt;t\u0026lt;\\infty $$ 比较正态分布的m.g.f $$ \\begin{aligned} \\psi(t)\u0026amp;=e^{\\mu t+\\frac{1}{2}\\sigma^2t^2}\u0026amp;\\text{ for }-\\infty\u0026lt;t\u0026lt;\\infty \\end{aligned} $$\n可以看出线性变化后的分布是一个均值为 $a\\mu+b$ 方差为 $a^2\\sigma^2$ 那么我们得到一个新的正态分布，并且新正态分布的参数和原正态分布有关系\n总结 本文主要介绍正态分布的数学性质。后面我们继续研究标准正态分布和对数正态分布。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-6-the-normal-distributions-p2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍正态分布的数学性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Normal Distributions\u003c/p\u003e","title":"【概率论】5-6:正态分布(The Normal Distributions Part II)"},{"content":"Abstract: 本文介绍正态分布第一部分，关于正态分布的基本知识 Keywords: The Normal Distributions\n正态分布 要把原来的一课拆成三篇博客，说明什么？说明这个真的很重要。 正态分布是我们要用到的最多的，也是自然界中最基础的一种连续随机分布，可以说是重点中的重点，并且这个函数的p.d.f不存积分（不定积分不可求），所以计算正态分布的某段的概率一般要用数值方法或者查询c.d.f.的表格得出对应的数字，就是这么不好计算的一个分布，为何如此重要呢？\n正态分布的重要性 Importance of the Normal Distributions 到目前为止，在统计学中的单变量分布，本文介绍的正太分布是最重要的。主要原因如下：\n 数学性质，如果一个有正态分布的随机变量使用很多常见的函数进行变换后产生的新的分布可以很轻松的计算，并且很多形式都很简单。 科学家发现很多物理现象中产生的数据接近正态分布，比如正态分布经常可以用来建模某一些人的身高和体重。或者有时候一些数据经过一点简单的变化就能满足正态分布。 中心极限定理，将在6.3中讲到，这是统计学和误差分析学的基础  总结 本文作为基础介绍，简单的介绍了正态分布的基本信息，下一节讲其定义和数学性质，难度会比较大，大家加油。 待续。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-6-the-normal-distributions-p1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍正态分布第一部分，关于正态分布的基本知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Normal Distributions\u003c/p\u003e","title":"【概率论】5-6:正态分布(The Normal Distributions Part I)"},{"content":"Abstract: 本文介绍负二项分布，几何分布的基础知识 Keywords: The Negative Binomial Distribution，The Geometric Distribution\n负二项分布 到目前为止，所有的分部都是从Bernoulli 分布衍生出来的：\n 二项分布，$n$ 次Bernoulli试验的结果中，每次试验的分布不变，结果为1的次数 $X$ 的分布 超几何分布，$n$ 次Bernoulli试验，每次试验分布发生改变，结果为1的次数 $X$ 的分布，当试验分布变化不大的时候和二项分布结果相同 泊松分布，用来在某种特殊情况下（$n$ 比较大， $p$ 比较小，而 $np$ 又不是很大的情况下）近似二项分布，当n趋近于无穷的时候等同于二项分布。  今天我们还是从二项分布出发，研究这样一个事实，对于Bernoulli过程，我们设定，当某个结果出现固定次数的时候，整个过程的数量，比如我们生产某个零件，假设每个零件的合格与否都是相互独立的，且分布相同，那么当我们生产出了五个不合格零件时，一共生产了多少合格的零件，这个数量就是一个负二项分布。 为什么叫负二项分布而不是正二项分布？ 有两种说法，第一我们上面说到的例子，多半是失败到了固定次数时 $X$ 的分布，另一种是站在分布的系数上来观察的，在下面我们可以看得到。\n负二项分布的定义和含义 Definition and Interpretation 废话中给出的生产零件的例子就是引出定义的关键。我们来先看一个定理，描述上面过程的定理：\n Theorem Sampling until a Fixed Number of Success.Suppose that an infinite sequence of Bernoulli trails with probability of success $p$ are available.The number $X$ of failures that occur before the $r$th success has the following p.d.f. $$ f(x|r,p)= \\begin{cases} \\begin{pmatrix} r+x-1\\ x \\end{pmatrix}p^r(1-p)^x\u0026amp;\\text{for }x=0,1,2,\\dots\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n 证明如下 首先我们必须分析一下这个过程，当成功的次数达到目标后停止试验，也就是说最后一次必然是成功的，不然试验不会结束，所以我们需要的是在已经进行了的 $x+r-1$ 次实验中完成 $r-1$ 次成功，$x$ 次失败，那么从计数原理角度，概率为： $$ \\begin{aligned} Pr(A_n)\u0026amp;=\\begin{pmatrix}n-1\\r-1\\end{pmatrix}p^{r-1}(1-p)^{(n-1)-(r-1)}p\\ \u0026amp;=\\begin{pmatrix}n-1\\r-1\\end{pmatrix}p^{r}(1-p)^{(n-r)}p \\end{aligned} $$ 然后这就是我们的目标了，n就是总的试验次数包括成功和失败。\n Definition Negative Binomial Distribution.A random variable $X$ has the negative binomial distribution with parameters $r$ and $p$ ( $r=1,2,\\dots$ and $0 \u0026lt; p \u0026lt; 1$) if $X$ has a discrete distribution for which the p.f. $f(x|r,p)$ is as specified by theorem upside.\n 定义，告诉你，上面定理里面的p.f.叫负二项分布 因为存在关系： $$ \\begin{pmatrix} r+x-1\\ x \\end{pmatrix}={\\frac {(x+r-1)\\dotsm (r)}{x!}}=(-1)^{x}{\\frac {(-r)(-r-1)(-r-2)\\dotsm (-r-k+1)}{x!}}=(-1)^{x}{\\binom {-r}{x}} $$ 所以负二项分布可以写成： $$ f(x|r,p)= \\begin{cases} (-1)^{x}{\\binom {-r}{x}}p^r(1-p)^x\u0026amp;\\text{for }x=0,1,2,\\dots\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ 这就是命名的由来（参考《统计推断第二版》）\n几何分布 The Geometric Distribution 我们先学了超几何分布然后又跑回来学几何分布，难道我们跑错了？当然不是，因为几何分布跟负二项分布非常相关。 当负二项分布 $r=1$ 的时候产生的分布叫做几何分布，文字解释就是当我们第一个不合格的产品出现时，我们就停止生产，这是生产的合格产品的数量为随机变量 $X$\n Definition Geometric Distribution.A random varibale X has the geometric distribution with parameter p ($0 \u0026lt; p \u0026lt; 1$ )if X has a discrete distribution for which the p.f. f(x|1,p) is as follows: $$ f(x|1,p)= \\begin{cases} p(1-p)^x\u0026amp;\\text{for }x=0,1,2,\\dots\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n 这个跟负二项分布一模一样，所以就不再重复证明了\n Theorem if $X_1,\\dots,X_r$ are i.i.d. random variable and if each $X_i$ has the geometric distribution with parameter $p$ ,then the sum $X_1+\\dots+X_r$ has the negative binomial distribution with parameters $r$ and $p\n 又是加法规则，多个几何分布相加的结果是负二项分布。 证明过程也就是分析过程，首先我们假设我们有一个Bernoulli过程，那么如果只要有0发生就停止，这样产生的是几何分布，如果第一个几何分布 $X_i$ 产生以后，我们继续按照规则进行，那么接着会产生 $X_2,\\dots,X_n$ 并且之间相互独立，而产生的到第 $n$ 个的时候，就产生了一个当0发生 $n$ 次的负二项分布。所以这 $n$ 个随机变量相加就是负二项分布。\n这个证明不太严谨。但是从逻辑的角度上是成立的。\n负二项分布和几何分布的性质 Properties of Negative Binomial and Geometric Distributions 接下来我们看看性质\n距生成函数 m.g.f.  Theorem Moment Generating Function.If X has the negative binomial distribution with parameters r and p ,then the m.g.f. of X is as follow: $$ \\psi(t)=(\\frac{p}{1-(1-p)e^t})^r\\text{ for } t\u0026lt; log(\\frac{1}{1-p}) $$ $r=1$ 的时候上述表达式为几何分布的p.d.f. ，有点复杂了，我们来证明下。\n 证明： 要用到m.g.f.的定理就是多个独立的随机变量和的m.g.f.是其m.g.f.的积：\n Teorem 4.4.4 Suppose that $X_1,\\dots,X_n$ are $n$ independent random varibales;and for $i=1,\\dots,n$ . let $\\psi_i$ denote the m.g.f. of $X_i$ .Let $Y=X_1+\\dots+X_n$ ,and let the m.g.f. of $Y$ be denoted by $\\psi$ .Then for every value of t such that $\\psi_i(t)$ is finite for $i=1,\\dots,n$ , $$ \\psi(t)=\\Pi^{n}_{i=1}\\psi_i(t) $$\n 以及上面的定理：多个同参数 $p$ 的独立几何分布的和是负二项分布，假设这些i.i.d的随机变量为 $X_1,\\dots,X_n$ 于是： $$ \\psi_i(t)=E(e^{tX_i})=p\\sum^{\\infty}{x=0}[(1-p)e^t]^x $$ 为了使上面的表达式结果有限，或者说让 $[(1-p)e^t]^x$ 收敛，我们应该有 $0\u0026lt; (1-p)e^t \u0026lt; 1$ 得到 $t\u0026lt;log(\\frac{1}{1-p})$ 根据微积分中级数的原理，当 $\\alpha\\in (0,1)$ 时： $$ \\sum^{\\infty}{x=0}\\alpha^x=\\frac{1}{1-\\alpha} $$\n带入到 $\\psi_i(t)$ 中就有 $$ \\psi_i(t)=\\frac{p}{1-(1-p)e^t} $$\n那么根据上面的定理4.4.4 我们就能得到结果 $$ \\psi(t)=(\\frac{p}{1-(1-p)e^t})^r\\text{ for } t\u0026lt; log(\\frac{1}{1-p}) $$\n均值和方差 Mean and Variance  Theorem if $X$ has the negative binomial distribution with parameters $r$ and $p$ the mean and the varance of $X$ must be $$ E(X)=\\frac{r(1-p)}{p} \\text{ and }Var(X)\\frac{r(1-p)}{p^2} $$ The mean and variance of the geometric distribution with parameter $p$ are the special case of equation upsite with $r=1$\n 如果有了m.g.f.求均值和方差都不是大问题，就是求两个导数，所以就直接写结果了，但是我们有更简单的方法，继续把负二项分布拆成几何分布，然后计算均值和方差后按照均值和方差的加法原则求负二项分布的均值和方差： $$ E(X_i)=\\psi\u0026rsquo;_i(0)=\\frac{1-p}{p}\\ Var(X_i)=\\psi\u0026rsquo;\u0026rsquo;_i(0)-[\\psi\u0026rsquo;_i(0)]^2=\\frac{1-p}{p^2} $$ 然后就是把 $r$ 个 $X_i$ 求和就得到订立中的结果了。\n集合分布的无记忆性 Memorless Property of Geometric Distributions 这条性质是第一次出现，所以值得注意\n Theorem Memoryless Property of Geometric Distributions.Let $X$ have the geometric distribution with parameter $p$ ,and let $k\\geq 0$ .Then for every integer $t\\geq 0$ , $$ Pr(X=k+t|X\\geq k)=Pr(X=t) $$\n 这个证明是个习题，我还没有做（嘻嘻嘻，后面补上），但是要指出的是，我们学的这些分布中只有几何分布有这种性质。\n此处有坑，记得补上\n总结 本文介绍了离散分布中最后两个，负二项分布，和几何分布，下一篇开始连续分布。\n","permalink":"https://go.face2ai.com/math/math-probability-5-5-the-negative-binomial-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍负二项分布，几何分布的基础知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Negative Binomial Distribution，The Geometric Distribution\u003c/p\u003e","title":"【概率论】5-5:负二项分布(The Negative Binomial Distribution)"},{"content":"Abstract: 本文介绍Poisson分布相关知识 Keywords: Poisson Distribution\n泊松分布 前面这几个分布包括今天说的泊松分布都是和二项分布，伯努利分布相互联系的，之间有各种各样的关系，我们的学习目的不是背诵所有这些分布的性质，而是在这些性质的推到过程。\n很多实验比较关注次数，比如一段时间内到达商店的顾客的人数，电话交换机每分钟受到的通话请求，洪水或者其他自然人为灾害发生的次数。泊松分布被用来建模，一段事件这些事情发生的次数，并且泊松分布也是用来近似当 $p$ 很小的时候的二项分布的一种方法。\n泊松分布的定义和性质 Definition and Properties of the Poisson Distributions 先来看一个商店一段时间有多少顾客到来的例子，这个例子会贯穿正片博客，大家应该好好读一下。\n 商店老板相信，顾客们以每个小时4.5 人次的数量来到商店，他想找到一个X的分布，这个X表示在未来某个一个小时，到店的客人数，并且他认为这些到来的客人之间相互独立，于是他的做法是按照一个小时3600秒计算，平均每秒来 0.00125 个人，并且假设一秒钟不会同时出现两个人同时到店的可能，那么某时间点，到达的人数为0或者1，为1的可能性是0.00125，整个过程是一个二项分布，n=3600，p=0.00125。 这看起来很正确也很流畅 于是他要计算p.f.了： $$ f(x|n=3600,p=0.00125)= \\begin{cases} \\begin{pmatrix} 3600\\x \\end{pmatrix}p^x(1-p)^{3600-x}\u0026amp;\\text{for }0\\leq x\\leq 3600\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n这个式子非常有意思，当参数 $\\begin{pmatrix} 360 0\\x \\end{pmatrix}$ 变大的时候， 参数 $p^x(1-p)^{3600-x}$ 似乎以同等的速度变小，而整体却变化不大，于是我们对相邻的两个随机变量值做个比较（以下把 $X$ 扩展到在0到 $n$ 之间变化） $$ \\begin{aligned} \\frac{f(x+1)}{f(x)}\u0026amp;= \\frac {\\begin{pmatrix}n\\x+1\\end{pmatrix}p^{x+1}(1-p)^{n-x-1}} {\\begin{pmatrix}n\\x\\end{pmatrix}p^{x+1}(1-p)^{n-x-1}}\\ \u0026amp;=\\frac{(n-x)p}{(x+1)(1-p)}\\ \u0026amp;\\approx\\frac{np}{x+1} \\end{aligned} $$\n那么根据这个比值，如果我们设 $\\lambda=np$ 那么我们会有一个递归关系： $$ f(1)=f(0)\\lambda\\ f(2)=f(1)\\frac{\\lambda}{2}=f(0)\\frac{\\lambda^2}{2}\\ f(3)=f(2)\\frac{\\lambda}{3}=f(0)\\frac{\\lambda^3}{6}\\ \\vdots\\ f(n)=f(n-1)\\frac{\\lambda}{n}==f(0)\\frac{\\lambda^n}{n!}\\ $$ 因为$f$ 是一个近似来的 p.f. 那么我们需要让他满足我们的条件，比如，所有随机变量对应的概率求和是1. $$ \\sum^{\\infty}{x=0}f(x)=1 $$ 因为整个关系式能调整的部分就只有 $f(0)$ 了，那么我们只好调整初始化条件来使得p.f.成立了， $$ \\sum^{\\infty}{x=0}f(0)\\frac{\\lambda^n}{n!}=1\\ f(0)\\sum^{\\infty}{x=0}\\frac{\\lambda^n}{n!}=1\\ \\text{for :}\\sum^{\\infty}{x=0}\\frac{\\lambda^n}{n!}=e^{\\lambda}\\ \\text{so :}f(0)=e^{-\\lambda} $$ 所以我们只需要让 $f(0)=e^{-\\lambda}$ 即可，关于求和等于 $e^{\\lambda}$ 的计算可以参开微积分书籍。\n那么我们就有了一个新的能够近似上面二项分布的新分布——Poisson Distribution： $$ f(x|\\lambda)= \\begin{cases} \\frac{e^{-\\lambda}\\lambda^x}{x!}\u0026amp;\\text{for }x=1,2,3,\\dots\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n这个分布就是我们今天的主角，也是概率论中非常重要的一个分布，可以用来描述一段时间内某事发生的次数的模型。\n Definition Poisson Distribution.Let $\\lambda \u0026gt; 0$ .A random variable X has the Poisson Distribution with mean $\\lambda$ if the p.f. of $X$ is as follow: $$ f(x|\\lambda)= \\begin{cases} \\frac{e^{-\\lambda}\\lambda^x}{x!}\u0026amp;\\text{for }x=1,2,3,\\dots\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n 还是传统的定义方法，告诉你，这个式子是泊松分布~\n泊松分布的均值 Mean  Theorem Mean. The mean of Poisson Distribution with p.f. equal to upside is $\\lambda$ . 怎么样！神奇不神奇~均值是 $\\lambda$ 我们接下来就来证明这一点。 直接使用期望的定义 $$ E(X)=\\sum^{\\infty}{x=0}xf(x|\\lambda) $$ 当x=0 时，值为0，我们直接从1开始 $$ \\begin{aligned} E(X)\u0026amp;=\\sum^{\\infty}{x=0}x\\frac{e^{-\\lambda}\\lambda^x}{x!}\\ \u0026amp;=\\sum^{\\infty}{x=1}\\frac{e^{-\\lambda}\\lambda^x}{(x-1)!}\\ \u0026amp;=\\lambda\\sum^{\\infty}{x=1}\\frac{e^{-\\lambda}\\lambda^{x-1}}{(x-1)!}\\ \\text{if we set } y=x-1\\ \u0026amp;=\\lambda\\sum^{\\infty}{y=0}\\frac{e^{-\\lambda}\\lambda^{y}}{y!} \\end{aligned} $$ 这样 $\\sum^{\\infty}{y=0}\\frac{e^{-\\lambda}\\lambda^{y}}{y!}$ 变成了一个对p.f.为$f(y|\\lambda)$ 的概率函数求和的计算，结果必然为1，那么我们就证明了泊松分布的期望是 —— $\\lambda$\n 泊松分布的方差 Varaince  Theorem Variance.The variance of Poisson distribution with mean $\\lambda$ is also $\\lambda$ 意外不意外！惊喜不惊喜！依旧是 $\\lambda$ 证明： 我们将用到和上面证明期望一样的方法就是通过凑，来使得求和里面变成 p.f.的样子 $$ \\begin{aligned} E[X(X-1)]\u0026amp;=\\sum^{\\infty}{x=0}x(x-1)f(x|\\lambda)\\ \u0026amp;=\\sum^{\\infty}{x=2}x(x-1)f(x|\\lambda)\\ \u0026amp;=\\sum^{\\infty}{x=2}x(x-1)\\frac{e^{-\\lambda}\\lambda^x}{x!}\\ \u0026amp;=\\lambda^2\\sum^{\\infty}{x=2}\\frac{e^{-\\lambda}\\lambda^{x-2}}{x-2!}\\ \\text{We set }y=x-2\\ E[X(X-1)]\u0026amp;=\\lambda^2\\sum^{\\infty}_{y=0}\\frac{e^{-\\lambda}\\lambda^y}{y!}\\ \u0026amp;=\\lambda^2 \\end{aligned} $$\n 然后我们祭出我们的大招 $E[X(X-1)]=E[X^2]-E[X]=E[X^2]-\\lambda=\\lambda^2$ 所以 $E[X^2]=\\lambda^2+\\lambda$ 那么 $$ Var(X)=E[X^2]-E^2[x]=\\lambda^2+\\lambda-\\lambda^2=\\lambda $$\n至此证毕，构造了 $E[X^2]$ 然后求出了 $Var(X)$\n泊松分布的距生成函数 m.g.f. 接着我们研究第三大工具，m.g.f.\n Theorem Moment Generating Function.The m.g.f. of the Poisson distribution with mean $\\lambda$ is $$ \\psi(t)=e^{\\lambda(e^t-1)} $$ for all real $t$ 证明如下： $$ \\psi(t)=E(e^{tX})=\\sum^{\\infty}{x=0}\\frac{e^{tx}e^{-\\lambda}\\lambda^x}{x!}=e^{-\\lambda}\\sum^{\\infty}{x=0}\\frac{(\\lambda e^t)^x}{x!} $$ 根据 $e$ 级数性质 $$ \\sum^{\\infty}_{x=0}\\frac{(\\lambda e^t)^x}{x!}=e^{\\lambda e^t} $$ 那么我们对于 $-\\infty \u0026lt; t\u0026lt; \\infty$ 有： $$ \\psi(t)=e^{-\\lambda}e^{\\lambda e^t}=e^{\\lambda(e^t-1)} $$\n 有了m.g.f就能得到期望，方差或者其他阶距。\n泊松分布随机变量相加  Theorem If the random variable $X_1,\\dots,X_k$ are independent and if $X_i$ has Poisson distribution with mean $\\lambda_i(i=1,\\dots,k)$ ,then the sum $X_1+\\dots+X_k$ has the Poisson distribution with mean $\\lambda_1+\\dots+\\lambda_k$\n 拥有相同参数的二项分布可以进行加法运算，这一点我们前面就已经证明过了，今天要证明的是Poisson分布也能进行加法，而且不需要参数一致，用到的方法是用m.g.f进行分析：\n证明： 首先令 $\\psi_i(t)$ 来定义 $X_i$ 的m.g.f. 并且$X_i$ 是均值为 $\\lambda_i$ 的Poisson分布。并且 $X_1,\\dots,X_n$ 之间相互独立，那么对于 $-\\infty\u0026lt;t\u0026lt;\\infty$ 我们有： $$ \\psi(t)=\\Pi^k_{i=1}\\psi_i(t)=\\Pi^k_{i=1}e^{\\lambda_i(e^t-1)}=e^{(\\lambda_1+\\dots+\\lambda_k)(e^t-1)} $$\n结合前面泊松分布的m.g.f.可见定理成立。\n二项分布的泊松近似 The Poisson Approximation to Binomial Distributions 接下来我们研究一下泊松分布近似二项分布的详细内容。\n Theorem Closeness of Binomial and Pisson Distribution.For each integer n and each $0 \u0026lt; p \u0026lt; 1$ ,let $f(x|n,p)$ denote the p.f. of the binomial distribtuion with parameters $n$ and $p$ .Let $f(x|\\lambda)$ denote the p.f. of the Poisson distribution with mean $\\lambda$ .Let ${{P_n}}^{\\infty}{n=1}$ be a sequence of numbers between 0 and 1 such that $lim{n\\to \\infty}np_n=\\lambda$ . Then $$ lim_{n\\to \\infty}f(x|n,p_n)=f(x|\\lambda) $$ for all $x=0,1\\dots$\n 定理表明了二项分布和Poisson分布的近似关系，虽然我们在开篇的例子里面已经提到了用Poisson分布来近似$n$ 比较大，$p$ 比较小，$np$ 又不大的问题，但是我们还是需要从理论上分析下二项分布和Poisson分布到底有什么关系。 证明： 首先我们写出二项分布 $$ f(x|n,p_n)=\\frac{n(n-1)\\dots(n-x+1)}{x!}p_n^x(1-p_n)^{n-x} $$ 提示，把组合运算展开写的。 然后我们令 $\\lambda_n=np_n$ 那么 $lim_{n\\to \\infty}\\lambda_n=\\lambda$ 这样我们就有 $$ f(x|n,p_n)=\\frac{\\lambda_n^x}{x!}\\frac{n}{n}\\cdot\\frac{n-1}{n}\\dots \\frac{n-x+1}{n}(1-\\frac{\\lambda_n}{n})^n(1-\\frac{\\lambda_n}{n})^{-x} $$\n对于每个 $x\\geq 0$ 来说，我们有： $$ lim_{n\\to \\infty}\\frac{n}{n}\\cdot\\frac{n-1}{n}\\dots \\frac{n-x+1}{n}(1-\\frac{\\lambda_n}{n})^{-x}=1 $$ 这个是微积分要解决的问题，不知道的同学需要去参考下微积分的知识。上文中倒数第二个定理，我们没有证明，但是那个结论在这里还需要再次使用 $$ lim_{n\\to \\infty}(1-\\frac{\\lambda_n}{n})^{n}=e^{-\\lambda} $$\n所以 $$ lim_{n\\to \\infty}f(x|n,p_n)=\\frac{e^{-\\lambda}\\lambda^x}{x!}=f(x|\\lambda) $$\n证毕。 接下来这个定理是说超几何分布和Poisson分布之间的关系的，没有证明，但是可以参考下结论。\n Theorem Closeness of Hypergeometric and Poisson Distribution.Let $\\lambda\u0026gt;0$ .Let $Y$ have the Poisson distribution with mean $\\lambda$ .For each postive integer $T$ ,let $A_T,B_T$ ,and $n_T$ be integers such that $lim_{T\\to \\infty}n_TA_T/(A_T+B_T)=\\lambda$ .Let $X_T$ have the hypergeometric distribution with parameters $A_T,B_T$ and $n_T$ .Tor each fixed $x=0,1,\\dots$ ， $$ lim_{T\\to \\infty}\\frac{Pr(Y=x)}{Pr(X_t=x)}=1 $$\n 泊松过程 Poisson Processes 前面我们第一个例子说的如何估算在一个小时内到店的客户，那么如果是我想知道半个小时或者15分钟的顾客数量呢？难道是要用2.25个或者1.125个作为平均数的Poisson Distribution建模么？于是我们使用Poisson过程来对这种情况建模。\n Definition Poisson Process.A Poisson process with rate $\\lambda$ per unit time is a process that satisfies the following two properties: i: The number of arrivals in every fixed interval of time of length $t$ has the Poisson distribution with mean $\\lambda t$ ii: The numbers of arrivals in every collection of disjoint time intervals are independent\n 泊松过程满足两点要求，首先固定时间段内平均时长 $\\lambda t$，其次每个不同时段之间人数彼此独立。 所以上面我们说是否能用改了 $\\lambda$ 的泊松分布建模这个答案是肯定的就是通过改变 $\\lambda$ 值来重新建模的。 后面有一个关于泊松过程的选读内容，有兴趣的同学可以在书上找到\n总结 本文介绍泊松分布，性质及用途，以及泊松过程 明天继续。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-4-the-poisson-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍Poisson分布相关知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Poisson Distribution\u003c/p\u003e","title":"【概率论】5-4:泊松分布(The Poisson Distribution)"},{"content":"Abstract: 本文主要介绍超几何分布 Keywords: Hypergeomtirc Distribution,Finite Population Correction\n超几何分布 实力这个东西是不能被完全表现出来的，中华民族传统文化告诉我们，有十分的能力，只显示一分，但是我们现在是有一分能力要显示出十分，这叫推销自己，而且我们自己根本不知道自己只有一分能力，人心浮躁，我们还是憋着看书学习吧，外面的是是非非自然有人去解决，我们要做的是推动人类文明的发展😆 上文书我们说到（改成单口相声了）二项分布就是若干个独立同分布的伯努利分布的随机变量的和的结果，而伯努利分布如果对应最原始的抽样的话应该是这样的场景： 如果我们有一个不透明的箱子，里面有 $A$ 个红球， $B$ 个蓝球，其被拿出来的可能性相等，在我们拿出之前我们不知道我们会拿到什么（也就是保证随机性）那么我们拿出一个球是红球(称为事件R)的概率是 $Pr(R)=\\frac{A}{A+B}$ ，如果我们连续进行本实验，那么就有两种取样方式，而这就导致了从伯努利到二项分布，和从伯努利到超几何分布的变化\n超几何分布定义和例子 Definition and Examples 首先我们用一个具体的例子来看。\n 继续上面说的拿球的例子，假设我们要连续拿出n个球，$n\\geq 0$ （这里我们只考虑 $n\\geq 2$ 的情况，$n=0$ 的时候说明试验不进行， $n=1$ 的时候是伯努利分布，上一课学习的东西，我们这里也不再说了） 我们假设每次取出时，拿到红球的随机变量为 $X_i=1$ 拿到蓝球的随机变量是 $X_i=0$ 并且每次试验是独立的，如果我们采用不放回的抽取方式，那么我们可以得出结论 $Pr(X_2=1|X_1=0)\\neq Pr(X_2=1|X_1=1)$ ，因为我们第一次拿球，里面一共有 $A+B$ 个球包含 $A$ 个红球，如果第一次取出了红球，那么第二次我们再取相当于从 $A+B-1$ 个球包含 $A-1$ 红球中取，或者如果第一次取到的是蓝球，那么第二次相当于从 $A+B-1$ 个球中含 $A$ 个红球中取。 $$ Pr(X_2=1|X_1=0)=\\frac{A}{A+B-1}\u0026gt;Pr(X_2=1|X_1=1)=\\frac{A-1}{A+B-1} $$\n 这就是简单的不放回抽样，前面我们研究过，如果从抽取范围非常大的样本中抽取少量的时候，可以不考虑其概率变化，但是如果样本集本来就不是很大，那么就要考虑这个概率变化了。 从有限的样本集中不放回的抽样，这就是我们今天研究的对象超几何分布的背景。 如果考虑从包含两种情况的样本集合中抽取n个样本，其中是某一情况的样本个数X，其就是一个超几何分布。\n Theorem Probability Function .The distribuiton of $X$ is Example has the p.f. $$ f(x|n,A,B)= \\begin{cases} \\frac{ \\begin{pmatrix}A\\x\\end{pmatrix} \\begin{pmatrix}B\\n-x\\end{pmatrix} }{ \\begin{pmatrix}A+B\\n\\end{pmatrix} }\u0026amp;\\text{for }max(0,n-B)\\leq x\\leq min(n,A)\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n 这个定理的证明只是用到了计数法则中的乘法法则，简化成分阶段的试验，但是 $x$ 范围要要注意一下，就是确保从A中来的和从B中来的结果总数是在范围内的，或者说，以取出红球的数量为x看来说，$x$ 不能超过 $A$，$n-x$ 是蓝球的数量不能超过 $B$ 写成数学形式就是 $max(0,n-B)\\leq x\\leq min(n,A)$\n Definition Hypergeomtric Distribution.Let $A,B$ and $n$ be nonnegative integers with $n\\leq A+B$ .If a random variable $X$ has a discrete distribution with p.f. as in upside,then it is said that $X$ has the hypergeometric distribution with parameters $A,B$ and $n$\n 这个定义依旧简单明了，和前面二项分布的定义基本一个套路，给一个p.f.然后告诉你这个就是xx分布。\n我们分析下超几何分布有什么特点，与二项分布有什么不同，首先他们的基础实验都是伯努利分布，但是不同的在于二项分布每一步的试验都是恒定分布的伯努利试验，但是超几何分布不是，其每一步的伯努利分布都有不同的分布，且当前分布和前面试验结果有关，如果按照这种思路，其实超几何分布的每一次取出都是一次和前面所有取出相关的伯努利分布（而二项分布是相互独立的）。\n为什么叫超几何分布？ 超几何分布的名字和二项分布的名字来源相似都是和某个展开式的系数有关系，超几何函数的级数展开系数和我们今天的超几何分布有关系，所以就命名为超几何分布。\n几何分布和超几何分布的关系？ 几何分布是二项分布中的一种极端情况，就是前 $k-1$ 次必须失败，第 $k$ 次成功，然后试验结束，换句话说就是在伯努利过程中当结果是 $X_k=1$ 时完成整个过程，这时的概率是 $p(1-p)^{k-1}$ 。那么如果 $X$ 是几何分布，那么其p.f.就是 $f(x|1,p)=p(1-p)^x$ for $x=0,1,2,\\dots$\n超几何分布的均值和方差 The Mean and Variance for a Hypergeomtirc Distribution 基础了解的差不多了我们就开始研究下数字特征了。\n Theorem Mean and Variance.Let X have a hypergeometric distribution with strictly positive parameters A,B and n.Then: $$ E(X)=\\frac{nA}{A+B}\\ Var(X)=\\frac{nAB}{(A+B)^2}\\times \\frac{A+B-n}{A+B-1} $$\n 证明有点复杂了，要用到我们前面用到的一种思想，因为是有限的离散分布，我们总能把它转换成穷举的模式 首先证明期望： 思路，以拿球为例子说明，$A$ 个红球， $B$ 个蓝球 如果第i次试验拿到了红球 随机变量 $X_i=1$ 一共抽取 $n$ 个球。 我们假设所有球都不同，把他们所有可能的排列都列出来，然后把我们抽取过程等效成从所有可能的排列中选一个，观察前n个球中红球的数量。那么我们会有 $(A+B)!$ 行，每一个行对应一个随机结果，且等可能性，那么我们关心的是前 $n$ 个球是怎么排布样的（因为我们就抽取 $n$ 个球），这时候我们看列，第一列到第 $n$ 列，其中红球的平均数就是超几何分布的期望，每一列红球出现的比例都是 $\\frac{A}{A+B}$ 原因是 红球和蓝球等可能出现， 有点复杂，我们来举个例子，三个球，一个红球，两个蓝球，我们按照上面的思路把所有可能的出场情况都写出来，共 $3!=6$ 种，我们只观察前两个球（假设从三个球中拿两个）的分布 $$ R_1,B_1,B_2\\ R_1,B_2,B_1\\ B_1,R_1,B_2\\ B_1,B_2,R_1\\ B_2,R_1,B_1\\ B_2,B_1,R_1 $$\n我们现在观察第一列，也就是我们取出第一个球的时候，红球的期望是 $1\\times\\frac{2}{6}=1\\times\\frac{1}{1+2}$ ，同样第二列和第一列相同，那么前n列都是这样的，所以期望是 $\\frac{nA}{A+B}$ 证毕 这里用到了两个思想，\n 把全部的取球过程列举出来，避免加权，我们把所有球都当做不同球。 当把所有的结果列举出来，每行之间是互斥的。把前面每步取球相互影响的试验变成了互不影响的试验  这个证明使用了等价问题转化的思想。 同理我们可以证明每一列的方差是： $$ Var(X_i)=\\frac{AB}{(A+B)^2} $$ 因为每列之间并不独立，所以要用到前面方差求和的公式： $$ Var(X)=\\sum^{n}{i=1}Var(X_i)+2{\\sum\\sum}{i\u0026lt;j}Cov(X_i,X_j) $$ 因为考虑到所有 $Cov(X_i,X_j)=Coc(X_1,X_2)\\text{for }i\\neq j$ 最后的结果就是： $$ \\begin{aligned} Cov(X_1,X_2)\u0026amp;=-\\frac{AB}{(A+B)^2(A+B-1)}\\ Var(X_i,X_j)\u0026amp;=\\frac{nAB}{(A+B)^2}-\\frac{n(n-1)AB}{(A+B)^2(A+B-1)}\\ \u0026amp;=\\frac{nAB(A+B-1)-n(n-1)AB}{(A+B)^2(A+B-1)}\\ \u0026amp;=\\frac{nAB}{(A+B)^2}\\frac{A+B-n}{(A+B-1)} \\end{aligned} $$ 证毕\n纯计算过程，但可以看出我们的证明也越来越复杂了。\n抽样方法比较 Comparison of Sampling Methods 上面我们就大致完成了超几何分布的知识，但是要说的是我们又涉及到了采样，我在上面说过，当我们不放回采样的对象，数量不多的时候，我们要用超几何分布来建模，当数量非常大的时候，不放回采样每次采集出现目标的概率变化非常小的时候，我们可以把超几何分布当做二项分布处理，而且我们仔细观察超几何分布的期望和方差，期望和二项分布一致，方差只差一个系数 $\\alpha=\\frac{T-n}{T-1}$ 这个系数是有名字 \u0026ldquo;finite population correction\u0026rdquo; 有限整体校正参数。有限就证明了超几何分布的采样是从有限个样本中进行的，如果样本数量无限大，这个参数接近于1，就变成了二项分布。而当T较小的时候，这个参数对整体结果影响较大，这时超几何分布和二项分布差异较大。 分析系数 $\\alpha$ 也有很多有趣的，比如$n=T$ 的时候，结果是0，只有一种方法，方差是0.并且 $\\alpha\\in [0,1]$ 其越接近1，证明放回抽样和不放回抽样差距越小，越接近零表示其差距越大。\n Theorem $a_n$ and $c_n$ be sequences of real numbers such that $a_n$ converges to $0$ ,and $c_na_n^2$ converges to $0$ .Then $$ lim_{n\\to \\infty}(1-a_n)^{c_n}e^{-a_nc_n}=1 $$ In particular,if $a_nc_n$ converges to $b$ ,then $(1+a_n)^{c_n}$ converges to $e^b$\n 这个定理看起来跟我们前面写的没什么关系，而且证明也没有给出，但是用微积分的知识可以进行证明，我们下面还是看看二项分布和超几何分布之间的区别吧。\n Theorem Closeness of Binomial and Hypergeometric Distribution .Let $0\u0026lt; p \u0026lt; 1$ ,and let $n$ be a positive integer.Let $Y$ have the binomial distribution with parameters $n$ and $p$ .For each positive integer $T$ ,let $A_T$ and $B_T$ be integers such that $lim_{T\\to \\infty}A_{T}=\\infty$ , $lim_{T\\to \\infty}B_{T}=\\infty$ ,and $lim_{T\\to \\infty} A_T/(A_T+B_T)=p$ .Let $X_T$ have the hypergeometric distribution with parameters $A_T,B_T$ ,and $n$ .For each fixed $n$ and each $x=0,\\dots,n$ $$ lim_{T\\to \\infty}\\frac{Pr(Y=x)}{Pr(X_T=x)}=1 $$\n 这个证明用到了上面未证明的定理，而整个证明也比较复杂，书上有详细的过程，我就不写了，想了解详细过程的同学去看看书吧，今天大概就这样了。\n总结 本文介绍了超几何分布的相关知识，下一篇我们继续研究Poison分布。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-3-the-hypergeomtirc-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍超几何分布\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Hypergeomtirc Distribution,Finite Population Correction\u003c/p\u003e","title":"【概率论】5-3:超几何分布(The Hypergeomtric Distribution)"},{"content":"Abstract: 本文介绍Bernoulli Distribution （伯努利分布）和Binomial Distribution（二项分布） Keywords: Bernoulli Distributions，Binomial Distributions\n伯努利和二项分布 吐血更，一天三篇，虽然上一篇只能算一段，但是确实应该加快总结的步伐了，给后面的新内容腾出足够的时间\n 一杯敬自由，一杯敬死亡\n 在本章的开始，我们从离散分布下手，看看每个分布有这什么样的特点，然后用我们的工具分析研究其内在的性质，当然要从最简单的开始，逐步构建出我们要研究的有代表性的这些分布，第一个被处理的就是伯努利分布（bernoulli Distribution） 随机变量 $X$ 只有两个取值，0或者1，并且取1的概率固定是$p$ 那么我们就说 $X$ 有一个参数为 $p$ 的伯努利分布。如果我们只知道试验输出对应的随机变量只有两个结果，非此即彼，那么这个随机变量的分布就是伯努利族中的一个随机变量。 如果随机变量 $X_1,X_2,\\dots,X_n$ 有相同的伯努利分布，他们的和就是其中为1的随机变量的个数，这个个数也是随机的，其对应的分布为二项分布。\n伯努利分布 The Bernoulli Distributions 上来先来个例子：\n 临床试验，对于某种治疗，我们简单的把结果划分成两种，一种有效，一种无效，我们用随机变量来表示这两个结果，$X=1$ 表示治疗有效 $X=0$ 表示治疗无效，那么我们要做的是得到这个概率就是 $Pr(X=1)=p$ 的值就是我们关心的结果。$p$ 的取值范围在 $[0,1]$ 对应于不同的 $p$ 我们就有了伯努利分布族。\n  Definition Bernoulli Distribution.A random variable X has the Bernoulli distribution with parameter $p$ ( $0\\leq p\\leq 1$ )if X can take only the values 0 and 1 and the probabilities are $$ Pr(X=1)=p $$ and $$ Pr(X=0)=1-p $$\n 其概率函数可以被写成： $$ f(x|p)= \\begin{cases} p^x(1-p)^{1-x}\u0026amp;\\text{ for }x=0,1\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ p.f.的表示方法可以看出伯努利分布是依赖于参数 $p$ 的，所以 $p$ 可以看成一个条件，那么我们后面所有类似的分布都可以将其p.f.或者p.d.f.写成这种形式。 c.d.f.（似乎我们学c.d.f的时候已经讲过了）可以被写成： $$ F(x|p)= \\begin{cases} 0\u0026amp;\\text{ for }x\u0026lt;0 \\ 1-p\u0026amp;\\text{ for }0 \u0026lt; x \u0026lt; 1 \\ 1\u0026amp;\\text{ for }x\\geq 1 \\end{cases} $$\n期望 Expectation 当我们研究完其p.f.和c.d.f.以后就研究研究他的期望吧，也没啥可研究的了，随机变量 $X$ 有参数为 $p$ 的伯努利分布，那么其期望： $$ E(X)=p\\times1 + 0\\times(1-p)=p $$ 然后我们研究一下随机变量 $X^2$ 的概率分布 $$ E(X^2)=p\\times1^2 + (1-p)\\times0^2=p $$\n方差 Variance 期望完了当然是方差了，同样是随机变量 $X$ 有参数为 $p$ 的伯努利分布，那么其方差： $$ Var(X)=E[(X-E(X))^2]=(1-p)^2p+(-p)^2(1-p)=p(1-p)(1-p+p)=p(1-p) $$ 或者通过更简单的公式： $$ Var(X)=E[X^2]-E^2[X]=p-p^2=p(1-p) $$ 结果一致。\n距生成函数 m.g.f. 我们说过除了p.d.f./p.f.和c.d.f.，m.g.f.也是非常重要的分布标书工具，所以伯努利分布自然也有m.g.f. $$ \\begin {aligned} \\psi(t)=E[e^{tX}]=p(e^{t\\times 1})+(1-p)(e^{t\\times 0}) \u0026amp;\\text{ for } -\\infty\u0026lt;t\u0026lt;\\infty \\end {aligned} $$ 这个写起来应该没啥难度，注意好 $X$ 就行，然后就是期望对应的概率值。\n伯努利过程 Bernoulli Trials/Process 说到序列我就想起了数学分析，Tao的分析我们已经开始更新了，但是我想把概率基础部分先写完，然后一边研究数理统计一边写分析的博客，想到分析的原因是我看到了序列 如果一个序列不论是否有限，每一个元素都是独立同分布的（i.i.d.）的伯努利随机变量，那么我们就叫他们伯努利序列或者伯努利过程。\n Definition Bernoulli Trails/Process.If the random variables in a finite or infinite sequence $X_1,X_2,\\dots$ and i.i.d.,and if each random variable $X_i$ has the Bernoulli distribution with parameter p,then it is said that $X_1,X_2,\\dots$ are Bernoulli trials with parameter $p$ .An infinite sequence of Bernoulli trials is also called a Bernoulli Process.\n 伯努利过程的例子最简单的就是连续丢同一枚硬币，组成的结果正反，就组成了伯努利过程。\n二项分布 The Binomial Distributions 举个例子，这个例子和上面伯努利过程有关，连续生产一批零件，每个零件有一定的合格率，，所有零件组成的序列是一个伯努利过程，那么么我们想知道这些随机变量的和满足怎么样的分布。\n Definition Binomial Distribution.A random variable $X$ has the binomial distribution with parameters $n$ and $p$ if $X$ has a discrete distribution for which the p.f. is as follow: $$ f(x|n,p)= \\begin{cases} \\begin{pmatrix}n\\x\\end{pmatrix} p^x(1-p)^{n-x }\u0026amp;\\text{ for }x=0,1,\\dots\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ in this distribution ,$n$ must be a positive integer, and $p$ must lie in the interval $0\\leq p\\leq 1$\n 这个定义确实是以定义的语言风格来写的，直接明了的告诉你，什么东西，叫什么名字，来源出处并不是定义要阐述的，但是我们要从理论上分析为啥这就是二项分布了呢？二项分布首先是因为这个分布产生系数和二项式系数一致，而且中有两个项，而其来源是多个独立同分布的伯努利分布随机变量求和结果。\n注意：二项分布是概率论和数理统计的重要基础！\n Theorem If the random varibales $X_1,\\dots,X_n$ from $n$ Bernoulli trials with parameter $p$ ,and if $X=X_1+\\dots+X_n$ ,then $X$ has the binomial distribution with parameters $n$ and $p$\n 这个定理的证明用到的是前面计数方法以及乘法法则，加法法则，也就是 $n$ 个样本中每一个都有 $p$ 的概率是1，其余是0，总和是 $x$ 的组合方法共有 $\\begin{pmatrix}n\\x\\end{pmatrix}$ 种，所以把这些种概率 $p^x(1-p)^{n-x }$ 相加就得到了结果，被定义为二项分布。\n根据上面这条定理，我们可以很轻松的计算二项分布的数字特征了。终于知道学习那些数字特征的计算法则的用途了，下面将会非常简单。\n期望 Expectation 随机变量 $X$ 是一个参数为 $n$ 和 $p$ 的二项分布，那么其期望是： $$ E(X)=\\sum^{n}_{i=0}E(X_i)=np $$ 用到的法则：\n 独立的随机变量的和的期望，等于期望的和  方差 Variance 随机变量 $X$ 是一个参数为 $n$ 和 $p$ 的二项分布，那么其方差是： $$ Var(X)=\\sum^{n}_{i=1}=np(1-p) $$ 用到的法则：\n 独立的随机变量的和的方差，等于方差的和  如果使用别的方法求方差会非常麻烦，比如定义或者 $Var(X)=E[X^2]-E^2[X]$ 别问我怎么知道的。\n距生成函数 m.g.f. 随机变量 $X$ 是一个参数为 $n$ 和 $p$ 的二项分布，那么其距生成函数是： $$ \\psi(t)=E(e^{tX})=\\Pi^{n}_{i=1}E(e^{tX_i})=(pe^t+1-p)^n $$ 用到的法则：\n 独立的随机变量的和的m.g.f.，等于m.g.f.的累积  二项分布随机变量相加  Theorem If $X_1,\\dots,X_n$ are independent random varibales,and if $X_i$ has the binomial distribution with parameters $n_i$ and $p$ ( $i=1,\\dots,k$ ) ,then the sum $X_1+\\dots+X_k$ has the binomial distribution with parameters $n=n_1+\\dots+n_k$ and $p$ .\n 当多个二项分布有不同的 $n$ 但是有相同的 $p$ 那么他们可以相加，$n$ 是所有 $n$ 的和， $p$ 不变，这个可以根据将二项分布打散成伯努利分布然后再加起来可以看出来定理是正确的\n那么什么时候可以使用上述定理呢？\n 所有随机变量相互独立 参数 $p$ 必须相同  这两点有任何一点不成立，上面的定理都不成立。 书上接着给了个大长例子，讲的是血液检验，还有到了二分查找法，可以看看\n总结 本文介绍伯努利分布和二项分布，分析了其对应数字特征，和m.g.f下一篇我们继续研究分布——超几何分布。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-2-the-bernoulli-and-binomial-distributions.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍Bernoulli Distribution （伯努利分布）和Binomial Distribution（二项分布）\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Bernoulli Distributions，Binomial Distributions\u003c/p\u003e","title":"【概率论】5-2:伯努利和二项分布(The Bernoulli and Binomial Distributions)"},{"content":"Abstract: 本文介绍本章关于分布的内容提要 Keywords: Distribution\n各分布介绍 这篇就是个介绍，会非常短，但是还是有点点信息在里面的比如，给了一个分布的家族分类。 我们知道一个随机变量的全部信息都在其分部里面，知道了随机变量的分布就相当于完全掌握了他，所以了解一些分布的性质是比较重要的。\n各分布介绍 Introduction 我们主要把研究对象放在单随机变量的分布上，可以大致分为三类\n   Num. Discrete Continuous other     1 Bernoulli normal Weibull   2 binomial lognormal Pareto   3 hypergeometric gamma ~   4 Poisson exponential ~   5 negative binomial beta ~   6 geometric ~ ~    一共13个特殊分布，我们本章就用各种前面提到的工具，包括数字特征，基本工具，如p.d.f，c.d.f.等对这些特殊的分布进行性质分析。并且这些分布是有关联的，比如binomial分布是从Bernoulli分布发展来的，接着二项分布有得出了超几何分布，然后从二项分布可以得出Poisson分布。而连续随机变量的分布是从正态分布引申出了后面的几个分布。\n这些分布并不是全部的分布，自然界中的各种各样的随机变量满足各种各样的分布，这只是被人类破解的若干个在实际应用中常用的，我们要学会记住的并不是这些分布的性质，或者这些分布本身，而是如何研究这些分布，和这些研究工具。 我们进一步要学会的就不止研究这些已知的分布了，而是针对不同的问题提出我们自己的分布，分析研究这些分布是否能解决问题。\n总结 一篇介绍性的文章，梳理整章脉络。 待续。。\n","permalink":"https://go.face2ai.com/math/math-probability-5-1-special-distributions.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍本章关于分布的内容提要\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Distribution\u003c/p\u003e","title":"【概率论】5-1:各分布介绍(Special Distribution Introduction)"},{"content":"Abstract: 本文介绍期望的条件版本，也就是条件期望 Keywords: Expectation，Prediction，Law of Total Probability\n条件期望 说到条件，我们前面反复说，所有概率都是条件的，随机变量也是，那么这几天我们学到的各种数字特征就应该也有条件版本，而我们学的这几个数组特征都是建立在期望的基础上，所以我们只要研究了条件期望，其他各特征的条件版本就是在此基础上的函数版本。 本文还有一个重要的部分就是prediction——预测，机器学习的除了发现事物本身内在的原理，另一个目的就是预测，而我们要预测的这个变量可能我们并不知其分布性质，而是知道另一个跟他有关系的随机变量的分布，那么我们就要用到全概率法则的条件版本了，具体我们来详细说清楚\n条件期望的定义和基本性质 Definition and Basic Properties 在我们举个例子之前我们回忆一下，我们整章都在说的期望，一个随机变量的期望取决于分布，而且我们提到过，不同的随机变量有同样的分布的时候，期望是一样的，那么我们可以进一步说每个分布对应唯一的期望，但是我们知道分布是有条件版本的，所以对应的期望就是条件分布的期望，而期望这个数值在预测过程中满足最小M.S.E. 的要求，所以某些时候用条件期望来预测某个值的出现是合理的。\n 举个例子： 首先统计了某一个片区所有人家的家庭成员和手机持有数量，然后得到了下面这个表： 那么当我们随机选取这个调查中的某一家人，其中有n个家庭成员，那么他们的手机持有量是多少呢？\n 这个问题就是一个典型的预测问题。\n Definition Conditional Expectation/Mean.Let $X$ and $Y$ be random variables such that the mean of $Y$ exists and is finite.The conditional expectation(or conditional mean) of $Y$ given $X=x$ is denoted by $E(Y|x)$ and is defined to be the expectation of the conditional distribution of $Y$ given $X=x$ .\n 这个定义就是条件期望或者条件均值的定义了，如果 $Y$ 存在并且有限，条件 $X=x$ 条件期望 $E(Y|x)$\n 举个例子 如果Y是一个连续随机变量，给定条件 $X=x$ 其条件p.d.f. $g_2(y|x)$ 那么他的条件期望： $$ E(Y|x)=\\int^{\\infty}{-\\infty}yg_2(y|x)dy\\text{\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;(1)} $$ 同理如果Y是一个离散随机变量，给定条件 $X=x$ 其条件p.d.f. 那么 $$ g_2(y|x)=\\sum{\\text{ All }y}yg_2(y|x)\\text{\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;(2)} $$\n 当然上面这个定义对条件有一定要求，因为条件 X也是随机变量，所以其取值也有范围，比如有些情况下 $f_1(x)=0$ 这种情况就很尴尬了，不过也没关系，因为条件发生的概率都是0了，那么在此条件下再发生别的更是不可能，所以这种情况变得无关紧要；另一种尴尬就是$f_1(x)\\neq 0$ 也就是条件是某个正常的值了，但是这时候Y可能不存在期望，或者期望是无穷的情况，这时条件期望未定义；\n Definition Conditional Means as Random Variables.Let $h(x)$ stand for the function of $x$ that is denoted $E(Y|x)$ in either (1) or (2).Define the symble $E(Y|X)$ to mean $h(X)$ and call it the conditional mean of $Y$ given $X$\n 想一下，我们前面给出了条件的具体取值，比如当给定 $X=x_0$ 的条件下，$Y$ 的期望是什么。如果我们不给定条件特定的值，那么条件变成一个变量，从微积分的角度来看求条件期望的公式如下 $$ E(Y|X=x)=\\int^{\\infty}_{-\\infty}Yf_1(Y|X=x)dy $$ 如果$X$ 也是变量那么这个两个变量的单积分表达式的结果就是个 $X$ 的函数.\n 一个🌰 ： 临床试验，一定数量的患者接受治疗，只有治愈和未治愈两种结果，假设 $P$ 是大量试验的一个结果统计的成功比例，设 $X_i=0$ 为失败（未治愈） $X_i=1$ 为治愈，并假设 $X_1,X_2,\\dots,X_n$ 之间在条件 $P=p$ 之下独立，并有 $Pr(X_i=1|P=p)=p$ ,我们现在来计算X的条件P下的期望，因为 $X$ 是参数为 $p，n$ 的二项分布，所以 $E(X|p)=np$ 以及 $E(X|P)=nP$ 后面我们会计算当我们已知X时如何求 $P$ ，这就是预测问题了\n 注意，当给定条件 $X$ 时的条件期望 $E(Y|X)$ 是一个随机变量，有自己的分布;给定条件$X=x$ 时，条件概率 $h(x)=E(Y|x)$ 是一函数，这个函数与其他普通函数一样使用； 他们的联系是 $X$ 有一定个概率等于 $x$ 这时候， $E(Y|X=x)=h(x)$ 按照函数 $h$ 来完成计算。\n接着我们提出条件版的全概率公式：\n Theorem Law of Total Probability for Expectation.Let $X$ and Y be random variables such that Y has finite mean.Then $$ E[E(Y|X)]=E(Y) $$\n 证明: 假设X和Y是连续的随机变量并有一个连续的联合分布 $$ \\begin{aligned} E[E(Y|X)]\u0026amp;=\\int^{\\infty}{-\\infty}E(Y|x)f_1(x)dx\\ \u0026amp;=\\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}yg_2(y|x)f_1(x)dydx\\ \\end{aligned} $$ 又因为 $g_2(y|x)=f(x,y)/f_1(x)$ 那么就有： $$ E[E(Y|X)]=\\int^{\\infty}{-\\infty}\\int^{\\infty}_{-\\infty}yf(x,y)dydx=E(Y) $$ 证毕\n证明第一步用到了函数的期望，谁是函数？我上面说啦，$h(x)=E(Y|x)$ 可以当做函数使用，然后又用到了条件概率分布和边缘分布的关系，最后得到预料之内的结论。\n接下来这个定理反应的是当给定条件 $X=x$ 下的概率，和把 $X$ 当做已知常数 $x$ 是一致的.\n Let $X$ and $Y$ be random variables,and let $Z=r(X,Y)$ for some function r.The conditional distribution of $Z$ given $X=x$ is the same as the conditional distribution of $r(x,Y)$ given $X=x$\n 证明： 当X和Y有一个连续的联合分布： $$ E(Z|x)=E(r(x,Y)|x)=\\int^{\\infty}_{-\\infty}r(x,y)g_2(y|x)dy $$ 根据前面关于期望的全概率法则，我们有： $$ E{E[r(X,Y)|X]}=E[r(X,Y)] $$ 证毕\n上面这个定理是说当某个随机变量 $Z$ 是另外两个随机变量 $X,Y$ 的某个函数 $r$ 结果，那么当其中一个随机变量 $X$ 或者 $Y$ 被给定为条件的时候 $E(Z|X=x)=E[r(x,Y)|X=x]$\n作为期望的第一步扩展，我们这里给出条件方差的定义，当然也有条件距，条件偏度，条件m.g.f，多变量的条件协方差等，篇幅时间都有限，不可能都一一练习，大家要多看例子，不然后面数理统计容易懵逼\n Defintion Conditional Variance.For every given value $x$ ,let $Var(Y|x)$ denote the variance ofthe conditional distribution of $Y$ given $X=x$ .That is $$ Var(Y|x)=E{[Y-E(Y|x)]^2|x} $$\n 这就是我们说当给定 $X=x$ 时 $Y$ 的条件期望是 $Var(Y|x)$\n条件期望用来预测 Prediction 上面我们临床试验的例子描述了一种场景，并说如果在一个随机样本空间 $n$ （足够大） 中有 $X$ 个成功治愈的样例，那么我们怎么估计 $P$ 这就是一个非常接近数理统计的例子，或者说这就是一个统计了题目。\n Theorem The prediction $d(X)$ that minimizes $E{[Y-d(X)]^2}$ is $d(X)=E(Y|X)$\n 证明过程略复杂，当时我们还是详细的写一下，毕竟后面统计要用到。在证明之前我们先仔细看看这个定理说的到底是个什么事，我们想要预测某个随机变量 $X$ 的某个函数( $d$ )的结果，误差函数被定义为 $[Y-d(X)]^2$ 其结果是 $d(X)=E(Y|X)$ 误差函数或者损失函数是机器学习里的名词，这里指的就是类似于前面 M.S.E. 和M.A.E. 的一个指标，最小化时有最优结果。 证明： 我们证明X有一个连续分布，离散分布与连续情况基本一致。\n 令 $d(X)=E(Y|X)$ , $d\u0026rsquo;(X)$ 是一个任意的预测结果，我们只需要证明 $E{[Y-d(X)]^2}\\leq E{[Y-d\u0026rsquo;(X)]^2}$ 成立 根据 $E{E[r(X,Y)|X]}=E[r(X,Y)]$ 有 $Z\u0026rsquo;=[Y-d\u0026rsquo;(X)]^2$ 并且 $h\u0026rsquo;(x)=E(Z\u0026rsquo;|x)$ 可以把1中的不等式转化成连续函数的期望的形式 $$ \\int h(x)f_1(x)dx\\leq \\int h\u0026rsquo;(x)f_1(x)dx $$ 所以我们就把上面要证明的目标转换成了证明 $h(x)\\leq h\u0026rsquo;(x)$ 那么我们如果证明了 $E{[Y-d(X)]^2|x}\\leq E{[Y-d\u0026rsquo;(X)]^2|x}$ 就相当于证明了3 假定4中的 $x$ 为常数，根据MSE很容易证明结论  证毕\n这个证明有点粗糙，但是大概框架给出了，大家需要查阅这两天的博客就能得出完成的证明过程了。\nM.S.E. 还和 $Var$ 有关系：当我们预测在给定条件 $X=x$ 的条件下预测 $Y$ 为 $E(Y|x)$ 时其M.S.E就是 $Var(Y|x)$ 如果在 $X$ 未知的情况下最佳预测就是 $E[Y]$\n Theorem Law of Total Probability for Variance.If $X$ and $Y$ are arbitrary random variables for which the necessary expectations and variances exist,then $Var(X)=E[Var(Y|X)]+Var[E(Y|X)]$\n 总结 本文我们介绍了条件版本的期望，然后扩展到了条件版本的方差，以及预测的相关知识。下一篇开始介绍各种各样的分布。 待续\n","permalink":"https://go.face2ai.com/math/math-probability-4-7-conditional-expectation.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍期望的条件版本，也就是条件期望\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Expectation，Prediction，Law of Total Probability\u003c/p\u003e","title":"【概率论】4-7:条件期望(Conditional Expectation)"},{"content":"Abstract: 本文介绍协方差和相关性的基础知识，以及部分性质 Keywords: Covariance,Correlation,Properties of Covariance and Correlation\n协方差和相关性 概率论基础知识，基础工具已经进入到后半部分了，接下来后面就是对特定分布的研究和分析了，使用的工具就是我们已经介绍过的这些知识，融汇贯通是所有知识学习的唯一考量，掌握的知识点如果不能融入体系，一个月后就相当于没学过，但是成体系的知识不同，只要有一个根节点，就能联系到整个一颗知识树。\n 一杯敬朝阳，一杯敬月光\n 我们前面几个重要的数字特征针对的基本都是单一随机变量，我们很清楚，我们在实际操作中面对的基本都是多随机变量的联合分布，那么我们接下来就想研究下，两个或者多个随机变量之间是怎么互相影响的。 协方差(Covariance)，相关性(Correlation)是度量随机变量间独立性的一种数字特征，但是必须注意，这两个数字特征度量的是随机变量之间的 线性相关程度 ，这里要好好注意一下！线性相关程度。 注意，协方差和相关性，只刻画线性相关程度！\n协方差 Covariance 当我们将随机变量从一个扩展到多个，前面提到的期望，方差，中值等这些针对单个随机变量的数字特征就只能刻画联合分布的某一边缘分布的性质了。所以我们提出了新的数字特征，这个数字特征能描述两个随机变量之间有没有变化上的关系，比如他们经常同时变大或者变小，或者总是一个变大另一个变小，这种关联的关系。 通过这种数字特征，我们能够在求出若干个这种变量的方差，以及通过已经得到的几个随机变量的结果来预测其他几个。如果确定了这几个随机变量之间的关联，这些似乎都是可行的。\n Definition Covariance. Let $X$ and $Y$ be random variables having finite means.Let $E(X)=\\mu_X$ and $E(Y)=\\mu_Y$ The covariance of X and Y,which is denoted by $Cov(X,Y)$ ,is defined as $$ Cov(X,Y)=E[(X-\\mu_X)(Y-\\mu_Y)] $$ if the expectation exists.\n 没错我们本章就是在研究期望，所以，本章所有的数字特征都来自期望，期望的存在性也左右了这些数字特征的存在性。 如果 X 和Y的都有有限的方差，那么期望存在，并且 $Cov(X,Y)$ 存在且有限，但是正负不受限制，可以是正数，负数，0\n 举个🌰 ： 已知随机变量 $X$ 和 $Y$ 有联合p.d.f. $$ f(x,y)= \\begin{cases} 2xy+0.5\u0026amp;\\text{ for } 0\\leq x\\leq 1 \\text{ and } 0\\leq y\\leq 1\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ 我们来计算 $X$ 和 $Y$ 的协方差。\n 首先我们要做的是计算均值，求 $\\mu_X,\\mu_Y$\n$$ \\begin{aligned} \\mu_X\u0026amp;=\\int^{1}{0}\\int^{1}{0}[2x^2y+0.5x]dydx\\ \u0026amp;=\\int^{1}_{0}[x^2+0.5x]dx\\ \u0026amp;=\\frac{7}{12} \\end{aligned} $$\n同理可以求出 $\\mu_Y=\\frac{7}{12}$ 接下来就是求协方差了: $$ \\int^{1}{0}\\int^{1}{0}(x-\\frac{7}{12})(y-\\frac{7}{12})(2xy+0.5)dydx $$ 求积分就不写了，很简单，结果是 $Cov(X,Y)=\\frac{1}{144}$\n按照定义算肯定不是最优的，有一个公理好像是说你永远不能一下就找到最优方法。计算协方差也是一样的。\n Theorem For all random variables X and Y such that $\\sigma^2_{X}\u0026lt;\\infty$ and $\\sigma^2_{Y}\u0026lt;\\infty$ , $$ Cov(X,Y)=E(XY)-E(X)E(Y) $$\n 这个定理是说当两个随机变量都有方差的时候，他们的联合分布的协方差可以用他们的期望来求得，这是个定理，定理都是可以被证明（定义不行） 证明： $$ \\begin{aligned} Cov(X,Y)\u0026amp;=E(XY-\\mu_X Y-\\mu_Y X + \\mu_X\\mu_Y)\\ \u0026amp;=E(XY)-\\mu_X E(Y)-\\mu_y E(X) + \\mu_X\\mu_Y)\\ \\end{aligned} $$ 就得到了上面定理的结论，证明过程非常简单。\n协方差的的主要用途就是来刻画两个或者多个变量的相关程度，比如两个随机变量同时都变大或者同时都变小，或者一个变大一个变小。 观察定义我们可以注意到，当协方差大于0的时候，一般情况下如果出现了 $ X \u0026gt; \\mu_X$ 就基本上会出现 $Y \u0026gt; \\mu_Y$ 。是否一定会出现 $Y \u0026gt; \\mu_Y$ ？这个是不确定的，但是发生概率极大。 同样的情况适合于协方差是负数，或者$ X \u0026lt; \\mu_X$ 的情况 当协方差是0，那么 $X$ 与 $Y$ 对应于其均值的大小变换比较随意，没有太大的一致性.\n上面介绍的就是协方差的一些情况，接下来就是相关性的引入。\n相关性 Correlation 今天讲解两个数字特征，协方差和相关性，这两个数字特征最终目的一样都是想描述多个变量之间一致性变化的特点，比如，当 $X$ 为较大值的时候 $Y$ 有很大的可能取较大值，注意，我们前面给出的协方差的大小就是这个可能性的一种描述，但是，这个描述也有问题，他不稳定，为啥不稳定， 比如说随机变量 $X$ 和随机变量 $Y$ 他们的协方差是 $Cov(X,Y)$ 根据协方差的计算法则，当我们把随机变量变成 $2X$ 和 $Y$ 的时候 $Cov(2X,Y)=2Cov(X,Y)$ ，但他们的一致性关系应该是不变的，只是对应的随机变量的可能值变化了不少，一致性并不改变，我们需要一种数字特征能描述这种一致性，不因为随机变量伸缩而改变。\n Definition Correlation.Let X and Y be random variables with finite variances $\\sigma^2_{X}$ and $\\sigma^2_{Y}$ ,respectively. Then the correlation of $X$ and $Y$ ,which is denoted by $\\rho(X,Y)$ ,is defined as follow: $$ \\rho(X,Y)=\\frac{Cov(X,Y)}{\\sigma_X^2 \\sigma_Y^2} $$\n 回想一下，我们似乎见过这种比值形式的定义，没错，4-4中的偏度也是这种定义形式，其给出的解释去除分母上的特征对目标特征的影响，于是我们可以看出，当协方差去除分布离散程度以后，就是我们的相关度特征了。 接着我们有一个重要的不等式需要了解。\n Theorem Schwarz Inequality.For all random variables $U$ and $V$ such that $E(UV)$ exists, $$ [E(UV)]^2\\leq E(U^2)E(V^2) $$ If,in addition,the right-hand side of $[E(UV)]^2\\leq E(U^2)E(V^2)$ is finite,then the two sides of it equal the same value if and only if there are nonzero constants $a$ and $b$ such that $aU+bV=0$ with probability 1.\n 首先给出了两个随机变量的期望的相关不等式，我们会在接下来完成证明，但是补充条款更有意思，说的是，小于等于号右边如果是有限的，那么等号成立当且仅当存在非零常数 $a$ 和 $b$ 使得 $aU+bV=0$ 横成立（或者叫做概率为1）\n那么我们来证明这个定理。 证明：\n 如果 $E(U^2)=0$ 那么 $Pr(U=0)=1$ 所以必然有 $Pr(UV=0)=1$ 那么 $E(UV)=0$ 于是不等式成立。 同理可以证明 $E(V^2)=0$ 的情况。 当 $E(U^2)$ 或者 $E(V^2)$ 为无限的时候，不等式也成立。 接下来证明 $0 \u0026lt; E(U^2) \u0026lt; \\infty$ , $0 \u0026lt; E(V^2) \u0026lt; \\infty$ 的情况，对于所有的 $a$ 和 $b$ 那么： 不等式一： $$ 0\\leq E[(aU + bV)^2]=a^2E(U^2)+b^2E(V^2)+2abE(UV) $$ 以及，不等式二： $$ 0\\leq E[(aU - bV)^2]=a^2E(U^2)+b^2E(V^2)-2abE(UV) $$ 如果 令$a=[E(V^2)]^{1/2},b=[E(U^2)]^{1/2}$ 那么就有下面的关系： 不等式三： $$ E(UV)\\geq -[E(U^2)E(V^2)]^{1/2} $$ 根据不等式二，就有不等式四： $$ E(UV)\\leq [E(U^2)E(V^2)]^{1/2} $$ 上面两个不等式，不等式三和不等式四得出定理中的结论。 不等式中等号成立，当且仅当不等式三和不等式四等号成立，不等式三等号成立，当且仅当不等式一等于0成立，也就是当且仅当 $E[(aU+bV)^2]=0$ 成立，当且仅当 $aU+bV=0$ 恒成立。 同理可以得到 $aU-bV=0$ 恒成立，至此证毕！   Theorem Cauchy-Schwarz Inequality.Let $X$ and $Y$ be random variables with finite variance.Then $$ [Cov(X,Y)]^2\\leq \\sigma^2_X\\sigma^2_Y $$ and $$ -1\\leq \\rho(X,Y)\\leq 1 $$ Furthermor,the inequality in $[Cov(X,Y)]^2\\leq \\sigma^2_X\\sigma^2_Y$ is an equality if and only if there are nonzero constants $a$ and $b$ and a constant $c$ such that $aX+bY=c$ with probability 1.\n Cauchy-Schwarz不等式，柯西是谁不介绍了，Schwarz翻译成中文叫施瓦茨。 这个不等式给出了相关性的关键信息，也就是相关性在 $[-1,1]$ 范围内，接下来我们看看如何证明他们： 证明\n 令$U=X-\\mu_X$ 和 $V=Y-\\mu_Y$ 根据协方差定理 $Cov(X,Y)=E(XY)-E(X)E(Y)$ 可以直接得到 $[Cov(X,Y)]^2\\leq \\sigma^2_X\\sigma^2_Y$ 然后就可以得到 $-1\\leq \\rho(X,Y)\\leq 1$ 这个结论  这个证明非常简单，只用到了前面协方差的一个计算定理，所以，可见相关性在 $[-1,1]$ 之间波动。\n Definition Positively/Negatively Correlation/Uncorrelated.It is said that $X$ and $Y$ are positively correlated if $\\rho (X,Y)\u0026gt;0$ ,that $X$ and $Y$ are negatively correlated if $\\rho(X,Y) \u0026lt; 0$ ,and that $X$ and $Y$ are uncorrelated if $\\rho(X,Y)=0$\n 定义正相关，负相关，还是不相关。\n接下来我们就要研究协方差，和相关性的性质了。\n相关性和协方差的的性质 Properties of Covariance and Correlation 性质1：独立的随机变量的相关性\n If $X$ and $Y$ are independent random varibales with $0\u0026lt;\\sigma^2_X\u0026lt;\\infty$ and $0\u0026lt;\\sigma^2_Y\u0026lt;\\infty$ ,then $$ Cov(X,Y)=\\rho(X,Y)=0 $$\n 证明，如果随机变量 $X$ 和 $Y$ 独立，那么 $E(XY)=E(X)E(Y)$ ，根据定理 $Cov(X,Y)=E(XY)-E(X)E(Y)$ 有 $Cov(X,Y)=0$ 那么就有 $\\rho(X,Y)=0$ 证毕。\n注意注意注意，两个变量独立可以推导出其相关性是0，但是相关性是0并不能推到出随机变量独立。\n 这里举个例子 随机变量 $X,Y$ 的联合分布是在一个圆范围内的均匀分布，可以得到其p.d.f. 是 $$ f(x)= \\begin{cases} \\frac{1}{2\\pi}\u0026amp;\\text{for } x^2+y^2 \\leq 1\\ 0\u0026amp;\\text{otherwise } \\end{cases} $$ 因为随机变量变化范围是个圆而不是矩形，所以很明显 X和Y不独立（参考随机变量的独立性），但是可以计算其协方差为 $Cov(X,Y)=E[XY]-E[X]E[Y]=0-0=0$ 那么其相关性也是 0 ，于是相关的两个随机变量，其协方差，相关性也可以是0.  性质2：如果两个随机变量是线性关系，那么相关性为1\n Theorem Suppose that $X$ is a random variable such that $0\u0026lt;\\sigma^2_X\u0026lt;\\infty$ ,and $Y=aX+b$ for some constants $a$ and $b$ ,where $a\\neq 0$ ,If $a \u0026gt; 0$ the $\\rho(X \u0026lt; Y)=1$ If $a \u0026lt; 0$ ,then $\\rho(X,Y)=-1$\n 证明：\n 如果 $y=ax+b$ 那么 $\\mu_Y=a\\mu_X+b$ ,$Y-\\mu_Y=a(X-\\mu_X)$ 根据协方差定义有 $Cov(X,Y)=aE[(X-\\mu_X)^2]=a\\sigma^2_X$ 因为有 $\\sigma_Y=|a|\\sigma_X$ 所以定理结论得到证明 （这步可由柯西-施瓦茨不等式得出） 证毕  这个定理告诉我们，相关性就是衡量两个随机变量的线性相关程度的。越接近线性，相关性的绝对值越接近1，反之越接近0， 注意相关性只用来衡量线性相关。相关性越接近零并不代表随机变量不相关，而是只代表他们不线性相关。\n性质三：两个随机变量相加，其协方差和单个变量方差的关系\n Theorem If $X$ and $Y$ are random variables such that $Var(X)\u0026lt;\\infty$ and $Var(Y)\u0026lt;\\infty$ ,then $$ Var(X+Y)=Var(X)+Var(Y)-2Cov(X,Y) $$\n 证明： 因为 $E[X+Y]=\\mu_X+\\mu_Y$ ,所以 $$ \\begin{aligned} Var(X+Y)\u0026amp;=E[(X+Y-\\mu_X-\\mu_Y)^2]\\ \u0026amp;=E[(X-\\mu_X)^2+(Y-\\mu_Y)^2+2(X-\\mu_X)(Y-\\mu_Y)]\\ \u0026amp;=Var(X)+Var(Y)+2Cov(X,Y) \\end{aligned} $$ 简单的计算，就不啰嗦了。\n接着是一个推论，在上面定理成立的情况下，我们有：\n Corollary Let a,b and c be constants.Under the conditions of theorem upside $$ Var(aX+bY+c)=a^2Var(X)+b^2Var(Y)+2abCov(X,Y) $$\n 还有一种特殊的情况就是 $$ Var(X-Y)=Var(X)+Var(Y)-2Cov(X,Y) $$\n性质四：根据性质三推广到多个随机变量的情况：\n Theorem If $X_1,\\dots,X_n$ are random variables scuh that $Var(X_i)\u0026lt;\\infty$ for $i=0,\\dots,n$ then $$ Var(\\sum^{n}{i=1}X_i)=\\sum^{n}{i=1}Var(X_i)+2{\\sum\\sum}_{i\u0026lt;j}Cov(X_i,X_j) $$ 这个定理的证明相对要麻烦点， 证明：\n  首先 $$ Var(\\sum^{n}{i=1}X_i)=Cov(\\sum^{n}{i=1}X_i,\\sum^{n}{j=1}X_j)=\\sum^{n}{i=1}\\sum^{n}_{j=1}Cov(X_i,X_j) $$ 把上面的求和分成两部分一部分是 $i=j$ 一部分是 $i\\neq j$ ，因为 $Var(x_i,x_j)=Var(x_j,x_i)$ $$ \\begin{aligned} Var(\\sum^{n}{i=1}X_i)\u0026amp;=\\sum^{n}{i=1}Var（X_i）+{\\sum\\sum}{i\\neq j}Cov(X_i,X_j)\\ \u0026amp;=\\sum^{n}{i=1}Var(X_i)+2{\\sum\\sum}_{i\u0026lt;j}Cov(X_i,X_j) \\end{aligned} $$  证明过程大致如上所述，很简单的计算过程，如有疑问可以去参考下原文\n上述定理得出一个推论\n Corollary If $X_1,\\dots,X_n$ are uncorrelated random varibales,then $$ Var(\\sum^{n}{i=1}X_i)=\\sum^{n}{i=1}Var(X_i) $$\n 总结 今天一下介绍了两个多随机变量的数字特征，所描述的性质类似，但是又各有各的用法，这部分内容在机器学习中非常常见，大家要好好研究，多做练习。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-4-6-covariance-and-correlation.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍协方差和相关性的基础知识，以及部分性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Covariance,Correlation,Properties of Covariance and Correlation\u003c/p\u003e","title":"【概率论】4-6:协方差和相关性(Covariance and Correlation)"},{"content":"Abstract: 本文介绍均值和中值的对比，以及最小平方误差，最小绝对误差 Keywords: Mean，Median，Mean Squared Error，Mean Absolute Error\n均值和中值 昨天犯了个大错误，google分析配置出错了，所以这两天博客访问一直显示零，所以昨天都很沮丧，生活有时候就这样，一些错误，后果非常让人非常沮丧，我们在面对这些沮丧的结果时的态度能决定我们的所有。 均值是度量分布中心位置的一种方法，中值也是，这就是我们上一篇说到的，关于一个属性的定义，我们现在定义分布的中心位置，就有了两种方法，这两种都能定义中心的合理方法，各有各的优点，也有自己的缺点，所以我们今天就来对比下这两种中心位置的数字特点。\n中位数 The Median 4.1中介绍过一个分布的的期望，是在随机变量所在的数轴的重心位置，这种角度下，期望是一个中心位置。 另一种就是假设存在某个随机变量 $m_0$ 小于 $m_0$ 对应的概率是 $1/2$ 大于 $m_0$ 的对应概率为 $1/2$ 这从某种意义上说也是一个中心位置。 两个不一样的定义方式，就有两种不同的方法用于不同的问题 最简单的例子就是图像处理里面两种不同的滤波，均值滤波和中值滤波，对应处理的噪声也完全不同。 均值，也就是期望我们已经研究了4篇了，今天我们主要研究一下中值，虽然在c.d.f中有介绍，但是我们还是重新说说。 值得一提的是与均值不同，分布的均值可以有一个或者没有，而中值可以有一个，还可以有很多个，这个我们后面会说到。\n Definition Median.Let $X$ be a random varibale.Every number $m$ with the following prperty is called a median of the distribution of $X$: $$ \\begin{aligned} Pr(X\\leq m)\\geq 1/2\n\\end{aligned} \\text{ and } \\begin{aligned} Pr(X\\geq m)\\geq 1/2 \\end{aligned} $$\n 中值的定义如上，还有一种跟奇特的说法和上面是等价的。\n Firest,if m is included with the values of X to the left of m,then $$ Pr(X\\leq m)\\geq Pr(X\u0026gt;m) $$\n  seconde ,if m is included with the values of X to the right of m,then $$ Pr(X\\geq m)\\geq Pr(X\u0026lt;m) $$\n  If there is a number $m$ such that $Pr(X \u0026lt; m)=Pr(X \u0026gt; m)$ ,that is,if the number $m$ does actually divide the total Probability into two equal parts,then $m$ will of course be a median of the distribution of $X$\n 两种定义中值的方法得到一样的结果。值得注意的就是一点，中值可能不止一个，当中值不止一个的时候我们这里选用最小的作为中值，当然，也可以选中间的，或者最大的，这取决于你自己的需求。\n文章写到这里，书上开始写🌰，目测有一斤🌰 。例子的核心就是中值不一定只有一个。\n均值和中值的比较 Comparison of the Mean and the median 最重要的一个区别就是期望有些分布是没有的，但是中值绝对存在，而且有很多时候还不止一个。所以对于某些应用中值的稳定性更好，最简单的例子就是我们伟大祖国各个城市的平均收入贼高，但是你和周围的小伙伴总发现自己拖后腿，那么这时候平均值可能真的不能反映实际的情况，需要用中值，如果你的工资连中值都没到，说明你确实不行，你说你比不过马云马化腾我们理解，你连你隔壁都不如，那就是你的问题了。 下面定义一个双射函数，这个证明我不写了，书上有，但是我觉得不完美，因为我马上要在数学分析那个系列里面讲这个情况，所以这里只给出定理，着急的小伙伴就自己查查资料，不着急的，等我数学分析。\n Theorem One-to-One Function.Let $X$ be a random variable that takes values in an interval $I$ of real numbers.Let $r$ be a one-to-one function defined on the interval $I$.If $m$ is a median of $X$ ,then $r(m)$ is a median of $r(X)$\n 其实用到的主要特性是双射函数的可逆性质，换句话说就是能找到反函数。\n接下来我们开始进入到很贴近应用的部分了。\n最小均方误差 Minimizing the Mean Squared Error 假设某随机变量 $X$ 其期望是 $\\mu$ 方差是 $\\sigma^2$ ，并且假设，$X$ 是通过某种试验得到的结果，现在我们还没有做实验，但是我们希望预测下结果，应该怎么做。 这个课题熟悉不？买彩票，买股票，赌博，各位老铁，激动了么，说实话，想指着学了这点概率发家致富，你还是去睡觉吧，可能睡觉致富更快一点。 为了预测出一个靠谱的结果，我们要做的第一件事不是找结果，而是告诉大家啥样的猜测算靠谱。你总预测我能当美国总统也不现实。 靠谱，就是错误小，没法生的时候，就是发生错误的可能性小，换句话说，就是犯错的期望值最小。\n Definition Mean Squared Error/M.S.E.. The number $E[(X-d)^2]$ is called the mean squared error(M.S.E) of prediction $d$ .\n 这个地方定义了错误，也就是什么是不靠谱的概率，也就是靠谱的概率。\n Theorem Let $X$ be a random varibale with finite variance $\\sigma^2$ ,and let $\\mu=E(X)$ .For every number $d$ , $$ E[(X-\\mu)^2]\\leq E[(X-d)^2] $$ Furthermore ,there will be equality in the relation if and only if $d=\\mu$\n 上面这个定理是说任何随机变量的均方误差都是大于等于方差的，等于号当且仅当d是均值的时候成立，我们需要证明下： $$ \\begin{aligned} E[(X-d)^2]\u0026amp;=E(X^2+2dX+d^2)\\ \u0026amp;=E(X^2)-2d\\mu+d^2 \\end{aligned} $$\n上面式子中，只有当 $d=\\mu$ 的时候才能得到最小值，这个最小值也就是方差，简单的求最值，这里就不啰嗦了。\n所以在一个分布已知的情况下，预测就预测均值，其均方误差一定是最小的。\n最小化绝对误差均值 Minimizing the Mean Absolute Error 跟上面的最小均方误差出发点类似的，也是我们先要定义一个误差，这个误差越大表示结果越不好，那么：\n Definition Mean Absolute Error/M.A.E. The number $E(|X-d|)$ is called the mean absolute error (M.A.E) of the prediction $d$ .\n 接下来我们要看到的是，在这种误差定义下，我们得到的最优猜测是中值，而不是均值。\n Theorem Let $X$ be a random variable with finite mean,and let $m$ be a median of the distribution of $X$ .For every number $d$ , $$ E(|X-m|)\\leq E(|X-d|) $$ Furthermore,there will be equality in the relation if and only if $d$ is also a median of the distribution of $X$\n 上面定义的绝对误差期望 M.A.E.在最小化时得到的最优猜测是中值，我们必须要证明一下。 证明： 我们假设连续随机变量 $X$ 的p.d.f.是 $f$ 所有不同的分布都是类似的。 首先假设 $d \u0026gt; m$ 其中 $m$ 是中值\n$$ E(|X-d|)-E(|X-m|)=\\int^{\\infty}{-\\infty}(|x-d|-|x-m|)f(x)dx\\ =\\int^{m}{-\\infty}(d-m)f(x)dx+\\int^{d}{m}(d+m-2x)fxdx+\\int^{\\infty}{d}(m-d)f(x)dx\\ \\geq \\int^{m}{-\\infty}(d-m)f(x)dx+\\int^{d}{m}(d+m)fxdx+\\int^{\\infty}_{d}(m-d)f(x)dx\\ =(d-m)[Pr(X\\leq m)-Pr(X \u0026gt; m)] $$\n因为 $d \u0026gt; m$ 且 $m$ 是中值,那么： $$ Pr(X\\leq m)\\geq 1/2 \\geq Pr(X\u0026gt;m) $$ 所以上的结果必然是个非负数，最小值只能是当 $d=m$ 时候才相等，因此 $E(|X-d|)\\geq E(|X-m|)$.\n同理可以证明 $d \u0026lt; m$ 时候的情况。\n关于M.S.E. 和M.S.A. 是否有限的重要结论 观察 M.S.E和M.A.E我们发现，每个分布都有中值，但是这两个误差是否有限其实和中值无关，从定义上看，只有随机变量的方差存在的时候，M.S.E才有限，只有当分布的期望存在时， M.A.E 才有限。\n总结 对标均值和中值，得到一些重要的结论，尤其是后面有关于预测的部分，这个是机器学习里面比较常用到的基础知识，大家要好好掌握！ 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-4-5-the-mean-and-the-median.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍均值和中值的对比，以及最小平方误差，最小绝对误差\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Mean，Median，Mean Squared Error，Mean Absolute Error\u003c/p\u003e","title":"【概率论】4-5:均值和中值(The Mean and the Median)"},{"content":"Abstract: 本文介绍另一个基于期望的随机变量分布的数字特征——“矩” （不知道咋翻译的，英文moment表示时刻,瞬间等意思） Keywords: Moments，Moments Generating Function\n距 能继续写关于基础数学知识的博客，靠的全是毅力吧，昨天一天博客的访问量是1，可能大家真的都对这个没什么兴趣。 有没有人看都要继续，因为我认为这个有用，而且值得写。 距这个名词第一次听说我记得非常清楚，2014年10月左右，在尚都蜗居的时候（挨饿的时候），准备去大疆面试，晚上看书在一本机器学习的书上看到概念，然后通过博客查了一下，得到了很多总结，还记得那个博主解释了3阶距4阶距什么的，但是不得其精妙，今天本篇的目的是掌握数学特性，后面再研究机器学习应用的时候就能领悟其中的奥秘了。看博客知乎学的知识，多半是碎片化的，系统的学习还是需要大量时间的 上一篇我们研究了第一个期望的衍生数字特征——方差，今天我们来第二个，Moments，中文翻译叫距，其本质是随机变量 $X$ 的 $k$ （正整数）次幂的期望，也就是 $E[X^k]$ 那么这个距可以看做期望的更一般形式，也就是说距其实包含了期望，期望是一种特殊的距，即 $k=1$ 当然当$k\\geq 2$ 时，这个距开始有更大的用途了。 另外就是m.g.f.了，和p.d.f.以及c.d.f.一样重要的一个描述随机变量的函数，中文翻译叫“生成距函数”？不知道中文叫啥，但是他的作用很大，可以帮助得到多个独立随机变量的和的分布，也可以用来确定分布的某些限制属性。\n距的存在 Existence of Moments 距就是一个随机变量指数函数的分布的期望，期望第一课就能求距，但是距的性质使他成为一个重要的数字特征。 对于任意随机变量 $X$ 以及正整数 $k$ ，期望$E[X^k]$ 被称为随机变量 $X$ 的 $k$ 阶距。那么如果按照距的说法，我们4.1中的期望就是1阶距。 就像不是所有随机变量都有期望一样，因为距也是期望，所以有些随机变量没有距，几阶都没有。\n距存在的条件 $(E[X^k]\u0026lt;\\infty)$ 那么：\n 如果随机变量有界，那么距存在（也就是如果有有限的数字 $a,b$ 使得 $Pr(a\\leq X\\leq b)=1$ 成立） 即使随机变量没有界，那么也有可能存在所有的距。 下面定理表示如果存在 $k$ 阶距，那么存在所有小于 $k$ 阶的距。   Theorem If $E(|X|^k)\u0026lt;\\infty$ for some positive integer $k$ ,then $E(|X|^j)\u0026lt;\\infty$ for every positive integer $j$ such that $j \u0026lt; k$\n 下面我们要做的就是证明这个定理了 证明： 证明之前，我们假定研究的对象是连续随机变量，并且其 p.d.f. 为 $f$ 那么： $$ \\begin {aligned} E(|X|^j)\u0026amp;=\\int^{\\infty}{-\\infty}|x|^jf(x)dx\\ \u0026amp;=\\int{|x|\\leq 1}|x|^jf(x)dx+\\int_{|x|\u0026gt;1}|x|^jf(x)dx\\ \u0026amp;\\leq \\int_{|x|\\leq 1}1\\cdot f(x)dx+\\int_{|x|\u0026gt;1}|x|^kf(x)dx\\ \u0026amp;\\leq Pr(|X|\\leq 1)+E(|X|^k) \\end{aligned} $$\n证明思路是利用了指数运算的计算性质，第一个小于等于号是首先小于一部分的随机变量被1替代，其次是后面大于1的随机变量指数变大，所以会产生一个小于等于，其实等号不会成立，因为定义中 $j\\leq k$ 所以后面那一项一定是大于前一项的；第二个小于等于号就是把上面两个积分对应下来，一个是Pr的定义，一个是期望的定义的扩展，证毕。\n中心距 Central Moments 进一步我们提出中心距的概念。\n Suppose that $X$ is a random variable for which $E(X)=\\mu$ .For every positive integer $k$ ,the expectation $E [(X-\\mu)^k]$ is called the $k$th central moment of $X$ or the $k$th moment of $X$ about the mean.\n 距的更广泛的一中定义，上面我们的原始定义可以看做中心距的均值为0的版本。 因为减去了期望，所以一阶中心距肯定是0了。如果不是说明你算错了。\n更仔细观察，发现，方差是二街中心距。 方差也好，距也好都是期望的产品，所以其间必然有着各种关系。 中心距的另一个性质就是如果 $k$ 是奇数，同时 $X$ 的分布是一个对称的函数，那么其期望必然是0. 举个🌰 但是不给出答案了： 一个对称的 p.d.f 假设X有一个连续分布，其p.d.f.如下： $$ f(x)=ce^{-(x-3)^2/2} \\text{ for } -\\infty\u0026lt;x\u0026lt;\\infty $$ 我们需要求出他的期望，以及中心距。\n这个例子可以好好研究一下，用了不少小trick，以及分部积分等微积分的知识。这里就不写详细的过程了，但是希望大家去看看书。\n偏度 Skewness 偏度，上面我们说过，有些对称随机变量的奇数中心距是0，然后我们就想，有没有什么数字特征可以描述一个随机变量的对称程度。\n Definition Let $X$ be a random variable with mean $\\mu$ ,standard deviation $\\sigma$ ,and finite third moment. The skewness of $X$ is defined to be $E[(X-\\mu)^3]/\\sigma^3$\n 三阶中心距与标准差的三次方的比值，定义了偏度，除以一个标准差的三次方其实是为了让偏度与分布的离散程度无关，而只描述对称程度。对称的分布偏度是0，越偏越大，因为偏这个描述本身就没有定义，而且分散程度这个也没有数量定义，所以我们就用方差直接定义了分散程度，而这个偏度说是去除了分散程度对他的影响，就是除去了方差，这些其实想推理一下是有困难的，因为这个是定义，定义就是一个描述，然后起个名字，你完全可以用你的定义方法去定义偏度，但是只要别人的定义也不违反定理，那你就没有任何理由说别人的定义错。\n然后关于偏度书上也有个例子，这里我就不写了，都是计算的例子，大家自己做两道题熟悉下就好了\n距生成函数 Moment Generating Functions 这个可以说是本篇的大Boss，因为这是个新的工具，来研究分布，就是m.g.f，中文翻译过来是距生成函数？这个工具与p.d.f 和c.d.f一样重要，是一个描述随机变量重要的方法，但是这个方法跟随机变量的距关系比较大，而不像 p.d.f 或者 c.d.f 那样跟随机变量的概率关系较大。\n Definition Moment Generating Function. Let X be a random variable.For each real number t,define $$ \\psi(t)=E(e^{tX}) $$ The function $\\psi(t)$ is called the moment generating function(abbreviated m.g.f.)of $X$\n 注意，m.g.f 只与随机变量的分布有关系，同一个分布只有一个m.g.f. 如果两个随机变量的m.g.f. 那么这两个随机变量分布也一定是一样的。 接着还是存在性问题，就是每个随机变量的m.g.f都存在么？显然也不是因为这里也用到了期望，因为期望不总存在所以m.g.f不总存在。\n 随机变量有界，所以 $e^{tX}$ 对所有 $t$ 都是有限的。 随机变量没有界，那么有些 $t$ 是有限的，有些 $t$ 则不是 $t=0$ 这种特殊情况，m.g.f. 为1  那么我们有个问题了，我们上面说的那么热闹，我们还不知道这个函数到底跟距有啥关系，而且叫距生成函数，是距生出来的函数，还是这个函数能生距？\n Theorem Let $X$ be a random variables whose m.g.f $\\psi(t)$ is finite for all values of $t$ in some open interval around the point $t=0$ .Then,for each integer $n\u0026gt;0$ ,the $n$th moment of $X$ ,$E(X)$,is finite and equals the $n$th derivative $\\psi^{(n)}(t)$ at $t=0$ ,That is $E(X^n)=\\psi^{(n)}(0)$ for $n=1,2\\dots$\n 定理看明白没？没看明白我用大白话讲一下，就是m.g.f的n阶导数在 $t=0$ 处的值，刚好是随机变量的n阶距。很神奇吧，原因还是 $e^x$ 的导数的性质造成的这种现象，我们可以举个例子算算看：\n随机变量 $X$ 的p.d.f. 是 $$ f(x)=\\begin{cases} e^{-x} \u0026amp; \\text{for } x\u0026gt;0\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ 那么我们可以计算出 $X$ 的 m.g.f 和他的方差，先求m.g.f. $$ \\psi(x)=E(e^{tX})=\\int^{\\infty}{0}e^{tx}e^{-x}dx\\ =\\int^{\\infty}{0}e^{(t-1)x}dx $$\n这个积分只有当 $t-1\u0026lt;0$ 的情况下才可以求积分，这个是微积分的基础知识，不知道的去看微积分。那么只有当 $t\u0026lt;1$ 的时候，积分结果： $$ \\psi(t)=\\frac{1}{1-t} $$ 那么接着就是对 $\\psi(t)$ 求导了： $$ \\psi\u0026rsquo;(t)=\\frac{1}{(1-t)^2}\\ \\psi\u0026rsquo;\u0026rsquo;(t)=\\frac{2}{(1-t)^3} $$\n因为 $E(X)=\\psi\u0026rsquo;(0)=1$ 以及 $E(X^2)=\\psi\u0026rsquo;\u0026rsquo;(0)=2$ 那么其方差： $$ Var(X)=\\psi\u0026rsquo;\u0026rsquo;(0)-[\\psi\u0026rsquo;(0)]^2=1 $$\n如果不是使用m.g.f. 那么这个期望和方差求起来有些麻烦，首先就是求 $\\int^{\\infty}_{-\\infty}xe^{-x}dx$ 这应该使用分部积分，然后再求方差，比较麻烦，m.g.f.则相对简单些。\n距生成函数的性质 Properties of Moment Generating Functions 知道m.g.f是怎么生成距了以后我们就要系统的研究下，m.g.f的性质了。\n Theorem Let $X$ be a random varibale for which the m.g.f. is $\\psi_1$ ;Let $Y=aX+b$ ,where $a$ and $b$ are given constants; and let $\\psi_2$ denote the m.g.f. of $Y$ .Then for every value of $t$ such that $\\psi_1(at)$ is finite, $$ \\psi_2(t)=e^{bt}\\psi_1(at) $$\n 随机变量的m.g.f的线性变换，有点复杂，但是就是带到公式里去得到的。 证明： $$ \\psi_2(t)=E(e^{tY})=E[e^{(aX+b)t}]=e^{bt}E(e^{atX})=e^{bt}\\psi_1(at) $$ 证毕\n接着就是一个重要的性质了，一定数量不相关的随机变量的和的m.g.f.有一个非常简单的形式，这使得我们研究多个不相关的随机变量的和的时候有了一个好工具！\n Theorem Suppose that $X_1,\\dots,X_n$ are $n$ independent random varibales;and for $i=1,\\dots,n$ . let $\\psi_i$ denote the m.g.f. of $X_i$ .Let $Y=X_1+\\dots+X_n$ ,and let the m.g.f. of $Y$ be denoted by $\\psi$ .Then for every value of t such that $\\psi_i(t)$ is finite for $i=1,\\dots,n$ , $$ \\psi(t)=\\Pi^{n}_{i=1}\\psi_i(t) $$\n 证明需要用到前面关于不相关的随机变量的积的期望等于期望的积的性质，不记得的童鞋查阅4.1和4.2， $$ \\psi(t)=E(e^{tY})=E(e^{tX_1+\\dots+tX_n})=E(\\Pi^{n}{i=1}e^{tX_i})\\ E(\\Pi^{n}{i=1}e^{tX_i})=\\Pi^{n}{i=1}E(e^{tX_i})\\ \\psi(t)=\\Pi^{n}{i=1}\\psi_i(t) $$\n其中只有第二步有点技术含量，用到了独立随机变量的性质，接着我们看几个别的性质。\n二项分布的距生成函数 The Moment Generating Function for the Binomial Distribution 二项分布 $(n,p)$ 的m.g.f 问题： 二项分布是多个独立的伯努利分布加起来的结果，根据上面的性质，独立随机变量求和，m.g.f性质，那么我们有： $$ \\begin{aligned} \\psi_i(t)\u0026amp;=E(e^{tX_i})\\ \u0026amp;=p\\times e^{t}+(1-p)\\times e^{0}\\ \u0026amp;=pe^t+1-p \\end{aligned} $$\n根据上面定理 $$ \\psi(t)=(pe^t+1-p)^n $$\n证毕。\n距生成函数的唯一性 Uniqueness of Moment Generating Functions 重要性质，但是受到我们知识的限制，我们没办法证明。\n Theorem If the m.g.f. of two random varibales $X_1$ and $X_2$ are finite and identical for all values of $t$ in an open interval around the point $t=0$ ,then the probability distributions of $X_1$ and $X_2$ must be identical.\n 在0附近这个条件要注意一下，其他的基本比较好动，目前我们还无法证明的话，那就留着呗，早晚有一天我们会证明他的。 而这条不会证明的定理也说明了m.g.f.可以用来完整的描述分布。\n二项分布的可加性 The Additive Property of the Binomial Distribution 二项分布的可加性，m.g.f.对独立随机变量的加法贡献，使得我们又得到了两个独立的二项分布的随机变量加法结果：\n Theorem If $X_1$ and $X_2$ are independent random variables,and if $X_i$ has the binomial distribution with parameters $n_i$ and $p$ ($i=1,2,\\dots$ ),then $X_1+X_2$ has the binomial distirbution with parameters $n_1+n_2$ and $p$\n 定理就是说两个独立的二项分布的随机变量的和的情况。 证明： 对于 $i=1$ 和 $i=2$ $$ \\psi_i(t)=(pe^{t}+1-p)^{n_i} $$ 那么两个二项分布的随机变量的和的m.g.f. 就是：\n$$ \\psi(t)=(pe^{t}+1-p)^{n_1+n_2} $$\n根据前两条定理，m.g.f能唯一确定分布，又根据二项分布的m.g.f.，我们可以确定上面的这个m.g.f.就是 $n_1+n_2,p$ 的二项分布 证毕！\n总结 总结就是这一篇知识点比较多，大家看的时候要多对应例子，不然定理没办法深入理解的。 明天继续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-4-4-moments.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍另一个基于期望的随机变量分布的数字特征——“矩” （不知道咋翻译的，英文moment表示时刻,瞬间等意思）\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Moments，Moments Generating Function\u003c/p\u003e","title":"【概率论】4-4:距(Moments)"},{"content":"Abstract: 本文介绍继期望之后分布的另一个重要数学性质，方差 Keywords: Variance,Standard Deviation\n方差 这两天更新有点频繁，但是没办法，必须快速的完成的基础知识积累，毕竟时间是有限的，还要留出更多的时间用于更进一步的深入研究，打牢基础的同时尽可能的提升速度。\n 如果我虚度光阴，那就请结束我的一生。如果你用奉承掐媚愚弄我，那我便自得取乐，如果你用荣华富贵诱惑我，那即便我的末日来临，我也要赌个输赢！\n 虽然期望很有用，但是并不能够完全的反应分布的信息，这么思考，首先，一个分布是一个公式确定的，这个公式的结构，和参数，如果能用一个参数全部概括，那么我们就有了一个超级模型一样的东西，这显然是不存在的，所以我们要用更多的数字特征来描述，代表一个分布的样子。 本文我们就介绍一款特征可以表示分布的离散程度（英文叫 “spread out”）——方差，他的衍生小弟叫做标准差是他的平方根，目前还不知道有啥特殊用途。\n先举个例子，股票。。 一个股波动范围在 $[25,35]$ 之间，均匀分布，第二个股分布在 $[15,45]$ 之间的均匀分布。 那么其图像显示如下： 可见这个图像上，均值一致，都是30，但是分布有着明显区别，我们开始介绍我们的主角——“方差”\n方差和标准差的定义 Definitions of the Variance and the Standard Deviation  Definition Variance/Standard Deviation.Let $X$ be a random variable with finite mean $\\mu=E(X)$ ,The variance of $X$ denoted by $Var(x)$ ,is defined as follows: $$ Var(X)=E[(X-\\mu)^2] $$\n 上面是关于方差和标准差的定义，首先随机变量的必须有一个有限的期望，然后再这个期望的基础上，每个变量和均值做差然后求其平方的期望，一共两步，用到了两次期望，可见方差其实就是随机变量函数的期望，而这个函数内又包含期望的运算。 注意无限的均值，或者不存在均值，都会导致方差无法计算，这是我们说随机变量没有方差，比如柯西分布，没有均值，也就没有方差，可见不是所有分布都有均值和方差的，同样，后面所有用到期望求的数字特征都没有。 方差用希腊字母 $\\sigma^2$ 表示，标准差用希腊字母 $\\sigma$ 表示，这是单个变量的分布时，当有多个变量的时候只需要对$\\sigma$ 加以区分就可以，比如加下标 $\\sigma_a$ so so\n那么我们来计算个🌰 ： 计算上面例子中第一种股票的方差： 在$[25,35]$ 的均匀分布 $$ Var(A)=\\int^{35}{25}(a-30)^2\\frac{1}{10}da=\\frac{1}{10}\\frac{x^3}{3}\\arrowvert^5{x=-5}=\\frac{25}{3} $$\n上面的积分，和积分限的计算请自行打草稿，这里不再赘述了。 下面开始看看方差有哪些定理\n Theorem Alternative Method for Calculating the Variance.For every random variable $X$ , $Var(X)=E(X^2)-[E(X)]^2$\n 这个定理中文不知道叫啥，二选一定理？不知道，反正结论是方差可以直接用两个期望计算，一个是随机变量的平方的期望，另一个是期望的平方。\n证明： $$ \\begin{aligned} Var(X) \u0026amp; =E[(X-\\mu)^2]\\ \u0026amp; =E(X^2-2\\mu X+\\mu^2)\\ \u0026amp; =E(X^2)-2\\mu E(X)+ \\mu^2\\ \u0026amp; =E(X^2)-\\mu^2 \\end{aligned} $$ Q.E.D\n简单的计算过程，用到的性质都是期望的性质，所以又不懂回到前两篇重新研究。\n方差和标准差只取决于其分布，而且其实际意义就是随机变量对均值 $\\mu$ 的离散程度，值越大说明越分散，相反，越小表示与均值越聚集。 这里可以有个例子，这里我就不写啦，大家自己看书吧\n方差的性质 Properties of the Variance 学了定义，该学性质了，看起来方差的性质没有期望多，期望用了整整一课来说明，方差只是一个小section。\n Theorem For each $X$ , $Var(X)\\geq 0$ .If $X$ is a bounded random varibale,then $Var(X)$ must exist and be finite.\n 对于所有随机变量，其方差永远是大于等于零的，其为0的情况是当且仅当， $Pr(X=\\mu)=1$ 如果 随机变量 $X$ 有界，那么其方差必然存在，并且是有限的。\n这个定理的证明要靠概念，没有逻辑过程，首先根据定义，方差是个平方，所以其必然大于等于0，又因为方差存在与否取决于两个期望，如果这两个期望都存在，方差没有不存在的理由，故而有界随机变量存在期望，故成立，证毕。\n Theorem $Var(X=0)$ if and only if there exists a constant c such that $Pr(X=c)=1$ ,then $\\mu=c$ and $Pr[(X-c)^2=0]=1$\n 哈哈，刚才一不小心把这个定理先直播出来了，那么我们就直接证明，证明 if and only if 要证明两个方向：\n  假设存在一个随机变量 $X$ 和一个常数c 满足 $Pr(X=c)=1$ 那么 $E(X)=c$ 并且 $Pr[(X-c)^2=0]=1$ 然后就有了 $$ Var(X)=E[(X-c)^2]=0 $$\n  反过来假设 $Var(X)=0$\n 那么就有 $Pr[(X-\\mu)^2\\geq 0]=1$ 。 但是又因为 $E[(X-\\mu)^2]=0$ 也就是 $0=\\int^{\\infty}_{-\\infty}Pr【(x-\\mu)^2】(x-\\mu)^2dx$ 根据定理（ Theorem Suppose that $E(x)=a$ and that either $Pr(X\\geq a)=1$ or $Pr(X\\leq a)=1$ Then $Pr(X=a)=1$） 可以得到 $$ Pr[(X-\\mu)^2=0]=1 $$ 证毕(其实2中用的那个定理有点问题，我还没想明白)     Theorem For constant $a$ and $b$ let $Y=aX+b$ Then $$ Var(Y)=a^2Var(X) $$ and $\\sigma_Y=|a|\\sigma_X$ 定理表明线性关系下，随机变量的方差的变化 证明：令 $\\mu=E(X)$ 那么根据上一篇我们有 $E(Y)=aE(X)+b$ $$ \\begin{aligned} Var(Y) \u0026amp; =E[(aX+b-a\\mu-b)^2]=E[(aX-a\\mu)^2]\\ \u0026amp; =a^2E[(X-\\mu)^2]=a^2Var(X) \\end{aligned} $$\n 求平方根就能得到关于标准差的公式。 当线性变换中 $a=1$ 的时候就变成给分布搬家了，而其形状完全不变： 图上就是搬家计算了，图例中最后一个是错的，应该是 $x-3$\n根据上面定理还能推导出一些其他关系式，比如说： $$ Var(-x)=Var(x) $$\n Theorem If $X_1,\\dots,X_n$ are independent random variable with finite means,Then $$ Var(X_1+\\dots +X_n)=Var(X_1)+\\dots+Var(X_n) $$\n 独立随机变量之和的方差等去其方差之和，证明过程如下： 证明： 我们只证明两个独立随机变量的情况 假设 $E[X_1]=\\mu_1$ 以及 $E[X_2]=\\mu_2$ 然后有 $$ E[X_1]+E[X_2]=\\mu_1+\\mu_2 $$\n那么 $$ \\begin {aligned} Var(X_1+X_2)\u0026amp; =E[(X_1+X_2-\\mu_1-\\mu_2)^2]\\ \u0026amp;=E[(X_1-\\mu_1)^2+(X_2-\\mu_2)^2+2(X_1-\\mu_1)(X_2-\\mu_2)]\\ \u0026amp;=E[(X_1-\\mu_1)^2]+E[(X_2-\\mu_2)^2]+E[2(X_1-\\mu_1)(X_2-\\mu_2)]\\ \u0026amp;=Var(X_1)+Var(X_2)+E[2(X_1-\\mu_1)(X_2-\\mu_2)] \\end{aligned} $$ 根据随机变量期望的性质 $$ \\text{When } X_1 \\text{ and } X_2 \\text{ are independent}\\ E[2(X_1-\\mu_1)(X_2-\\mu_2)]=2\\times E[(X_1-\\mu_1)]E[(X_2-\\mu_2)]=0 $$ 证毕\n这就是关于独立随机变量方差之间的关系，但是如果不是独立的随机变量，他们的方差会是什么样的呢？这是个很有意思的课题，后面我们会介绍相关话题。\n Corollary If $X_1,\\dots,X_n$ are independent random varibales with finite means,and if $a_1,\\dots,a_n$ then $$ Var(a_1X_1+\\dots+a_nX_n)=a^2Var(X_1)+\\dots+a_n^2Var(X_n) $$\n 这个推论的证明用到了上面两个已经被证明的定理，所以我们就不用证明了，没错，我又开始偷懒了。。\n二项分布的方差 The Variance of a Binomial Distribution 二项式分布的方差： 独立同伯努利分布的随机变量的和是满足二项分布的随机变量，这个我们前面已经说过了，后面下一章还会再说，我们现在就假装知道他们是独立的就行，根据独立随机变量的方差性质。 $$ Var(X)=\\sum^{n}{i=1}Var(X_i) $$ 然后我们根据二选一定理，某个伯努利随机变量 $Var(X_i)=E(X_i^2)-[E(X_i)]^2$ 来计算方差，首先要得到 $X_i^2$ 的方差，因为伯努利分布只有0和1，那么$X_i^2$ 也是0和1，故 $X_i^2$ 的分布于原始 $X_i$ 的分布一样，均值是 $p$ （参考不努力分布的期望）那么，我们就有 $$ Var(X_i)=E(X_i^2)-[E(X_i)]^2=p-p^2 $$ 这是对于某一个随机变量的方差，因为他们互不相关，所以把他们加起来就好了，最后结果： $$ \\begin{aligned} Var(X) \u0026amp; =\\sum^{n}{i=1} Var(X_i)\\ \u0026amp; =\\sum^{n}_{i=1}(p-p^2)\\ \u0026amp;=np(1-p) \\end{aligned} $$ 行了，就这么样了，二项分布的方差就是上面这个了。\n四分位数范围 Interquartile Range 我们是否还记得方差的实际意义，他描述的是分布距离均值的离散程度，方差可以没有，也就是说当期望不存在或者无限的时候，方差可以不存在，但是描述分布的离散程度，这个可以有啊，所以我们就提出个新的数字特征，这个特征能帮忙解决没有方差，比如柯西分布，这种特殊的分布的离散程度的刻画。\n Definition Interquartile Range(IQR). Let X be a random varibale with quatile function $F^{-1}(p)$ for $0\u0026lt;p\u0026lt;1$ .The interquartile range (IQR) is defined to be $F^{-1}(0.75)-F^{-1}(0.25)$\n 换句话说，IQR就是四分之一分位数，和四分之三分位数之间的距离。\n总结 继期望过后，我们用期望引申出了一个更复杂的，刻画分布另一个性质的期望。 下几篇还是期望，我们本章就叫期望。 待续\n","permalink":"https://go.face2ai.com/math/math-probability-4-3-variance.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍继期望之后分布的另一个重要数学性质，方差\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Variance,Standard Deviation\u003c/p\u003e","title":"【概率论】4-3:方差(Variance)"},{"content":"Abstract: 本文介绍关于期望的性质，主要是计算性质，所以本文会有非常多公式定理，例子可能较少 Keywords: Properties of Expectation\n期望的性质 更新了下博客主题，添加了RSS订阅功能，这个功能看起来不错，以前听说过，但是一直也没用过，今天下了个软件，注册了个账号，帮忙收集信息也是不错，效率高很多，欢迎大家订阅。 本文介绍期望的一些性质，计算性质，而且很多是比较常见的随机变量函数的期望，从这篇看起来我们的套路有点越来越接近国内教材了，定义完了是计算性质，但是这个计算性质确实是必须的，不掌握好后面很多内容学起来就会吃力，就像我前两天看了一会儿统计，发现很多关于计算的的性质，在统计书籍里是直接使用的，如果不掌握好那就是好几脸懵逼。\n期望的基本定理 Basic Theorems  Linear Function:.If Y=aX+b,where a and b are finite constants,then $$ E(Y)=aE(X)+b $$\n 线性关系，最简单的变化， $a,b$ 是有限的常数，那么新的随机变量的期望和原始变量的关系满足上式，其实用上一篇的关于随机变量函数的方法就能证明这个问题，我们来计算一下： $$ E(Y)=E(aX+b)=\\int^{\\infty}{\\infty}(ax+b)f(x)dx\\ =a\\int^{\\infty}{-\\infty}xf(x)dx+b\\int^{\\infty}_{-\\infty}f(x)dx\\ =aE(x)+b $$\n上面用到上一篇的公式，然后用到了积分的线性性质，完成了证明。\n Corollary If $X=c$ with probability 1 ,then $E(X)=c$\n 证明： $$ E(X)=\\int^{\\infty}{-\\infty}cf(x)dx\\ =c\\int^{\\infty}{-\\infty}f(x)dx=c $$ Q.E.D\n Theorem If there exists a constant such that $Pr(X\\geq a)=1$, then $E(X)\\geq a$. If there exists a constant $b$ such that $Pr(X\\leq b)=1$,then $E(X)\\leq b$\n 这个定理说明当存在一个常数 $a$ 满足 $Pr(X\\geq a)=1$ 那么 $E(X)\\geq a$ 另一部分是反过来的，所以我们只要证明了一半，另一半可以用同样的方法得到结论。 证明： $$ E(X)=\\int^{\\infty}{-\\infty}xf(x)dx=\\int^{\\infty}{a}xf(x)dx\\ \\geq \\int^{\\infty}{a}af(x)dx=aPr(X\\geq a)=a $$ Q.E.D 其中这一步 $\\int^{\\infty}{a}xf(x)dx\\geq \\int^{\\infty}{a}af(x)dx$ 用到的条件是 $x\\geq a$ 而 $\\int^{\\infty}{a}af(x)dx=aPr(X\\geq a)$ 用到的是积分的线性性质，和概率的相关定义。\n Theorem Suppose that $E(x)=a$ and that either $Pr(X\\geq a)=1$ or $Pr(X\\leq a)=1$ .Then $Pr(X=a)=1$\n 定理解释当知道一个随机变量的期望值是 $a$ 时，那么如果知道 $Pr(X\\geq a)=1$ 或者 $Pr(X\\leq a)=1$ 必然有 $Pr(X=a)=1$ 。 证明当X时离散情况下 $Pr(X\\geq a)=1$ 其他情况类似，假设 $x_1,x_2,\\dots$ 包含所有 $x\u0026gt;a$ 那么 $Pr(X=x)\u0026gt;0$ 令 $p_0=Pr(X=a)$ 那么 $$ E(X)=p_0a+\\sum^{\\infty}_{j=1}x_jPr(X=x_j) $$\n每个 $x_j$ 都大于 $a$ 其和不能变大 因为 $$ E(X)\\geq p_0a + \\sum^{\\infty}_{j=1}aPr(X=x_j)=a $$ 证毕。 其实离散情况想一下就正大概知道定理的正确性，但是连续情况下用微积分证明难度就有点大了。\n Theorem If $X_1,\\dots,X_n$ are $n$ random variables such that each expectation $E(X_i)$ is finite $(i=0,\\dots,n)$ ,then $$ E(X_1+\\dots+X_n)=E(X_1)+\\dots+E(X_n) $$\n 证明期望的加法性质，连续双随机变量证明过程如下，其他情况类似： $$ E(X_1+X_2)=\\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}(x_1+x_2)f(x_1,x_2)dx_1dx_2\\ =\\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}x_1f(x_1,x_2)dx_1dx_2+\\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}x_2f(x_1,x_2)dx_1dx_2\\ =\\int^{\\infty}{-\\infty}x_1f_1(x_1)dx_1+\\int^{\\infty}{-\\infty}x_2f_2(x_2)dx_2\\ =E(X_1)+E(X_2) $$\n证明过程最关键一步是 $\\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}x_1f(x_1,x_2)dx_1dx_2=\\int^{\\infty}{-\\infty}x_1f_1(x_1)dx_1$ 的过程，首先调换积分变量的次序 $\\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}x_1f(x_1,x_2)dx_2dx_1$ 这样，内层积分中$x_1$ 是常量，那么就可以提出来 $\\int^{\\infty}{-\\infty}x_1[\\int^{\\infty}_{-\\infty}f(x_1,x_2)dx_2]dx_1$ 这样中括号里面的部分就是 $x_1$ 的边缘变量了，同理可得 $x_2$ 的情况，故得到最后结论。\n上述证明过程证明了随机变量的和的期望等于期望的和，而不需要考虑其联合分布，同理可以推广到多变量情况\n下面我们就要考虑多变量线性关系了\n Corollary Assume that $E(x_i)$ is finite for $i=1,\\dots,n$ For all constants $a_1,\\dots,a_n$ and $b$ $$ E(a_1X_1+\\dots + a_nX_n+b)=a_1E(X_1)+\\dots a_nE(X_n)+b $$\n 这个引理的证明是上面加法性质的证明以及前面第一个线性关系的扩展，这里就不再证明了。\n注意：上面线性的函数g可以有 $E[g(x)]\\neq g(E[x])$ 的关系，但其他函数没有这种关系！Jensen\u0026rsquo;s inequality 会给出其他函数之间两者的关系\n Definition Convex Functions A function g of a vector argument is convex if ,for every $\\alpha\\in (0,1)$ and every x and y, $$ g[\\alpha x+(1-\\alpha)y] \\geq \\alpha g(x)+(1-\\alpha)g(y) $$\n 这个定义就是凸函数的一般定义\n Theorem Jensen\u0026rsquo;s Inequality. Let g be a convex function,and let $X$ be a random vector with finite mean.Then $E[g(X)]\\geq g(E[X])$\n 詹森不等式，说明了函数的期望和期望的函数之间的一般大小关系，等号当且仅当$g$ 是线性函数时成立，证明过程书上没写，等我思考出完整的证明后再来补充一下。\n独立随机变量之积的期望关系 Expectation of a Product of Independent Random Variables  If $X_1,\\dots,X_n$ are $n$ independent random variables such that each expectation $E(X_i)$ is finite $(i=1,\\dots,n)$ then $$ E(\\Pi^{n}{i=1}X_i)=\\Pi^{n}{i=1}E(X_i) $$\n 证明，为了方便我们假设这组连续随机变量的联合分布p.d.f. 是 $f$ 并且 $f_i$ 是其中 变量 $i$ 的边缘分布，因为他们之间彼此独立，所以 $$ f(x_1,\\dots,x_i)=\\Pi^{n}{i=1}f_i(x_i) $$ 因此： $$ E(\\Pi^{n}{i=1}X_i)\\ =\\int^{\\infty}{-\\infty}\\dots \\int^{\\infty}{-\\infty}(\\Pi^{n}{i=1}x_i)f(x_1,\\dots,x_n)dx_1,\\dots,x_n\\ =\\int^{\\infty}{-\\infty}\\dots \\int^{\\infty}{-\\infty}\\Pi^{n}{i=1}[x_if_i(x_i)]dx_1,\\dots,x_n\\ =\\Pi^{n}{i=1} \\int^{\\infty}{-\\infty}x_if_i(x_i)dx_i $$\n最后一步的拆分大家可能会感到疑惑不解，把积分内层拆开，那么每个除了当前要积分的 $x_i$ 以外，其他变量都是已知的就可以挪到本层积分的外面，然后本层积分的结果也必然是是个数字，所以一层一层的拆开就是最后的式子，如果实在看不懂，试试两个独立现需随机变量的计算请款个就知道了。\n总结 总结就是今天我们研究了上一篇的扩展部分，我们得到了很多期望的计算性质，而这些兴致将在后面的计算中非常有用处！ 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-4-2-properties-of-expectations.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍关于期望的性质，主要是计算性质，所以本文会有非常多公式定理，例子可能较少\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Properties of Expectation\u003c/p\u003e","title":"【概率论】4-2:期望的性质(Properties of Expectation)"},{"content":"Abstract: 本篇介绍期望的第二部分，关于随机变量的函数的期望 Keywords: Expectation\n随机变量的期望 本来这篇可以和前一篇合在一起的，但是看了下还是有点长，控制下篇幅来保证质量，有的时候追求多快就会影响到质量。\n函数的期望 The Expecatation of a function 一个函数的期望，首先这个函数一定是个随机变量的函数，那么其结果也是个随机变量，是随机变量就有分布，所以就有可能有期望。 举个🌰 ： 一个家电制造公司制造的电器每年出现故障的比率是 $X$ ，$X$ 是一个当前不知道的随机变量，如果我们感兴趣的是这个电器是失效前能运行多久，我们可能会使用 $1/X$ 来表示这个时间，那么我们计算 $Y=1/X$ 的平均值呢？ 这就是我们今天要讨论的问题，如何求一个关于随机变量函数的期望。\n单随机变量的函数 Function of a Single Random Variable 首先来看函数的自变量只有一个随机变量的情况。\n Function of a Single Random Variable : If $X$ is a random variable for which the p.d.f. is $f$ ,then the expectation of each real-valued function $r(X)$ can be found by applying the definition of expectation to the distribution of $r(X)$ as follows:Let $Y=r(X)$ ,determine the probability distribution of $Y$ ,and then determine $E(Y)$ by applying either expectation for a discrete distribution or expectation for a continous distribution.For example suppose that $Y$ has a continuous distribution with the p.d.f. $g$ .Then $$ E[r(X)]=E(Y)=\\int^{\\infty}_{-\\infty}yg(y)dy $$\n 这段文字的意思就是，一个随机变量的函数会产生一个新的随机变量，新的随机变量会有分布，这个分布如果有期望，那么就可以按照期望的原始定义计算这个新的随机变量的期望，也就是老随机变量的函数的期望了。 所继续上面的例子，如果 $X$ 的p.d.f.是： $$ f(x)= \\begin{cases} 3x^2\u0026amp;\\text{ if }0\u0026lt;x\u0026lt;1\\ 0\u0026amp;\\text{oterwise} \\end{cases} $$\n那么根据上面例子的关系 $r(x)=\\frac{1}{x}$，我们可以得到 $Y=r(x)$ 的p.d.f.： $$ g(y)= \\begin{cases} 3y^{-4}\u0026amp;\\text{ if }y\u0026gt;1\\ 0\u0026amp;\\text{oterwise} \\end{cases} $$ 那么Y的期望就是： $$ E(Y)=\\int^{\\infty}_{0}y^3y^{-4}dy=\\frac{3}{2} $$\n Theorem Law of the Unconscious Statistician.Let $X$ be a random varibale,and let $r$ be a real-valued function of a real variable.If $X$ has a continuous distribution,then $$ E[r(x)]=\\int^{\\infty}{-\\infty}r(x)f(x)dx $$ If the mean exists.If X has a discrete distribution ,then $$ E[r(X)]=\\sum{\\text{All } x}r(x)f(x) $$ if the mean exists.\n 这个定理的名字翻译成中文非常尴尬，叫“未知统计家法则” ，哪部分是未知的？明显自变量随机变量的分布已知，和新的随机变量的关系 $r$ 也是已知，而新的随机变量的分布是未知，所以我觉得未知应该就是指的是新随机变量的分布，这个分布是不需要求出来就能求期望的，这一点是很神奇的，而计算方法从公式上的体现就是直接用$r(x)$ 替换掉了以前的 $x$ 而p.d.f还是用原始的$x$ 的p.d.f. 我们可以证明下上面的这个定理，我们只证明离散情况的，连续情况下的大家可以自己证明。 证明： 假设X的分布式离散的，那么Y的分布必然也是离散的，我们来使 $g$ 为 $Y$ 的 p.d.f. 那么 $$ \\sum_{y}yg(y)=\\sum_{y}yPr[r(x)=y]\\ =\\sum_{y}y\\sum_{x:r(x)=y}f(x)\\ =\\sum_{y}\\sum_{x:r(x)=y}r(x)f(x)=\\sum_{x}r(x)f(x) $$ Q.E.D\n证明过程中最关键的一步是 $Pr[r(x)=y]=\\sum_{x:r(x)=y}f(x)$ 这一步是把原始随机变量和新随机变量联系起来的关键步骤，然后根据加法分配原理，把 $y$ 塞到求和公式里面，用 $r(x)$ 做代换，就得到了$\\sum_{x:r(x)=y}r(x)f(x)$ 最后是合并两个求和，第一个求和是按照变量y来求，而里面的是根据满足y的所有x的求和，那么综合起来就是求所有x的集合，因为每个x都要被映射成一个y，举个例子，因为这个式子还是有点难度的，你有三个盒子，每个盒子里有不同的若干块糖，你要做的是先把每个盒子里糖数出来，然后再把每个盒子的糖的数量加起来求总数量，和你把所有糖都集中起来，计数，结果是一样的。\n知识点完毕按道理来说，这里应该多看几个例子。\n注意： $E[g(x)]\\neq g(E[x])$\n多随机变量的函数 Function of Several Random Variables 同样的问题，如果上面的r对应的自变量随机变量不是一个随机变量呢？已知随机变量 $x$ 和 $y$ 的分布，那么求随机变量 $Z=X^2+Y^2$ 的期望，这事怎么办？ 继续我们的尴尬😓 定理\n Theorem Law of Unconscious Statistician:Suppose $X_1,\\dots,X_n$ are random variables with the joint p.d.f $f(x_1,\\dots,x_n)$ Let $r$ be a real-valued function of $n$ real varibales,and suppose that $Y=r(X_1,\\dots,X_n)$ .Then $E(Y)$ can be determined directly from the relation $$ E(Y)=\\underbrace{\\int\\dots\\int}{R_n}r(x_1,\\dots,x_n)f(x_1,\\dots,x_n) $$ if the mean exists.Similarly ,if $X_1,\\dots,X_n$ have a discrete joint distribution with p.f. $f(x_1,\\dots,x_n)$ the mean of $Y=r(X_1,\\dots,X_n)$ is $$ E(Y)=\\sum{\\text{All }x_1,\\dots,x_n}r(x_1,\\dots,x_n)r(x_1,\\dots,x_n)f(x_1,\\dots,x_n) $$ if the mean exists\n 没错，就是上面单变量的扩展版本。证明过程也是一样的，就是把单变量改成向量而已，基本没有难度，甚至如果你对线性代数很了解，基本上就是把一定范围的数的求和引申的某个空间上所有向量的求和，然后就能得出结论，连续情况下，积分同理！\n总结 期望是一个分布的一个非常重要的属性，分布确定就能确定唯一的期望，这种一对一的关系也好，或者是其本身含义也好，都决定了其在概率学中的重要地位，我们本章学的基本都是分布的一些数字性质，而且都是用期望作为基础得到的，我们后面继续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-4-1-the-expectation-of-a-random-variable-p2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本篇介绍期望的第二部分，关于随机变量的函数的期望\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Expectation\u003c/p\u003e","title":"【概率论】4-1:随机变量的期望(The Expectation of a Random Variable Part II)"},{"content":"Abstract: 本文主要介绍期望的基础之知识，第一部分介绍连续和离散随机变量的期望。 Keywords: Expectation\n随机变量的期望 好像大家比较喜欢关于学习方面的废话，那么以后就不说社会现象了，哈哈哈。 期望是整个这一章的基础，概率论学习例子最重要，前面几节例子都写的不多，所以让大家多看书，博客只能算个总结性的东西，而期望这个概念更是需要用练习去理解，我做数学的目的是为了研究机器学习，不是为了做习题，但是做习题是最快速的学数学的方法。 为了使得基础扎实，所以把本来可以一篇完成的博客拆分成了两篇，第一篇写离散和连续随机变量的期望，下一篇写随机变量函数的期望。\n本章引言 一个随机变量的全部信息被保存在他的分布中，当事件到随机变量的确定后，随机变量的分布唯一描述这个随机变量的全部性质。 但是整个分布包含太多信息了，比如一个复杂的分布，参数可能有几百上千个，有些性质就变得不那么明显了。 举个通俗的例子，我们描述一个人的身材（把身材当做随机变量），最完整的方法就像做CT，把整个人的三维模型数据采集出来，这就相当于其分布函数，但是这个数据量也好，耗时也好，都是非常大的，而且有些数据也没啥大作用，我们可能只关心这个人的射高体重，就能大概猜测出来这个人的大概样子，而不关心他的脑袋有多大，眼睛有多大。 这个例子是个很通俗的解释，但是类比的很恰当（为自己鼓掌）。 我们的目的就像找到身材中的身高和体重一样，找到分布中的某几个关键数值，这些数值可以反映出分布的某些重要性质——期望！\n离散分布的期望 Expectation for a Discrete Distribution 先举个不切实际的例子，买股票，通过某种计算，我们知道了某只股票的赚钱的分布，只有两种请款个，一种是赚10块钱，概率是90%，一种是赔100块钱，概率是10%。那么我们要不要买这只股票。 分析，首先事件是两个，一个是赚10元，一个是赔100，那么我们把这两个事件映射成随机变量 10，-100，那么离散分布：$Pr(10)=0.9,Pr(-100)=0.1$ 我们可能赚多少钱，相当于随机变量的加权平均，也就是 $E=10\\times 0.9+(-100)\\times 0.1 =-1$ 我们买这只股票的赚钱期望值是-1 ，这个-1其实是没有意义的，因为我们从事件到随机变量的映射其实只做了两个事件的一对一映射，我们得到的 -1 这个随机变量根本不知道对应什么事件，但是我们可以把第一步的从事件到随机变量的映射改成一个线性的函数，也就是收益 $a$ (可正可负)对应是随机变量是 $X=a$ 那么这样就存在逆映射，随机变量-1对应赔了一块钱。\n Definition Mean of Bounded Discrete Random Variable. Let $X$ be a bounded discrete random variable whose p.f. is $f$ .the expectation of $X$ denoted by $E(X)$ ,is a number define as follow: $$ E(X)=\\sum_{\\text{All }x}xf(x) $$ The expectation of $X$ is also referred to as the mean of $X$ or the expected value of $X$\n 上面定义了一个有限的离散分布的期望，每个分布对应唯一的期望，有限的离散分布都有期望，但是后面要说的连续的分布可能没有期望。\n一个例子，但是很重要，重要到可以当做一个定理： 一个随机变量X有一个参数为p的伯努利分布，那么他的期望是什么？ $$ E(X)=p\\times 1+(1-p)\\times 0=p $$ 简单的例子，但是是后面很多求解的基础组成，这个值得我们关注一下。\n上面我们讲的都是有限个离散分布的情况，当X是无限的时候其实也可以求期望，也就是求所有可能的值的加权平均数\n Definition Mean of General Discrete Random Variable. Let X be a discrete random variable whose p.f. is f.Suppose that at least one of the following sums is finite: $$ \\sum_{\\text{Positive }x}xf(x) , \\sum_{\\text{Negative }x}xf(x) $$ Then the mean,expectation,or expected value of $X$ is said to exist and is defined to be $$ E(x)=\\sum_{\\text{All } x}xf(x) $$\n 这个定义跟我在其他书上看到的还是有点区别，首先是分了两类，正的随机变量求了一个加权和，负的随机变量也求了一个加权和，判断了一下这两个和是不是有限的，如果其中至少一个是有限的的，那么就能得出其期望是 $E(x)=\\sum_{\\text{All } x}xf(x)$ 为啥两个都是无限的不行，因为没办法确定符号，当两个和有一个是无限的，我们可以认定其符号是正还是负，其值肯定是无穷，所以我们能得到一个明确的结论，是正无穷还是负无穷。 如果两个和都是无穷，一个正无穷，一个负无穷，那么他们的和将会没有意义，所以，我们的期望定义就变成了上面这个样子。\n那我们就来举一个例子，无边界离散随机变量期望不存在的例子：离散随机变量X有分布如下 $$ f(x)= \\begin{cases} \\frac{1}{2|x|(|x|+1)}\u0026amp;\\text{ if }x=\\pm 1,\\pm 2,\\pm 3,\\dots\\ 0\u0026amp; \\text{ otherwise } \\end{cases} $$ 那么他的期望是：\n$$ \\sum^{-\\infty}{x=-1} x\\frac{1}{2|x|(|x|+1)}=-\\infty \\ \\sum^{\\infty}{x=1} x\\frac{1}{2|x|(|x|+1)}=\\infty $$\n所以期望不存在。 期望可以是任意一个实数，当然也包括 $\\pm \\infty$ 前提是必须明确知道这个实数是什么。\n注意：期望之和分布唯一相关，和其他任何东西都无关，如果两个随机变量有同样的分布，那么他俩就有一样的期望，即使他俩是风马牛不相及的事物。 所以我们常说一个分布的期望是多少，甚至不知道这个分布的随机变量是啥都无所谓。 期望只和分布有关系！\n连续分布的期望 Expectation for a Continuous Distribution 到了连续情况下，我们就要用积分取代上面的所有求和，还有一个问题就是 p.f. 过度到p.d.f 的过程，p.f. 每个点对应的值就是其概率，但是p.d.f.对应的点并不是概率，那么这个区别我们要怎么处理呢？ 首先我们应该不考虑p.f.对应的值是概率这个想法，而是把它仅仅当做一个权值，每个随机变量对应不同的权值，这些权值的特点是相加的和是1，同样，对于连续随机变量，有无数个随机变量，也有无数个权值，虽然这些权值不是其对应的概率，但是这些权值的和也就是积分结果也是1，满足加权平均的要求，期望的公式中，$f(x)$ 只是一个权重，虽然他有时可以是随机变量对应的概率，有时也可以不是。\n Definition Mean of Bounded Continuous Random Variable. Let X be a bounded continuous random variable whose p.d.f. is f.The expectation of X ,denote E(X),is defined as follows: $$ E(X)=\\int^{\\infty}_{-\\infty}xf(x)dx $$\n 从求和变成了积分，求得的结果也叫做均值或者期望值。 这是有限情况下的结果，有限的随机变量必然有期望。而一般情况下的定义如下：\n Definition Mean of General Continuous Random Variable.Let X be a continuous random variable whose p.d.f. is f.Suppose that at least one of the following integrals is finite: $$ \\int^{\\infty}{0}xf(x)dx,\\int^{0}{-\\infty}xf(x)dx $$ Then the mean,expectation,or expected value of X is said to exist and is defined to be $$ E(X)=\\int^{\\infty}_{-\\infty}xf(x)dx $$\n 和离散情况套路一致，如果两个积分结果都是无限的，那么这个分布期望不存在，如果其中一个有限，那么期望结果就是确定的，原因也一致。\n下面这个例子必须给出，一个伟大的人给出的特例，柯西分布： 计算以下分布函数的期望： $$ f(x)=\\frac{1}{\\pi(1+x^2)} \\text{ for } -\\infty \u0026lt; x \u0026lt; \\infty $$ 求积分可以知道其结果是1，因为 $$ \\frac{d}{dx}tan^{-1}(x)=\\frac{1}{1+x^2} $$ 具体求积分这段就不写了，容易证明，柯西分布是一个合法的分布，但是其期望： $$ \\int^{\\infty}{0}\\frac{x}{\\pi(1+x^2)}dx=\\infty $$ 同理 $$ \\int^{0}{-\\infty}\\frac{x}{\\pi(1+x^2)}dx=-\\infty $$\n所以其期望不存在，那么是什么原因导致本来收敛的积分变得不确定了呢？原因是乘以了x，使得p.d.f. 的收敛性发生了极大的变化。\n期望的表达 Interpretation of the Expectation 其中有些关于期望的性质要说一下： 均值和重心的关系，一个分布的均值一般来说是在整个分布的重心。 对称分布的期望一般在对称轴所在的随机变量处。 当我们计算一个分布的期望之前要确定期望是否存在。 分布的变化对期望影响很大，期望小小的变化都会引起期望的剧烈变化。所以期望可以作为分布的一个重要特征。\n总结 本文介绍了期望的定义和确定期望的方法，包含有限离散，有界连续，无限无界离散连续分布的期望求法，下文我们将介绍期望的性质。\n","permalink":"https://go.face2ai.com/math/math-probability-4-1-the-expectation-of-a-random-variable-p1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍期望的基础之知识，第一部分介绍连续和离散随机变量的期望。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Expectation\u003c/p\u003e","title":"【概率论】4-1:随机变量的期望(The Expectation of a Random Variable Part I)"},{"content":"Abstract: 本文介绍多随机变量的函数 Keywords: 离散多随机变量的函数，连续多随机变量的函数，卷积\n多随机变量函数 任何一个领域的顶级人才都是需要很久的基础知识积累的，所以根据自己的定位可以适当的补充自己的基础知识：\n 如果你想进入机器学习这个行业，了解基础技术更重要，你需要会使用python，各种工具包，TensorFlow等基础工具 如果你想在机器学习这个行业稳定的输出而不是撞大运式的调参，你需要了解下网络结构，基础算法，并且你需要非常多的经验去调参。 如果你想成为机器学习的研究者，很遗憾的告诉你，你有一大堆数学要学而且真的不是一两年能学完的，所以还没有毕业的铜须有志于进入研究行列的，大家请多学习数学。  以上为个人理解，每一个等级难度都会提升，但是不保证收入和难度完全成正比。\n上文书我们讲到单个随机变量的函数变换，本文我们只进行简单变换，因为我们从试验结果到事件进行了一次映射，事件到随机变量又是一次映射，如果从随机变量再到另一个随机变量还是一个映射，这个过程可能都不是一对一的，所以这个过程是对原始信息的总结和提取，提取对我们有用的信息的方法。通过总结归纳出一个或者多个结构化的函数，可以反映信息的容积。\n有离散联合分布的多随机变量 Random Variables with a Discrete Joint Distribution  Theorem Functions of Discrete Random Variables. Suppose that $n$ random varibales $X_1,\\dots ,X_n$ have a discrete joint distribution for which the joint p.f. is $f$ and that $m$ functions $Y_1,\\dots ,Y_m$ of these $n$ random variables are defined as follows: $$ Y_1=r_1(X_1,\\dots,X_n),\\ Y_2=r_2(X_1,\\dots,X_n),\\ \\vdots\\ Y_m=r_m(X_1,\\dots,X_n) $$ For given values $y_1,\\dots,y_m$ fo the $m$ random variables $Y_1,\\dots,Y_m$ let $A$ denote the set of all points $(x_1,\\dots,x_n)$ such that: $$ r_1(x_1,\\dots,x_n)=y_1\\ r_2(x_1,\\dots,x_n)=y_2\\ \\vdots\\ r_m(x_1,\\dots,x_n)=y_m\\ $$ Then the value of the joint p.f. $g$ of $Y_1,\\dots,Y_m$ is specified at the point $(y_1,\\dots,y_m)$ by the relation $$ g(y_1,\\dots,y_m)=\\sum_{(x_1,\\dots,x_n)\\in A}f(x_1,\\dots,x_n) $$\n 和单变量函数的套路基本一致，最后的公式是最关键的逻辑核心，也就是 $(x_1,\\dots,x_n)\\in A$ 是解决问题的关键，换句话说，多变量也好，单变量也好，最后我们要做的都是一个逆向的求解，或者叫做穷举的方法，因为我们并没计算公式能够得到全部的向量 $\\vec{x}=(x_1,\\dots,x_n)$ 保证其满足 $\\vec{x}\\in A$ 所以$g$ 和 $f$ 的关系也就是这么确定的，找到所有f的输入 $\\vec{x}$ 使其满足 $\\vec{y_0}$ 的需求，求的所有满足条件的概率和。 这部分和单离散随机变量完全一致，只是随机变量变成了随机变量向量了。\n下面的定理关于二项分布和伯努利分布：\n Theorem Binomial and Bernoulli Distributions. Assume that $X_1,\\dots,X_n$ are i.i.d. random variables having the Bernoulli distribution with parameter $p$ .Let $Y=X_1+\\dots X_n$ . Then $Y$ has the binomial distribution with parameters $n$ and $p$\n 当随即向量 $\\vec{x}=(x_1,\\dots,x_n)$ 是独立同伯努利分布的随机变量的时候，且其概率为 $p$ ，其函数 $Y=f(x_1,\\dots,x_n)$ 的分布是二项分布 参数是 $n$ 和 $p$\n证明：\n 可以明确的是，当 $y=x_1+\\dots+x_n$ 时，$y$ 的值在 $[0,n]$ 之间 设 $m\\in [0,n]$ 那么，根据加法和伯努利分布的性质，其中m个随机变量为1，另外n-m个随机变量为0: $Pr(Y=m)=\\begin{pmatrix}n\\m\\end{pmatrix}p^{m}(1-p)^{n-m}$ 很显然，Y的分布是二项分布。 Q.E.D  有连续联合分布的多随机变量 Random Variables with a Continuous Joint Distribution 先来个🌰，不然全文没有例子有点不像概率论学习该有的样子，顺便补充一句，博客只能是总结精华部分，如果想和熟练的掌握，需要去做大量的练习，也就是我们这里的例子也好课后习题也好，反正要练习。 排队的🌰 : 假设队伍里面的前两个客户计划同时离开，$X_i$ 表示第 $i$ 为客户用的时间 $i=1,2$ 假设 $X_1$ 和 $X_2$ 是独立的随机变量，并有相同的分布: $f(x)=2e^{-2x}$ 其中 $x\u0026gt;0$ 因为两个客户想同时离开（也就是先完成的人要等待没完成的人），所以我们感兴趣的是他们用的总时间：$Y=X_1+X_2$ 所以 $Y$ 的p.d.f. 是我们要求的：\n$$ \\text{for each } y\\text{,let }\\ A_y={(x_1,x_2):x_1+x_2\\leq y}\\ $$ 那么当 $Y\\leq y$ 当且仅当 $(X_1,X_2)\\in A_y$ 集合 $A_y$ 如图所示 如果我们让 $G(y)$ 来定义 $Y$ 的 c.d.f. 那么对于 $y\u0026gt;0$ 我们有： $$ G(y)=Pr((X_1,X_2)\\in A_y)=\\int^{y}{0}\\int^{y-x_2}{0}4e^{-2x_1-2x_2}dx_1dx_2\\ =\\int^{y}{0}2e^{-2x_2}[1-e^{-2(y-x_2)}]dx_2=\\int^{y}{0}[2e^{-2x_2-2e^{-2y}}]dx_2\\ =1-e^{-2y}-2ye^{-2y} $$\n上面这个例子用到的主要数学技巧是微积分，多元微积分，而得到积分表达式却用到了概率的知识，配合示意图，这个例子变得很清晰，但是其原理还是值得我们研究的。\n Theorem Brute-Force Distribution of a Function.Suppose that the joint p.d.f. of $\\vec{X}=(X_1,\\dots X_n)$ is $f(\\vec{x})$ and that $Y=r(\\vec{X})$ For each real number $y$ ,define $A_y={x:r(x)\\leq y}$ ,Then the c.d.f. G(y) of Y is: $$ G(y)=\\underbrace{\\int\\dots \\int}_{A_y} f(x)dx $$\n 这是个简单暴力的方法来确定一个连续多随机变量分布，和多离散随机变量相似，都是把满足条件的所有的积分（求和）重新得到新变量的 c.d.f ，其证明也很容易： proof: $$ G(y)=Pr(Y\\leq y)=Pr[r(\\vec{X})\\leq y]=Pr(\\vec{X}\\in A_y) $$\n上面的方法适合于变量较少，而且分布比较简单的情况下，当情况复杂了，这种方法将会非常酷男，困得部分也是确定积分范围的部分，也就是说我们基本没什么办法直接得到 $\\vec{X}$ 使其满足 $r(\\vec{X})\\leq y$ ，这个问题将成为一个大问题，如果 $r$ 是可逆的，这个就好办，但是如果r是个多对一的不可逆函数，情况就变得复杂了。 当然我们还是可以研究最简单的情况 —— 线性情况\n Theorem Linear Function of Two Random Varibales Let $X_1$ and $X_2$ have joint p.d.f. $f(x_1,x_2)$ and let $Y=a_1X_1+a_2X_2+b$ with $a_1\\neq 0$ The $Y$ has a continuous distribution whose p.d.f. is $$ g(y)=\\int^{\\infty}_{-\\infty}f(\\frac{y-b-a_2x_2}{a_1},x_2)\\frac{1}{|a_1|}dx_2 $$\n 上面的公理给出了线性双连续变量的分布公式，我们来证明一下：\n  首先我们发现 Y的 c.d.f. G的导数是g，也就是上面定理中的g\n  对于每一个y，定义 $A_y={(x_1,x_2):a_1x_1+a_2x_2+b\\leq y}$\n  $A_y$ 和上面的图（本文就一张图，没错，就是上面例子的那张图） 的情况类似\n  写出积分限，外部积分到$x_2$ 里层积分是 $x_1$ ,然后就有： $$ G(y)=\\int_{A_y}\\int f(x_1,x_2)dx_1dx_2=\\int^{\\infty}{-\\infty}\\int^{\\frac{(y-b-a_2x_2)}{a_1}}{-\\infty}f(x_1,x_2)dx_1dx_2 $$\n  上面内层积分限有点小复杂，因为y是我们关心的变量，放在内层处理起来会麻烦，所以把他挪到外层。方法就是换元, $z=a_1x_1+a_2x_2+b$ ，那么$x_1=\\frac{z-a_2x_2-b}{a_1}$ 那么就有 $dx_1=dz_1/a_1$ 于是内层积分就变成了下面这个式子： $$ \\int^{y}_{-\\infty}f(\\frac{z-b-a_2x_2}{a_1},x_2)\\frac{1}{a_1}dz $$\n  然后我们使用积分的性质做下面的计算： $$ G(y)=\\int^{\\infty}{-\\infty}\\int^{y}{-\\infty}f(\\frac{z-b-a_2x_2}{a-1}，x_2)\\frac{1}{a_1}dzdx_2\\ =\\int^{y}{-\\infty}\\int^{\\infty}{-\\infty}f(\\frac{z-b-a_2x_2}{a_1},x_2)dx_2dz $$\n  我们可以让内层积分成为一个函数 $t(z)=\\int^{\\infty}{-\\infty}f(\\frac{z-b-a_2x_2}{a_1},x_2)dx_2$ ,然后我们就能得到 $G(y)=\\int^{y}{-\\infty}g(z)dz$ 根据微积分基本定理II 其求导等于t(z) ,而 $t{z}$ 就是我们上面定理中的 $g(y)$\n  Q.E.D\n  精彩的部分在换元，通过换元来得到了我们的目标函数，这个应该算是微积分技巧，跟概率原理没太大关系，但是可以看出，微积分基础是多么重要啊。\n然而这个定理你以为就完了？还没有，有更劲爆的还在后面。 当 $\\int^{y}{-\\infty}\\int^{\\infty}{-\\infty}f(\\frac{z-b-a_2x_2}{a_1},x_2)dx_2dz$ 中，$a_1=a_2=1$ 并且 $b=0$ 的时候，这个式子改名叫卷积，没错卷积，神经网络来的同学激动不？看了这么久了，终于慢慢的靠上边了。\n Definition Convolution.Let $X_i$ be independent continuous random variables and let $Y=X_1+X_2$ The distribution of $Y$ is called the convolution of the distributions of $X_1$ and $X_2$ .The p.d.f. of $Y$ is sometimes called convolution of the p.d.f.\u0026rsquo;s of $X_1$ and $X_2$.\n 如果我们把$X_i$ 的p.d.f. 写成 $f_i$ 其中 $i=1,2$ 的话那么 $Y=X_1+X_2$ 的 p.d.f.是： $$ g(y)=\\int^{\\infty}{-\\infty}f_1(y-t)f_2(t)dt $$ 其中t是个哑变量，或者叫做临时变量。 同理，交换$X_1$ 和 $X_2$ 能得到： $$ g(y)=\\int^{\\infty}{-\\infty}f_1(t)f_2(y-t)dt $$\n怎么样，像卷积了吧，不过不要忘了这是双连续随机变量的线性函数变换后的pd.f.的关系！这句话有点复杂？那就好好多读几遍。 本想举个🌰就结束，结果发现这几个🌰是连续的，所以这里就不再多写了，大家可以参考\u0026rsquo;Probability and Statistics 4th\u0026rsquo;中的例子，都非常精彩\n总结 本文扩展上文介绍了多随机变量的函数，从离散到连续，遵循和单变量类似的法则，但是用处却大大扩展了，下一篇开始就要进入第四章了，我们一起加油。。\n","permalink":"https://go.face2ai.com/math/math-probability-3-9-functions-of-two-or-more-random-variables.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍多随机变量的函数\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 离散多随机变量的函数，连续多随机变量的函数，卷积\u003c/p\u003e","title":"【概率论】3-9:多随机变量函数(Functions of Two or More Random Variables)"},{"content":"Abstract: 本文介绍通过函数这个工具，来研究随机变量 Keywords: The Probability Integral Transformation，Simulation，Pseudo-Random Numbers，General Function\n随机变量函数 我们到目前为止对概率的研究经过了试验结果，事件，随机变量大概这三个过程，其实每个过程都是更高层的抽象，比如，对于直观的事实，实验结果，我们通过一种函数，或者称为一种收集，将结果抽象成了事件，而对事件研究了一段时间后又将事件通过函数（随机变量）映射到了实数域，整个过程，更加抽象，更加复杂，但是计算和模拟现实中的试验结果变得更加容易更加准确。 对于实数的研究，函数是绕不开的话题，而函数的微分积分等又是现代科学的基础，所以本文简要的介绍下随机变量的函数。 问题的描述变成了当我们已知一个随机变量 $X$ 具有某个p.f. 或者 p.d.f 那么随机变量 $Y=f(x)$ 分布是什么。\n有离散分布的随机变量 Random Variable with a Discrete Distribution 先看一个例子： 离散随机变量 $X$ 在 $[1,\\dots ,9]$ 有一个均匀分布，我们关系的是随机变量距离区间中心5的距离Y的分布情况。 这时候 $Y$ 的定义的数学化表示是： $Y=|X-5|$ 其分布函数不太好写，但是可以列举出来： $$ Y\\in {0,1,2,3,4}\\ Pr(Y=1)=Pr(X\\in {4,6})=\\frac{2}{9}\\ Pr(Y=2)=Pr(X\\in {3,7})=\\frac{2}{9}\\ Pr(Y=3)=Pr(X\\in {2,8})=\\frac{2}{9}\\ Pr(Y=4)=Pr(X\\in {1,9})=\\frac{2}{9}\\ Pr(Y=0)=Pr(X\\in {5})=\\frac{1}{9}\\ $$\n这就是一个最简单的例子，关于离散随机变量的函数的分布问题。\n Theorem Function of a Discrete Random Variable. Let $X$ have a discrete distribution with p.f. $f$ and let $Y=r(X)$ for some function of $r$ defined on the set of possible values of $X$ For each possible value y of $Y$ the p.f. $g$ of $Y$ is $$ g(y)=Pr(Y=y)=Pr[r(X)=y]=\\sum_{x;r(x)=y}f(x) $$\n 解读下上面的公式，其实公式写的很清楚，当我们知道函数r了以后，满足 $r(X)=y$ 的所有X对应的概率最后组合成了Y，所以要进行求和，其实这一步跟从试验结果得到事件的过程是一样的。但是下面对于连续分布来说，就非常不一样了。\n有连续分布的随机变量 Random Variable with a Continuous Distribution 继续用例子开头 已知连续随机变量 $X$ 有在 $[-1,1]$ 一个均匀分布，那么我们求连续随机变量 $Y=X^2$ 的分布：\n 根据条件我们有： $$ f(x)= \\begin{cases} \\frac{1}{2}\u0026amp;\\text{for } -1\\leq x\\leq 1\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ 分析我们的目标函数 $Y=X^2$ 其定义域 $[-1,1]$ 其值域 $[0,1]$ 那么随机变量 $Y$ 的c.d.f. 应该是： $$ G(y)=Pr(Y\\leq y)=Pr(X^2\\leq y)\\ =Pr(-y^{1/2}\\leq X\\leq y^{1/2})\\ =\\int^{y^{1/2}}_{-y^{1/2}}f(x)dx=y^{1/2} $$ 因为$0\u0026lt;y\u0026lt;1$ 所以 c.d.f. g(y)是 $$ g(y)=\\frac{dG(y)}{dy}=\\frac{1}{2y^{1/2}} $$ 或者其他情况是0。  分析下上面的例子，并没有直接从随机变量X来计算随机变量Y的分布，原因很简单，没办法算，因为这个是个连续随机变量，没办法像离散随机变量一样得到满足 $r(X)=y$ 的所有X，因为连续情况下有无限多组这样的对应，而且上面的例子并不是一对一的双射函数（双射函数的定义可以参考数学分析教材）这就是最重要的麻烦，只要得到一对一的双射关系（本例中是X的p.d.f.的一个区间 对应 Y的一个p.d.f的区间）所以，上面用的就是这个思路，最后得到了相应的解。 现在我们只给出最简单的函数变换，线性变换对随机变量的影响，其推理过程和上面例子相似，只是这里我们直接给出结果:\n Theorem Linear Function: Suppose that X is a random variable for which the p.d.f. is f and that $Y=aX+b(a\\neq 0)$ .Then the p.d.f. of Y is $$ g(y)=\\frac{1}{|a|}f(\\frac{y-b}{a}) \\text{ for } -\\infty \u0026lt;y\u0026lt;\\infty $$\n 因为连续随机变量的函数变换比较复杂，这里面只给出了一个线性变换的变换定理，其他变换方式变换需要用到上面例子那种分析方法来获取。同样线性的结论也可以用例子中的方法来证明。\n概率积分变换 The Probability Integral Transformation 接下来我们进入今天的难点，就是概率的积分问题，并且会引出一个非常重要非常有用的结论。 🌰 ： 连续随机变量X 的p.d.f. $$ f(x)= \\begin {cases} e^{-x}\u0026amp; \\text{ for } x\u0026gt;0\\ 0\u0026amp;\\text{ otherwise} \\end {cases} $$ 那么X 的c.d.f.： $$ F(x)= \\begin {cases} 1-e^{-x}\u0026amp; \\text{ for } x\u0026gt;0\\ 0\u0026amp;\\text{ otherwise} \\end {cases} $$\n如果按照我们前面定义的随机变量Y跟X的关系满足 $Y=r(x)$ ,那么我们另 $r=F$ 会得到什么结论了，换句话说，随机变量 $Y=F(x)$ 那么Y的c.d.f.怎么求（X的c.d.f.的c.d.f.）\n$$ G(y)=Pr(Y\\leq y)=Pr(1-e^{-x}\\leq y)=Pr(X\\leq -log(1-y))\\ =F(-log(1-y))=1-e^{-[-log(1-y)]}=y $$ 上面的结果是 $G(y)=y$ 是 $[0,1]$ 的上的均匀分布！这是个意外还是个巧合？一个随机变量的c.d.f函数(或者叫做积分)变化来的随机变量的c.d.f.居然是 $[0,1]$ 均匀分布！\n Theorem Probability Integral Transformation .Let $X$ have a continuous c.d.f. $F$ and let $Y=F(x)$. (This transformation from $X$ to $Y$ is called the probability integral transformation)This distribution of $Y$ is uniform distribution on the interval of $[0,1]$\n 证明过程如下：\n $F$ 是单随机变量 $X$ 的c.d.f. 那么其定义域可以是 $\\mathbb{R}$ 但是其值域为 $[0,1]$ 因为 $Y=F(X)$ 所以 $Y\\in [0,1]$ 并且 $Pr(Y\u0026gt;1)=Pr(Y\u0026lt;0)=0$ 那么如果我们想找到 $X$ 中的某个点 $x_0$ 和 $Y$ 中的 $y_0$ 对应相当于3.3中关于分位数的计算中，$X$ 的 $y_0$ 分位数，并且每个分位数和对应的分位是一一对应的 这样的话，根据意义对应的关系，如果要保证 $Y\\leq y_0$ 的充分必要条件是 $X\\leq F^{-1}(y_0)=x_0$ 那么对应的概率关系就是 $G(y)=Pr(Y\\leq y)=Pr(X\\leq x_0)=F(x_0)=y$ Q.E.D  看起来很神奇的一个结论，就被我们这么证明了，最重要的一点就是对于连续随机变量，其c.d.f.后的变量可以和原始变量产生一个一对一满射的关系，继续进行c.d.f的计算就能得到一个均匀分布的c.d.f\n上面的结论进一步精简就是任意连续分布的c.d.f. 函数 $F$ 的c.d.f.函数 $F$ 就是均匀分布，那么一个均匀分布的 $F^{-1}$ 就是一个任意连续分布的c.d.f（ $F$ ）\n Corollary Let $Y$ have the uniform distribution on the interval $[0,1]$ ,and let $F$ be a continuous c.d.f. with quanntile function $F^{-1}$.Then $X=F^{-1}(Y)$ has c.d.f. $F$\n 上面的定理给我们了一个重要的引用结论，也就是如何通过均匀生成任意的分布的随机变量\n模拟随机 Simulation 上面的结论帮助我们仿真生成随机数，我们知道我们的计算机上用于生成随机数的算法是伪随机的，用的是周期非常大的一个函数来假装生成均匀分布的随机数，有时候我们需要特定分布的随机数，那么就需要用上面的原理将均匀分布的随机变量转换成任意分布的随机变量了。 如果非常详细的描述这个过程是不可能的，因为这个课题在CS和数学界已经发展成一个专门的课题了，我没办法用我目前的知识去讲解，所以我们大概知道下任意分布的随机变量的生成：\n 使用系统api生成若干个均匀分布的随机数 $\\vec{x}$ 求目标分布的c.d.f的逆函数 $F^{-1}$ 求目标c.d.f.分布的随机变量 $F^{-1}(\\vec{x})$ 上面这种算法被叫做分位数法，当然我们现在大部分软件中用的都是更高级的算法，有兴趣的同学可以自己去查资料。  总结 本文主要介绍单个随机变量的函数，最重要的一点是连续随机变量的c.d.f的c.d.f是均匀分布，这个在后面的应用中会经常用到，推理过程都很粗糙，因为本篇的内容实在太深奥，这里只能是盲人摸象一样给出自己的理解，我们下一篇继续。。\n","permalink":"https://go.face2ai.com/math/math-probability-3-8-fuctions-of-a-random-variable.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍通过函数这个工具，来研究随机变量\n\u003cstrong\u003eKeywords:\u003c/strong\u003e The Probability Integral Transformation，Simulation，Pseudo-Random Numbers，General Function\u003c/p\u003e","title":"【概率论】3-8:随机变量函数(Functions of a Random Variable)"},{"content":"Abstract: 本文介绍CUDA线程束执行的本质的后半部分，包括资源，延迟，同步，扩展性等严重影响性能的线，吞吐量，带宽，占用率，CUDA同步\n理解线程束执行的本质(Part II) 最近这几篇应该是CUDA最核心的部分，并不是编程模型，而是执行模型，通过执行模型我们去了解GPU硬件的具体运行方式，这样才能保证我们写出更快更好的程序。 由于访问量太少，转载请保留本条广告，各位老铁欢迎访问Tony的网站：http://www.face2ai.com\n资源分配 我们前面提到过，每个SM上执行的基本单位是线程束，也就是说，单指令通过指令调度器广播给某线程束的全部线程，这些线程同一时刻执行同一命令，当然也有分支情况，上一篇我们已经介绍了分支，这是执行的那部分，当然后有很多线程束没执行，那么这些没执行的线程束情况又如何呢？我给他们分成了两类，注意是我分的，不一定官方是不是这么讲。我们离开线程束内的角度（线程束内是观察线程行为，离开线程束我们就能观察线程束的行为了），一类是已经激活的，也就是说这类线程束其实已经在SM上准备就绪了，只是没轮到他执行，这时候他的状态叫做阻塞，还有一类可能分配到SM了，但是还没上到片上，这类我称之为未激活线程束。 而每个SM上有多少个线程束处于激活状态，取决于以下资源：\n 程序计数器 寄存器 共享内存  线程束一旦被激活来到片上，那么他就不会再离开SM直到执行结束。 每个SM都有32位的寄存器组，每个架构寄存器的数量不一样，其存储于寄存器文件中，为每个线程进行分配，同时，固定数量的共享内存，在线程块之间分配。 一个SM上被分配多少个线程块和线程束取决于SM中可用的寄存器和共享内存，以及内核需要的寄存器和共享内存大小。\n这是一个平衡问题，就像一个固定大小的坑，能放多少萝卜取决于坑的大小和萝卜的大小，相比于一个大坑，小坑内可能放十个小萝卜，或者两个大萝卜，SM上资源也是，当kernel占用的资源较少，那么更多的线程（这是线程越多线程束也就越多）处于活跃状态，相反则线程越少。 关于寄存器资源的分配：\n关于共享内存的分配：\n上面讲的主要是线程束，如果从逻辑上来看线程块的话，可用资源的分配也会影响常驻线程块的数量。 特别是当SM内的资源没办法处理一个完整块，那么程序将无法启动，这个是我们应该找找自己的毛病，你得把内核写的多大，或者一个块有多少线程，才能出现这种情况。\n以下是资源列表：\n当寄存器和共享内存分配给了线程块，这个线程块处于活跃状态，所包含的线程束称为活跃线程束。 活跃的线程束又分为三类：\n 选定的线程束 阻塞的线程束 符合条件的线程束  当SM要执行某个线程束的时候，执行的这个线程束叫做选定的线程束，准备要执行的叫符合条件的线程束，如果线程束不符合条件还没准备好就是阻塞的线程束。 满足下面的要求，线程束才算是符合条件的：\n 32个CUDA核心可以用于执行 执行所需要的资源全部就位  Kepler活跃的线程束数量从开始到结束不得大于64，可以等于。 任何周期选定的线程束小于等于4。 由于计算资源是在线程束之间分配的，且线程束的整个生命周期都在片上，所以线程束的上下文切换是非常快速的，。 下面我们介绍如何通过大量的活跃的线程束切换来隐藏延迟\n延迟隐藏 延迟隐藏，延迟是什么，就是当你让计算机帮你算一个东西的时候计算需要用的时间，举个宏观的例子，比如一个算法验证，你交给计算机，计算机会让某个特定的计算单元完成这个任务，共需要十分钟，而接下来这十分钟，你就要等待，等他算完了你才能计算下一个任务，那么这十分钟计算机的利用率有可能并不是100%，也就是说他的某些功能是空闲的，你就想能不能再跑一个同样的程序不同的数据（做过机器学习的这种情况不会陌生，大家都是同时跑好几个版本）然后你又让计算机跑，这时候你发现还没有完全利用完资源，于是有继续加任务给计算机，结果加到第十分钟了，已经加了十个了，你还没加完，但是第一个任务已经跑完了，如果你这时候停止加任务，等陆陆续续的你后面加的任务都跑完了共用时20分钟，共执行了10个任务，那么平局一个任务用时 $\\frac{20}{10}=2$ 分钟/任务 。 但是我们还有一种情况，因为任务还有很多，第十分钟你的第一个任务结束的时候你继续向你的计算机添加任务，那么这个循环将继续进行，那么第二十分钟你停止添加任务，等待第三十分钟所有任务执行完，那么平均每个任务的时间是： $\\frac{30}{20}=1.5$ 分钟/任务，如果一直添加下去，$lim_{n\\to\\infty}\\frac{n+10}{n}=1$ 也就是极限速度，一分钟一个，隐藏了9分钟的延迟。 当然上面的另一个重要参数是每十分钟添加了10个任务，如果每十分钟共可以添加100个呢，那么二十分钟就可以执行100个，每个任务耗时： $\\frac{20}{100}=0.2$ 分钟/任务 三十分钟就是 $\\frac{30}{200}=0.15$ 如果一直添加下去， $lim_{n\\to\\infty}\\frac{n+10}{n\\times 10}=0.1$ 分钟/任务 。 这是理想情况，有一个必须考虑的就是虽然你十分钟添加了100个任务，可是没准添加50个计算机就满载了，这样的话 极限速度只能是：$lim_{n\\to\\infty}\\frac{n+10}{n\\times 5}=0.2$ 分钟/任务 了。\n所以最大化是要最大化硬件，尤其是计算部分的硬件满跑，都不闲着的情况下利用率是最高的，总有人闲着，利用率就会低很多，即最大化功能单元的利用率。利用率与常驻线程束直接相关。 硬件中线程调度器负责调度线程束调度，当每时每刻都有可用的线程束供其调度，这时候可以达到计算资源的完全利用，以此来保证通过其他常驻线程束中发布其他指令的，可以隐藏每个指令的延迟。\n与其他类型的编程相比，GPU的延迟隐藏及其重要。对于指令的延迟，通常分为两种：\n 算术指令 内存指令  算数指令延迟是一个算术操作从开始，到产生结果之间的时间，这个时间段内只有某些计算单元处于工作状态，而其他逻辑计算单元处于空闲。 内存指令延迟很好理解，当产生内存访问的时候，计算单元要等数据从内存拿到寄存器，这个周期是非常长的。 延迟：\n 算术延迟 10~20 个时钟周期 内存延迟 400~800 个时钟周期  下图就是阻塞线程束到可选线程束的过程逻辑图： 其中线程束0在阻塞两短时间后恢复可选模式，但是在这段等待时间中，SM没有闲置。 那么至少需要多少线程，线程束来保证最小化延迟呢？ little法则给出了下面的计算公式 $$ \\text{所需线程束} = \\text{延迟} \\times \\text{吞吐量} $$\n 注意带宽和吞吐量的区别，带宽一般指的是理论峰值，最大每个时钟周期能执行多少个指令，吞吐量是指实际操作过程中每分钟处理多少个指令。\n 这个可以想象成一个瀑布，像这样，绿箭头是线程束，只要线程束足够多，吞吐量是不会降低的： 下面表格给出了Fermi 和Kepler执行某个简单计算时需要的并行操作数： 另外有两种方法可以提高并行：\n 指令级并行(ILP): 一个线程中有很多独立的指令 线程级并行(TLP): 很多并发地符合条件的线程  同样，与指令周期隐藏延迟类似，内存隐藏延迟是靠内存读取的并发操作来完成的，需要注意的是，指令隐藏的关键目的是使用全部的计算资源，而内存读取的延迟隐藏是为了使用全部的内存带宽，内存延迟的时候，计算资源正在被别的线程束使用，所以我们不考虑内存读取延迟的时候计算资源在做了什么，这两种延迟我们看做两个不同的部门但是遵循相同的道理。 我们的根本目的是把计算资源，内存读取的带宽资源全部使用满，这样就能达到理论的最大效率。 同样下表根据Little 法则给出了需要多少线程束来最小化内存读取延迟，不过这里有个单位换算过程，机器的性能指标内存读取速度给出的是GB/s 的单位，而我们需要的是每个时钟周期读取字节数，所以要用这个速度除以频率，例如C 2070 的内存带宽是144 GB/s 化成时钟周期： $\\frac{144GB/s}{1.566GHz}=92 B/t$ ,这样就能得到单位时间周期的内存带宽了。 得出下表的数据 需要说明的是这个速度不是单个SM的而是整个GPU设备的，以内们用的内存带宽是GPU设备的而不是针对一个SM的。 Fermi 需要并行的读取74的数据才能让GPU带宽满载，如果每个线程读取4个字节，我们大约需要18500个线程，大约579个线程束才能达到这个峰值。 所以，延迟的隐藏取决于活动的线程束的数量，数量越多，隐藏的越好，但是线程束的数量又受到上面的说的资源影响。所以这里就需要寻找最优的执行配置来达到最优的延迟隐藏。\n那么我们怎么样确定一个线程束的下界呢，使得当高于这个数字时SM的延迟能充分的隐藏，其实这个公式很简单，也很好理解，就是SM的计算核心数乘以单条指令的延迟， 比如32个单精度浮点计算器，每次计算延迟20个时钟周期，那么我需要最少 32x20 =640 个线程使设备处于忙碌状态。\n占用率 占用率是一个SM种活跃的线程束的数量，占SM最大支持线程束数量的比， 我们前面写的程序7_deviceInformation 中添加几个成员的查询就可以帮我们找到这个值： 完整代码：https://github.com/Tony-Tan/CUDA_Freshman\n// 省略了上半部分 printf(\u0026#34;----------------------------------------------------------\\n\u0026#34;); printf(\u0026#34;Number of multiprocessors: %d\\n\u0026#34;, deviceProp.multiProcessorCount); printf(\u0026#34;Total amount of constant memory: %4.2f KB\\n\u0026#34;, deviceProp.totalConstMem/1024.0); printf(\u0026#34;Total amount of shared memory per block: %4.2f KB\\n\u0026#34;, deviceProp.sharedMemPerBlock/1024.0); printf(\u0026#34;Total number of registers available per block: %d\\n\u0026#34;, deviceProp.regsPerBlock); printf(\u0026#34;Warp size %d\\n\u0026#34;, deviceProp.warpSize); printf(\u0026#34;Maximum number of threads per block: %d\\n\u0026#34;, deviceProp.maxThreadsPerBlock); printf(\u0026#34;Maximum number of threads per multiprocessor: %d\\n\u0026#34;, deviceProp.maxThreadsPerMultiProcessor); printf(\u0026#34;Maximum number of warps per multiprocessor: %d\\n\u0026#34;, deviceProp.maxThreadsPerMultiProcessor/32); return EXIT_SUCCESS; 结果 最大64个线程束每个SM。 CUDA工具包中提供一个叫做UCDA占用率计算器的电子表格，填上相关数据可以帮你自动计算网格参数： 上图是书上的截图，吐个槽，这些人居然写了个表格，为啥不写个程序？\n上面我们已经明确内核使用寄存器的数量会影响SM内线程束的数量，nvcc的编译选项也有手动控制寄存器的使用。 也可以通过调整线程块内线程的多少来提高占用率，当然要合理不能太极端：\n 小的线程块：每个线程块中线程太少，会在所有资源没用完就达到了线程束的最大要求 大的线程块：每个线程块中太多线程，会导致每个SM中每个线程可用的硬件资源较少。  同步 并发程序对同步非常有用，比如pthread中的锁，openmp中的同步机制，这没做的主要目的是避免内存竞争 CUDA同步这里只讲两种：\n 线程块内同步 系统级别  块级别的就是同一个块内的线程会同时停止在某个设定的位置，用\n__syncthread(); 这个函数完成，这个函数只能同步同一个块内的线程，不能同步不同块内的线程，想要同步不同块内的线程，就只能让核函数执行完成，控制程序交换主机，这种方式来同步所有线程。\n内存竞争是非常危险的，一定要非常小心，这里经常出错。\n可扩展性 可扩展性其实是相对于不同硬件的，当某个程序在设备1上执行的时候时间消耗是T当我们使用设备2时，其资源是设备1的两倍，我们希望得到T/2的运行速度，这种性质是CUDA驱动部分提供的特性，目前来说 Nvidia正在致力于这方面的优化，如下图：\n总结 今天效率很高，主要是这个部分之前已经研究透彻了，第三章是Freshman阶段的最核心部分，需要大家多查资料，多思考，多练习，待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-2-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8-p2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA线程束执行的本质的后半部分，包括资源，延迟，同步，扩展性等严重影响性能的线，吞吐量，带宽，占用率，CUDA同步\u003c/p\u003e","title":"【CUDA 基础】3.2 理解线程束执行的本质(Part II)"},{"content":"Abstract: 本文继续上文的内容，讲解多变量分布的条件分布，全概率公式和贝叶斯公式。提出直方图的概念。 Keywords: Conditional Distributions，Law of Total Probability，Bayes\u0026rsquo; Theorem，Histograms\n多变量分布 向往自由的人更懂得约束自己，约束自己的行为，自由自己的思想，而我们现在周围的人更多的是反过来的，约束自己的思想，告诉自己不许瞎想，但是自由自己的行为，想干嘛干嘛，毕竟改革开放的红利使得大部分人从温饱过渡到有车有房还可以随意出门旅游看看世界了，而思维思想，包括知识，还停留在改革开放之前，这就导致了很诡异的一系列行为\u0026ndash;有钱没文化。 本文继续上文，将前面的内容扩展至多变量的联合分布，最后引出关于直方图的一些知识。\n多变量条件分布 Conditional Distributions 条件概率，是3.6中讲解的内容，我们反复的强调所有概率都是条件概率，所有分布也都是条件分布，所以与其说条件概率（分布）与一般的概率（分布）行为一致，不如说所有的所有的行为都是在条件概率的基础上进行的，只是我们省略了某些必然达成的条件。 假设 $n$ 个随机变量 $X_1,\\dots,X_n$ 有一个连续的联合分布，其联合分布式p.d.f. 是 $f$ 并且 $f_0$ 定义了其中 $k \u0026lt; n$ 个随机变量的边缘分布有 $f_0(x_1,\\dots,x_n) \u0026gt; 0$ 那么条件分布，当条件 $X_1=x_1,\\dots X_n=x_n$ 给定时 $(x_{k+1},\\dots,x_n)$ 的p.d.f.是：\n$$ g_{k+1,\\dots,x_n}(x_{k+1},\\dots,x_{n}|x_1,\\dots,x_k)=\\frac{f(x_1,\\dots ,x_n)}{f_0(x_1,\\dots,x_k)} $$\n Definition 3.3.7 Conditional p.f. or p.d.f. Suppose that the random vector $\\vec{X}=(X_1,\\dots,X_n)$ is divided into two subvectors $\\vec{Y}$ and $\\vec{Z}$ ,where $\\vec{Y}$ is a k-dimensional random vector comprising $k$ of the $n$ random variables in $\\vec{X}$ and $\\vec{Z}$ is an $(n-k)$-dimensional random vector comprising the other $n-k$ random variables in $\\vec{X}$ .Suppose also that the $n$-dimensional joint p.f. or p.d.f. of $(\\vec{Y},\\vec{Z})$ is $f$ and that the marginal $(n-k)$-dimensional joint p.f. ,p.d.f. or p.f./p.d.f. of $\\vec{Z}$ is $f_2$ .Then for every given point $z\\in\\mathbb{R}^{n-k}$ such that $f_2(z)\u0026gt;0$ ,the conditional $k$-dimensional p.f. p.d.f.or p.f./p.d.f. $g_1$ of $\\vec{Y}$ given $\\vec{Z}=\\vec{z}$ is defined as follows: $$ g_1(\\vec{y}|\\vec{z})=\\frac{f(\\vec{y},\\vec{z})}{f_2(\\vec{z})} \\text{ for }\\vec{y}\\in \\mathbb{R}^k $$\n 这就是完整的定义，抄一遍下来还真写了不少字，但是整个思路很清晰，首先就是把多变量形成向量的形式，再把向量拆成两个小的空间当然上面这个定义的公式也可以写成：\n$$ f(\\vec{y},\\vec{z})=g_1(\\vec{y}|\\vec{z}) f_2(\\vec{z}) $$\n写成乘法原理的形式，解决分母是0的尴尬局面。也可以看出，通过条件分布和边缘分布得到联合分布的方法是正确的（乘法原理的正确性）。\n多变量全概率公式，贝叶斯定理 Law of Total Probability \u0026amp; Bayes\u0026rsquo; Theorem 条件概率和乘法原则得到证明后接着就可以扩展到全概率公式和贝叶斯公式：\n Theorem Multivariate Law of Total Probability and Bayes\u0026rsquo; Theorem .Assume the conditions and notation given in Definition 3.3.7 .If $\\vec{Z}$ has a continuous joint distribution ,the marginal p.d.f. of $\\vec{Y}$ is $$ f_1(\\vec{y})=\\underbrace{\\int^{\\infty}{-\\infty} \\dots \\int^{\\infty}{-\\infty}}_{n-k}g_1(\\vec{y}|\\vec{z})f_2(\\vec{z})d\\vec{z} $$\n and the conditional p.d.f. of $\\vec{Z}$ given $\\vec{Y}=\\vec{y}$ is $$ g_2(\\vec{z}|\\vec{y})=\\frac{g_1(\\vec{y}|\\vec{z})f_2(\\vec{z})}{f_1(\\vec{y})} $$\n上面的积分如果在离散概率情况要变成求和。\n条件独立多随机变量 Conditionally Independent Random Variables 当我们定义某两个或某几个随机变量独立的时候，是看其概率分布的关系是否满足乘法关系，但是当我们说某两个或几个随机变量条件独立的时候，条件一定是同一个条件，比如 $\\vec{x}$ 在条件 $\\vec{z}$ 下和 $\\vec{y}$ 相互独立。 意味着 $f(\\vec{x},\\vec{y}|\\vec{z})=f_1(\\vec{x}|\\vec{z})f_2(\\vec{y}|\\vec{z})$ ,不可能出现这种情况：$\\vec{x}$ 在条件 $\\vec{z}$ 下和 $\\vec{y}$ 在条件 $\\vec{w}$ 下相互独立。\n Definition Conditionally Independent Random Variables.Let Z be a random vector with joint p.f.,p.d.f. or p.f./p.d.f. f_0(\\vec{z}) .Several random variables X_1,\\dots,X_n are conditionally independent given Z if,for all z such that f_0(z)\u0026gt;0 we have $$ g(\\vec{x}|\\vec{z})=\\prod_{i=1}^ng_i{x_i}|\\vec{z}) $$\n 注意上面公式中 $g(\\vec{x}|\\vec{z})$ 是多变量的条件随机分布，而后面的乘数 $g_i{x_i}|\\vec{z})$ 条件是多变量的而 $x_i$ 是单变量\n进一步扩展，我们现在已经基本知道了前面所有分布定理的条件版本，但是只有一个条件，当条件是多个的时候，我们进行相同的操作，比如多条件的贝叶斯公式： $$ g_2(\\vec{z}|\\vec{y},\\vec{w})=\\frac{g_1(\\vec{y}|\\vec{z},\\vec{w})f_2(\\vec{z}|\\vec{w})}{f_1(\\vec{y}|\\vec{w})} $$\n还是前面说的，所有分布都是条件的，独立性也一样，独立是条件独立的特殊情况。 当我们证明某个条件随机变量或者某些条件独立同分布的随机变量满足某一结论的时候，那么其非条件形式也满足！\n直方图 Histograms 直方图用处很多，Excel中，高考试卷上，图像处理里面，直方图是我们收集到的数据的直观显示，直方图对于显示条件独立同分布的随机变量是最有效的，换句话说，如果收集到的数据不是（条件）独立同分布的，那么显示出来没有意义，因为我们抽样的时候都是假设他们独立同分布的i.i.d\n Definition Histogram. Let $x_1,\\dots,x_n$ be a collection of numbers that all lie between two values $a \u0026lt; b$ That is $a\\leq x_i \\leq b$ for all $i=1,\\dots,n$ Choose some integer $k\\geq 1$ and divide the interval $[a,b]$ into $k$ equal-legth subintervals of length $\\frac{b-a}{k}$ .For each subinterval,count how many of the numbers $x_1,\\dots,x_n$ are in the subinterval,Let $c_i$ be the count for subinterval $i$ for $i=1,\\dots,k$ ,Choose a number $r\u0026gt;0$ (Typically, $r+1$ ,or $r=n$ or $r=\\frac{n(b-a)}{k})$ Draw a two-dimensional graph with the horizonal axis running from $a$ to $b$ .For each subinterval $i=1,\\dots,k$ draw a rectangular bar of width $\\frac{b-a}{k} and height equal to $\\frac{c_i}{r}$ over the midpoint of the $i$th interval.such a graph is called a histogram.\n 上面是直方图的基本定义，interval被分成多个等长的subinterval 当然我们也可以开发出不等长的subinterval 然后用每个直方图内的方块的面积表示这个subinterval中的数量。\n总结 今天我们就把随机变量一般化了，这更加复合实际应用，而且操作也不是很复杂。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-3-7-multivariate-distributions-p2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文继续上文的内容，讲解多变量分布的条件分布，全概率公式和贝叶斯公式。提出直方图的概念。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Conditional Distributions，Law of Total Probability，Bayes\u0026rsquo; Theorem，Histograms\u003c/p\u003e","title":"【概率论】3-7:多变量分布(Multivariate Distributions Part II）"},{"content":"Abstract: 本文介绍CUDA执行模型最核心的部分，线程束的执行实质第一部分 Keywords: CUDA分支，线程束分化\n理解线程束执行的本质(Part I) 我们前面已经大概的介绍了CUDA执行模型的大概过程，包括线程网格，线程束，线程间的关系，以及硬件的大概结构，例如SM的大概结构，而对于硬件来说，CUDA执行的实质是线程束的执行，因为硬件根本不知道每个块谁是谁，也不知道先后顺序，硬件(SM)只知道按照机器码跑，而给他什么，先后顺序，这个就是硬件功能设计的直接体现了。 从外表来看，CUDA执行所有的线程，并行的，没有先后次序的，但实际上硬件资源是有限的，不可能同时执行百万个线程，所以从硬件角度来看，物理层面上执行的也只是线程的一部分，而每次执行的这一部分，就是我们前面提到的线程束。\n线程束和线程块 线程束是SM中基本的执行单元，当一个网格被启动（网格被启动，等价于一个内核被启动，每个内核对应于自己的网格），网格中包含线程块，线程块被分配到某一个SM上以后，将分为多个线程束，每个线程束一般是32个线程（目前的GPU都是32个线程，但不保证未来还是32个）在一个线程束中，所有线程按照单指令多线程SIMT的方式执行，每一步执行相同的指令，但是处理的数据为私有的数据，下图反应的就是逻辑，实际，和硬件的图形化 线程块是个逻辑产物，因为在计算机里，内存总是一维线性存在的，所以执行起来也是一维的访问线程块中的线程，但是我们在写程序的时候却可以以二维三维的方式进行，原因是方便我们写程序，比如处理图像或者三维的数据，三维块就会变得很直接，很方便。 在块中，每个线程有唯一的编号（可能是个三维的编号），threadIdx。 网格中，每个线程块也有唯一的编号(可能是个三维的编号)，blockIdx 那么每个线程就有在网格中的唯一编号。 当一个线程块中有128个线程的时候，其分配到SM上执行时，会分成4个块：\nwarp0: thread 0,........thread31 warp1: thread 32,........thread63 warp2: thread 64,........thread95 warp3: thread 96,........thread127 当编号使用三维编号时，x位于最内层，y位于中层，z位于最外层，想象下c语言的数组，如果把上面这句话写成c语言，假设三维数组t保存了所有的线程，那么(threadIdx.x,threadIdx.y,threadIdx.z)表示为\nt[z][y][x]; 计算出三维对应的线性地址是： $$ tid = threadIdx.x+threadIdx.y\\times blockDim.x+threadIdx.z\\times blockDim.x \\times blockDim.y $$\n上面的公式可以借助c语言的三维数组计算相对地址的方法，如果有人做过图像，或者矩阵，那么这个计算过程应该没啥纠结的。但是对于初学者，这个地方经常性的绕晕，就行我刚开始写图像算法的时候，经常搞不清楚长和宽。 一个线程块包含多少个线程束呢？ $$ \\text{WarpsPerBlock}=\\text{ceil}\\begin{pmatrix}\\frac{\\text{ThreadsPerBlock}}{\\text{warpSize}}\\end{pmatrix} $$ ceil函数是向正无穷取整的函数，比如$ceil(\\frac{9}{8})=2$ 线程束和线程块，一个是硬件层面的线程集合，一个是逻辑层面的线程集合，我们编程时为了程序正确，必须从逻辑层面计算清楚，但是为了得到更快的程序，硬件层面是我们应该注意的。\n线程束分化 线程束被执行的时候会被分配给相同的指令，处理各自私有的数据，还记得前文中的分苹果么？每次分的水果都是一样的，但是你可以选择吃或者不吃，这个吃和不吃就是分支，在CUDA中支持C语言的控制流，比如if\u0026hellip;else, for ,while 等，CUDA中同样支持，但是如果一个线程束中的不同线程包含不同的控制条件，那么当我们执行到这个控制条件是就会面临不同的选择。 这里要讲一下CPU了，当我们的程序包含大量的分支判断时，从程序角度来说，程序的逻辑是很复杂的，因为一个分支就会有两条路可以走，如果有10个分支，那么一共有1024条路走，CPU采用流水线话作业，如果每次等到分支执行完再执行下面的指令会造成很大的延迟，所以现在处理器都采用分支预测技术，而CPU的这项技术相对于gpu来说高级了不止一点点，而这也是GPU与CPU的不同，设计初衷就是为了解决不同的问题。CPU适合逻辑复杂计算量不大的程序，比如操作系统，控制系统，GPU适合大量计算简单逻辑的任务，所以被用来算数。 如下一段代码：\nif (con) { //do something } else { //do something } 假设这段代码是核函数的一部分，那么当一个线程束的32个线程执行这段代码的时候，如果其中16个执行if中的代码段，而另外16个执行else中的代码块，同一个线程束中的线程，执行不同的指令，这叫做线程束的分化。 我们知道在每个指令周期，线程束中的所有线程执行相同的指令，但是线程束又是分化的，所以这似乎是相悖的，但是事实上这两个可以不矛盾。 解决矛盾的办法就是每个线程都执行所有的if和else部分，当一部分con成立的时候，执行if块内的代码，有一部分线程con不成立，那么他们怎么办？继续执行else？不可能的，因为分配命令的调度器就一个，所以这些con不成立的线程等待，就像分水果，你不爱吃，那你就只能看着别人吃，等大家都吃完了，再进行下一轮（也就是下一个指令）线程束分化会产生严重的性能下降。条件分支越多，并行性削弱越严重。 注意线程束分化研究的是一个线程束中的线程，不同线程束中的分支互不影响。 执行过程如下： 因为线程束分化导致的性能下降就应该用线程束的方法解决，根本思路是避免同一个线程束内的线程分化，而让我们能控制线程束内线程行为的原因是线程块中线程分配到线程束是有规律的而不是随机的。这就使得我们根据线程编号来设计分支是可以的，补充说明下，当一个线程束中所有的线程都执行if或者，都执行else时，不存在性能下降；只有当线程束内有分歧产生分支的时候，性能才会急剧下降。 线程束内的线程是可以被我们控制的，那么我们就把都执行if的线程塞到一个线程束中，或者让一个线程束中的线程都执行if，另外线程都执行else的这种方式可以将效率提高很多。 下面这个kernel可以产生一个比较低效的分支：\n__global__ void mathKernel1(float *c) { int tid = blockIdx.x* blockDim.x + threadIdx.x; float a = 0.0; float b = 0.0; if (tid % 2 == 0) { a = 100.0f; } else { b = 200.0f; } c[tid] = a + b; } 这种情况下我们假设只配置一个x=64的一维线程块，那么只有两个个线程束，线程束内奇数线程（threadIdx.x为奇数）会执行else，偶数线程执行if，分化很严重。 但是如果我们换一种方法，得到相同但是错乱的结果C，这个顺序其实是无所谓的，因为我们可以后期调整。那么下面代码就会很高效\n__global__ void mathKernel2(float *c) { int tid = blockIdx.x* blockDim.x + threadIdx.x; float a = 0.0; float b = 0.0; if ((tid/warpSize) % 2 == 0) { a = 100.0f; } else { b = 200.0f; } c[tid] = a + b; } 第一个线程束内的线程编号tid从0到31，tid/warpSize都等于0，那么就都执行if语句。 第二个线程束内的线程编号tid从32到63，tid/warpSize都等于1，执行else 线程束内没有分支，效率较高。\n完整代码：https://github.com/Tony-Tan/CUDA_Freshman\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026#34;freshman.h\u0026#34;__global__ void warmup(float *c) { int tid = blockIdx.x* blockDim.x + threadIdx.x; float a = 0.0; float b = 0.0; if ((tid/warpSize) % 2 == 0) { a = 100.0f; } else { b = 200.0f; } //printf(\u0026#34;%d %d %f \\n\u0026#34;,tid,warpSize,a+b); \tc[tid] = a + b; } __global__ void mathKernel1(float *c) { int tid = blockIdx.x* blockDim.x + threadIdx.x; float a = 0.0; float b = 0.0; if (tid % 2 == 0) { a = 100.0f; } else { b = 200.0f; } c[tid] = a + b; } __global__ void mathKernel2(float *c) { int tid = blockIdx.x* blockDim.x + threadIdx.x; float a = 0.0; float b = 0.0; if ((tid/warpSize) % 2 == 0) { a = 100.0f; } else { b = 200.0f; } c[tid] = a + b; } __global__ void mathKernel3(float *c) { int tid = blockIdx.x* blockDim.x + threadIdx.x; float a = 0.0; float b = 0.0; bool ipred = (tid % 2 == 0); if (ipred) { a = 100.0f; } else { b = 200.0f; } c[tid] = a + b; } int main(int argc, char **argv) { int dev = 0; cudaDeviceProp deviceProp; cudaGetDeviceProperties(\u0026amp;deviceProp, dev); printf(\u0026#34;%s using Device %d: %s\\n\u0026#34;, argv[0], dev, deviceProp.name); //set up data size \tint size = 64; int blocksize = 64; if (argc \u0026gt; 1) blocksize = atoi(argv[1]); if (argc \u0026gt; 2) size = atoi(argv[2]); printf(\u0026#34;Data size %d \u0026#34;, size); //set up execution configuration \tdim3 block(blocksize,1); dim3 grid((size - 1) / block.x + 1,1); printf(\u0026#34;Execution Configure (block %d grid %d)\\n\u0026#34;, block.x, grid.x); //allocate gpu memory \tfloat * C_dev; size_t nBytes = size * sizeof(float); float * C_host=(float*)malloc(nBytes); cudaMalloc((float**)\u0026amp;C_dev, nBytes); //run a warmup kernel to remove overhead \tdouble iStart, iElaps; cudaDeviceSynchronize(); iStart = cpuSecond(); warmup\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt; (C_dev); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; printf(\u0026#34;warmup\t\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt;elapsed %lf sec \\n\u0026#34;, grid.x, block.x, iElaps); //run kernel 1 \tiStart = cpuSecond(); mathKernel1 \u0026lt;\u0026lt;\u0026lt; grid,block \u0026gt;\u0026gt;\u0026gt; (C_dev); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; printf(\u0026#34;mathKernel1\u0026lt;\u0026lt;\u0026lt;%4d,%4d\u0026gt;\u0026gt;\u0026gt;elapsed %lf sec \\n\u0026#34;, grid.x, block.x, iElaps); cudaMemcpy(C_host,C_dev,nBytes,cudaMemcpyDeviceToHost); //for(int i=0;i\u0026lt;size;i++) \t//{ \t//\tprintf(\u0026#34;%f \u0026#34;,C_host[i]); \t//} \t//run kernel 2 \tiStart = cpuSecond(); mathKernel2 \u0026lt;\u0026lt;\u0026lt;grid,block \u0026gt;\u0026gt;\u0026gt; (C_dev); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; printf(\u0026#34;mathKernel2\u0026lt;\u0026lt;\u0026lt;%4d,%4d\u0026gt;\u0026gt;\u0026gt;elapsed %lf sec \\n\u0026#34;, grid.x, block.x, iElaps); //run kernel 3 \tiStart = cpuSecond(); mathKernel3 \u0026lt;\u0026lt; \u0026lt;grid, block \u0026gt;\u0026gt; \u0026gt; (C_dev); cudaDeviceSynchronize(); iElaps = cpuSecond() - iStart; printf(\u0026#34;mathKernel3\u0026lt;\u0026lt;\u0026lt;%4d,%4d\u0026gt;\u0026gt;\u0026gt;elapsed %lf sec \\n\u0026#34;, grid.x, block.x, iElaps); cudaFree(C_dev); free(C_host); cudaDeviceReset(); return EXIT_SUCCESS; } 代码中warmup部分是提前启动一次GPU，因为第一次启动GPU时会比第二次速度慢一些，具体原因未知，可以去查一下CUDA的相关技术文档了解内容。我们可以通过nvprof分析一下程序执行过程：\nnvprof --metrics branch_efficiency ./divergence 然后得到下面这些参数：\n可以看到这里面所有kernel的分支效率都是100%，而这个值的计算是这样的：\n$$ \\text{Branch Efficiency}=\\frac{\\text{Branches − DivergentBranches}}{\\text{Branches}} $$ 但是这又有问题了，明明我们kernel1的分支效率应该是50%但是，为啥测试结果是100%呢？\n因为编译器帮我们进行了优化，这个具体原因我们不在Freshman系列中介绍了，下一系列我们会深入进行，所以这里先挖个坑 但是下面我们用另一种方式，编译器就不会优化了：\n//kernel 3 __global__ void mathKernel3(float *c) { int tid = blockIdx.x* blockDim.x + threadIdx.x; float a = 0.0; float b = 0.0; bool ipred = (tid % 2 == 0); if (ipred) { a = 100.0f; } else { b = 200.0f; } c[tid] = a + b; } 执行结果还是上面那张图，那里面已经有kernel3了。我们也可以通过编译选项禁用分值预测功能，这样kernel1和kernel3的效率是相近的。如果使用nvprof,会得到下面的结果，没有优化的结果如下： 和我们预测的结果基本一致。 我们考察一下事件计数器：\nnvprof --events branch,divergent_branch ./divergence_g nvcc 在1和3上优化有限，但是也超过了50%以上的利用率。\n事件和指标 上面我们提到了事件event，事件是可计算的活动，比如这个分支就是一个可以计算的活动，对应一个在内和执行期间被搜集的硬件计数器。 指标是内核的特征，有一个或多个事件计算得到。\n总结 总结，我们今天介绍了分支的一部分，后面的部分我们下一篇继续。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-2-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8-p1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA执行模型最核心的部分，线程束的执行实质第一部分\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA分支，线程束分化\u003c/p\u003e","title":"【CUDA 基础】3.2 理解线程束执行的本质(Part I)"},{"content":"Abstract: 本文将3.4，3.5，3.6的内容扩展到多个随机变量中去，并得到相对应的结论，由于内容较多，故分为两部分完成 Keywords: Joint Distributions,Mixed Distributions,Marginal Distributions,Independent Random Variable\n多变量分布 今天讲的废话可能真是废话，因为这个道理真是自古以来经过无数次的验证，关于合作，合作就是多个人在一起做一件事，有钱出钱有力出力，所有的想法，工作和信息都要共享。这是一个团队存在的必要条件，但是有两种东西千万别想着与团队内部人员分享，首先是别人的收益，你不能指望别人把收益分给你，这个不现实也不讲道理的，说好是谁的就是谁的，不能拿别人的任何收益，这是保证团队不崩溃的底线；第二不要让别人跟你分担风险，你的风险就是你的风险，你出资赔钱你就要忍着，不可能让别人补偿你，这是不符合规矩的，别人也没这个义务，甚至别人主动补偿都不能要；最后一点就在于沟通，有些话说了必须负责任，任何事给出预期，同时必须提示风险，别总拍着胸脯保证什么什么，尤其是没有发生的事，这样的结果就是，一旦负面情况发生，你的责任就会非常大了，而且大家会对你这个产生怀疑！ 接着就是正经的废话了，关于概率论，我这两天尝试着看数理统计方面的书，发现，难度有点提升的过快，尤其是概率论掌握的不是很熟练的时候，我的概率论现在什么水平？看了一遍书，写了下书后的习题，目前也就这样，但是写完博客会是一个很大的提升，整个思路和认知都会有所提升，所以我决定先把概率论的博客写完再继续数理统计，到时候应该能通常一点了 本文是对前三节内容的扩展，我们学习概率论从试验，到事件再到随机变量，从概率，到概率分布，都是从简单的可见的，到复杂的抽象的，这篇就把前面的限制进一步减小，从单个随机变量到两个随机变量，再到今天的多个随机变量的过程\n联合分布 Joint Distributions 当一个分布中随机变量的个数超过两个的时候，我们称之为多变量概率分布；在实际应用中多变量随机分布应用更广。\n联合离散分布 Joint Discrete Distribution  Definition Joint Distribution Function/c.d.f.:The joint c.d.f. of $n$ random variables $X_1,\\dots ,X_n$ is the function $F$ whose value at every point $(x_1,\\dots ,x_n)$ in n-dimensional space $\\mathbb{R}^n$ is specified by the relation $$ F(x_1,\\dots , x_n)=Pr(X_1\\leq x_1,X_2\\leq x_2,\\dots X_n\\leq x_n) $$\n 多变量c.d.f.和前面单变量双变量c.d.f有相似的性质 举个🌰 ： 假设某设备有三个部件，其中部件 $i$ 有可能在 $X_i$ 时刻失效，$i$ 可能是1，2，3，那么 $X_1,X_2,X_3$ 的联合 c.d.f. 是 $$ F(x_1,x_2,x_3)=\\begin{cases}(1-e^{x_1})(1-e^{-2x_2})(1-e^{-3x_3})\u0026amp;\\text{ for }x_1,x_2,x_3\\geq 0\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$\n使用向量记录多个随机变量，像这样： $\\vec{X}=(x_1,\\dots ,x_n)$ 称为随机向量，对于一个多变量的c.d.f. $F(x_1,x_2,x_3)$ 我们可以简写成：$F(\\vec{X})$ 这里需要注意的是当我们用向量来管理多个变量的时候，注意其维度，这时的c.d.f被定义在一个 $\\mathbb{R}^n$ 的空间上。\n Definition Joint Discrete Distribution/p.f. It is said that $n$ random variables $X_1,\\dots ,X_n$ have a discrete joint distribution if the random vector $(X_1,\\dots ,X_n)$ can have only a finite number or an infinite sequence of different possible values $(x_1,\\dots,x_n)$ in $\\mathbb{R}^n$ .The joint p.f. of $X_1,\\dots,X_n$ is then defined as the function $f$ such that for every point $(x_1,\\dots,x_n)\\in \\mathbb{R}^n$ $$ f(x_1,\\dots,x_n)=Pr(X_1=x_1,\\dots,X_n=x_n) $$\n 这个定义说明，随机向量 $\\vec{X}$ 有一个离散分布，其每个点 $\\vec{x} \\in \\mathbb{R}^n$ 的概率是 $$ f(\\vec{x})=Pr(\\vec{X}=\\vec{x}) $$\n下面这个定理和3.4中双变量分布相似\n Theorem If $X$ has a joint discrete distribution with joint p.f. $f$ then for every subset $C\\subset \\mathbb{R}^n$ , $$ Pr(\\vec{X}\\in C)=\\sum_{x\\in C}f(x) $$ 如果每个随机变量$X_1,\\dots,X_n$ 每个随机变量都有离散的分布，那么 $\\vec{X}=(X_1,\\dots,X_n)$ 就有一个离散的联合分布\n 联合连续分布 Joint Continuous Distribution  Definition Continuous Distribution/p.d.f. It is said that $n$ random variables $X_1,\\dots,X_n$ have a continuous joint distribution if there is a nonnegative function $f$ defined on $\\mathbb{R}^n$ such that for every subset $C\\subset \\mathbb{R}^n$, $$ Pr[(X_1,\\dots,X_n)\\in C ]=\\underbrace{\\int\\dots\\int}_{C}f(x_1,\\dots,x_n)dx_1,\\dots,dx_n $$\n 上面的定义也可以用向量来重新表示，向量表示更加简单，但是使用时要注意区分：\n$$ Pr[\\vec{X}\\in C ]=\\underbrace{\\int\\dots\\int}_{C}f(\\vec{x})d\\vec{x} $$\n Theorem If the joint distribution of $X_1,\\dots,X_n$ is continuous,then the joint p.d.f. $f$ can be derived from the joint c.d.f. $F$ by using the relation $$ f(x_1,\\dots,x_n)=\\frac{\\partial^nF(x_1,\\dots,x_n)}{\\partial x_1\\dots \\partial x_n} $$ at all points $(x_1,\\dots,x_n)$ at which the derivative in this relation exists.\n 上面的定理和3.4中的定理非常相似，只是进行了响应的扩展，但是理论基础一致。 举个🌰 ： 找出上面例子的joint p.d.f $$ F(x_1,x_2,x_3)=\\begin{cases}(1-e^{x_1})(1-e^{-2x_2})(1-e^{-3x_3})\u0026amp;\\text{ for }x_1,x_2,x_3\\geq 0\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ 求偏导得到 $$ f(x_1,x_2,x_3)=\\begin{cases}6e^{-x_1-2x_2-3x_3}\u0026amp;\\text{ for }x_1,x_2,x_3\u0026gt; 0\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ 注意，这里 $x_1,x_2,x_3$ 的范围是大于0的，而前面的例子是C.D.F. 是$x_1,x_2,x_3$ 的范围是大于等于0的\n插播新闻 突发事件，就在我写本文的时候，Stephen Hawking 教授去世，享年76岁；非亲非故，也不是相关专业的学生，Howking先生的著作也没拜读过，但是感觉莫名的失落，希望晚辈们能继承先贤们的遗志，为了人类的进步事业做出贡献。\n混合分布 Mixed Distributions 混合分布就是一个联合分布里有连续的随机变量也有离散的随机变量，而处理起来也和双变量联合分布一样，连续部分就用积分和微分处理，离散就用求和做差处理。\n Definition Joint p.f./p.d.f. Let $X_1,\\dots ,X_n$ be random variables,some of which have a continuous joint distribution and some of which have discrete distributions ,their joint distribution would then be represented by a function $f$ that we call the joint p.f./p.d.f .The function has the property that the probability that $X$ lies in a subset $C \\subset \\mathbb{R}^n$ is calculated by summing $f(x)$ over the values of the coordinates of $x$ that correspond to the discrete random variables and integrating over those coordinates that correspond to the continuous random variables for all piints $\\vec{x}\\in C$\n 对于连续和离散混合的分布，还是那句话对不同的敌人用不同的战术，但总体思路都是一样的\n边缘分布 Marginal Distributions 计算边缘概率密度函数 Deriving a Marginal p.d.f. 对于一个多元连续的联合分布，求其边缘概率密度函数的方法是： $$ f_1(x_1)=\\underbrace{\\int^\\infty_{-\\infty}\\dots \\int^\\infty_{-\\infty}}_{n-1}f(x_1,\\dots,x_n)dx_2\\dots dx_n $$ 求谁的边缘分布就把其他的所有变量求积分就可以了，当然也可以求某两个，三个，多个变量的边缘分布。 如果里面对应的变量是离散型的，就把对应的积分换成求和就可以了\n计算边缘概率累积函数 Deriving a Marginal c.d.f. 对于一个联合分布，其c.d.f. 为$F$ ,其中x_1的边缘c.d.f.为： $$ F_1(x_1)=Pr(X_1\\leq x_1,X_2\u0026lt;\\infty,\\dots ,X_n\u0026lt;\\infty)\\ =lim_{x_2,\\dots,x_n \\to \\infty}F(x_1,x_2,\\dots,x_n) $$\n由上可见，跟双变量的操作也是如出一辙，只是变量多了一些，计算起来更加复杂，需要小心一点\n独立随机变量 Independent Random Variable 多维的随机变量的独立性相对要复杂一些，但是原理还是相似的。\n Definition Independent Random Variables,It is said that n random varibales $X_1,\\dots,X_n$ are independent if for every $n$ sets $A_1,A_2,\\dots A_n$ of real numbers $$ Pr(X_1\\in A_1,X_2\\in A_2,\\dots,X_n\\in A_n)=Pr(X_1\\in A_1)Pr(X_2 \\in A_2)\\dots Pr(X_n\\in A_n) $$\n 上面定义的一个推广就是如果 $(X_1,X_2,\\dots,X_n)$ 相互独立那么其非空子空间内的变量也相互独立。\n Theorem Let $F$ denote the joint c.d.f. of $X_1,\\dots X_n$ and let $F_i$ denote the marginal univariate c.d.f. of $X_i$ for $i=1,\\dots,n$ The variables $X_1,\\dots,X_n$ are independent if and only if,for all points $(x_1,x_2,\\dots,x_n)\\in \\mathbb{R}^n$ $$ F(x_1,x_2,\\dots,x_n)=F_1(x_1)F_2(x_2)\\dots F_n(x_n) $$\n 上述定理说明了独立随机变量的联合c.d.f.的求法，下面看看我们后面要经常看到了一种随机变量，他们之间的关系叫做独立同分布的随机变量，简称，i.i.d.\n Defintion Random Samples/i.i.d./Sample Size:Consider a given probability on the real line that can be represented by either a p.f. or a p.d.f. $f$ It is said that $n$ random variables $X_1,\\dots,X_n$ form a random sample from this distribution if these random varibales are independent and the marginal p.f. or p.d.f. of each of them is $f$ Such random variables are also said to be independent and identically distributed ,abbrevuated i.i.d.We refer to the number $n$ of random varibales as the sample size\n 上面定义了独立同分布的随机变量，样本大小，以及涉及到了一个简单的采样过程。 对于i.i.d.的随机变量，其联合p.f或者p.d.f.为 $$ g(x_1,\\dots,x_n)=f(x_1)f(x_2)\\dots f(x_n) $$\n因为i.i.d讲究的的是独立同分布，同分布就保证了不可能出现混合分布的形式，否则会自相矛盾。\n总结 至此我们讲完了上半部分关于多变量的扩展，理论上难度并不是很大，但是实际操作起来可能会出现各种小问题，需要大家谨慎。 致敬Stephen Hawking 先生！ 待续。。\n","permalink":"https://go.face2ai.com/math/math-probability-3-7-multivariate-distributions-p1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文将3.4，3.5，3.6的内容扩展到多个随机变量中去，并得到相对应的结论，由于内容较多，故分为两部分完成\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Joint Distributions,Mixed Distributions,Marginal Distributions,Independent Random Variable\u003c/p\u003e","title":"【概率论】3-7:多变量分布(Multivariate Distributions Part I）"},{"content":"Abstract: 本文介绍CUDA执行模型，只比硬件高一层的抽象 Keywords: CUDA SM，SIMT，SIMD，Fermi，Kepler\nCUDA执行模型概述 这一篇开始我们开始接近CUDA最核心的部分，就是有关硬件，和程序的执行模型，用CUDA的目的其实说白了就是为计算速度快，所以压榨性能，提高效率其实就是CUDA学习的最终目的，没人学CUDA为了去显示Hello world。 前面几篇我们学了编写，启动核函数，计时，统计时间，然后学习了线程，内存模型，线程内存部分我们会在后面用几章的篇幅进行大书特书，而本章，我们介绍最底层最优理论指导意义的知识。 什么时候我们沿着硬件设计的思路设计程序，我们就会得到百战百胜；什么时候我们背离了硬件设计的思路去设计程序，我们就会得不到好结果。\n概述 CUDA执行模型揭示了GPU并行架构的抽象视图，再设计硬件的时候，其功能和特性都已经被设计好了，然后去开发硬件，如果这个过程模型特性或功能与硬件设计有冲突，双方就会进行商讨妥协，知道最后产品定型量产，功能和特性算是全部定型，而这些功能和特性就是变成模型的设计基础，而编程模型又直接反应了硬件设计，从而反映了设备的硬件特性。 比如最直观的一个就是内存，线程的层次结构帮助我们控制大规模并行，这个特性就是硬件设计最初设计好，然后集成电路工程师拿去设计，定型后程序员开发驱动，然后在上层可以直接使用这种执行模型来控制硬件。 所以了解CUDA的执行模型，可以帮助我们优化指令吞吐量，和内存使用来获得极限速度。\nGPU架构概述 GPU架构是围绕一个流式多处理器（SM）的扩展阵列搭建的。通过复制这种结构来实现GPU的硬件并行。 上图包括关键组件：\n CUDA核心 共享内存/一级缓存 寄存器文件 加载/存储单元 特殊功能单元 线程束调度器  SM GPU中每个SM都能支持数百个线程并发执行，每个GPU通常有多个SM，当一个核函数的网格被启动的时候，多个block会被同时分配给可用的SM上执行。\n注意: 当一个blcok被分配给一个SM后，他就只能在这个SM上执行了，不可能重新分配到其他SM上了，多个线程块可以被分配到同一个SM上。\n在SM上同一个块内的多个线程进行线程级别并行，而同一线程内，指令利用指令级并行将单个线程处理成流水线。\n线程束 CUDA 采用单指令多线程SIMT架构管理执行线程，不同设备有不同的线程束大小，但是到目前为止基本所有设备都是维持在32，也就是说每个SM上有多个block，一个block有多个线程（可以是几百个，但不会超过某个最大值），但是从机器的角度，在某时刻T，SM上只执行一个线程束，也就是32个线程在同时同步执行，线程束中的每个线程执行同一条指令，包括有分支的部分，这个我们后面会讲到，\nSIMD vs SIMT 单指令多数据的执行属于向量机，比如我们有四个数字要加上四个数字，那么我们可以用这种单指令多数据的指令来一次完成本来要做四次的运算。这种机制的问题就是过于死板，不允许每个分支有不同的操作，所有分支必须同时执行相同的指令，必须执行没有例外。 相比之下单指令多线程SIMT就更加灵活了，虽然两者都是将相同指令广播给多个执行单元，但是SIMT的某些线程可以选择不执行，也就是说同一时刻所有线程被分配给相同的指令，SIMD规定所有人必须执行，而SIMT则规定有些人可以根据需要不执行，这样SIMT就保证了线程级别的并行，而SIMD更像是指令级别的并行。 SIMT包括以下SIMD不具有的关键特性：\n 每个线程都有自己的指令地址计数器 每个县城都有自己的寄存器状态 每个线程可以有一个独立的执行路径  而上面这三个特性在编程模型可用的方式就是给每个线程一个唯一的标号（blckIdx,threadIdx），并且这三个特性保证了各线程之间的独立\n32 32是个神奇数字，他的产生是硬件系统设计的结果，也就是集成电路工程师搞出来的，所以软件工程师只能接受。 从概念上讲，32是SM以SIMD方式同时处理的工作粒度，这句话这么理解，可能学过后面的会更深刻的明白，一个SM上在某一个时刻，有32个线程在执行同一条指令，这32个线程可以选择性执行，虽然有些可以不执行，但是他也不能执行别的指令，需要另外需要执行这条指令的线程执行完，然后再继续下一条，就像老师给小朋友们分水果： 第一次分苹果，分给所有32个人，你可以不吃，但是不吃也没别的，你就只能在那看别人吃，等别人吃完了，老师会把没吃的苹果回收，防止浪费。 第二次分橘子，你很爱吃，可是有别的小朋友不爱吃，当然这时候他也不能干别的，只能看你吃完。吃完后老师继续回收刚才没吃的橘子。 第三次分桃子，你们都很爱吃，大家一起吃，吃完了老师发现没有剩下的，继续发别的水果，一直发到所有种类的水果都发完了。今天就可以放学了。\n简单的类比，但过程就是这样。\nCUDA编程的组件与逻辑 下图从逻辑角度和硬件角度描述了CUDA编程模型对应的组件。 SM中共享内存，和寄存器是关键的资源，线程块中线程通过共享内存和寄存器相互通信协调。 寄存器和共享内存的分配可以严重影响性能！\n因为SM有限，虽然我们的编程模型层面看所有线程都是并行执行的，但是在微观上看，所有线程块也是分批次的在物理层面的机器上执行，线程块里不同的线程可能进度都不一样，但是同一个线程束内的线程拥有相同的进度。 并行就会引起竞争，多线程以未定义的顺序访问同一个数据，就导致了不可预测的行为，CUDA只提供了一种块内同步的方式，块之间没办法同步！ 同一个SM上可以有不止一个常驻的线程束，有些在执行，有些在等待，他们之间状态的转换是不需要开销的。\nFermi 架构 Fermi架构是第一个完整的GPU架构，所以了解这个架构是非常有必要的，就像几十年过去了，我们的微机原理学的还是386一样，祖宗的基因代代相传，学好了祖宗后面的孙子辈都好掌握。 Fermi架构逻辑图如上，具体数据如下:\n 512个加速核心，CUDA核 每个CUDA核心都有一个全流水线的整数算数逻辑单元ALU，和一个浮点数运算单元FPU CUDA核被组织到16个SM上 6个384-bits的GDDR5 的内存接口 支持6G的全局机栽内存 GigaThread疫情，分配线程块到SM线程束调度器上 768KB的二级缓存，被所有SM共享  而SM则包括下面这些资源：\n 执行单元（CUDA核） 调度线程束的调度器和调度单元 共享内存，寄存器文件和一级缓存  每个多处理器SM有16个加载/存储单元所以每个时钟周期内有16个线程（半个线程束）计算源地址和目的地址\n特殊功能单元SFU执行固有指令，如正弦，余弦，平方根和插值，SFU在每个时钟周期内的每个线程上执行一个固有指令。 每个SM有两个线程束调度器，和两个指令调度单元，当一个线程块被指定给一个SM时，线程块内的所有线程被分成线程束，两个线程束选择其中两个线程束，在用指令调度器存储两个线程束要执行的指令（就像上面例子中分水果的水果一样，我们这里有两个班，两个班的老师各自控制的自己的水果，老师就是指令调度器） 像第一张图上的显示一样，每16个CUDA核心为一个组，还有16个加载/存储单元或4个特殊功能单元。当某个线程块被分配到一个SM上的时候，会被分成多个线程束，线程束在SM上交替执行： 上面曾经说过，每个线程束在同一时间执行同一指令，同一个块内的线程束互相切换是没有时间消耗的。 Fermi上支持同时并发执行内核。并发执行内核允许执行一些小的内核程序来充分利用GPU，如图： Kepler 架构 Kepler架构作为Fermi架构的后代，有以下技术突破：\n 强化的SM 动态并行 Hyper-Q技术  技术参数也提高了不少，比如单个SM上CUDA核的数量，SFU的数量，LD/ST的数量等： kepler架构的最突出的一个特点就是内核可以启动内核了，这使得我们可以使用GPU完成简单的递归操作，流程如下。 Hyper-Q技术主要是CPU和GPU之间的同步硬件连接，以确保CPU在GPU执行的同事做更多的工作。Fermi架构下CPU控制GPU只有一个队列，Kepler架构下可以通过Hyper-Q技术实现多个队列如下图。 计算能力概览： 使用Profile进行优化（Profile-Driven Optimization） 中文翻译的这个标题是配置文件驱动优化，驱动这个词在这里应该是个动词，或者翻译的人直接按照字面意思翻译的，其实看完内容以后的意思是根据profile这个文件内的信息对程序进行优化。 性能分析通过以下方法来进行：\n 应用程序代码的空间(内存)或时间复杂度 特殊指令的使用 函数调用的频率和持续时间  程序优化是建立在对硬件和算法过程理解的基础上的，如果都不了解，靠试验，那么这个结果可想而知。理解平台的执行模型也就是硬件特点，是优化性能的基础。 开发高性能计算程序的两步：\n 保证结果正确，和程序健壮性 优化速度  Profile可以帮助我们观察程序内部。\n 一个原生的内核应用一般不会产生最佳效果，也就是我们基本不能一下子就写出最好最快的内核，需要通过性能分析工具分析性能。找出性能瓶颈 CUDA将SM中的计算资源在该SM中的多个常驻线程块之间进行分配，这种分配方式可能导致一些资源成为性能限制因素，性能分析工具可以帮我们找出来这些资源是如何被使用的 CUDA提供了一个硬件架构的抽象。它能够让用户控制线程并发。性能分析工具可以检测和优化，并肩优化可视化  总结起来一句话，想优化速度，先学好怎么用性能分析工具。\n nvvp nvprof  限制内核性能的主要包括但不限于以下因素\n 存储带宽 计算资源 指令和内存延迟  总结 想写出更快更好的CUDA程序，学习硬件执行模型，学习使用测试工具，多思考多总结多练习，就可以做的更好！\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-3-1-cuda%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA执行模型，只比硬件高一层的抽象\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA SM，SIMT，SIMD，Fermi，Kepler\u003c/p\u003e","title":"【CUDA 基础】3.1 CUDA执行模型概述"},{"content":"Abstract: 本文介绍联合分布的构建，也就是条件分布部分的扩展和应用 Keywords: 乘法法则，贝叶斯定理，随机变量的全概率公式\n条件分布 今天这篇是上一篇的后半部分，其实应该是一篇，但是上一篇由于长时间没写博客导致写作速度下降，所以不得已分成两篇，最近除了写概率的博客，还有数学分析的博客，CUDA系列的也在更新，所以有点要累吐血的感觉，同时还在学习数理统计，数理统计用的是陈希孺先生的概率论与数理统计的数理统计部分，看了二十几页，发现他说的90%我基本都能看懂，但是真的不知道为啥上大学的时候，有老师讲还一脸懵x，是我智商进化了？还是书本难度降低了？这个就不得而知了，除非把大学教材重新拿过来比较一下，那就有点浪费时间了，我的目标是学好数学去研究机器学习，而不是做教材点评，难道不是么？\n条件概率乘法法则 Multiplication Rule for Conditional Probability 乘法法则我们在事件的概率部分学过了传送到条件概率，也是通过条件概率过度出来的，并且乘法法则相对于条件概率适用面更广，因为条件概率有除法计算，所以必然会对概率为0的分母有所忌惮，但是乘法法则无所谓，0可以随便来： $$ Pr(A|B)=\\frac{Pr(A,B)}{Pr(B)} \\text{ for } Pr(B)\\neq 0\\ Pr(A,B)=Pr(A|B)\\times Pr(B) \\text{ for } Pr(B)\\geq 0 $$ 根据随机变量的定义，我们知道随机变量是个函数，可以把事件映射成数字，如果我们将上面的条件概率转化成条件分布，应该怎么转呢？我们先看个例子 前面我们说过所有概率都是条件概率只是有些条件在题设中已经明确固定了，我们就没有必要再分布中再反复的体现了。 举个🌰：\n  还是零件加工的问题，假设我们明确的知道加工这批零件的合格率是90%（我是怎么知道？上帝说的！就是知道，这也是概率论理想情况的抽象，就像物理中的质点一样），那么我们生产了100个零件，其中合格的零件为x个的事件的概率是多少： 分析，很简单的一个离散概率模型，设X是有零件x合格的事件，二项分布 $$ Pr(X=x)=\\begin{pmatrix}100\\x\\end{pmatrix}0.9^x(1-0.9)^{100-x} \\text{ for } $$\n  那么这是我们的初级阶段，从初级到高级的一种变化方式就是把条件不确定化，比如上面的例子，我们条件中有两个已知数，90%和100 ，那么如果我们把100变成变量n呢？这个变量将会是一个普通的变量，或者说是输入变量，由我们自己决定。同时我们把事件转换到随机变量，那么例子就变成了 $$ g_1(x)=\\begin{pmatrix}n\\x\\end{pmatrix}0.9^x(1-0.9)^{n-x} \\text{ for }x=0,1,2,\\dots $$ 这种情况下，n是个普通可控制的变量，因为我们可以想象，如果你有一个工厂，生产一批零件，不管好坏，总数量肯定是你控制的，如果你控制不了，说明这个厂子你已经失去控制权了，也就是说，不管怎么样，这个n完全归我们管。那么我们下一步复杂。\n  我们在1中蛮横不讲理的说90%是上帝告诉我们的，那么上帝是怎么知道这个数呢？《上帝掷骰子么》，那么如果他掷骰子这个事就又要归概率论处理了，那么我们接着引入变量p作为合格率(原始例子中的90%)，这个变量与n的最大差别是我们控制不了他，控制不了的就是随机的，随机的就可以用一个p.f.或者p.d.f来描述他，也就是说这个条件是要考虑其分布了，那么我们的例子近一步进化： $$ g_1(x|p)=\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x} \\text{ for }x=0,1,2,\\dots $$ 又因为我们上一篇已经研究过了离散，连续，混合随机变量的条件分布，那么这个例子很明显中p是连续的，x是离散的，是个混合条件概率，那么从概率转移到分布有： $$ g_1(x|p)=\\frac{f(x,p)}{f_2(p)} $$ 其中 $1\\geq p\\geq 0$ 那么我们就有，X和P的联合p.f.或者p.d.f，若果我们假设$f_2(p)$ 是一个0到1区间内的均匀分布我们有： $$ f(x,p)=g_1(x|p)f_2(p)=\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x} \\times f_2(p)\\ =\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x} \\times 1)=\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x}\\ \\text{ for }x=0,\\dots ,n \\text{ and } 0 \\leq p \\leq 1 $$ 上面这个式子原文只给出： $$ f(x,p)=g_1(x|p)f_2(p)=\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x} \\text{ for }x=0,\\dots ,n \\text{ and } 0 \\leq p \\leq 1 $$ 看了半天才明白，他直接把1省略掉了，为啥是1 不知道？去看分布的文章。\n  那么我们就可以正式的提出我们的定理了：\n Multiplication Rule for Distributions: Let $X$ and $Y$ be random variables such that $X$ has p.f. or p.f.d. $f_1(x)$ and $Y$ has p.f. or p.d.f $f_2(y)$. Also,assume that the conditional p.f. or p.d.f. of X given $Y=y$ is $g_1(x|y)$ while the conditional p.f. or p.d.f. of Y given $X=x$ is $g_2(y|x)$ .Then for each y such that $f_2(y)\u0026gt;0$ and each $x$, $$ f(x,y)=g_1(x|y)f_2(y) $$ where $f$ is the joint p.f. or p.d.f.,or p.f./p.d.f. of $X$ and $Y$ ,Similarly,for each $x$ such that $f_1(x)\u0026gt;0$ and each $y$, $$ f(x,y)=g_2(y|x)f_1(x) $$\n 上面公理虽然形式上和事件的条件概率很相似，但是注意他们的区别就是事件的概率公式中每个部分都是概率，而这里都是概率分布或者概率密度函数，一个是数，一个是函数，这个区别非常明显。 上面定理我们可以发现，两个边缘分布（也就是$f_1$ 和 $f_2$） 可以存在是0的情况，这并不影响什么，定理照样生效。 再举个🌰： 还是生产零件，假设我们生产了n个零件其中合格的比例是 $P=p$ 那么X个合格产品的分布是： $$ f(x,p)=g_1(x|p)f_2(p)=\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x} \\text{ for }x=0,\\dots ,n \\text{ and } 0 \\leq p \\leq 1 $$\n我们要计算在已知条件为 $X=x$ 时 $P$ 的分布 :\n  计算边缘分布 $f_1(x)$ : $$ f_1(x)=\\int^1_0\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x}dp $$\n  带入定理中的公式可以得到： $$ g_2(p|x)=\\frac{\\begin{pmatrix}n\\x\\end{pmatrix}p^x(1-p)^{n-x}}{\\begin{pmatrix}n\\x\\end{pmatrix}\\int^1_0p^x(1-p)^{n-x}dp}\\ =\\frac{p^x(1-p)^{n-x}}{\\int^1_0p^x(1-p)^{n-x}dp} $$\n  $\\begin{pmatrix}n\\x\\end{pmatrix}$ 是个常数，说以可以提出到积分外面，又因为其不为0，所以可以消去。 至此我们已经得到了通用解，但是我们为了好玩和形象，带入两个数进去看看，比如 $n=2,x=1$ 带入就有： $$ \\int^1_0p(1-p)dp=\\frac{1}{2}-\\frac{1}{3}=\\frac{1}{6} $$\n那么\n$$ g_2(p|1)=6p(1-p) \\text{ for } 0\\leq p \\leq 1 $$\n随机变量的贝叶斯定理，全概率计算 Bayes\u0026rsquo; Theorem and Law of Total Probability for Random Varibales 我们前面在事件条件概率之后提出之后也是提出了贝叶斯公式和全概率公式，那么我们还按照这个套路提出贝叶斯定理的分布形式，全概率公式的分布形式。 我们从事件到随机变量一路走过来，套路基本相同，逻辑相似，但是我们必须进行区分，毕竟随机变量是事件的函数过程，这样的过程是会造成很多不同的。我们后面也要时刻注意，不要再用事件的方法思考问题了，因为我们已经把事件数学化了。\n Theorem Law of Total Probability for Random Variables.If $f_2(y)$ is the marginal p.f. or p.d.f. of a random variabl $Y$ and $g_1(x|y)$ is the conditional p.f. or p.d.f. of $X$ given $Y=y$,then the marginal p.f. or p.d.f. of $X$ is $$ f_1(x)=\\sum_{y} g_1(x|y)f_2(y) $$ if $y$ is discrete.If $y$ is continuous ,the marginal p.f. of p.d.f. of X given $Y=y$,then the conditional p.f. or p.d.f. of Y given $X=x$ is $$ f_1(x)=\\int^\\infty_{-\\infty}g_1(x|y)f_x(y)dy $$\n 上面的定义同样适用于Y，这里就不再详细叙述了，理论推理就是乘法原理（乘法部分）加上边缘分布的算法（积分部分），就能得到随机变量的全概率分布。\n Bayes\u0026rsquo; Theorem for Random Variables.If $f_2(y)$ is the marginal p.f. or p.d.f. of a random variable $Y$ and $g_1(x|y)$ is the conditional p.f. or p.d.f. of $X$ given $Y=y$ ,then the conditional p.f. or p.d.f. of $Y$ given $X=x$ is $$ g_2(y|x)=\\frac{g_1(x,y)f_2(y)}{f_1(x)} $$ where $f_1(x)$ can be obtained form \u0026lsquo;Theorem Law of Total Probability for Random Variables\u0026rsquo; .Similarly,the conditional p.f. or p.d.f. of X given $Y=y$ is $$ g_1(x|y)=\\frac{g_2(x|y)f_1(x)}{f_2(y)} $$ where $f_2(y)$ can be obtained form \u0026lsquo;Theorem Law of Total Probability for Random Variables\u0026rsquo; .\n 上面连续给出两发定理，都是旧瓶装新酒，但是酒还是有区别的，从数字到函数，这个就是最大的变化。下面开始举例子。 举个🌰： 假设x是 $[0,1]$ 的均匀分布的随机变量，那么我们有： $$ f_1(x)= \\begin{cases}1 \u0026amp; \\text{for } 0\u0026lt;x\u0026lt;1\\ 0 \u0026amp; \\text{otherwise} \\end{cases} $$ 随机变量y是 $[x,1]$ 区间内的均匀分布，那么我们有，当 $X=x$ 给定的时候： $$ g_2(y|x)= \\begin{cases}\\frac{1}{1-x} \u0026amp; \\text{for } x\u0026lt; y\u0026lt;1\\ 0 \u0026amp; \\text{otherwise} \\end{cases} $$\n那么根据乘法原理： $$ f(x,y)= \\begin{cases}\\frac{1}{1-x} \\times f_1(x)=\\frac{1}{1-x} \u0026amp; \\text{for } 0\u0026lt;x\u0026lt; y\u0026lt;1\\ 0\\times f_1(x)= 0 \u0026amp; \\text{otherwise} \\end{cases} $$\n那么根据边缘分布的计算方法，可以得到y的边缘分布: $$ f_2(y)=\\int^\\infty_{-\\infty} f(x,y)dx=\\int^y_0\\frac{1}{1-x}dx=-log(1-y) $$\n独立随机变量 Independent Random Varibales 和事件独立套路依旧一致，随机变量的独立并不代表对立的你死我活，也不是说相交为空的互斥，而是相关的一种状态。\n Theorem Independent Random Variables.Suppose that $X$ and $Y$ are two random variables having a joint p.f.,p.d.f.or p.f./p.d.f. $f$ Then $X$ and $Y$ are independent if and only if for every value of $y$ such that $f_2(y)\u0026gt;0$ and every value of $x$ , $$ g_1(x|y)=f_1(x) $$\n 前面我们在3.5中就提到过两个分布独立的充分必要条件就是$f(x,y)=f_1(x)f_2(y)$ 那么根据我们的乘法原理，我们知道$f(x,y)=f_2(y)g_1(x|y)$ 且 $f_2(y) \\neq 0$ 那么$g_2(y|x)=f_2(y)$ 这样就证明了only if . 如果已知 $g_2(y|x)=f_2(y)$ 根据乘法原理，$f(x,y)=f_1(x)g_2(y|x)=f_1(x)f_2(y)$ if过程证明完成。\n前面在事件的条件概率过程我们说过所有概率都是条件概率，没有条件就没有概率，只是有些条件我们不列出，或者不考虑，那么当我们要考虑某些条件的时候就出现了我们的条件概率，条件分布，但是我们具体操作者写条件概率和条件分布和普通的概率分布是一样的，因为他们都是条件的，只是有人写出来了，有人隐藏了。\n总结 这一节分两篇看来分的还是挺正确的，不然一篇就太长了，我们下一篇继续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-3-6-conditional-distributions-p2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍联合分布的构建，也就是条件分布部分的扩展和应用\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 乘法法则，贝叶斯定理，随机变量的全概率公式\u003c/p\u003e","title":"【概率论】3-6:条件分布(Conditional Distributions Part II）"},{"content":"Abstract: 本文只介绍一个功能，如何获取设备（一个或多个）信息 Keywords: CUDA Device Information\nGPU设备信息 今天跑了一天，然后晚上写了今天的代码，虽然都是printf的内容，用到的api就那么一两个，但是我还是自己打了一遍，算是深入学习一下。 我们用CUDA的时候一般有两种情况，一种自己写完自己用，使用本机或者已经确定的服务器，这时候我们只要查看说明书或者配置说明就知道用的什么型号的GPU，以及GPU的所有信息，但是如果我们写的程序是通用的程序或者框架，我们在使用CUDA前要先确定当前的硬件环境，这使得我们的程序不那么容易因为设备不同而崩溃，本文介绍两种方法，第一种适用于通用程序或者框架，第二种适合查询本机或者可登陆的服务器，并且一般不会改变，那么这时候用一条nvidia驱动提供的指令查询设备信息就很方便了。\nAPI查询GPU信息 在软件内查询信息，用到如下代码：\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; int main(int argc,char** argv) { printf(\u0026#34;%s Starting ...\\n\u0026#34;,argv[0]); int deviceCount = 0; cudaError_t error_id = cudaGetDeviceCount(\u0026amp;deviceCount); if(error_id!=cudaSuccess) { printf(\u0026#34;cudaGetDeviceCount returned %d\\n-\u0026gt;%s\\n\u0026#34;, (int)error_id,cudaGetErrorString(error_id)); printf(\u0026#34;Result = FAIL\\n\u0026#34;); exit(EXIT_FAILURE); } if(deviceCount==0) { printf(\u0026#34;There are no available device(s) that support CUDA\\n\u0026#34;); } else { printf(\u0026#34;Detected %d CUDA Capable device(s)\\n\u0026#34;,deviceCount); } int dev=0,driverVersion=0,runtimeVersion=0; cudaSetDevice(dev); cudaDeviceProp deviceProp; cudaGetDeviceProperties(\u0026amp;deviceProp,dev); printf(\u0026#34;Device %d:\\\u0026#34;%s\\\u0026#34;\\n\u0026#34;,dev,deviceProp.name); cudaDriverGetVersion(\u0026amp;driverVersion); cudaRuntimeGetVersion(\u0026amp;runtimeVersion); printf(\u0026#34; CUDA Driver Version / Runtime Version %d.%d / %d.%d\\n\u0026#34;, driverVersion/1000,(driverVersion%100)/10, runtimeVersion/1000,(runtimeVersion%100)/10); printf(\u0026#34; CUDA Capability Major/Minor version number: %d.%d\\n\u0026#34;, deviceProp.major,deviceProp.minor); printf(\u0026#34; Total amount of global memory: %.2f MBytes (%llu bytes)\\n\u0026#34;, (float)deviceProp.totalGlobalMem/pow(1024.0,3)); printf(\u0026#34; GPU Clock rate: %.0f MHz (%0.2f GHz)\\n\u0026#34;, deviceProp.clockRate*1e-3f,deviceProp.clockRate*1e-6f); printf(\u0026#34; Memory Bus width: %d-bits\\n\u0026#34;, deviceProp.memoryBusWidth); if (deviceProp.l2CacheSize) { printf(\u0026#34; L2 Cache Size: %d bytes\\n\u0026#34;, deviceProp.l2CacheSize); } printf(\u0026#34; Max Texture Dimension Size (x,y,z) 1D=(%d),2D=(%d,%d),3D=(%d,%d,%d)\\n\u0026#34;, deviceProp.maxTexture1D,deviceProp.maxTexture2D[0],deviceProp.maxTexture2D[1] ,deviceProp.maxTexture3D[0],deviceProp.maxTexture3D[1],deviceProp.maxTexture3D[2]); printf(\u0026#34; Max Layered Texture Size (dim) x layers 1D=(%d) x %d,2D=(%d,%d) x %d\\n\u0026#34;, deviceProp.maxTexture1DLayered[0],deviceProp.maxTexture1DLayered[1], deviceProp.maxTexture2DLayered[0],deviceProp.maxTexture2DLayered[1], deviceProp.maxTexture2DLayered[2]); printf(\u0026#34; Total amount of constant memory %lu bytes\\n\u0026#34;, deviceProp.totalConstMem); printf(\u0026#34; Total amount of shared memory per block: %lu bytes\\n\u0026#34;, deviceProp.sharedMemPerBlock); printf(\u0026#34; Total number of registers available per block:%d\\n\u0026#34;, deviceProp.regsPerBlock); printf(\u0026#34; Wrap size: %d\\n\u0026#34;,deviceProp.warpSize); printf(\u0026#34; Maximun number of thread per multiprocesser: %d\\n\u0026#34;, deviceProp.maxThreadsPerMultiProcessor); printf(\u0026#34; Maximun number of thread per block: %d\\n\u0026#34;, deviceProp.maxThreadsPerBlock); printf(\u0026#34; Maximun size of each dimension of a block: %d x %d x %d\\n\u0026#34;, deviceProp.maxThreadsDim[0],deviceProp.maxThreadsDim[1],deviceProp.maxThreadsDim[2]); printf(\u0026#34; Maximun size of each dimension of a grid: %d x %d x %d\\n\u0026#34;, deviceProp.maxGridSize[0], deviceProp.maxGridSize[1], deviceProp.maxGridSize[2]); printf(\u0026#34; Maximu memory pitch %lu bytes\\n\u0026#34;,deviceProp.memPitch); exit(EXIT_SUCCESS); } 主要用到了下面API，了解API的功能最好不要看博客，因为博客不会与时俱进，要查文档，所以我这里不挨个解释用法，对于API的不了解，解决办法：查文档，查文档，查文档！\ncudaSetDevice cudaGetDeviceProperties cudaDriverGetVersion cudaRuntimeGetVersion cudaGetDeviceCount 运行的效果如下：\n这里面很多参数是我们后面要介绍的，而且每一个都对性能有影响：\n CUDA驱动版本 设备计算能力编号 全局内存大小（1.95G,原文有错误，写成MBytes了） GPU主频 GPU带宽 L2缓存大小 纹理维度最大值，不同维度下的 层叠纹理维度最大值 常量内存大小 块内共享内存大小 块内寄存器大小 线程束大小 每个处理器硬件处理的最大线程数 每个块处理的最大线程数 块的最大尺寸 网格的最大尺寸 最大连续线性内存  上面这些都是后面要用到的关键参数，这些会严重影响我们的效率。后面会一一说到，不同的设备参数要按照不同的参数来使得程序效率最大化，所以我们必须在程序运行前得到所有我们关心的参数。\nNVIDIA-SMI nvidia-smi是nvidia驱动程序内带的一个工具，可以返回当前环境的设备信息： 这个命令可以加各种参数，当然参数你要查文档查文档查文档：\n利用下面这个参数可以精简上面那么一大堆的信息，而这些可以在脚本中帮我们得到设备信息，比如我们可以写通用程序时在编译前执行脚本来获取设备信息，然后在编译时固化最优参数，这样程序运行时就不会被查询设备信息的过程浪费资源。 也就是我们可以用一下两种方式编写通用程序：\n 运行时获取设备信息：  编译程序 启动程序 查询信息，将信息保存到全局变量 功能函数通过全局变量判断当前设备信息，优化参数 程序运行完毕   编译时获取设备信息  脚本获取设备信息 编译程序，根据设备信息调整固化参数到二进制机器码 运行程序 程序运行完毕    详细信息使用\nnvidia-smi -q -i 可以得到如下信息，过于详细，不贴图了：\n==============NVSMI LOG============== Timestamp : Sun Mar 11 00:01:39 2018 Driver Version : 384.111 Attached GPUs : 1 GPU 00000000:01:00.0 Product Name : GeForce GTX 1050 Ti Product Brand : GeForce Display Mode : Disabled Display Active : Disabled Persistence Mode : Disabled Accounting Mode : Disabled Accounting Mode Buffer Size : 1920 Driver Model Current : N/A Pending : N/A Serial Number : N/A GPU UUID : GPU-9d4a4647-c82e-6302-bc62-b0a23e916877 Minor Number : 0 VBIOS Version : 86.07.3A.00.27 MultiGPU Board : No Board ID : 0x100 GPU Part Number : N/A Inforom Version Image Version : N/A OEM Object : N/A ECC Object : N/A Power Management Object : N/A GPU Operation Mode Current : N/A Pending : N/A GPU Virtualization Mode Virtualization mode : None PCI Bus : 0x01 Device : 0x00 Domain : 0x0000 Device Id : 0x1C8C10DE Bus Id : 00000000:01:00.0 Sub System Id : 0x39D017AA GPU Link Info PCIe Generation Max : 3 Current : 1 Link Width Max : 16x Current : 8x Bridge Chip Type : N/A Firmware : N/A Replays since reset : 0 Tx Throughput : 0 KB/s Rx Throughput : 0 KB/s Fan Speed : N/A Performance State : P8 Clocks Throttle Reasons Idle : Active Applications Clocks Setting : Not Active SW Power Cap : Not Active HW Slowdown : Not Active Sync Boost : Not Active SW Thermal Slowdown : Not Active FB Memory Usage Total : 2001 MiB Used : 243 MiB Free : 1758 MiB BAR1 Memory Usage Total : 256 MiB Used : 2 MiB Free : 254 MiB Compute Mode : Default Utilization Gpu : 0 % Memory : 0 % Encoder : 0 % Decoder : 0 % Encoder Stats Active Sessions : 0 Average FPS : 0 Average Latency : 0 Ecc Mode Current : N/A Pending : N/A ECC Errors Volatile Single Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Double Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Aggregate Single Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Double Bit Device Memory : N/A Register File : N/A L1 Cache : N/A L2 Cache : N/A Texture Memory : N/A Texture Shared : N/A CBU : N/A Total : N/A Retired Pages Single Bit ECC : N/A Double Bit ECC : N/A Pending : N/A Temperature GPU Current Temp : 28 C GPU Shutdown Temp : 102 C GPU Slowdown Temp : 97 C GPU Max Operating Temp : 94 C Memory Current Temp : N/A Memory Max Operating Temp : N/A Power Readings Power Management : N/A Power Draw : N/A Power Limit : N/A Default Power Limit : N/A Enforced Power Limit : N/A Min Power Limit : N/A Max Power Limit : N/A Clocks Graphics : 139 MHz SM : 139 MHz Memory : 405 MHz Video : 544 MHz Applications Clocks Graphics : N/A Memory : N/A Default Applications Clocks Graphics : N/A Memory : N/A Max Clocks Graphics : 1911 MHz SM : 1911 MHz Memory : 3504 MHz Video : 1708 MHz Max Customer Boost Clocks Graphics : N/A Clock Policy Auto Boost : N/A Auto Boost Default : N/A Processes Process ID : 970 Type : G Name : /usr/lib/xorg/Xorg Used GPU Memory : 182 MiB Process ID : 1398 Type : G Name : compiz Used GPU Memory : 59 MiB 下面这些nvidia-smi -q -i 0 的参数可以提取我们要的信息(这样我们就不需要用正则表达式了，😆)\n MEMORY UTILIZATION ECC TEMPERATURE POWER CLOCK COMPUTE PIDS PERFORMANCE SUPPORTED_CLOCKS PAGE_RETIREMENT ACCOUNTING  比如我们想得到内存信息：\nnvidia-smi -q -i 0 -d MEMORY 得到如下：\n多设备时，我们只要把上面的0改成对应的设备号就好了。\n总结 今天没有理论行的东西，都是技术层面的，没存，技术问题最好的解决方法就是查文档，而原理部分就要看书看教程了，至此CUDA的编程模型大概就是这些了，核函数，计时，内存，线程，设备参数，这些足够能写出比CPU块很多的程序了，但是追求更快的我们要深入研究每一个细节，从下一篇开始，我们深入硬件，研究背后的秘密 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-2-4-%E8%AE%BE%E5%A4%87%E4%BF%A1%E6%81%AF.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文只介绍一个功能，如何获取设备（一个或多个）信息\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA Device Information\u003c/p\u003e","title":"【CUDA 基础】2.4 GPU设备信息"},{"content":"Abstract: 本文介绍CUDA模型中的线程组织模式 Keywords: Thread,Block,Grid\n组织并行线程 2.0 CUDA编程模型中我们大概的介绍了CUDA编程的几个关键点，包括内存，kernel，以及今天我们要讲的线程组织形式，2.0中还介绍了每个线程的编号是依靠，块的坐标（blockIdx.x等），网格的大小（gridDim.x 等），线程编号（threadIdx.x等），线程的大小（tblockDim.x等） 这一篇我们就详细介绍每一个线程是怎么确定唯一的索引，然后建立并行计算，并且不同的线程组织形式是怎样影响性能的：\n 二维网格二维线程块 一维网格一维线程块 二维网格一维线程块  使用块和线程建立矩阵索引 多线程的优点就是每个线程处理不同的数据计算，那么怎么分配好每个线程处理不同的数据，而不至于多个不同的线程处理同一个数据，或者避免不同的线程没有组织的乱访问内存。如果多线程不能按照组织合理的干活，那么就相当于一群没训练过的哈士奇拉雪橇，往不同的方向跑，那么是没办法前进的，必须有组织，有规则的计算才有意义。 我们的线程模型前面2.0中已经有个大概的介绍，但是下图可以非常形象的反应线程模型，不过注意硬件实际的执行和存储不是按照图中的模型来的，大家注意区分： 这里(ix,iy)就是整个线程模型中任意一个线程的索引，或者叫做全局地址，局部地址当然就是(threadIdx.x,threadIdx.y)了，当然这个局部地址目前还没有什么用处，他只能索引线程块内的线程，不同线程块中有相同的局部索引值，比如同一个小区，A栋有16楼，B栋也有16楼，A栋和B栋就是blockIdx，而16就是threadIdx啦 图中的横坐标就是： $$ ix=threadIdx.x+blockIdx.x \\times blockDim.x $$ 纵坐标是： $$ iy=threadIdx.y+blockIdx.y \\times blockDim.y $$ 这样我们就得到了每个线程的唯一标号，并且在运行时kernel是可以访问这个标号的。前面讲过CUDA每一个线程执行相同的代码，也就是异构计算中说的多线程单指令，如果每个不同的线程执行同样的代码，又处理同一组数据，将会得到多个相同的结果，显然这是没意义的，为了让不同线程处理不同的数据，CUDA常用的做法是让不同的线程对应不同的数据，也就是用线程的全局标号对应不同组的数据。 设备内存或者主机内存都是线性存在的，比如一个二维矩阵 $(8\\times 6)$，存储在内存中是这样的： 我们要做管理的就是：\n 线程和块索引（来计算线程的全局索引） 矩阵中给定点的坐标（ix,iy） (ix,iy)对应的线性内存的位置  线性位置的计算方法是： $$ idx=ix+iy*nx $$ 我们上面已经计算出了线程的全局坐标，用线程的全局坐标对应矩阵的坐标，也就是说，线程的坐标(ix,iy)对应矩阵中(ix,iy)的元素，这样就形成了一一对应，不同的线程处理矩阵中不同的数据，举个具体的例子，ix=10,iy=10的线程去处理矩阵中(10,10)的数据，当然你也可以设计别的对应模式，但是这种方法是最简单出错可能最低的。 我们接下来的代码来输出每个线程的标号信息：\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; __global__ void printThreadIndex(float *A,const int nx,const int ny) { int ix=threadIdx.x+blockIdx.x*blockDim.x; int iy=threadIdx.y+blockIdx.y*blockDim.y; unsigned int idx=iy*nx+ix; printf(\u0026#34;thread_id(%d,%d) block_id(%d,%d) coordinate(%d,%d)\u0026#34; \u0026#34;global index %2d ival %2d\\n\u0026#34;,threadIdx.x,threadIdx.y, blockIdx.x,blockIdx.y,ix,iy,idx,A[idx]); } int main(int argc,char** argv) { initDevice(0); int nx=8,ny=6; int nxy=nx*ny; int nBytes=nxy*sizeof(float); //Malloc  float* A_host=(float*)malloc(nBytes); initialData(A_host,nxy); printMatrix(A_host,nx,ny); //cudaMalloc  float *A_dev=NULL; CHECK(cudaMalloc((void**)\u0026amp;A_dev,nBytes)); cudaMemcpy(A_dev,A_host,nBytes,cudaMemcpyHostToDevice); dim3 block(4,2); dim3 grid((nx-1)/block.x+1,(ny-1)/block.y+1); printThreadIndex\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(A_dev,nx,ny); CHECK(cudaDeviceSynchronize()); cudaFree(A_dev); free(A_host); cudaDeviceReset(); return 0; } 这段代码输出了一组我们随机生成的矩阵，并且核函数打印自己的线程标号，注意，核函数能调用printf这个特性是CUDA后来加的，最早的版本里面不能printf，输出结果： 由于截图不完全，上面有一段打印信息没贴全，但是我们可以知道每一个线程已经对应到了不同的数据，接着我们就要用这个方法来进行计算了，最简单的当然就是二维矩阵加法啦。\n二维矩阵加法 我们利用上面的线程与数据的对应完成了下面的核函数：\n__global__ void sumMatrix(float * MatA,float * MatB,float * MatC,int nx,int ny) { int ix=threadIdx.x+blockDim.x*blockIdx.x; int iy=threadIdx.y+blockDim.y*blockIdx.y; int idx=ix+iy*ny; if (ix\u0026lt;nx \u0026amp;\u0026amp; iy\u0026lt;ny) { MatC[idx]=MatA[idx]+MatB[idx]; } } 下面我们调整不同的线程组织形式，测试一下不同的效率并保证得到正确的结果，但是什么时候得到最好的效率是后面要考虑的，我们要做的就是用各种不同的相乘组织形式得到正确结果.\n二维网格和二维块 首先来看二维网格二维模块的代码：\n// 2d block and 2d grid dim3 block_0(dimx,dimy); dim3 grid_0((nx-1)/block_0.x+1,(ny-1)/block_0.y+1); iStart=cpuSecond(); sumMatrix\u0026lt;\u0026lt;\u0026lt;grid_0,block_0\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,C_dev,nx,ny); CHECK(cudaDeviceSynchronize()); iElaps=cpuSecond()-iStart; printf(\u0026#34;GPU Execution configuration\u0026lt;\u0026lt;\u0026lt;(%d,%d),(%d,%d)\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec\\n\u0026#34;, grid_0.x,grid_0.y,block_0.x,block_0.y,iElaps); CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost)); checkResult(C_host,C_from_gpu,nxy); 运行结果： 红色框内是运行结果，用cpu写一个矩阵计算，然后比对结果，发现我们的运算结果是正确的，用时0.002152秒。\n一维网格和一维块 接着我们使用一维网格一维块：\n// 1d block and 1d grid dimx=32; dim3 block_1(dimx); dim3 grid_1((nxy-1)/block_1.x+1); iStart=cpuSecond(); sumMatrix\u0026lt;\u0026lt;\u0026lt;grid_1,block_1\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,C_dev,nx*ny ,1); CHECK(cudaDeviceSynchronize()); iElaps=cpuSecond()-iStart; printf(\u0026#34;GPU Execution configuration\u0026lt;\u0026lt;\u0026lt;(%d,%d),(%d,%d)\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec\\n\u0026#34;, grid_1.x,grid_1.y,block_1.x,block_1.y,iElaps); CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost)); checkResult(C_host,C_from_gpu,nxy); 运行结果： 同样运行结果是正确的。\n二维网格和一维块 二维网格一维块：\n// 2d block and 1d grid dimx=32; dim3 block_2(dimx); dim3 grid_2((nx-1)/block_2.x+1,ny); iStart=cpuSecond(); sumMatrix\u0026lt;\u0026lt;\u0026lt;grid_2,block_2\u0026gt;\u0026gt;\u0026gt;(A_dev,B_dev,C_dev,nx,ny); CHECK(cudaDeviceSynchronize()); iElaps=cpuSecond()-iStart; printf(\u0026#34;GPU Execution configuration\u0026lt;\u0026lt;\u0026lt;(%d,%d),(%d,%d)\u0026gt;\u0026gt;\u0026gt; Time elapsed %f sec\\n\u0026#34;, grid_2.x,grid_2.y,block_2.x,block_2.y,iElaps); CHECK(cudaMemcpy(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost)); checkResult(C_host,C_from_gpu,nxy); 运行结果： 总结 用不同的线程组织形式会得到正确结果，但是效率有所区别：\n   线程配置 执行时间     CPU单线程 0.060022   (128,128),(32,32) 0.002152   (524288,1),(32,1) 0.002965   (128,4096),(32,1) 0.002965    观察结果没有多大差距，但是明显比CPU快了很多，而且最主要的是我们本文用不同的线程组织模式都得到了正确结果，并且：\n 改变执行配置（线程组织）能得到不同的性能 传统的核函数可能不能得到最好的效果 一个给定的核函数，通过调整网格和线程块大小可以得到更好的效果  第三章的执行模型，我们才会深入到硬件层面，追寻影响效率的根本原因。 代码库中有完整代码：https://github.com/Tony-Tan/CUDA_Freshman\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-2-3-%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA模型中的线程组织模式\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Thread,Block,Grid\u003c/p\u003e","title":"【CUDA 基础】2.3 组织并行线程"},{"content":"Abstract: 本文介绍CUDA核函数计时方法 Keywords: gettimeofday,nvprof\n给核函数计时 继续更新CUDA，同时概率和数学分析也在更新，欢迎大家访问www.face2ai.com 昨天晚上开始折腾ubuntu，上一篇用腾讯云搭建服务器来调试CUDA，现在有机器了，所以装个ubuntu准备调试cuda，但是出现了下面的纠结问题，搞了将近五个多小时，才解决，首先我的笔记本是联想R720 1050Ti的显卡，安装ubuntu 16.04 发现源中的驱动安装好后，安装CUDA 9.1 local版本出现问题，没办法安装成功，以为是驱动问题，安装新的驱动也不行，于是想起来之前用的是17.04，打开镜像网站发现17.04已经不再支持了，找了old版本中，找到下载安装，发现没有源可以用，放弃，安装17.10，开机就出错，于是又退回16.04，安装自带的驱动，安装了cuda 9.0 run版，成功了，安装cmake，ssh-server，于是我们成功了： 编程模型中我们介绍了内存，线程相关的知识，接着我们启动了我们的核函数，这些只是大概的勾勒出CUDA编程的外貌，通过前几篇可以写出一般的可运行程序，但是想获得最高的效率，需要反复的优化，以及对硬件和编程细节的详细了解，怎么评估效率，时间是个很直观的测量方式。\n用CPU计时 使用cpu计时的方法是测试时间的一个常用办法，我记得很有趣的一件事时，我们在写C程序的时候最多使用的计时方法是：\nclock_t start, finish; start = clock(); // 要测试的部分 finish = clock(); duration = (double)(finish - start) / CLOCKS_PER_SEC; 其中clock()是个关键的函数，“clock函数测出来的时间为进程运行时间，单位为滴答数(ticks)”；字面上理解CLOCKS_PER_SEC这个宏，就是没秒中多少clocks，在不同的系统中值可能不同。必须注意的是，并行程序这种计时方式有严重问题！如果想知道具体原因，可以查询clock的源代码（c语言标准函数） 这里我们使用gettimeofday() 函数\n#include \u0026lt;sys/time.h\u0026gt;double cpuSecond() { struct timeval tp; gettimeofday(\u0026amp;tp,NULL); return((double)tp.tv_sec+(double)tp.tv_usec*1e-6); } gettimeofday是linux下的一个库函数，创建一个cpu计时器，从1970年1月1日0点以来到现在的秒数，需要头文件sys/time.h 那么我们使用这个函数测试核函数运行时间： 我把代码部分贴出来，完整的访问代码库：https://github.com/Tony-Tan/CUDA_Freshman\n#include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; __global__ void sumArraysGPU(float*a,float*b,float*res,int N) { int i=blockIdx.x*blockDim.x+threadIdx.x; if(i \u0026lt; N) res[i]=a[i]+b[i]; } int main(int argc,char **argv) { // set up device.....  // init data ......  //timer  double iStart,iElaps; iStart=cpuSecond(); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d,nElem); cudaDeviceSynchronize(); iElaps=cpuSecond()-iStart; // ...... } 主要分析计时这段，首先iStart是cpuSecond返回一个秒数，接着执行核函数，核函数开始执行后马上返回主机线程，所以我们必须要加一个同步函数等待核函数执行完毕，如果不加这个同步函数，那么测试的时间是从调用核函数，到核函数返回给主机线程的时间段，而不是核函数的执行时间，加上了\ncudaDeviceSynchronize(); 函数后，计时是从调用核函数开始，到核函数执行完并返回给主机的时间段，下面图大致描述了执行过程的不同时间节点： 我们可以大概分析下核函数启动到结束的过程：\n 主机线程启动核函数 核函数启动成功 控制返回主机线程 核函数执行完成 主机同步函数侦测到核函数执行完  我们要测试的是2~4的时间，但是用CPU计时方法，只能测试1~5的时间，所以测试得到的时间偏长。 接着我们调整下我们的参数，来看看不同线程维度对速度的影响，看看计时能不能反映出来点问题，这里我们考虑一维线程模型\n 2的幂次数据量 1\u0026laquo;24，16兆数据：  每个块256个线程  每个块512个线程  每个块1024个线程    2的非幂次数据量 (1\u0026laquo;24)+1，16兆加一个数据：  每个块256个线程  每个块512个线程  每个块1024个线程     对于我这个cpu这三个参数的性能差距比较小，但是需要注意的是当数据不能被完整切块的时候性能滑铁卢了，这个我们可以使用一点小技巧，比如只传输可完整切割数据块，然后剩下的1，2个使用cpu计算，这种技巧后面有介绍，以及包括如何选择系数。我们本篇之关系计时函数的工作状态，目前看起来还不错。\n用nvprof计时 CUDA 5.0后有一个工具叫做nvprof的命令行分析工具，后面还要介绍一个图形化的工具，现在我们来学习一下nvprof，学习工具主要技巧是学习工具的功能，当你掌握了一个工具的全部功能，那就是学习成功了。 nvprof的用法如下：\n$ nvprof [nvprof_args] \u0026lt;application\u0026gt;[application_args] 于是我们执行命令得到\n出现错误：\n======== Error: unified memory profiling failed. 原因是权限问题，因为安全原因，Mac os和Linux当你调试程序时，一个程序（比如IDE）要接入别的进程（被调试进程），这时候需要权限保证安全，否则一些坏程序会肆意干扰别的程序，出现问题，所以操作系统不允许线程间任意通信。 解决办法是加上sudo，但是还是有问题，超级用户的环境变量里没有nvprof： 如果我们使用完整的nvprof路径加上sudo执行 工具不仅给出了kernel执行的时间，比例，还有其他cuda函数的执行时间，可以看出核函数执行时间只有4%左右，其他内存分配，内存拷贝占了大部分事件，nvprof给出的核函数执行时间2.1011ms，上面cpuSecond计时结果是2.282ms 可见，nvprof可能更接近真实值。 nvprof这个强大的工具给了我们优化的目标，分析数据可以得出我们重点工作要集中在哪部分。\n理论界限最大化 得到了实际操作值，我们需要知道的是我们能优化的极限值是多少，也就是机器的理论计算极限，这个极限我们永远也达不到，但是我们必须明确的知道，比如理论极限是2秒，我们已经从10秒优化到2.01秒了，基本就没有必要再继续花大量时间优化速度了，而应该考虑买更多的机器或者更新的设备。 各个设备的理论极限可以通过其芯片说明计算得到，比如说：\n Tesla K10 单精度峰值浮点数计算次数：745MHz核心频率 x 2GPU/芯片 x（8个多处理器 x 192个浮点计算单元 x 32 核心/多处理器） x 2 OPS/周期 =4.58 TFLOPS Tesla K10 内存带宽峰值： 2GPU/芯片 x 256 位 x 2500 MHz内存时钟 x 2 DDR/8位/字节 = 320 GB/s 指令比：字节 4.58 TFLOPS/320 GB/s =13.6 个指令： 1个字节  总结 本文我们简单介绍了CUDA核函数的计时方法，以及如何评估理论时间下届，也就是效率的极限值，了解性能瓶颈和性能极限，是优化性能的第一步。 转载请标明来源www.face2ai.com\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-2-2-%E6%A0%B8%E5%87%BD%E6%95%B0%E8%AE%A1%E6%97%B6.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA核函数计时方法\n\u003cstrong\u003eKeywords:\u003c/strong\u003e gettimeofday,nvprof\u003c/p\u003e","title":"【CUDA 基础】2.2 给核函数计时"},{"content":"Abstract: 首先介绍随机变量的条件分布，随后介绍随机变量条件分布下的乘法法则，贝叶斯公式和全概率公式 Keywords: Discrete Conditional Distributions，Continuous Conditional Distributions，Multiplication Rule，Bayes\u0026rsquo; Therom，Law of Total Probability\n条件分布 隔了半个月没有研究数学，早上起来还激动不已，看了两页书好几脸懵x\n “如何变成一个真正的行家”\n  花几年时间进行紧张的学习，直到你觉得自己在行 离开几年，去探索更多其他的领域，无论是否有关 回到原来的领域，换个角度，重新掌握它 也许这与传统观点相悖，但你可以不用练习就升级，有时候这是升级的唯一办法  上面这段话引自Francois Chollet\n这段话不适合小菜，只适合大牛们冲击天花板，我们小菜现在要做的就是第一步，花几年时间进行紧张的学习。 简短的回顾下前面的内容，我们从试验出发，然后得到事件，从事件引出对应的概率，然后把事件数字化后，随机变量作为一个函数成为我们研究的对象，在研究事件的时候我们研究了形如 $Pr(A|B)$ 的事件的条件概率，并且把它用到了全概率公式，贝叶斯公式等，并了解到其性质和普通事件的概率一致，甚至所有事件都可以定义为条件事件，条件概率从一开始就注定成为我们研究的重要一部分，所以当事件数字化之后，条件分布也就成了研究的重点，没错，我们今天这一大篇都是研究条件分布的，目前之研究两个随机变量的条件分布，多变量的可以依靠两个变量的推导出来。 在联合分布中的条件分布，上一篇的边缘分布也是我们要使用到的，所以上一篇的内容需要大家详细掌握。\n离散条件分布 Discrete Conditional Distributions 上来先举个🌰 ： 保险公司想要研究哪种型号的车更容易被盗，研究出了下面这个表的数据： 表中1表示被盗，0表示没有被盗，Y表示车型，保险公司会根据不同的车型设定保险金（奸商都特别会算账，不然会赔到死），如果我们不知道你是什么车，从表上我们只能根据X的边缘密度 $Pr(x=1)=\\sum_{y}Pr(x=1,y)=0.024$ 来估计你的车被盗的风险，但是如果你要是告诉我，你的车型是3，那么你被盗 的可能性就是 $Pr(x=1,y=3)=0.001$ 了\n所以，当一个联合分布中，我们知道一个随机变量x发生了，另一个随机变量y发生的概率从原来的$Pr(y)$ 变成了 $Pr(y|x)$ 而从相对关系上来看满足下面的关系： $$ Pr(X=x|Y=y)=\\frac{Pr(X=x \\text{ and } Y=y)}{Pr(Y=y)}=\\frac{f(x,y)}{f_2(y)} $$\n所以我们就能引出定义：\n Definition Conditional Distribution/p.f. Let X and Y have a discrete joint distribution eith joint p.f. $f$ .Let $f_2$ denote the marginal p.f. of Y Fot each y such that $f_2(y)\u0026gt;0$ ,define: $$ g_1(x|y)=\\frac{f(x,y)}{f_2(y)} $$ Then $g_1$ is called the conditional p.f. of X given Y.The discrete distribution whose p.f. is $g_1(\\cdot |y)$ is called the conditional distribution of $X$ given that $Y=y$\n 定义大概就是上面的样子了，但是我们需要确认一下 $g_1(x|y)$ 这货到底是不是个分布，证明如下，假设 $f_2(y)\u0026gt;0$ ,$g_1(x|y)\u0026gt;0$ 那么对于所有 $x$ 来说: $$ \\sum_x g_1(x|y)=\\frac{1}{f_2(y)}\\sum_xf(x,y)=\\frac{1}{f_2(y)}f_2(y)=1 $$\n一个随机变量的概率分布必须满足所有值都大于0 ，$g_1(x|y)\u0026gt;0$ 满足条件，并且所有可能的概率和是1，上面式子也证明了，所以 $g_1$ 是一个概率分布，Q.E.D\n举个计算的🌰 ： 根据上面的数据计算p.f. of $Y$ given $X=2$ $$ g_2(y|2)=\\frac{f(2,y)}{f_1(x=2)}=\\frac{f(2,y)}{0.6} $$ 因为本🌰是个离散有限的，可以很容易的求出所有情况下的值：$g_2(1|2)=1/2$ $g_2(2|2)=0$ $g_2(3|2)=1/6$ $g_2(4|2)=1/3$\n注意当边缘分布中对应的是0的情况，也就是分母是0的情况，是没有意义的，为什么？首先我们可以从代数的角度理解，分母不能为零，其次，我们从概率的角度理解，不可能发生的事件或随机变量概率是0，如果这个事件发生了，那么他就不可能有概率0，所以前后矛盾，分母不能为0。\n连续条件分布 Continuous Conditional Distributions 上面说明白了离散情况下的条件分布，用到了前一篇中的边缘分布，那么连续情况下的条件分布会是什么样呢？ 还是先举个🌰 ：\n 一个工序需要两步完成，第一阶段需要Y分钟，整个过程需要X分钟（包括前面的Y分钟），假设X和Y满足下面的连续分布，joint p.d.f.如下：\n $$ f(x,y)= \\begin{cases} e^{-x}\u0026amp;\\text{ for }0\\leq y\\leq x\u0026lt;\\infty \u0026amp;\\ 0 \u0026amp;\\text{otherwise}\u0026amp; \\end{cases} $$\n当我们知道Y用了多久以后，我们就能重新评估X的分布，换句话说，当我们得知 $Y=y$发生时 求 $g_1(x|Y=y)$ 的分布\n Definition 3.6.2 p.d.f. :Let $X$ and $Y$ have a continuous joint distribution with joint p.d.f. $f$ and respective marginals $f_1$ and $f_2$ .Let $y$ be a value such that $f_2(y)\u0026gt;0$ .Then the conditional p.d.f. $g_1$ of $X$ given that $Y=y$ is defined as follows: $$ g_1(x|y)=\\frac{f(x,y)}{f_2(y)}\\text{ for }-\\infty\u0026lt;x\u0026lt;\\infty $$ For values of y such that $f_2(y)=0$ ,we are free to define $g_1(x|y)$ however we wish ,so long as $g_1(x|y)$ is a p.d.f. as a function of $x$\n 上面就是关于连续随机变量的条件pdf的定义，与离散情况下的条件p.f.的定义非常相似，但是需要注意的是，一个是p.d.f.一个是p.f. 这个就是本质的区别\n Theorem: For each $y$ , $g_1(x|y)$ defined in Definition 3.6.2 is a p.d.f. as a function of $x$\n 这个定理要证明的也是，经过我们一些列计算，得到的新的函数，是否满足p.d.f.的要求，证明如下：\n $f_2(y)=0$ 分母是0，没有计算意义 $f_2(y)\u0026gt;0$ 明显有$g_1(x|y)\\geq0$ if $f_2(y)\u0026gt;0$ $$ \\int^{\\infty}{-\\infty}g_1(x|y)dx=\\frac{\\int^{\\infty}{-\\infty}f(x,y)dx}{f_2(y)}=\\frac{f_2(y)}{f_2(y)}=1 $$ Q.E.D  定理为了确定我们一系列计算得到仍然是p.d.f，证明了三点性质（其实是2点），保证函数满足p.d.f.的基本需求。参考，p.d.f.的定义\n继续上面关于工序的例子,我们来计算条件分布，当已知 $Y=y$ 的时候我们知道 $x\\geq y$所以计算边缘分布： $$ f_2(y)=\\int^\\infty_{y}e^{-x}dx=e^{-y} $$ 对于所有的 $y\u0026gt;0$ : $$ g_1(x|y)=\\frac{f(x,y)}{f_2(y)}=\\frac{e^{-x}}{e^{-y}}=e^{y-x},\\text{ for }x\\geq y $$ 当 $x\u0026lt;y$ 的时候 $g_1(x|y)=0$\n这个例子暂时告一段落，我们下面展示一张图来可视化一下连续随机变量的条件分布，因为离散情况下很容易想象，所以我们把连续的随机变量表示一下： 看图说话，完整的曲面是二维随机变量的联合p.d.f. 那么其中一个切片 $x=x_0$ 或者 $y=y_0$ 都能得到一个切片，这个切片即使条件分布的一个伸缩，为什么是伸缩，因为其积分不是1，为了让他的积分为1，或者说正规化，我们需要给他一个系数：$\\frac{1}{f_1(x)}$ 或者 $\\frac{1}{f_2(y)}$ ，这样就能保证其积分为 $\\int^\\infty_{-\\infty} \\frac{f(x_0,y)}{f_1(x_0)}=1$ 或者 $\\int^\\infty_{-\\infty} \\frac{f(x,y_0)}{f_2(y_0)}=1$\n一点需要注意，我们说过，p.d.f和pf的区别在于，p.d.f.的单点对应的函数值没有意义，其区间内的积分才能反映区间的概率，那么上述式子中 $f(x_0,y)$ 是0肯定没错了因为这个单变量函数没有体积，如果你还不明白，就看上面的图，并且确定二维随机变量只有一块区域内的体积才有意义，那么$x=x_0$ 确定的平面，不管怎么算体积都是0，也就是对应的概率是0. 那么实际上严谨的连续条件分布的定义应该是这样的： $$ g_1(x|y)=lim_{\\epsilon \\to 0}\\frac{\\partial}{\\partial x}Pr(X\\leq x|y-\\epsilon \u0026lt; Y \\leq y+ \\epsilon) $$ 是这样的一个极限，首先用到的是c.d.f.到p.d.f.的求偏导，然后是给了y一个小区间，使得积分有意义。\n剩下的就是混合分布了，一个连续随机变量一个离散随机变量，做法也很简单，各算各的，互不干扰\n Definition Conditional p.f. or p.d.f. from Mixed Distribution: Let $X$ discrete and let $Y$ be continuous with joint p.f./p.d.f. f.Then the conditional p.f. of $X$ given $Y=y$ is defined by Eq.(3.6.2) and the conditional p.d.f of $Y$ given $X=x$ is defined by Eq.(3.6.3)\n 上面说的3.6.2 就是离散条件分布定义中的计算公式，同样的3.6.3就是连续条件分布定义中的计算公式。\n总结 本文较前几篇有点短小，但是介绍的东西确实重要很多，Part II 介绍条件概率分布的构成，欢迎大家继续收看。\n","permalink":"https://go.face2ai.com/math/math-probability-3-6-conditional-distributions-p1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 首先介绍随机变量的条件分布，随后介绍随机变量条件分布下的乘法法则，贝叶斯公式和全概率公式\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Discrete Conditional Distributions，Continuous Conditional Distributions，Multiplication Rule，Bayes\u0026rsquo; Therom，Law of Total Probability\u003c/p\u003e","title":"【概率论】3-6:条件分布(Conditional Distributions Part I）"},{"content":"Abstract: 介绍三种网页抓取的方法 Keywords: 正则表达式,Beautiful Soup,Lxml\n数据抓取 本文创建于哈尔滨，完成于深圳，也算是边走边工作了，刚做了下新年计划，这个软件还不错，需要付费，但是能实时提醒下自己，安排下计划也不错 今年还是要忍住外界各种压力，把该打好的基础做好，然后恢复下身体，争取多走出人生的几步，去年前年都走了不少步，有对有错，希望今年也走几步（没病走两步~~） 还是原来那句：我不是爬虫专家或者前端后台专家，我的努力方向也不是这个方向，我只是想要运用这套技术，但是我又希望对整个过程有一个比较详细的了解，所以我在本系列只是简单介绍，有些东西可能含糊不清，需要详细学习的同学可以去查询相关资料\n分析网页 前面讲的三只虫已经讲解了如何把静态的网页下载回本地，前两篇HTTP讲了下背后的理论基础，就是向服务器发送请求，得到相应的过程，按照Introduction中介绍的处理过程\n我们在得到内容后就要进行分析内容了，也就是抓取感兴趣的信息，可以是数据，比如文章，图片，也可以是链接，那么就是下一步要访问的页面了。 分析网页的方法是使用浏览器自带的工具，比如Safari和Chrome的右键都能显示源代码（检查元素）之类的功能 这个源代码（html）就是我们上面用虫子下载到的，而接下来，就是要提取里面的信息了，想要啥，就拿啥。\n抓取方法 正则表达式 正则表达式，是比较好使用，但是通用性不强的一种方法，其实也不是很好用，因为你要首先掌握正则表达式的语法，而掌握了正则表达式的语法以后，你会发现，处理字符串类的文件会变得极其简单，高效，但是正则表达式本身就是个难点，会正则表达式的人也不是那么特别的多，比如我们看下面这个例子，还是上文我们讲的最后一只虫子：\n## -*- coding:utf-8 -*- import urllib2 import re import urlparse import datetime import time class Throttle: \u0026#34;\u0026#34;\u0026#34; 限速 \u0026#34;\u0026#34;\u0026#34; def __init__(self,delay): self.delay=delay self.domains={} def wait(self,url): domain=urlparse.urlparse(url).netloc last_accessed=self.domains.get(domain) if self.delay\u0026gt;0 and last_accessed is not None: sleep_secs=self.delay-(datetime.datetime.now()- last_accessed).seconds if sleep_secs\u0026gt;0: time.sleep(sleep_secs) print \u0026#39;sleep %dsec\u0026#39;%sleep_secs self.domains[domain]=datetime.datetime.now() def get_links(html): print html webpage_regex =re.compile(\u0026#39;\u0026lt;a[^\u0026gt;]+href=[\u0026#34;\\\u0026#39;](.*?)[\u0026#34;\\\u0026#39;]\u0026#39;,re.IGNORECASE) return webpage_regex.findall(html) def link_crawler(seed_url,link_regex,delay=1): throttle=Throttle(delay) crawl_queue=[seed_url] seen=set(crawl_queue) while crawl_queue: url=crawl_queue.pop() throttle.wait(url) html=download(url) for link in get_links(html): if re.search(link_regex,link): link=urlparse.urljoin(seed_url,link) if link not in seen: seen.add(link) crawl_queue.append(link) def download(url,user_agent=\u0026#39;tony\u0026#39;,proxy=None,num_retries=2): print \u0026#39;Downloading:\u0026#39;, url headers = {\u0026#39;User-agent\u0026#39;: user_agent} request=urllib2.Request(url,headers=headers) opener=urllib2.build_opener() if proxy: proxy_params={urlparse.urlparse(url).scheme:proxy} opener.add_handler(urllib2.ProxyHandler(proxy_params)) try: response = opener.open(request).read() except urllib2.URLError as e: print \u0026#39;download error:\u0026#39;, e.reason response = None if num_retries \u0026gt; 0: if hasattr(e, \u0026#39;code\u0026#39;) and 500 \u0026lt;= e.code \u0026lt; 600: print \u0026#39;download retry\u0026#39; return download(url, num_retries - 1) return response if __name__==\u0026#39;__main__\u0026#39;: url=\u0026#39;http://example.webscraping.com\u0026#39; link_crawler(url,\u0026#39;/(index|view)\u0026#39;) 在函数get_link里面\ndef get_links(html): print html webpage_regex =re.compile(\u0026#39;\u0026lt;a[^\u0026gt;]+href=[\u0026#34;\\\u0026#39;](.*?)[\u0026#34;\\\u0026#39;]\u0026#39;,re.IGNORECASE) return webpage_regex.findall(html) html是服务器返回的响应，这里的类型就是字符串：\n\u0026lt;!--[if HTML5]\u0026gt;\u0026lt;![endif]--\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;!-- paulirish.com/2008/conditional-stylesheets-vs-css-hacks-answer-neither/ --\u0026gt; \u0026lt;!--[if lt IE 7]\u0026gt;\u0026lt;html class=\u0026#34;ie ie6 ie-lte9 ie-lte8 ie-lte7 no-js\u0026#34; lang=\u0026#34;en-us\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt; \u0026lt;!--[if IE 7]\u0026gt;\u0026lt;html class=\u0026#34;ie ie7 ie-lte9 ie-lte8 ie-lte7 no-js\u0026#34; lang=\u0026#34;en-us\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt; \u0026lt;!--[if IE 8]\u0026gt;\u0026lt;html class=\u0026#34;ie ie8 ie-lte9 ie-lte8 no-js\u0026#34; lang=\u0026#34;en-us\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt; \u0026lt;!--[if IE 9]\u0026gt;\u0026lt;html class=\u0026#34;ie9 ie-lte9 no-js\u0026#34; lang=\u0026#34;en-us\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt; \u0026lt;!--[if (gt IE 9)|!(IE)]\u0026gt;\u0026lt;!--\u0026gt; \u0026lt;html class=\u0026#34;no-js\u0026#34; lang=\u0026#34;en-us\u0026#34;\u0026gt; \u0026lt;!--\u0026lt;![endif]--\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Example web scraping website\u0026lt;/title\u0026gt; \u0026lt;!--[if !HTML5]\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge,chrome=1\u0026#34;\u0026gt; \u0026lt;![endif]--\u0026gt; \u0026lt;!-- www.phpied.com/conditional-comments-block-downloads/ --\u0026gt; \u0026lt;!-- Always force latest IE rendering engine (even in intranet) \u0026amp; Chrome Frame Remove this if you use the .htaccess --\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;!-- http://dev.w3.org/html5/markup/meta.name.html --\u0026gt; \u0026lt;meta name=\u0026#34;application-name\u0026#34; content=\u0026#34;places\u0026#34; /\u0026gt; \u0026lt;!-- Mobile Viewport Fix j.mp/mobileviewport \u0026amp; davidbcalhoun.com/2010/viewport-metatag device-width: Occupy full width of the screen in its current orientation initial-scale = 1.0 retains dimensions instead of zooming out if page height \u0026gt; device height user-scalable = yes allows the user to zoom in --\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;/places/static/images/favicon.ico\u0026#34; type=\u0026#34;image/x-icon\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;apple-touch-icon\u0026#34; href=\u0026#34;/places/static/images/favicon.png\u0026#34;\u0026gt; \u0026lt;!-- All JavaScript at the bottom, except for Modernizr which enables HTML5 elements \u0026amp; feature detects --\u0026gt; \u0026lt;script src=\u0026#34;/places/static/js/modernizr.custom.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- include stylesheets --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;!-- // These variables are used by the web2py_ajax_init function in web2py_ajax.js (which is loaded below).  var w2p_ajax_confirm_message = \u0026#34;Are you sure you want to delete this object?\u0026#34;; var w2p_ajax_disable_with_message = \u0026#34;Working...\u0026#34;; var w2p_ajax_date_format = \u0026#34;%Y-%m-%d\u0026#34;; var w2p_ajax_datetime_format = \u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;; var ajax_error_500 = \u0026#39;An error occured, please \u0026lt;a href=\u0026#34;/places/default/index\u0026#34;\u0026gt;reload\u0026lt;/a\u0026gt; the page\u0026#39; //--\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;meta name=\u0026#34;tags\u0026#34; content=\u0026#34;web2py, python, web scraping\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;generator\u0026#34; content=\u0026#34;Web2py Web Framework\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;Richard Penman\u0026#34; /\u0026gt; \u0026lt;script src=\u0026#34;/places/static/js/jquery.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;link href=\u0026#34;/places/static/css/calendar.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; /\u0026gt;\u0026lt;script src=\u0026#34;/places/static/js/calendar.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script src=\u0026#34;/places/static/js/web2py.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;link href=\u0026#34;/places/static/css/web2py.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; /\u0026gt;\u0026lt;link href=\u0026#34;/places/static/css/bootstrap.min.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; /\u0026gt;\u0026lt;link href=\u0026#34;/places/static/css/bootstrap-responsive.min.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; /\u0026gt;\u0026lt;link href=\u0026#34;/places/static/css/style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; /\u0026gt;\u0026lt;link href=\u0026#34;/places/static/css/web2py_bootstrap.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; /\u0026gt; \u0026lt;!-- uncomment here to load jquery-ui \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;http://ajax.googleapis.com/ajax/libs/jqueryui/1.10.3/themes/ui-lightness/jquery-ui.css\u0026#34; type=\u0026#34;text/css\u0026#34; media=\u0026#34;all\u0026#34; /\u0026gt; \u0026lt;script src=\u0026#34;http://ajax.googleapis.com/ajax/libs/jqueryui/1.10.3/jquery-ui.min.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; uncomment to load jquery-ui //--\u0026gt; \u0026lt;noscript\u0026gt;\u0026lt;link href=\u0026#34;/places/static/css/web2py_bootstrap_nojs.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; /\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Navbar ================================================== --\u0026gt; \u0026lt;div class=\u0026#34;navbar navbar-inverse\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;flash\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;navbar-inner\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;!-- the next tag is necessary for bootstrap menus, do not remove --\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-navbar\u0026#34; data-toggle=\u0026#34;collapse\u0026#34; data-target=\u0026#34;.nav-collapse\u0026#34; style=\u0026#34;display:none;\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;icon-bar\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;ul id=\u0026#34;navbar\u0026#34; class=\u0026#34;nav pull-right\u0026#34;\u0026gt;\u0026lt;li class=\u0026#34;dropdown\u0026#34;\u0026gt;\u0026lt;a class=\u0026#34;dropdown-toggle\u0026#34; data-toggle=\u0026#34;dropdown\u0026#34; href=\u0026#34;#\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;Log In\u0026lt;/a\u0026gt;\u0026lt;ul class=\u0026#34;dropdown-menu\u0026#34;\u0026gt;\u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/places/default/user/register?_next=/places/default/index\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;icon icon-user glyphicon glyphicon-user\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; Sign Up\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class=\u0026#34;divider\u0026#34;\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;/places/default/user/login?_next=/places/default/index\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;icon icon-off glyphicon glyphicon-off\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; Log In\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; \u0026lt;div class=\u0026#34;nav\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;nav\u0026#34;\u0026gt;\u0026lt;li class=\u0026#34;web2py-menu-first\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/places/default/index\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class=\u0026#34;web2py-menu-last\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;/places/default/search\u0026#34;\u0026gt;Search\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;!--/.nav-collapse --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\u0026lt;!--/top navbar --\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;!-- Masthead ================================================== --\u0026gt; \u0026lt;header class=\u0026#34;mastheader row\u0026#34; id=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;span12\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;page-header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt; Example web scraping website \u0026lt;small\u0026gt;\u0026lt;/small\u0026gt; \u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;section id=\u0026#34;main\u0026#34; class=\u0026#34;main row\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;span12\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;results\u0026#34;\u0026gt; \u0026lt;table\u0026gt;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Afghanistan-1\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/af.png\u0026#34; /\u0026gt; Afghanistan\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Aland-Islands-2\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/ax.png\u0026#34; /\u0026gt; Aland Islands\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Albania-3\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/al.png\u0026#34; /\u0026gt; Albania\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Algeria-4\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/dz.png\u0026#34; /\u0026gt; Algeria\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/American-Samoa-5\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/as.png\u0026#34; /\u0026gt; American Samoa\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Andorra-6\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/ad.png\u0026#34; /\u0026gt; Andorra\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Angola-7\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/ao.png\u0026#34; /\u0026gt; Angola\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Anguilla-8\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/ai.png\u0026#34; /\u0026gt; Anguilla\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Antarctica-9\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/aq.png\u0026#34; /\u0026gt; Antarctica\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;div\u0026gt;\u0026lt;a href=\u0026#34;/places/default/view/Antigua-and-Barbuda-10\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/places/static/images/flags/ag.png\u0026#34; /\u0026gt; Antigua and Barbuda\u0026lt;/a\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;pagination\u0026#34;\u0026gt; \u0026amp;lt; Previous | \u0026lt;a href=\u0026#34;/places/default/index/1\u0026#34;\u0026gt;Next \u0026amp;gt;\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt;\u0026lt;!--/main--\u0026gt; \u0026lt;!-- Footer ================================================== --\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;footer class=\u0026#34;footer span12\u0026#34; id=\u0026#34;footer\u0026#34;\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- /container --\u0026gt; \u0026lt;!-- The javascript ============================================= (Placed at the end of the document so the pages load faster) --\u0026gt; \u0026lt;script src=\u0026#34;/places/static/js/bootstrap.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;/places/static/js/web2py_bootstrap.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!--[if lt IE 7 ]\u0026gt; \u0026lt;script src=\u0026#34;/places/static/js/dd_belatedpng.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; DD_belatedPNG.fix(\u0026#39;img, .png_bg\u0026#39;); //fix any \u0026lt;img\u0026gt; or .png_bg background-images \u0026lt;/script\u0026gt; \u0026lt;![endif]--\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 然后我们在这个字符串里找出所有满足\n\u0026#39;\u0026lt;a[^\u0026gt;]+href=[\u0026#34;\\\u0026#39;](.*?)[\u0026#34;\\\u0026#39;]\u0026#39; 的模式，于是我们就得到了\n[ \u0026#39;/places/default/index\u0026#39;, \u0026#39;#\u0026#39;, \u0026#39;/places/default/user/register?_next=/places/default/index\u0026#39;, \u0026#39;/places/default/user/login?_next=/places/default/index\u0026#39;, \u0026#39;/places/default/index\u0026#39;, \u0026#39;/places/default/search\u0026#39;, \u0026#39;/places/default/view/Afghanistan-1\u0026#39;, \u0026#39;/places/default/view/Aland-Islands-2\u0026#39;, \u0026#39;/places/default/view/Albania-3\u0026#39;, \u0026#39;/places/default/view/Algeria-4\u0026#39;, \u0026#39;/places/default/view/American-Samoa-5\u0026#39;, \u0026#39;/places/default/view/Andorra-6\u0026#39;, \u0026#39;/places/default/view/Angola-7\u0026#39;, \u0026#39;/places/default/view/Anguilla-8\u0026#39;, \u0026#39;/places/default/view/Antarctica-9\u0026#39;, \u0026#39;/places/default/view/Antigua-and-Barbuda-10\u0026#39;, \u0026#39;/places/default/index/1\u0026#39; ] 对比原始HTML确实提取到了所有的url但是并不是很干净所以代码中进一步筛选根据的特性就是网站的所有连接里都有\u0026rsquo;view\u0026rsquo; 这个关键字\ndef link_crawler(seed_url,link_regex,delay=1): throttle=Throttle(delay) crawl_queue=[seed_url] seen=set(crawl_queue) while crawl_queue: url=crawl_queue.pop() throttle.wait(url) html=download(url) for link in get_links(html): if re.search(link_regex,link): link=urlparse.urljoin(seed_url,link) if link not in seen: seen.add(link) crawl_queue.append(link) 上面使用的是三只虫里面的代码，其中这两部用到了正则表达式，一个用于提取，一个用于确定，可以看出，正则表达式其实不是很通用，每个网站甚至每个网页都要对应自己的表达式，所以这个基本算是半自动模式，过段时间如果网页代码发生改变，那么正则表达式可能会失效。\nBeautiful Soup 上面的正则表达式是半自动武器，那么Beautiful Soup就是AK47，自动化，强壮，而且好用不贵。 安装方法就是\npip install Beautifulsoup4 与正则表达式不同的是，Beautifulsoup并不是把响应当成字符串处理，而是将html字符串解析成soup文档，相当于构建成了自己的数据结构，然后在使用自己的方法解析，查找，完成一些任务。 但是html有些常见的问题，比如有些语法的缺失：\n\u0026lt;ul class = country\u0026gt; \u0026lt;li\u0026gt;Area \u0026lt;li\u0026gt;Population \u0026lt;/ul\u0026gt; 其中Area和Population都缺失后边的\u0026rsquo;/li\u0026rsquo;，那么这种情况可以有两种解释，Population是Area的子元素，另一种他们两个是并列的。Beautifulsoup可以给出以下处理：\nfrom bs4 import BeautifulSoup as bs bad_html=\u0026#39;\u0026#39;\u0026#39;\u0026lt;ul class = country\u0026gt; \u0026lt;li\u0026gt;Area \u0026lt;li\u0026gt;Population \u0026lt;/ul\u0026gt;\u0026#39;\u0026#39;\u0026#39; soup=bs(bad_html,\u0026#39;lxml\u0026#39;) fixed_html=soup.prettify() print fixed_html 得到的输出是：\n\u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;ul class=\u0026#34;country\u0026#34;\u0026gt; \u0026lt;li\u0026gt; Area \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; Population \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 明显已经被修复了，层次已经明确了，就是并列关系。 bs中find命令和find_all能够找出第一个或者所有的你关心的标签\nfrom bs4 import BeautifulSoup as bs bad_html=\u0026#39;\u0026#39;\u0026#39;\u0026lt;ul class = country\u0026gt; \u0026lt;li\u0026gt;Area \u0026lt;li\u0026gt;Population \u0026lt;/ul\u0026gt;\u0026#39;\u0026#39;\u0026#39; soup=bs(bad_html,\u0026#39;lxml\u0026#39;) fixed_html=soup.prettify() print \u0026#39;find:\u0026#39; print soup.find(\u0026#39;li\u0026#39;) print \u0026#39;find_all:\u0026#39; print soup.find_all(\u0026#39;li\u0026#39;) 而且找到结果还是bs类的，可以继续使用find和find_all在结果中搜索。 代码过程复杂在构建bs数据结构那步，需要显示的初始化，其他部分就相对正则表达式简单多了。\nLxml xml解析库的python封装版本，解析速度比Beautiful Soup快很多，但是安装有点复杂。 其用法与BeautifulSoup类似。具体可以参考手册，而且其可以利用CSS选择器完成一些高难度的任务\n性能 以上三种方法性能是有差距的，很明显lxml使用c语言的封装的，应该跑的最快，而Beautiful Soup应该是最慢的，因为有重建过程。而正则表达式也应该是比较快的（也是C语言封装出来的工具箱） 经过测试，有一下的结论可供参考（来自“用python写网络爬虫”）\n   抓取方法 性能 使用难度 安装难度     正则表达式 快 困难 简单（内置模块）   Beautiful Soup 慢 简单 简单（纯python 模块）   Lxml 快 简单 相对困难    添加抓取回调 之前我们讲到的三只虫，只讲到如何访问网站所有的网页，这相当于我们转悠了一圈啥也没干，如果要处理点什么问题，比如下载个图片什么的，就要加入响应的功能，为了写一个通用的遍历网站的爬虫，处理问题部分用一个单独的函数，而这个函数作为参数传给遍历网站的模块，这种方法在C++中叫函数指针，这里我们可以称之为回调函数。 再次说明下，我们有一个框架，这个框架可以访问网站中的所有网页，但是框架为了通用性，没有设置每一页具体完成什么动作，而这个动作留作接口供大家发挥，这个接口就是回调函数（C++叫函数指针）\n下面我们改造下三只虫中的代码，加入回调函数：\ndef link_crawler(seed_url,link_regex,delay=1,scrape_callback=None,max_depth=10): throttle=Throttle(delay) crawl_queue=[seed_url] seen=set(crawl_queue) while crawl_queue: url=crawl_queue.pop() throttle.wait(url) html=download(url) links=[] if scrape_callback: links.extend(scrape_callback(url,html) or []) for link in links: if re.search(link_regex,link): link=urlparse.urljoin(seed_url,link) if link not in seen: seen.add(link) crawl_queue.append(link) 与原始函数不同的是，我们在这里没有使用固定的获取url的函数get_url(),而是把其作为参数传递进来，这样函数将会更加通用，提示一下，这里面关于最大深度的设定没有实现，如果想使用这段代码，请大家自己添加。 get_url也可以实现任何我们想要的功能，这里就不再赘述了，比如将数据写入文件什么的。\n总结 本文我们介绍了三种抓取数据的方法，当然是我们关心的数据，同时也通过回调函数的实现，使得框架更为通用。我们下一篇该讲如何高速下载了。 待续。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/crawler/crawler-3-3-%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 介绍三种网页抓取的方法\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 正则表达式,Beautiful Soup,Lxml\u003c/p\u003e","title":"【爬虫】3.3 数据抓取"},{"content":"Abstract: 本文介绍HTTP基础知识的入门讲解，没有深入，主要是为了让我们知道爬虫请求服务器响应的大致过程 Keywords: http,uri/url\n爬虫HTTP协议(二) 回家的假期马上就要句号了，返程图中写于哈尔滨友谊路392号全集酒店  I had been rejected,but I was still in love. \u0026ndash;Steven Jobs\n 还是需要强调的一点：我不是爬虫专家或者前端后台专家，我的努力方向也不是这个方向，我只是想要运用这套技术，但是我又希望对整个过程有一个比较详细的了解，所以我在本系列只是简单介绍，有些东西可能含糊不清，需要详细学习的同学可以去查询相关资料\nHTTP用于客户端和服务端的通信 回忆一下上一篇，客户端和服务器的定义，请求访问文本或图像等资源的一端称为客户端，提供资源响应的一端称为服务器端 两台计算机之间通信，也必须有一端扮演服务器，一端扮演客户端，时候可以进行角色互换，但是在一条通信线路来说，服务器客户端的角色是确定的，用HTTP协议能够明确区分哪端是服务器端\n通过请求和响应的交换达成通信 HTTP协议规定，请求从客户端发出，服务器相应该请求并返回，所以，如果客户端不发送请求，服务器端是没有响应的，如果服务器乱响应说明bug了，那么可以得出一个结论，通信肯定是从客户端开始建立的。 客户端发送给某个HTTP服务器端的请求报文中的内容 解释下上面的请求报文：\n GET表示请求访问服务器的类型，称为方法（Method） 字符串/index.html指明请求访问的资源对象，也叫做请求URI（request-URI） HTTP/1.1 HTTP的版本号，用来提示客户端使用的HTTP协议功能  合起来就是，请求某台HTTP服务器上的/index.html页面资源 报文组成：\n 请求方法 请求URI 协议版本 可选的请求首部字段 内容实体构成的  像这样： 详细研究下请求首部字段，接收到客户端请求的服务器，会用下面的处理结果以响应的形式返回 解释下服务器返回的结果：\n HTTP/1.1 表示HTTP的版本 200OK表示请求的处理结果的状态码status code和原因短语reason-phrase 接着是创建响应的日期时间，是首部字段header field内的一个属性 空行，后面的内容是资源实体entity body  响应报文基本上由上述四个模块组成 HTTP是不保存状态的协议 HTTP的另一个性质是不保存状态stateless协议，HTTP协议自身不对请求和响应之间的通信状态进行保存，也就是HTTP这个级别的协议对于发送过请求和相应都没有持久化处理（可以理解为：处理完就忘了） 不记忆之前处理的内容的一个好处就是处理大量的事物的时候会变得非常快，设计的这么简单的原因就是为了确保协议的可伸缩性。 但是现在我们可以发现当我们登录某个网站以后怎么跳转都不需要重新登录（按照上面我们说的HTTP协议无记忆功能，这个变得不可行），我们实现这个功能的方法是Cookie，有关Cookie的介绍在后面。\n请求uri定位资源 uri相当于一种地址格式，互联网上的所有可访问的资源都有一个这种地址，并且这种地址是唯一的： 使用uri的方法有两种，一种是直接写完整的uri： 另一种是写成Host加相对地址的方式，host可以使域名或者ip地址 如果并不是访问某个文件而是访问服务器，直接用*代替请求URI：\nOPTIONS * HTTP/1.1 告知服务器意图的HTTP方法 下面简单介绍HTTP/1.1中使用的方法（请求方法）\n GET 请求访问uri对应的资源，可以返回uri对应的文件，也可以是uri对应程序的结果  POST 传输实体的主体，虽然get也能传输实体的主体，但是一般不用get，post的功能与get很相似，但是post的主要目的并不是获取响应的主体内容  PUT 传输文件，与FTP传输文件一样，要求在请求报文的主题中包含文件内容，然后保存到请求uri指定的位置，但是由于HTTP/1.1中对PUT方法本身不带验证机制，，任何人都可以上传文件，存在安全性问题，因此一般的web网站不适用该方法  HEAD 获得报文首部，与GET方法一样，只是不返回报文主体部分，用于确认URI的有效性及资源更新的日期时间等  DELETE 删除文件，与put相反，DELETE方法删除uri对应的资源，与put一样不安全，一般web站点也不实用DELETE  OPTIONS 询问支持的方法，查询针对请求URI指定资源的方法  TRACE 追踪路径，让web服务器将之前的请求通信环回给客户端的方法，客户端通过trace方法可以查询发送出去的请求是怎么样被加工修改（篡改的），这是因为请求想要连接到源目标服务器可能会通过代理中转，TRACE方法就是用来确认连接过程中发生的一些列操作，但是TRACE不常用，而且其容易引发XST（Cross-Site Tracin个，跨站追踪）攻击，通常就更不会用到了。  CONNECT 用隧道协议连接代理，实现用隧道协议进行TCP，主要使用SSL合同TLS协议吧通信内容加密后经网络隧道传输   使用方法下达命令 方法是一种命令，给uri对应的资源的，在报文中传送这条命令，方法可以指定请求的资源按期望产生某种行为，方法中有GET、POST和HEAD等 下面我们列举一些方法，注意区分大小写： 使用cookie的状态管理 我们说过HTTP是无状态协议，他不会记住任何之前已经完成的请求和响应状态，如果一个网站不记录你的登录状态，你每次跳转都要重新登录，这是不现实的，假设要求登录认证的web本身无法进行状态管理，那么如果每次跳转不重新登录，就要每次请求报文中附加参数来管理登录状态。 又要保留不需要记忆的这个特性，同时又要不每次跳转都重新登录，于是引入Cookie技术，当服务器响应报文中有set-Cookie 的首部字段信息，客户端就会保存Cookie，下次客户端再往该服务器发送请求时，就会自动在报文中加入Cookie，服务器会在收到Cookie后对比记录，得到之前的状态信息。\n没有Cookie的报文\n有关响应报文内Cookie对应的首部字段，参考其他资料 这里我就不再介绍了\n总结 请求报文和响应报文在网络上传来传去，这些报文中的信息就是我们发送的和接受的，我们学习爬虫需要的也就是这部分内容，而具体的算法，实现，硬件部署，这些是网络工程师关心的问题，术业有专攻，我们就不深入的研究别人家的技术了，明天继续爬虫相关知识，待续。。 哈尔滨晚安\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/crawler/crawler-3-2-http%E5%8D%8F%E8%AE%AE-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍HTTP基础知识的入门讲解，没有深入，主要是为了让我们知道爬虫请求服务器响应的大致过程\n\u003cstrong\u003eKeywords:\u003c/strong\u003e http,uri/url\u003c/p\u003e","title":"【爬虫】3.2 HTTP协议(二)"},{"content":"Abstract: 本文介绍HTTP基础知识的入门讲解，没有深入，主要是为了让我们知道爬虫请求服务器响应的大致过程 Keywords: http,web,tcp/ip,uri/url\n爬虫HTTP协议(一) 还是昨天那套，我不是爬虫专家或者前端后台专家，我的努力方向也不是这个方向，我只是想要运用这套技术，但是我又希望对整个过程有一个比较详细的了解，所以我在本系列只是简单介绍，有些东西可能含糊不清，需要详细学习的同学可以去查询相关资料\nHTTP简史 我们访问网站，在地址栏中输入一个网址就能访问网址对应的网站内容，看起来很常用的一套操作，背后隐藏了什么样的技术，我们并不很清楚的知道，我们今天这篇和下一篇就是大概的介绍这个过程。 如果我们进一步抽象，我们可以把我们的浏览器或者其他工具称之为客户端，而响应我们请求或者存储我们想要的信息的成为服务器。 他们之间的一系列的运作流程使用http（超文本传输协议） 提出 而http的最初提出为了是科研机构之间的知识共享，最初的设计理念是：借助多文档之间的相互关联形成的超文本，连成可相互参阅的www万维网。 WWW的构建技术分别是：\n SGML（Standard Generalized Markup Language）作为页面的文本标记语言的HTML（HyperText Markup Language） 作为文档传递协议的HTTP 指定文档所在地址的URL（Uniform Resource Locator）  成长 Web的成长 日本第一个主页 -\u0026gt; HTML1.0 -\u0026gt; NCSA Mosaic bounce page -\u0026gt; The NCSA HTTPd Home Page\n网络基础TCP/IP TCP/IP协议族，这个学过计算机的都知道，完整学习，估计要很长很长很长一段时间，做通讯的可能要深入研究，我们这里还是学习其中浅显易懂的入门知识。\nTCP/IP协议族 计算机与网络设备通讯肯定是相互的，不能像我们寻找外星人一样，发出讯息几千年没有回应，我们访问网络需要的是立刻马上得到信息，而相互通讯的基础就是使用相同的方法一个说中文一个说英语，这样的沟通是低效的，高效的方法就是都说中文，或者都说英文。 访问网络设备，如何探测到通讯目标，由谁先发起通讯，使用什么语言，怎样结束都需要规定清楚，而且如果各个网站都不同，浏览器也没办法通用，所以大家最好都是按照一个规则，这样浏览器就会更容易设计。 这种规则称之为协议： 协议管的非常宽，包括电线规格到ip地址选定方法，寻找异地用户方法，建立通讯顺序都需要协议来规范，可想而知，这个协议的体量应该是超级大的。\nTCP/IP的分层结构 TCP/IP协议是分层的，这个是TCP/IP最重要的一个结构\n 应用层：向用户提供应用服务时通信的活动 传输层：对上层应用层，提供处于网络连接中的两台计算机间的数据传输 网络层（网络互连层）：处理网上流动的数据包，数据包是网络传输的最小单位，该层规定了通过怎样的路径所谓的传输路线到达对方计算机，并把数据包传送给对方 链路层（链路层，网络接口层）：硬件部分，操作系统，驱动，网卡，光纤  传输流 上图就是最典型的传输流，我们就考虑我们访问网站的过程，也就是HTTP说明： 首先客户端在应用层发出一个想看某个web网页的HTTP请求。 接着，为了传输方便，传输层（TCP协议）把从应用层收到的数据（HTTP请求报文）进行分割，并在各个报文上打上标记序号及端口号，转发给网络层。 在网络层（IP协议），增加作为通信目的地的MAC地址后转发给链路层，这样一来，发往网络的通信请求就准备齐全了。 到了服务器端，操作与上述基本相反，从链路层收到的数据按书序往上发送一直到应用程序： 每一层都要加包装，把数据信息包装起来的做法叫做封装\nIP/TCP/DNS与HTTP 负责传输的IP协议 几乎所有的网络系统都要使用IP协议，与IP地址不同，IP协议是一个很大的集合。 在发快递的时候，为了确保对方是真正的接收方，我们一般会写地址，和手机号，类似的IP协议中的做法是ip地址和mac地址。 一般情况下，我们不但要和内网的计算机通讯，更多的是和外网的计算机通讯，外网通讯一般要听过很多网络中转设备才能到目标服务器，这个过程和发快递也很相似，快递逐步转运；搜寻下一站去哪，就要使用MAC地址，采用ARP协议解析地址，根据通信方的IP就可以反查出对应的MAC地址（有点混乱，具体可查询相关资料） 网络上的情况是复杂的，没有人能确定当前网络中的传输状况，在到达目的地服务器之前，我们不确定数据包都经过了哪些设备，我们只能大致估算。无论哪台计算机，网络设备都无法掌握全部的互联网细节。 可靠的TCP协议 为了确保到达目标TCP进行三次握手，确认无误： 当然TCP还有别的策略保证通讯准确无误\nDNS域名解析 DNS和HTTP一样，也是应用层的一个协议，主机除了Ip地址也可以被赋予主机名和域名，比如www.face2ai.com这种字符串比IP地址和主机名好记多了，而域名对应的IP地址需要用dns服务器来解析： HTTP与各个协议 HTTP使用时，下面的简图可以大致反应各个协议发挥的作用：\nuri和url URI统一资源标识符，我们更熟悉URL，在web浏览器中输入www.face2ai.com就能访问相关地址\nUniform Resource Identifier的缩写，定义如下：\n Uniform 规定统一的格式可以方便处理不同类型的资源，而不需要根据上下文环境来识别资源指定的访问方式，另外加入新增的协议方案（比如http: 或https:）也很容易 Resource 资源的定义是可标识的任何东西，除了文档图片或者服务，都是资源 Identifier 可标识的对象，也称为标识符 举几个例子： URI表示的互联网的某一个资源 比如：http://www.iana.org/assignments/uri-schemes URL表示一个地址，可见，URL是URI的一个子集。 URI的格式分为绝对URI和相对URI，同样下面的URL也是分为绝对URL和相对URL，URI的格式如下：  http: 是访问资源用到的协议类型，也可以是data: javascript: 登录信息 用户名和密码是登录服务器必要信息 服务器地址 绝对uri必须制定访问服务器地址 服务器端口号 可选，如果省略就是默认端口号 带层次的文件路径 指定服务器文件路径来定位指定资源 查询字符串 对于指定的资源可以传入参数，此项也是可选的 片段标识符 使用片段标识符通常可标记处以获取资源中的子资源，文档的某个位置  总结 本文简单的介绍了HTTP相关的一些简单知识，下一文我们介绍更多详细内容。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/crawler/crawler-3-1-http%E5%8D%8F%E8%AE%AE-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍HTTP基础知识的入门讲解，没有深入，主要是为了让我们知道爬虫请求服务器响应的大致过程\n\u003cstrong\u003eKeywords:\u003c/strong\u003e http,web,tcp/ip,uri/url\u003c/p\u003e","title":"【爬虫】3.1 HTTP协议(一)"},{"content":"Abstract: 本文介绍第一个naive爬虫，同时也给出三种不同方式的 Keywords: 网站地图爬虫，ID遍历爬虫，链接爬虫\n三只爬虫 今天废话不多说，争取多写点爬虫的内容，后面回去了主要还是要进攻基础理论知识，心怀希望则不会迷茫！ 再次说明，爬虫对于我是非常有用的一个工具，但是我并不是也不想深入成为爬虫专家，所以这一系列的博客，都相对浅显入门，只给大家提供一个没有肉的骨架，具体应用请咨询更专业的书籍。\n下载网页 我们上网查资料一般都是通过网页上的文字获得的了，信息蕴藏在文字中，我们的爬虫为了获得信息，第一步就是获得文字。 我们要用浏览器访问地址才能得到服务器的响应，浏览器把服务器响应可视化，我们就能看到文字图片听到声音等，所以我们最关键，最重要的技术就是，使用python向服务器发送请求，然后把响应翻译筛选出有效信息，而且有些时候不能让服务器发现是python在访问而是让他错以为是人在操作。\nurllib2 python的urllib2是一种常用的下载工具，当然不是唯一的工具，对于工具的学习，我们分成两种，一种是自己本专业的工具，我们要非常熟悉，而且知道其原理和理论，比如我们研究图像处理，opencv的这个工具不但会用还要研究其算法和实现，而其他工具就不需要花费太多的精力在其原理上，而是更多关注其操作流程，及时查询文档，就能完成任务。 我们可以定义一个简单的下载函数：\n## -*- coding:utf-8 -*- import urllib2 def download(url): print \u0026#39;Downloading: \u0026#39;,url try: html=urllib2.urlopen(url).read() except urllib2.URLError as e: print \u0026#39;Downloading error:\u0026#39;,e.reason html=None return html if __name__==\u0026#39;__main__\u0026#39;: url=\u0026#39;https://www.baidu.com\u0026#39; html=download(url) print html 输出：\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;script\u0026gt; location.replace(location.href.replace(\u0026#34;https://\u0026#34;,\u0026#34;http://\u0026#34;)); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;noscript\u0026gt;\u0026lt;meta http-equiv=\u0026#34;refresh\u0026#34; content=\u0026#34;0;url=http://www.baidu.com/\u0026#34;\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 上面的这段代码作用是向url：\u0026lsquo;https://www.baidu.com\u0026rsquo; 所在的服务器发送请求，显示的html显示这是一个跳转，转到\u0026rsquo;http://www.baidu.com\u0026rsquo;， 服务器返回的就是上面的html代码，我们看到的就是： 而我们通过浏览器查看其源代码： 重试下载 上面的代码有个try的代码块，Python知识，try后的语句可能出现错误，如果出现错误，except语句开始执行，返回错误信息，有些时候下载网页有可能是网络暂时性问题，也就是说返回错误了虽然，但是再来一次很有可能成功，这类错误的代号是从500到600之间的，如果发生看404这类错误，很遗憾没有重试的必要了，不是撞墙了，就是人家没有这一页。 代码中加入重试：\ndef download(url,num_retries=2): print \u0026#39;Downloading:\u0026#39;, url try: response = urllib2.urlopen(url).read() except urllib2.URLError as e: print \u0026#39;download error:\u0026#39;, e.reason response = None if num_retries \u0026gt; 0: if hasattr(e, \u0026#39;code\u0026#39;) and 500 \u0026lt;= e.code \u0026lt; 600: print \u0026#39;download retry\u0026#39; return download(url, num_retries - 1) return response 当发现错误是500到600之间时重试，递归调用本函数，重试次数就是递归深度。\nhasattr(obj,name) 函数说明：判断obj对象中是否有name属性或方法。 我们可以测试一个我们预先设定好的网页，他作用就是返回错误500 输出结果：\nDownloading: http://httpstat.us/500 download error: Internal Server Error download retry Downloading: http://httpstat.us/500 download error: Internal Server Error download retry Downloading: http://httpstat.us/500 download error: Internal Server Error None 用户代理 像我们这种小菜如果控制了特别大的带宽的时候而又没控制好虫子，虫子很有可能把别人的网站爬崩溃，而我们通过上述简单的方式访问服务器时，服务器会识别出我们的用户是：python-urllib/2.7后面是版本号，前面是python-urllib。于是一些网站禁止这个用户访问自己的网站，所以我们要给自己改革名字，这个就是用户代理。\ndef download(url,user_agent=\u0026#39;tony\u0026#39;,num_retries=2): print \u0026#39;Downloading:\u0026#39;, url headers = {\u0026#39;User-agent\u0026#39;: user_agent} request = urllib2.Request(url,None,headers) try: response = urllib2.urlopen(request).read() except urllib2.URLError as e: print \u0026#39;download error:\u0026#39;, e.reason response = None if num_retries \u0026gt; 0: if hasattr(e, \u0026#39;code\u0026#39;) and 500 \u0026lt;= e.code \u0026lt; 600: print \u0026#39;download retry\u0026#39; return download(url, num_retries - 1) return response 这个用户名就变成了tony. 接下来就要写三种不同套路的小虫子了，茴香豆的茴字有四种写法，你知道么？\n网站地图 上文我们提到过网站地图，这一小结，我们就将利用这个地图爬取网站数据。 有些网站的sitemap中会给你所有的本站的地址例如： http://example.webscraping.com/places/default/sitemap.xml 我们使用下面代码，配合download能够得出结果：\ndef crawl_sitemap(url): sitemap=download(url) links=re.findall(r\u0026#39;\u0026lt;loc\u0026gt;(.*?)\u0026lt;/loc\u0026gt;\u0026#39;,sitemap) for link in links: html = download(link) ## do something print \u0026#39;crawl site:\u0026#39;,link 这是一个非常简单的方式，xml中、之间是一个连接，用正则表达式找出所有的连接，那么就获得了全部的访问地址，然后每一个都访问一下就可以了 正则表达式我们后面会有专题。\nID遍历爬虫 观察我们上面爬到的那个网站: http://example.webscraping.com/places/default/view/1\nhttp://example.webscraping.com/places/default/view/Afghanistan-1\n他们两个指向同一个页面，数据库端一般只用id来匹配数据，而这些别名Afghanistan-1，是用来方便搜索引擎工作的，那么按照这个例子，就可以不断修改第一个链接中的1，从而获得所有的网页了。 当然问题来了，如果1，2，3，4，5中我们删掉了一页，那么循环没办法继续下去了，我们就要设置另一种策略来避免这种尴尬，就是如果只有1，4，5，6等，我们可以当遇到2，3，时发生错误不退出，而是继续循环，那么就可以爬到后面的了，只要设置一个断开的距离不超过多少，如果超过了则跳出循环即可解决问题。\n链接爬虫 这种爬虫才是最正规的爬虫，前面两种虫子都是比较特例的，当我们分析完目标站时，发现其具有上述两种性质时，这两种虫子才有作用，当时对于一般网站，我们只能用一般的办法，链接虫子，一种通用的高端虫。其一般过程如下：\n把上面的过程代码化前，必须明确几个问题\n 就是网站内死循环的问题，网页A包含B的链接，B包含C的链接，C包含A的链接，这样下去循环往复，无休无止。解决办法就是用一个list或者其他数据结构保存已访问的网页，每次访问前判断是否重复访问，不重复，则访问。 网站内链接有些是以相对路径，也就是和只写了一半的路径，我们需要补全 robots文件的加入使爬虫正规化  简单的框架 import urllib2 import re def get_links(html): webpage_regex =re.compile(\u0026#39;\u0026lt;a[^\u0026gt;]+href=[\u0026#34;\\\u0026#39;](.*?)[\u0026#34;\\\u0026#39;]\u0026#39;,re.IGNORECASE) return webpage_regex.findall(html) def link_crawler(seed_url,link_regex): crawl_queue=[seed_url] while crawl_queue: url=crawl_queue.pop() html=download(url) for link in get_links(html): if re.search(link_regex,link): crawl_queue.append(link) def download(url,num_retries=2): print \u0026#39;Downloading:\u0026#39;, url try: response = urllib2.urlopen(url).read() except urllib2.URLError as e: print \u0026#39;download error:\u0026#39;, e.reason response = None if num_retries \u0026gt; 0: if hasattr(e, \u0026#39;code\u0026#39;) and 500 \u0026lt;= e.code \u0026lt; 600: print \u0026#39;download retry\u0026#39; return download(url, num_retries - 1) return response if __name__==\u0026#39;__main__\u0026#39;: url=\u0026#39;http://example.webscraping.com\u0026#39; link_crawler(url,\u0026#39;/(index|view)\u0026#39;) 这段代码看上去没啥问题，但是运行时会出现错误，就是我们上面问题2的问题，有些地址是相对地址：\nDownloading: http://example.webscraping.com Downloading: /places/default/index/1 Error ...... 支持相对地址 错误信息省略，那么我们怎么改进呢，当然是加上前半段的地址啦：\nimport urlparse def link_crawler(seed_url,link_regex): crawl_queue=[seed_url] while crawl_queue: url=crawl_queue.pop() html=download(url) for link in get_links(html): if re.search(link_regex,link): link=urlparse.urljoin(seed_url,link) crawl_queue.append(link) 就可以正确的下载了，当然，问题1还没有解决，就是循环访问的问题，我们的解决办法\n避免循环访问 我们添加一个集合的结构体来保存访问过（或者将要访问）的地址，需要访问时查询是否访问过，如果没有则加入将要访问的列表中：\ndef link_crawler(seed_url,link_regex): crawl_queue=[seed_url] seen=set(crawl_queue) while crawl_queue: url=crawl_queue.pop() html=download(url) for link in get_links(html): if re.search(link_regex,link): link=urlparse.urljoin(seed_url,link) if link not in seen: seen.add(link) crawl_queue.append(link) 上面这个版本如果没有问题就能访问网站中所有的网页了，这个爬虫是可用的。\nrobots 我们上一篇提到过robots.txt，里面有一些可以访问以及不可以访问的路径，我们可以通过python工具包robotparser来自动解析使用(贴一段书上的截图吧)： 支持代理 有些网站对地区等访问有限制，当然万里长城那个不是这类的，有些网站站长就不希望别的国家人访问，但是我们还想访问，怎么办，vpn或者通过代理，python有个requests模块可以实现这部分功能，这个部分应该在download中完成，那么新版本的download函数如下：\ndef download(url,user_agent=\u0026#39;tony\u0026#39;,proxy=None,num_retries=2): print \u0026#39;Downloading:\u0026#39;, url headers = {\u0026#39;User-agent\u0026#39;: user_agent} request=urllib2.Request(url,headers=headers) opener=urllib2.build_opener() if proxy: proxy_params={urlparse.urlparse(url).scheme:proxy} opener.add_handler(urllib2.ProxyHandler(proxy_params)) try: response = opener.open(request).read() except urllib2.URLError as e: print \u0026#39;download error:\u0026#39;, e.reason response = None if num_retries \u0026gt; 0: if hasattr(e, \u0026#39;code\u0026#39;) and 500 \u0026lt;= e.code \u0026lt; 600: print \u0026#39;download retry\u0026#39; return download(url, num_retries - 1) return response 上面的代码设置了代理服务器，urllib2的功能很强大，但是 用起来稍微可能麻烦点，我们下一篇介绍下代理服务器和整个响应过程的主要知识，所以这部分不明白流程的可以不用着急，结合下一篇看更明了！\n下载限速 如果我们为了自己爬数据把人家服务器搞崩了，其实不道德也耽误我们自己的进度，所以设置下载限速或者加入休眠，可以很好的解决这个问题。\nimport datetime import time class Throttle: \u0026#34;\u0026#34;\u0026#34; 限速 \u0026#34;\u0026#34;\u0026#34; def __init__(self,delay): self.delay=delay self.domains={} def wait(self,url): domain=urlparse.urlparse(url).netloc last_accessed=self.domains.get(domain) if self.delay\u0026gt;0 and last_accessed is not None: sleep_secs=self.delay-(datetime.datetime.now()- last_accessed).seconds if sleep_secs\u0026gt;0: time.sleep(sleep_secs) print \u0026#39;sleep %dsec\u0026#39;%sleep_secs self.domains[domain]=datetime.datetime.now() 同时在下载前一步加入如下语句：\ndef link_crawler(seed_url,link_regex,delay=1): throttle=Throttle(delay) crawl_queue=[seed_url] seen=set(crawl_queue) while crawl_queue: url=crawl_queue.pop() throttle.wait(url) html=download(url) for link in get_links(html): if re.search(link_regex,link): link=urlparse.urljoin(seed_url,link) if link not in seen: seen.add(link) crawl_queue.append(link) 这样就完成了一个两个链接之间的访问间隔设定。\n避免陷阱 设定访问深度，当达到深度后不再继续下去，因为有些链接可以无限向后链接的，比如有些日历，爬虫要避免落入深渊中，所以设定最大深度好处多多\n总结 两个小虫子，一个大虫子，大虫子比较通用，小虫子适合特定场景，明天继续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/crawler/crawler-3-0-%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍第一个naive爬虫，同时也给出三种不同方式的\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 网站地图爬虫，ID遍历爬虫，链接爬虫\u003c/p\u003e","title":"【爬虫】3.0 三只虫"},{"content":"Abstract: 分析网站，进行数据收集 Keywords: builtwith，网站规模\n分析目标网站 还是前一篇那个主旨，我的个人目标不是前端或者爬虫专家，我的主要目标是机器学习算法，所以爬虫系列爬虫显得浅显和幼稚，只给大家一个入门的建议，没有进阶或者深入部分。\n爬虫何时使用 爬虫使用还是收集数据或者分析网站数据时，关于爬虫合法性，这个各个国家都不一样，涉及法律问题请大家查询当地法律条文。\n网站分析 分析网站的前提是你知道要爬谁，而要爬谁这个要手工确定，这样我们可以精确打击了，如果不能精确打击，恭喜你，你做的爬虫可以改名叫搜索引擎了。\nRobots.txt 对目标网站的robots.txt进行访问，robots中包含了这个网站对于哪些内容是允许爬取的，哪些是不能的，相当于网站所有者或者管理者对于爬虫的行为约束，当然这个不是强制性的，数据收集者也就是我们，我们可以完全不在意他说什么，而尽情的收集我们所需要的，但是，这是不道德的 上图就是网易公开课的robots.txt文件的内容，其中允许我们爬取\u0026rsquo;/\u0026lsquo;下的内容，而且给我了我们一个地图，没错，这个就是下面我们要说的地图。\n网站大小与地图 在说地图前，我们先研究一下网站大小，我们可以利用搜索引擎来确定下网站的大致规模，用到的搜索引擎技术是在搜索框里使用\u0026rsquo;site:example.com\u0026rsquo;，就会得到大致的网页数量，从而可以大致的估算网站规模，比如我们来测试一下face2ai.com的规模：\n 先看我们的baidu，3个结果  看一下bing，35个结果  最后看一下google，285个结果   这个没办法说谁搜的好谁搜的快，但是从细节上来说，google确实还是要强一些 规模上来说face2ai.com确实不大，不止35个网页，当然也没有285那么多，我感觉大概也就是200个网页左右，所以google还是非常接近的。不得不再次说，google真心厉害。 接着就是地图啦，我们在观察网易公开课的robot.txt文件中我们看到了本网站的地图，这个地图会给我们提供非常多的信息，类似于主人邀请你去他家的宫殿游玩，主人不在，又怕你迷路，所以robots告诉你哪些可以去哪些不可以去，如果你想去，应该怎么走，这个应该怎么走就是地图啦。 其中第一行sitemap.org是提供标准的，具体可以访问查询，而这个网站地图后面我们将会使用它来引导我们访问全站。\n网站技术 接下来就是研究一下网站使用啥写的，我虽然不做网站，但是也知道构建网站的方式不止一种，于是针对不同技术，我们理论上是有不同的爬虫技术与其对应，当然这是高级的做法了，我们这种入门选手不需要这么专业，但是基础就是为了专业而大的，了解下网站技术，对我们精确打击是有很大帮助的，而获得信息的方式是使用python的一个叫做builtwith的工具包： 用到的代码：\nimport builtwith print builtwith.parse(\u0026#39;http://www.face2ai.com\u0026#39;) 就能得到这个网址对应的网站是使用什么技术搭建的。\n网站所有者 接着上面那个工具包，又一个工具包叫做whois，用来检测我们的目标网站属于谁，不能打了半连对方是谁都不清楚，那不就白打了么。\nimport whois print whois.whois(\u0026#39;http://www.face2ai.com\u0026#39;) 千万要注意，whois和python-whois是两个不同的工具包，我们要用的是python-whois，上面的信息是阿里云，但是所有者里好像没有我的名字啊。。老子可是花了钱的好不好，虽然是租的。。。\n爬取方案设计 爬虫设计方案主要看我们的目的。 有些网站我们需要根据主目录一页一页的爬取每个目录中对应连接中的内容，这是一个典型的广度优先，根据网站目录，每页搜索。 第二是漫无目的的搜索，就是当我们进入一个网页先按照深度优先的方式逐级推进至最深层次，然后返回以此迭代。 归根到底还是看我们的需求，所以深度优先搜索还是广度优先搜索，都有用处，但是场合不同。\n总结 这一集干货不多，也有一点，下面我们就要开始研究怎么欺骗或者访问服务器，让他们给我们发送回响应了，这才是难点！待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/crawler/crawler-2-0-%E5%88%86%E6%9E%90%E7%A1%AE%E5%AE%9A%E7%9B%AE%E6%A0%87.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 分析网站，进行数据收集\n\u003cstrong\u003eKeywords:\u003c/strong\u003e builtwith，网站规模\u003c/p\u003e","title":"【爬虫】2.0 分析目标网站"},{"content":"Abstract: 关于python网络爬虫的入门 Keywords: 爬虫，python\n爬虫介绍 数据收集过程 我们平时访问网络使用的主要是浏览器，其实浏览器和编译器以及操作系统这三大软件应该是软件界比较厉害的三种工具了，当然你要说出几个其他的比如Photoshop什么的当然，这也厉害，但是和我们CS相关的这三个应该是最顶级的工具了。 我们平时和网络上的数据进行信息交换主要通过浏览器，比如我们从网上找一些图片，或者一些新闻，这个过程人工进行是很慢的，所以我们的党政机关有不少人专门负责上网看新闻（铛铛铛，老乡，社区送温暖！）网站上的数据有时候我们想要，而且不止一部分而是全部，比如某个网站上全部的图片，我们需要用到的就是爬虫技术，之前不太了解爬虫，爬虫访问网站上的每一页的内容，识别出我们需要的数据，下载保存到本地，基本上就是我们使用爬虫的一般性过程，当然你如果就是为了统计点什么不需要把数据下载下来，也是可以的，这个过程大致的一个流程是：  分析目标，这部分针对特定网站，如果你想写一个通用的爬虫，那么这部分可以不在考虑范围，而是考虑文件特征和如何判断网页是否已经处理过了；其次是考虑网站的结构和技术，动态网站，静态网站以及其他的特殊网站 获取网站回应，通过1中得到的信息，向服务器发送请求，当然这部分最好能模拟成和浏览器一样的请求，迷惑服务器正常返回所需要的信息，这部分是技术核心，也是你难点，因为各种网站结构不同，也有相应的防止爬虫的技术，所以，成为大神，这部分是关键 分析网站返回的内容，从服务器返回的内容中找到目标内容，进行分析和处理（这部分可能产生新的url作为爬取目标） 判断是否继续进行搜索，如果继续返回 打完收工  使用到的主要技术 那么我们要学会那些技术呢？ 根据上面的爬取过程，根据我们前的知识，我们要用到但不限于以下技术：\n HTML技术，JavaScript，CSS，Json等，这类知识主要跟前端有关，必须承认，我对这些知识的了解基本为零，所以边学边查 分析服务器返回内容，通过正则表达式，或者python 中的beautifulsoup等工具重建提取信息 保存主要用到数据库或者其他文件系统  上面这些技术是简单总结，可能不全，后面可能补齐\n总结 技术类的东西可以从头学起，比如我们先学前端，再学服务器知识，再学网络等等，最后我们就变成了一个爬虫专家，但是我们的目标是数据专家，所以我们更多的时间应该放在数学和算法上，我们学爬虫，是为了服务于我们的数据分析算法的验证上，所以用到什么现学现卖，所以没有我们研究机器学习那么专业，我们主要用几个项目来学习过程 学习整个流程，当其中某些技术对于其他目标网站不奏效的时候，相应的调整，更换技术就能完成我们的新目标，我们主要参考下面这本书：\n正则表达式则参考：\n注意：正则表达式对于我们的机器学习和后面的开发用处较大，所以这部分内容更加详细\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/crawler/crawler-1-0-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 关于python网络爬虫的入门\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 爬虫，python\u003c/p\u003e","title":"【爬虫】1.0 介绍"},{"content":"Abstract: 本文继续上文介绍CUDA编程模型关于核函数以及错误处理部分 Keywords: CUDA核函数，CUDA错误处理\nCUDA编程模型概述（二） 继续CUDA编程模型的后半部分，关于核函数以及错误处理。\n 核函数  启动核函数 编写核函数 验证核函数   错误处理  核函数概述 核函数就是在CUDA模型上诸多线程中运行的那段串行代码，这段代码在设备上运行，用NVCC编译，产生的机器码是GPU的机器码，所以我们写CUDA程序就是写核函数，第一步我们要确保核函数能正确的运行产生正切的结果，第二优化CUDA程序的部分，无论是优化算法，还是调整内存结构，线程结构都是要调整核函数内的代码，来完成这些优化的。 我们一直把我们的CPU当做一个控制者，运行核函数，要从CPU发起，那么我们开始学习如何启动一个核函数\n启动核函数 启动核函数，通过的以下的ANSI C 扩展出的CUDA C指令：\nkernel_name\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(argument list); 其标准C的原型就是C语言函数调用\nfunction_name(argument list); 这个三个尖括号\u0026rsquo;\u0026laquo;\u0026lt;grid,block\u0026raquo;\u0026gt;\u0026lsquo;内是对设备代码执行的线程结构的配置（或者简称为对内核进行配置），也就是我们上一篇中提到的线程结构中的网格，块。回忆一下上文，我们通过CUDA C内置的数据类型dim3类型的变量来配置grid和block（上文提到过：在设备端访问grid和block属性的数据类型是uint3不能修改的常类型结构，这里反复强调一下）。 通过指定grid和block的维度，我们可以配置：\n 内核中线程的数目 内核中使用的线程布局  我们可以使用dim3类型的grid维度和block维度配置内核，也可以使用int类型的变量，或者常量直接初始化：\nkernel_name\u0026lt;\u0026lt;\u0026lt;4,8\u0026gt;\u0026gt;\u0026gt;(argument list); 上面这条指令的线程布局是： 我们的核函数是同时复制到多个线程执行的，上文我们说过一个对应问题，多个计算执行在一个数据，肯定是浪费时间，所以为了让多线程按照我们的意愿对应到不同的数据，就要给线程一个唯一的标识，由于设备内存是线性的（基本市面上的内存硬件都是线性形式存储数据的）我们观察上图，可以用threadIdx.x 和blockIdx.x 来组合获得对应的线程的唯一标识（后面我们会看到，threadIdx和blockIdx能组合出很多不一样的效果）\n接下来我们就是修改代码的时间了，改变核函数的配置，产生运行出结果一样，但效率不同的代码：\n 一个块：  kernel_name\u0026lt;\u0026lt;\u0026lt;1,32\u0026gt;\u0026gt;\u0026gt;(argument list); 32个块  kernel_name\u0026lt;\u0026lt;\u0026lt;32,1\u0026gt;\u0026gt;\u0026gt;(argument list); 上述代码如果没有特殊结构在核函数中，执行结果应该一致，但是有些效率会一直比较低。\n上面这些是启动部分，当主机启动了核函数，控制权马上回到主机，而不是主机等待设备完成核函数的运行，这一点我们上一篇文章也有提到过（就是等待hello world输出的那段代码后面要加一句）\n想要主机等待设备端执行可以用下面这个指令：\ncudaError_t cudaDeviceSynchronize(void); 这是一个显示的方法，对应的也有隐式方法，隐式方法就是不明确说明主机要等待设备端，而是设备端不执行完，主机没办法进行，比如内存拷贝函数：\ncudaError_t cudaMemcpy(void* dst,const void * src, size_t count,cudaMemcpyKind kind); 这个函数上文已经介绍过了，当核函数启动后的下一条指令就是从设备复制数据回主机端，那么主机端必须要等待设备端计算完成。\n所有CUDA核函数的启动都是异步的，这点与C语言是完全不同的\n编写核函数 我们会启动核函数了，但是核函数哪里来的？当然我们写的，核函数也是一个函数，但是声明核函数有一个比较模板化的方法：\n__global__ void kernel_name(argument list); 注意：声明和定义是不同的，这点CUDA与C语言是一致的\n在C语言函数前没有的限定符__global__ ,CUDA C中还有一些其他我们在C中没有的限定符，如下：\n   限定符 执行 调用 备注     __global__ 设备端执行 可以从主机调用也可以从计算能力3以上的设备调用 必须有一个void的返回类型   __device__ 设备端执行 设备端调用    __host__ 主机端执行 主机调用 可以省略    而且这里有个特殊的情况就是有些函数可以同时定义为 device 和 host ，这种函数可以同时被设备和主机端的代码调用，主机端代码调用函数很正常，设备端调用函数与C语言一致，但是要声明成设备端代码，告诉nvcc编译成设备机器码，同时声明主机端设备端函数，那么就要告诉编译器，生成两份不同设备的机器码。\nKernel核函数编写有以下限制\n 只能访问设备内存 必须有void返回类型 不支持可变数量的参数 不支持静态变量 显示异步行为  并行程序中经常的一种现象：把串行代码并行化时对串行代码块for的操作，也就是把for并行化。 例如： 串行：\nvoid sumArraysOnHost(float *A, float *B, float *C, const int N) { for (int i = 0; i \u0026lt; N; i++) C[i] = A[i] + B[i]; } 并行：\n__global__ void sumArraysOnGPU(float *A, float *B, float *C) { int i = threadIdx.x; C[i] = A[i] + B[i]; } 这两个简单的段不能执行，但是我们可以大致的看一下for展开并行化的样子，没吃过鸡肉的时候可以先看看鸡跑（我的博客是清真的）。\n验证核函数 验证核函数就是验证其正确性，下面这段代码上文出现过，但是同样包含验证核函数的方法： 代码库：https://github.com/Tony-Tan/CUDA_Freshman\n/* * https://github.com/Tony-Tan/CUDA_Freshman * 3_sum_arrays */ #include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; void sumArrays(float * a,float * b,float * res,const int size) { for(int i=0;i\u0026lt;size;i+=4) { res[i]=a[i]+b[i]; res[i+1]=a[i+1]+b[i+1]; res[i+2]=a[i+2]+b[i+2]; res[i+3]=a[i+3]+b[i+3]; } } __global__ void sumArraysGPU(float*a,float*b,float*res) { int i=threadIdx.x; res[i]=a[i]+b[i]; } int main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int nElem=32; printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *a_h=(float*)malloc(nByte); float *b_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte); float *res_from_gpu_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d,*res_d; CHECK(cudaMalloc((float**)\u0026amp;a_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;b_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;res_d,nByte)); initialData(a_h,nElem); initialData(b_h,nElem); CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); dim3 block(nElem); dim3 grid(nElem/block.x); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;,block.x,grid.x); CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); sumArrays(a_h,b_h,res_h,nElem); checkResult(res_h,res_from_gpu_h,nElem); cudaFree(a_d); cudaFree(b_d); cudaFree(res_d); free(a_h); free(b_h); free(res_h); free(res_from_gpu_h); return 0; } 在开发阶段，每一步都进行验证是绝对高效的，比把所有功能都写好，然后进行测试这种过程效率高很多，同样写CUDA也是这样的每个代码小块都进行测试，看起来慢，实际会提高很多效率。 CUDA小技巧，当我们进行调试的时候可以把核函数配置成单线程的：\nkernel_name\u0026lt;\u0026lt;\u0026lt;1,1\u0026gt;\u0026gt;\u0026gt;(argument list) 错误处理 所有编程都需要对错误进行处理，早起的编码错误，编译器会帮我们搞定，内存错误也能观察出来，但是有些逻辑错误很难发现，甚至到了上线运行时才会被发现，而且有些厉害的bug复现会很难，不总出现，但是很致命，而且CUDA基本都是异步执行的，当错误出现的时候，不一定是哪一条指令触发的，这一点非常头疼；这时候我们就需要对错误进行防御性处理了，例如我们代码库头文件里面的这个宏：\n#define CHECK(call)\\ {\\ const cudaError_t error=call;\\ if(error!=cudaSuccess)\\ {\\ printf(\u0026#34;ERROR: %s:%d,\u0026#34;,__FILE__,__LINE__);\\ printf(\u0026#34;code:%d,reason:%s\\n\u0026#34;,error,cudaGetErrorString(error));\\ exit(1);\\ }\\ } 就是获得每个函数执行后的返回结果，然后对不成功的信息加以处理，CUDA C 的API每个调用都会返回一个错误代码，这个代码我们就可以好好利用了，当然在release版本中可以去除这部分，但是开发的时候一定要有的。\n编译执行 接下来我们对上面那段向量加法的代码编译执行，观察结果，这里需要一张图，我明天早上连接服务器后运行给出，今晚网太差，所以请见谅， 编译指令：\nnvcc xxxx.cu -o xxxx 此处有图片\n总结 这篇可以说写的比前几篇流畅很多，因为这篇知识很连贯，不想漆面的概览，琐碎的知识，我们明天继续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-2-1-cuda%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B02.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文继续上文介绍CUDA编程模型关于核函数以及错误处理部分\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA核函数，CUDA错误处理\u003c/p\u003e","title":"【CUDA 基础】2.1 CUDA编程模型概述（二）"},{"content":"Abstract: 本文介绍CUDA编程模型的简要结构，包括写一个简单的可执行的CUDA程序，一个正确的CUDA核函数，以及相应的调整设置内存，线程来正确的运行程序。 Keywords: CUDA编程模型，CUDA编程结构，内存管理，线程管理，CUDA核函数，CUDA错误处理\nCUDA编程模型概述（一） 过年了，祝大家新年快乐，新年希望自己学习的东西能都学会 这是一只不爱学习的狗，总看电视！\n编程模型就是告诉我们如何写CUDA程序，如果做过C开发的同学或者其他开发的同学都知道做个完整的项目不只是写代码，还有需求分析，调试，优化，部署等一些列步骤。CUDA平台也提供了着一些列的工具供我们使用，我们这一章主要就是讲解这些工具怎么用，如何编写调试CUDA程序。以及编写两个矩阵运算有关的CUDA应用，以供大家把玩。\nCUDA编程模型概述 CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统），下面的结构图片能形象的表现他们之间的关系： 其中Communication Abstraction是编程模型和编译器，库函数之间的分界线。 可能大家还不太明白编程模型是啥，编程模型可以理解为，我们要用到的语法，内存结构，线程结构等这些我们写程序时我们自己控制的部分，这些部分控制了异构计算设备的工作模式，都是属于编程模型。 GPU中大致可以分为：\n 核函数 内存管理 线程管理 流  等几个关键部分。 以上这些理论同时也适用于其他非CPU+GPU异构的组合。 下面我们会说两个我们GPU架构下特有几个功能：\n 通过组织层次结构在GPU上组织线程的方法 通过组织层次结构在GPU上组织内存的方法  也就是对内存和线程的控制将伴随我们写完前十几篇。 从宏观上我们可以从以下几个环节完成CUDA应用开发：\n 领域层 逻辑层 硬件层  第一步就是在领域层（也就是你所要解决问题的条件）分析数据和函数，以便在并行运行环境中能正确，高效地解决问题。 当分析设计完程序就进入了编程阶段，我们关注点应转向如何组织并发进程，这个阶段要从逻辑层面思考。 CUDA模型主要的一个功能就是线程层结构抽象的概念，以允许控制线程行为。这个抽象为并行变成提供了良好的可扩展性（这个扩展性后面有提到，就是一个CUDA程序可以在不同的GPU机器上运行，即使计算能力不同）。 在硬件层上，通过理解线程如何映射到机器上，能充分帮助我们提高性能。\nCUDA编程结构 一个异构环境，通常有多个CPU多个GPU，他们都通过PCIe总线相互通信，也是通过PCIe总线分隔开的。所以我们要区分一下两种设备的内存：\n 主机：CPU及其内存 设备：GPU及其内存  注意这两个内存从硬件到软件都是隔离的（CUDA6.0 以后支持统一寻址），我们目前先不研究统一寻址，我们现在还是用内存来回拷贝的方法来编写调试程序，以巩固大家对两个内存隔离这个事实的理解。\n一个完整的CUDA应用可能的执行顺序如下图： 从host的串行到调用核函数（核函数被调用后控制马上归还主机线程，也就是在第一个并行代码执行时，很有可能第二段host代码已经开始同步执行了）。\n我们接下来的研究层次是：\n 内存 线程 核函数  启动核函数 编写核函数 验证核函数   错误处理  内存管理 内存管理在传统串行程序是非常常见的，寄存器空间，栈空间内的内存由机器自己管理，堆空间由用户控制分配和释放，CUDA程序同样，只是CUDA提供的API可以分配管理设备上的内存，当然也可以用CDUA管理主机上的内存，主机上的传统标准库也能完成主机内存管理。 下面表格有一些主机API和CUDA C的API的对比：\n   标准C函数 CUDA C 函数 说明     malloc cudaMalloc 内存分配   memcpy cudaMemcpy 内存复制   memset cudaMemset 内存设置   free cudaFree 释放内存    我们先研究最关键的一步，这一步要走总线的（郭德纲：我到底能不能走二环）\ncudaError_t cudaMemcpy(void * dst,const void * src,size_t count, cudaMemcpyKind kind) 这个函数是内存拷贝过程，可以完成以下几种过程（cudaMemcpyKind kind）\n cudaMemcpyHostToHost cudaMemcpyHostToDevice cudaMemcpyDeviceToHost cudaMemcpyDeviceToDevice  这四个过程的方向可以清楚的从字面上看出来，这里就不废话了，如果函数执行成功，则会返回 cudaSuccess 否则返回 cudaErrorMemoryAllocation\n使用下面这个指令可以吧上面的错误代码翻译成详细信息：\nchar* cudaGetErrorString(cudaError_t error) 内存是分层次的，下图可以简单地描述，但是不够准确，后面我们会详细介绍每一个具体的环节：\n共享内存（shared Memory）和全局内存（global Memory）后面我们会特别详细深入的研究，这里我们来个例子，两个向量的加法：\n代码库：https://github.com/Tony-Tan/CUDA_Freshman\n/* * https://github.com/Tony-Tan/CUDA_Freshman * 3_sum_arrays */ #include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026#34;freshman.h\u0026#34; void sumArrays(float * a,float * b,float * res,const int size) { for(int i=0;i\u0026lt;size;i+=4) { res[i]=a[i]+b[i]; res[i+1]=a[i+1]+b[i+1]; res[i+2]=a[i+2]+b[i+2]; res[i+3]=a[i+3]+b[i+3]; } } __global__ void sumArraysGPU(float*a,float*b,float*res) { int i=threadIdx.x; res[i]=a[i]+b[i]; } int main(int argc,char **argv) { int dev = 0; cudaSetDevice(dev); int nElem=32; printf(\u0026#34;Vector size:%d\\n\u0026#34;,nElem); int nByte=sizeof(float)*nElem; float *a_h=(float*)malloc(nByte); float *b_h=(float*)malloc(nByte); float *res_h=(float*)malloc(nByte); float *res_from_gpu_h=(float*)malloc(nByte); memset(res_h,0,nByte); memset(res_from_gpu_h,0,nByte); float *a_d,*b_d,*res_d; CHECK(cudaMalloc((float**)\u0026amp;a_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;b_d,nByte)); CHECK(cudaMalloc((float**)\u0026amp;res_d,nByte)); initialData(a_h,nElem); initialData(b_h,nElem); CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice)); CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice)); dim3 block(nElem); dim3 grid(nElem/block.x); sumArraysGPU\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(a_d,b_d,res_d); printf(\u0026#34;Execution configuration\u0026lt;\u0026lt;\u0026lt;%d,%d\u0026gt;\u0026gt;\u0026gt;\\n\u0026#34;,block.x,grid.x); CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost)); sumArrays(a_h,b_h,res_h,nElem); checkResult(res_h,res_from_gpu_h,nElem); cudaFree(a_d); cudaFree(b_d); cudaFree(res_d); free(a_h); free(b_h); free(res_h); free(res_from_gpu_h); return 0; } 然后使用nvcc编译我们的程序（我们代码库用cmake管理工程，更方便）\n解释下内存管理部分的代码：\ncudaMalloc((float**)\u0026amp;a_d,nByte); 分配设备端的内存空间，为了区分设备和主机端内存，我们可以给变量加后缀或者前缀h_表示host，d_表示device\n一个经常会发生的错误就是混用设备和主机的内存地址！！\n线程管理 当内核函数开始执行，如何组织GPU的线程就变成了最主要的问题了，我们必须明确，一个核函数只能有一个grid，一个grid可以有很多个块，每个块可以有很多的线程，这种分层的组织结构使得我们的并行过程更加自如灵活： 一个线程块block中的线程可以完成下述协作：\n 同步 共享内存  不同块内线程不能相互影响！他们是物理隔离的！ 接下来就是给每个线程一个编号了，我们知道每个线程都执行同样的一段串行代码，那么怎么让这段相同的代码对应不同的数据呢？首先第一步就是让这些线程彼此区分开，才能对应到相应从线程，使得这些线程也能区分自己的数据。如果线程本身没有任何标记，那么没办法确认其行为。 依靠下面两个内置结构体确定线程标号：\n blockIdx（线程块在线程网格内的位置索引） threadIdx（线程在线程块内的位置索引）  注意这里的Idx是index的缩写（我之前一直以为是identity x的缩写），这两个内置结构体基于 uint3 定义，包含三个无符号整数的结构，通过三个字段来指定：\n blockIdx.x blockIdx.y blockIdx.z threadIdx.x threadIdx.y threadIdx.z  上面这两个是坐标，当然我们要有同样对应的两个结构体来保存其范围，也就是blockIdx中三个字段的范围threadIdx中三个字段的范围：\n blockDim gridDim  他们是dim3类型(基于uint3定义的数据结构)的变量，也包含三个字段x,y,z.\n blockDim.x blockDim.y blockDim.z  网格和块的维度一般是二维和三维的，也就是说一个网格通常被分成二维的块，而每个块常被分成三维的线程。 注意：dim3是手工定义的，主机端可见。uint3是设备端在执行的时候可见的，不可以在核函数运行时修改，初始化完成后uint3值就不变了。他们是有区别的！这一点必须要注意。\n下面有一段代码，块的索引和维度：\n/* *1_check_dimension */ #include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;__global__ void checkIndex(void) { printf(\u0026#34;threadIdx:(%d,%d,%d) blockIdx:(%d,%d,%d) blockDim:(%d,%d,%d)\\ gridDim(%d,%d,%d)\\n\u0026#34;,threadIdx.x,threadIdx.y,threadIdx.z, blockIdx.x,blockIdx.y,blockIdx.z,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y,gridDim.z); } int main(int argc,char **argv) { int nElem=6; dim3 block(3); dim3 grid((nElem+block.x-1)/block.x); printf(\u0026#34;grid.x %d grid.y %d grid.z %d\\n\u0026#34;,grid.x,grid.y,grid.z); printf(\u0026#34;block.x %d block.y %d block.z %d\\n\u0026#34;,block.x,block.y,block.z); checkIndex\u0026lt;\u0026lt;\u0026lt;grid,block\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceReset(); return 0; } 可以运行得到不同线程分解方式\n此处有图，明天补上！\n接下来这段代码是检查网格和块的大小的：\n/* *2_grid_block */ #include \u0026lt;cuda_runtime.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;int main(int argc,char ** argv) { int nElem=1024; dim3 block(1024); dim3 grid((nElem-1)/block.x+1); printf(\u0026#34;grid.x %d block.x %d\\n\u0026#34;,grid.x,block.x); block.x=512; grid.x=(nElem-1)/block.x+1; printf(\u0026#34;grid.x %d block.x %d\\n\u0026#34;,grid.x,block.x); block.x=256; grid.x=(nElem-1)/block.x+1; printf(\u0026#34;grid.x %d block.x %d\\n\u0026#34;,grid.x,block.x); block.x=128; grid.x=(nElem-1)/block.x+1; printf(\u0026#34;grid.x %d block.x %d\\n\u0026#34;,grid.x,block.x); cudaDeviceReset(); return 0; } 这里也有图，明天补上\n网格和块的维度存在几个限制因素，块大小主要与可利用的计算资源有关，如寄存器共享内存。 分成网格和块的方式可以使得我们的CUDA程序可以在任意的设备上执行。\n总结 今天先介绍第一部分，主要是从宏观上给出内存，线程以及核函数的相互作用，通过这些特征的相互配合形成可以高速正确执行的CUDA程序，下一篇概述一下核函数的一些特征，待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-2-0-cuda%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B01.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍CUDA编程模型的简要结构，包括写一个简单的可执行的CUDA程序，一个正确的CUDA核函数，以及相应的调整设置内存，线程来正确的运行程序。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CUDA编程模型，CUDA编程结构，内存管理，线程管理，CUDA核函数，CUDA错误处理\u003c/p\u003e","title":"【CUDA 基础】2.0 CUDA编程模型概述（一）"},{"content":"Abstract: 介绍异构计算和CUDA概述，完成GPU输出Hello world！ Keywords: 异构计算，CUDA\n异构计算与CUDA 异构计算 异构计算，首先必须了解什么是异构，不同的计算机架构就是异构，上文书我们讲过计算机架构了，就是为了引出异构的概念，按照指令集划分或者按照内存结构划分，但是我觉得只要两片CPU型号不一样就应该叫异构（这个想法先保留，对错不确定）。 GPU本来的任务是做图形图像的，也就是把数据处理成图形图像，图像有个特点就是并行度很高，基本上一定距离意外的像素点之间的计算是独立的，所以属于并行任务。 GPU之前是不可编程的，或者说不对用户开放的，人家本来是做图形计算控制显示器的，虽然对用户不可编程，但是你只要把硬件卖给了我，就由不得你了，然后就有hacker开始想办法给GPU编程，来帮助他们完成规模较大的运算，于是他们研究着色语言或者图形处理原语来和GPU对话。后来黄老板发现了这个是个新的功能啊，然后就让人开发了一套平台，CUDA，然后深度学习火了，顺带着，CUDA也火到爆炸。 刚刚最新消息，英伟达新版本GPU架构会被命名为Turing，一丝欣慰，发自内心深处地敬那些为世界进步做出了杰出贡献的人们，他们是人类未来的希望。 x86 CPU+GPU的这种异构应该是最常见的，也有CPU+FPGA，CPU+DSP等各种各样的组合，CPU+GPU在每个笔记本或者台式机上都能找到。当然超级计算机大部分也采用异构计算的方式来提高吞吐量。 异构架构虽然比传统的同构架构运算量更大，但是其应用复杂度更高，因为要在两个设备上进行计算，控制，传输，这些都需要人为干预，而同构的架构下，硬件部分自己完成控制，不需要人为设计。\n异构架构 举一个我用的工作站的构成，我使用的是一台 intel i7-4790 CPU加上两台Titan x GPU构成的工作站，GPU插在主板的PCIe卡口上，运行程序的时候，CPU像是一个控制者，指挥两台Titan完成工作后进行汇总，和下一步工作安排，所以CPU我们可以把它看做一个指挥者，主机端，host，而完成大量计算的GPU是我们的计算设备，device。 上面这张图能大致反应CPU和GPU的架构不同。\n 左图：一个四核CPU一般有四个ALU，ALU是完成逻辑计算的核心，也是我们平时说四核八核的核，控制单元，缓存也在片上，DRAM是内存，一般不在片上，CPU通过总线访问内存。 右图：GPU，绿色小方块是ALU，我们注意红色框内的部分SM，这一组ALU公用一个Control单元和Cache，这个部分相当于一个完整的多核CPU，但是不同的是ALU多了，control部分变小，可见计算能力提升了，控制能力减弱了，所以对于控制（逻辑）复杂的程序，一个GPU的SM是没办法和CPU比较的，但是对了逻辑简单，数据量大的任务，GPU更搞笑，并且，注意，一个GPU有好多个SM，而且越来越多。  CPU和GPU之间通过PCIe总线连接，用于传递指令和数据，这部分也是后面要讨论的性能瓶颈之一。 一个异构应用包含两种以上架构，所以代码也包括不止一部分：\n 主机代码 设备代码  主机代码在主机端运行，被编译成主机架构的机器码，设备端的在设备上执行，被编译成设备架构的机器码，所以主机端的机器码和设备端的机器码是隔离的，自己执行自己的，没办法交换执行。 主机端代码主要是控制设备，完成数据传输等控制类工作，设备端主要的任务就是计算。 因为当没有GPU的时候CPU也能完成这些计算，只是速度会慢很多，所以可以把GPU看成CPU的一个加速设备。 NVIDIA目前的计算平台（不是架构）有：\n Tegra Geforce Quadro Tesla  每个平太针对不同的应用场景，比如Tegra用于嵌入式，Geforce是我们平时打游戏用到，Tesla是我们昨天租的那台腾讯云的，主要用于计算。\n上面是根据应用场景分类的几种平台。\n衡量GPU计算能力的主要靠下面两种容量特征：\n CUDA核心数量（越多越好） 内存大小（越大越好）  相应的也有计算能力的性能指标:\n 峰值计算能力 内存带宽  nvidia自己有一套描述GPU计算能力的代码，其名字就是“计算能力”，主要区分不同的架构，早其架构的计算能力不一定比新架构的计算能力强\n   计算能力 架构名     1.x Tesla   2.x Fermi   3.x Kepler   4.x Maxwell   5.x Pascal   6.x Volta    这里的Tesla架构，与上面的Tesla平台不同，不要混淆，一个是平台名字，一个是架构名字\n范例 CPU和GPU相互配合，各有所长，各有所短，不能说GPU就是比CPU强这种幼稚的话： 低并行逻辑复杂的程序适合用CPU 高并行逻辑简单的大数据计算适合GPU\n一个程序可以进行如下分解，串行部分和并行不分： CPU和GPU线程的区别：\n CPU线程是重量级实体，操作系统交替执行线程，线程上下文切换花销很大 GPU线程是轻量级的，GPU应用一般包含成千上万的线程，多数在排队状态，线程之间切换基本没有开销。 CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟，而GPU核则是大量线程，最大幅度提高吞吐量  CUDA：一种异构计算平台 CUDA平台不是单单指软件或者硬件，而是建立在Nvidia GPU上的一整套平台，并扩展出多语言支持 CUDA C 是标准ANSI C语言的扩展，扩展出一些语法和关键字来编写设备端代码，而且CUDA库本身提供了大量API来操作设备完成计算。\n对于API也有两种不同的层次，一种相对交高层，一种相对底层。\n CUDA驱动API CUDA运行时API  驱动API是低级的API，使用相对困难，运行时API是高级API使用简单，其实现基于驱动API。 这两种API是互斥的，也就是你只能用一个，两者之间的函数不可以混合调用，只能用其中的一个库。\n一个CUDA应用通常可以分解为两部分，\n CPU 主机端代码 GPU 设备端代码  CUDA nvcc编译器会自动分离你代码里面的不同部分，如图中主机代码用C写成，使用本地的C语言编译器编译，设备端代码，也就是核函数，用CUDA C编写，通过nvcc编译，链接阶段，在内核程序调用或者明显的GPU设备操作时，添加运行时库。\n注意：核函数是我们后面主要接触的一段代码，就是设备上执行的程序段\nnvcc 是从LLVM开源编译系统为基础开发的。\nCUDA工具箱提供编译器，数学库，调试优化等工具，当然CUDA的文档是相当完善的，可以去查阅，当然在我们基本了解基础结构的情况下，直接上来看文档会变得机械化。\n\u0026ldquo;Hello World!\u0026rdquo; Hello World是所有程序初学者都非常喜欢的，之前GPU是不能printf的，我当时就很懵，GPU是个做显示的设备，为啥不能输出，后来就可以直接在CUDA核里面打印信息了，我们写下面程序\n/* *hello_world.cu */ #include\u0026lt;stdio.h\u0026gt;__global__ void hello_world(void) { printf(\u0026#34;GPU: Hello world!\\n\u0026#34;); } int main(int argc,char **argv) { printf(\u0026#34;CPU: Hello world!\\n\u0026#34;); hello_world\u0026lt;\u0026lt;\u0026lt;1,10\u0026gt;\u0026gt;\u0026gt;(); cudaDeviceReset();//if no this line ,it can not output hello world from gpu  return 0; } 完整代码可以从以下项目中clone：\nhttps://github.com/Tony-Tan/CUDA_Freshman\n简单介绍其中几个关键字\n__global__ 是告诉编译器这个是个可以在设备上执行的核函数\nhello_world\u0026lt;\u0026lt;\u0026lt;1,10\u0026gt;\u0026gt;\u0026gt;(); 这句话C语言中没有\u0026rsquo;\u0026laquo;\u0026lt;\u0026raquo;\u0026gt;\u0026lsquo;是对设备进行配置的参数，也是CUDA扩展出来的部分。\ncudaDeviceReset(); 这句话如果没有，则不能正常的运行，因为这句话包含了隐式同步，GPU和CPU执行程序是异步的，核函数调用后成立刻会到主机线程继续，而不管GPU端核函数是否执行完毕，所以上面的程序就是GPU刚开始执行，CPU已经退出程序了，所以我们要等GPU执行完了，再退出主机线程。 一般CUDA程序分成下面这些步骤：\n 分配GPU内存 拷贝内存到设备 调用CUDA内核函数来执行计算 把计算完成数据拷贝回主机端 内存销毁  上面的hello world只到第三步，没有内存交换。\nCUDA C难么？ CPU与GPU的编程主要区别在于对GPU架构的熟悉程度，理解机器的结构是对编程效率影响非常大的一部分，了解你的机器，才能写出更优美的代码，而目前计算设备的架构决定了局部性将会严重影响效率。 数据局部性分两种\n 空间局部性 时间局部性  这个两个性质告诉我们，当一个数据被使用，其附近的数据将会很快被使用，当一个数据刚被使用，则随着时间继续其被再次使用的可能性降低，数据可能被重复使用。\nCUDA中有两个模型是决定性能的：\n 内存层次结构 线程层次结构  CUDA C写核函数的时候我们只写一小段串行代码，但是这段代码被成千上万的线程执行，所有线程执行的代码都是相同的，CUDA编程模型提供了一个层次化的组织线程，直接影响GPU上的执行顺序。\nCUDA抽象了硬件实现：\n 线程组的层次结构 内存的层次结构 障碍同步  这些都是我们后面要研究的，线程，内存是主要研究的对象，我们能用到的工具相当丰富，NVIDIA为我们提供了：\n Nvidia Nsight集成开发环境 CUDA-GDB 命令行调试器 性能分析可视化工具 CUDA-MEMCHECK工具 GPU设备管理工具  总结 本文从总体上粗略的介绍了CUDA这种高效的异构计算平台，并且概括了我们的将要遇到的苦难和使用到的工具，当我们学会的CUDA，那么编写高效异构计算就会像我们写串行程序一样流畅。 祝大家新年快乐。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-1-1-%E5%BC%82%E6%9E%84%E8%AE%A1%E7%AE%97-cuda.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 介绍异构计算和CUDA概述，完成GPU输出Hello world！\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 异构计算，CUDA\u003c/p\u003e","title":"【CUDA 基础】1.1 异构计算与CUDA"},{"content":"Abstract: 本文从总体上给出了CUDA编程的Big picture，后续所有的文章都在本文的基础上详细展开。 Keywords: 并行计算，串行编程，并行编程，计算机架构，并行性，异构架构，CUDA\n并行计算与计算机架构 听过一句话，电影里面的一句台词，不是名人说的，但我觉得非常值得我思考，“慢就是稳，稳就是快”，我们的发展太快了，以至于出现了各种《21天精通C++》《10天学会机器学习》这类东西。 稳住吧，打好基础，才有机会去更好的环境。\nBig Picture 我们学习CUDA主要参考《CUDA C编程权威指南》我们的博客也基本按照书中的章节进行。 结构： CUDA想要运行起来并不困难，但是想要写得好，真的需要研究一下，某乎上各路大牛给出的建议是看CUDA的官方文档，我之前也是过了一遍文档，但是文档教会你更多的是如何写代码，而没有讲解详细的硬件结构（可能在别的文档中，我只看了编程指导），我们学习编程应该同时理解语言，编程模型，硬件执行模型，以及优化方法，单纯的学会写代码，能运行，这是培训班的节奏。 还记得峰哥的话，知道编译原理和操作系统（软硬件），什么语言都一样。 读这个系列的文章需要以下知识：\n C/C++ 编程经验，这个不用说，如果C都没学会就要来CUDA，我觉得不理智，根基不稳，也是我一贯所反对的 本系列是Freshman，后面会有Junior，主要内容肯定有所不同，目前准备的是Freshman 主要介绍基础知识，包括硬件基础，编程模型，基本性能方面的考察，和简单的优化（包括内存等），以及项目实际中的一些技巧；Junior部分主要介绍更高级的性能优化技巧，比如PTX，更高级的内存处理等；优化空间最大的是并行算法的设计，当然不在本系列所讨论的范围内，那是另一个专题了。  并行计算 我们的计算机从最早的埃尼阿克到现在的各种超算，都是为了应用而产生的，软件和硬件相互刺激而相互进步，并行计算也是这样产生的，我们最早的计算机肯定不是并行的，但是可以做成多线程的，因为当时一个CPU只有一个核，所以不可能一个核同时执行两个计算，后来我们的应用逐步要求计算量越来越高，所以单核的计算速度也在逐步上升，后来大规模并行应用产生了，我们迫切的需要能够同时处理很多数据的机器，比如图像处理，以及处理大规模的同时访问的服务器后台。 并行计算其实设计到两个不同的技术领域：\n 计算机架构（硬件） 并行程序设计（软件）  这两个很好理解，一个生产工具，一个用工具产生各种不同应用。 硬件主要的目标就是为软件提供更快的计算速度，更低的性能功耗比，硬件结构上支持更快的并行。 软件的主要目的是使用当前的硬件压榨出最高的性能，给应用提供更稳定快速的计算结果。 我们传统的计算机结构一般是哈佛体系结构（后来演变出冯·诺依曼结构）主要分成三部分：\n 内存（指令内存，数据内存） 中央处理单元（控制单元和算数逻辑单元） 输入、输出接口  后面的冯诺依曼结构就把数据和指令都当做数据来处理了，这里就不再介绍了，再次安利《深入理解计算机系统》这本书，里面可以找到相关知识。 写并行和串行的最大区别就是，写串行程序可能不需要学习不同的硬件平台，但是写并行程序就需要对硬件有一定的了解了。\n并行性 写并行程序主要是分解任务，我们一般把一个程序看成是指令和数据的组合，当然并行也可以分为这两种：\n 指令并行 数据并行  我们的任务更加关注数据并行，所以我们的主要任务是分析数据的相关性，哪些可以并行，哪些不能不行。 如果你对并行不太了解，可以先去学习学习pThread和OpenMP，了解下载多核CPU上是怎么并行的，比如把用openmp把for并行。 任务并行多出现在各种管理系统，比如我们天天用的支付系统，基本上每时每刻都有很多人在同时使用，这时候就需要后台的处理能够并行执行这些请求，不然全国人民排队，那就比春运还热闹了。 我们研究的是大规模数据计算，计算过程比较单一（不同的数据基本用相同的计算过程）但是数据非常多，所以我们主要是数据并行，分析好数据的相关性，决定了我们的程序设计。 CUDA非常适合数据并行 数据并行程序设计，第一步就是把数据依据线程进行划分\n 块划分，把一整块数据切成小块，每个小块随机的划分给一个线程，每个块的执行顺序随机（关于线程的概念可以去看《深入理解计算机系统》）     thread 1 2 3 4 5     block 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15    周期划分，线程按照顺序处理相邻的数据块，每个线程处理多个数据块，比如我们有五个线程，线程1执行块1，线程2执行块2\u0026hellip;..线程5执行块5，线程1执行块6     thread 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5     block 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15    下面是示意图，注意颜色相同的块使用的同一个线程，从执行顺序上看如下： 下面是数据集上的划分上看： 不同的数据划分严重影响程序性能，所以针对不同的问题和不同计算机结构，我们要通过和理论和试验共同来决定最终最优的数据划分。\n计算机架构 Flynn\u0026rsquo;s Taxonomy 划分不同计算机结构的方法有很多，广泛使用的一种被称为佛林分类法Flynn\u0026rsquo;s Taxonomy，他根据指令和数据进入CPU的方式分类，分为以下四类： 分别以数据和指令进行分析：\n 单指令单数据SISD（传统串行计算机，386） 单指令多数据SIMD（并行架构，比如向量机，所有核心指令唯一，但是数据不同，现在CPU基本都有这类的向量指令） 多指令单数据MISD（少见，多个指令围殴一个数据） 多指令多数据MIMD（并行架构，多核心，多指令，异步处理多个数据流，从而实现空间上的并行，MIMD多数情况下包含SIMD，就是MIMD有很多计算核，计算核支持SIMD）  为了提高并行的计算能力，我们要从架构上实现下面这些性能提升：\n 降低延迟 提高带宽 提高吞吐量  延迟是指操作从开始到结束所需要的时间，一般用微秒计算，延迟越低越好。 带宽是单位时间内处理的数据量，一般用MB/s或者GB/s表示。 吞吐量是单位时间内成功处理的运算数量，一般用gflops来表示（十亿次浮点计算），吞吐量和延迟有一定关系，都是反应计算速度的，一个是时间除以运算次数，得到的是单位次数用的时间\u0026ndash;延迟，一个是运算次数除以时间，得到的是单位时间执行次数\u0026ndash;吞吐量。\n根据内存划分 计算机架构也可以根据内存进行划分：\n 分布式内存的多节点系统 共享内存的多处理器系统  第一个更大，通常叫做集群，就是一个机房好多机箱，每个机箱都有内存处理器电源等一些列硬件，通过网络互动，这样组成的就是分布式。 第二个是单个主板有多个处理器，他们共享相同的主板上的内存，内存寻址空间相同，通过PCIe和内存互动。 多个处理器可以分多片处理器，和单片多核（众核many-core），也就是有些主板上挂了好多片处理器，也有的是一个主板上就一个处理器，但是这个处理器里面有几百个核。 GPU就属于众核系统。当然现在CPU也都是多核的了，但是他们还是有很大区别的：\n CPU适合执行复杂的逻辑，比如多分支，其核心比较重（复杂） GPU适合执行简单的逻辑，大量的数据计算，其吞吐量更高，但是核心比较轻（结构简单）  总结 本文主要介绍了下计算机架构的划分和并行计算的基础简单介绍，后面我们继续介绍异构和CUDA\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-1-0-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%9E%B6%E6%9E%84.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文从总体上给出了CUDA编程的Big picture，后续所有的文章都在本文的基础上详细展开。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 并行计算，串行编程，并行编程，计算机架构，并行性，异构架构，CUDA\u003c/p\u003e","title":"【CUDA 基础】1.0 并行计算与计算机架构"},{"content":"Abstract: 本文主要为不支持CUDA的电脑用户提供一个可供选择的CUDA学习环境 Keywords: 腾讯云,CUDA,GPU云\n腾讯云CUDA环境搭建 Macbook 我没有意思想要炫耀macbook（前几年确实是高端货），这玩意去星巴克，一桌子都是，但那里用来写程序的不多，最近几年Mac在中国普及度非常高，以前见不到几个，现在真的是到处都是，对于开发者，大部分用来开发IOS应用，也有小部分用于开发其他，我对Macbook有着更多的恩怨情仇，这里就不说了。 我为什么选择Macbook？ 我的第一个mac里是有NVIDIA显卡的，为了这个mac我算是搭上了身家性命了，用mac研究了《三十天自制操作系统》，刚萨雷斯的数字图像处理，写了一百多篇CSDN博客（虽然质量不太高），当时坚持要买带显卡的，就为了学习cuda，然而，我已经换了第二台macbook了，这才刚刚开始深入研究CUDA，Macbook用久了真的感觉效率很高，linux桌面版说实话，我感觉稳定性并没有网上宣传的那么高（网上多半是说服务器版本的）桌面版本的谁用谁知道，天天给我弹出内核错误的消息，而macbook基本没啥问题，内核又是类unix，所以所有工具和linux很相似，但又更顺畅，当然价格也要高出很多。 但是，macbook上已经没有nvidia显卡了，肿么办？ 我的办法是又买了一台有nvidia显卡的笔记本电脑，事实证明，脑残了，Lenovo下的linux做日常工作很不顺利，windows又有很多工具用不爽（比如我们的后面要用到的cuda的一些调试工具）。 就在决定过年带哪个电脑的时候，我想起来了，可以找找有没有可以用的云服务器，这样，我们就能愉快的在mac上编码，然后远程调试运行就可以了，于是我先写了一篇搭建环境的文章，来供大家参考\n云 这个云那个云多到不行，提供GPU服务的也越来越多，第一选择本来是google云，但是google那边要申请gpu资源，还有先交35 USD 的费用，还要翻墙，然后我觉果断的拒绝了，同样的功能，腾讯这边能便宜不少，于是我开始配置在腾讯云上的环境。 首先就是买买买买，贵不贵？不便宜，但是看起来不错，每小时十几块钱（前几天去了一次网吧，我还以为两三块钱一个小时，结果服务员小妹妹说三十，一脸😳）\n腾讯云GPU购买 按照下面这个基本过程操作（2018年1月完成以下操作，如果过段时间腾讯界面修改，那么以当时的为准） 这里必须要注意一下，因为有的区没有GPU服务器，所以一定要选一个有的地区买，我之前就傻傻的在北京区折腾半天，我以为从“GPU云服务器”那个按钮点进来的，就每个机器都有GPU呢。。结果折腾半天也安装不上CUDA，后来发现，naive啊！ 选一个你能接受的，我们目前这点小知识，最最最简陋的就够了，后面多GPU的试验估计我已经回到公司了，机器多得是，哈哈 点击服务市场，里面有带有CUDA的操作系统（搜索就可以了），我们选ubuntu的CUDA7.5。这次我们会自己制作一个镜像，把我们安装的一些东西打包起来，下次再买的时候可以直接安装我们自己的镜像。就不用配置环境了，安装好的全家桶。 配置一些基本信息，包括登录密码，记住开启ssh 22端口，不然没办法登录了 等待云端配置。 配置完成，注意你的公网IP，我们一会儿要用这个地址登录。\n连接到云 简单的ssh登录，第一次要确认输入个yes，然后输入密码就可以了 登录成功后我们要做的就是安装CUDA和CMAKE，这两个是我们后面整个工程都需要的。\n安装CUDA7.5 安装CUDA比较简单，因为操作系统已经安装好NVIDIA驱动了，我们直接输入\nnvcc 就会有提示让你安装CUDA-Tookite，按照他的提示安装就好了。\n安装CMAKE  去官网下载linux版的源代码 解压源代码，进入源代码目录， 执行：./bootstrap 执行：make 执行：sudo make install  验证环境 验证环境可以很多中方法，编译CUDA例子，运行也可以，用自己的也可以，我们用我们自己写的小代码来测试下，看运行是否成功 先用nvidia-smi命令来看一下驱动是否正确安装 从我们的代码库中clone一份我们的学习cuda的代码，cmake一下\ngit clone https://github.com/Tony-Tan/CUDA_Freshman.git 别忘了给我来个星星 看起来一帆风顺，继续下一步，cmake完后make一下，所有都生成成功了，执行一个例子，发现运行成功了，获取到的GPU型号是Tesla M40 24G 足够我们干各种各样的试验了\n制作一个镜像 我们是按照小时租的服务器，所以我们争取在线下写的正确些，然后买了服务器以后就马上做各种试验观察结果，因为服务器是按照小时收费的，我们尽可能的要快一些，所以，如果每次都配置环境，浪费时间太多，我们用搭建好的环境做个镜像出来，下次再买服务器的时候可以直接安装，美滋滋 更多里面有个只做镜像的功能，写好描述就可以，别到时候自己都忘了\n及时销毁 我们是按照小时收费的，所以用完了以后马上去销毁掉 更多操作里面有销毁，等你确定备份好镜像了，可以关机了，就销毁就可以了。\n总结 好久没写这种流水账一样的文章，感觉写起来确实很流畅，没有啥精神压力，这篇文章只是写给那些没有nvidia设备还想学习CUDA的同学们，有nvidia显卡的同学可以直接在本地装CUDA和CMAKE方法类似。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/cuda/cuda-f-0-0-tencent-gpu-cloud.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要为不支持CUDA的电脑用户提供一个可供选择的CUDA学习环境\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 腾讯云,CUDA,GPU云\u003c/p\u003e","title":"【CUDA 基础】0.0 腾讯云CUDA环境搭建"},{"content":"Abstract: 本文承接上文，对于二维联合分布，如何求出二维变量中一个变量的一个分布，也就是标题所说的边缘分布；以及对独立随机变量的讨论。 Keywords: Marginal p.f.,Marginal p.d.f.,Independent\n边缘分布 今天这篇可能是农历新年前最后一篇关于数学的博客了，过年期间争取把CUDA系列的写出来，大家有兴趣的可以关注一下，过年本来是个休息的时间，但是说实话，现在真的很讨厌过年，尤其是那些关心你生活的所谓亲戚们，可能是变向找平衡，或者是炫耀，具体案例我不说，已经烂大街了，只是觉得有点恶心，人们在内心是相互攀比相互较量，表面还要装作一团和气，然后各种映射暗示你不如他的地方。 过年就应该是一家团聚，相互祝福，相互鼓励的。 真的想找个没人的地方看一春节书，改变不了就是试着逃避吧。 想要逃出生天，好好学习，可能还有机会。\n边缘分布 Marginal Distribution 我们继续我们的概率论，我们已经经历了概率论的变化过程是：从试验到样本空间，样本空间到事件，事件到概率（复合事件的概率，包括条件事件，独立事件等等扩展情况），样本空间到随机变量，随机变量的离散概率、连续概率，描述随机变量概率的工具（p.f.,p.d.f.,c.d.f.），然后随机变量被扩展为二维（离散的，连续的，混合的），今天我们在二维联合分布的情况下，推出今天的主要讨论目标：Marginal Distribution(边缘分布) 上文我们曾有一个小伏笔，我们想知道联合的p.f.或者p.d.f.怎么通过每个变量的p.f.或者p.d.f.求出的；或者我们反过来，如何通过联合的p.f.或者p.d.f.来得到每个变量自己的（一维的）p.f.或者p.d.f.。 这就是我们要说的今天的边缘分布，适用于p.d.f.或者p.d.或者c.d.f.\n求边缘概率（分布）函数 Deriving a Marginal p.f. or a Marginal p.d.f. 离散分布边缘概率函数 Discrete 首先我们来从简单的二维联合分布来看，从有限的离散二维联合分布，举个🌰 ： 对于一个二维分布，上文说得到的扔骰子和丢硬币，下面我们的例子硬币和骰子都不是均匀的，所以产生的概率分布与原始例子不太一样：\n    Head Tail $f_1(x)$     1 $\\frac{1}{24}$ $\\frac{3}{24}$ $\\frac{1}{6}$   2 $\\frac{2}{24}$ $\\frac{2}{24}$ $\\frac{1}{6}$   3 $\\frac{3}{24}$ $\\frac{1}{24}$ $\\frac{1}{6}$   4 $\\frac{1}{24}$ $\\frac{3}{24}$ $\\frac{1}{6}$   5 $\\frac{1}{24}$ $\\frac{3}{24}$ $\\frac{1}{6}$   6 $\\frac{2}{24}$ $\\frac{2}{24}$ $\\frac{1}{6}$   $f_2(y)$ $\\frac{5}{12}$ $\\frac{7}{12}$ 1    陈希孺先生在书中说，简单的来说，边缘概率就是上面这个表的边缘，下面的一行，右边的一列所表现出来的分布，下面的一行，就是变量y，试验（丢硬币的）的边缘分布，对应的右侧一列对应的是随机变量x对应的试验是扔骰子。\n Definition Marginal c.d.f./p.f./p.d.f. Suppose that X and Y have a joint distribution.The c.d.f. of X derived by theorem 3.4.5. is called the marginal c.d.f. of X.Similarly,the p.f. or p.d.f. of X associated with the marginal c.d.f. of X is called the marginal p.f. or marginal p.d.f. of X\n 写书的好处就是可以省略掉很多前面已经写了的东西，然后一个指针指过去就可以了，写博客也可以，就像这样3-4，但是我们还是重写一遍定理3.4.5：\nTheorem Let $X$ and $Y$ have a joint c.d.f. $F$.The c.d.f. $F_1$ of just the single random variable $X$ can be derived from the joint c.d.f. $F$ as $F_1(x)=lim_{y\\to \\infty}F(x,y)$.Similarly,the c.d.f. $F_2$ of $Y$ equals $F_2(y)=lim_{x\\to \\infty}F(x,y)$ ,for $0\u0026lt;y\\leq \\infty$\n定义3.4.5告诉我们如何把一个联合c.d.f.拆分出来，通过把一个变量写成无穷大，来得到另一个变量的c.d.f.，这个过程就是一个边缘c.d.f.的过程，对于p.f.和p.d.f.同样的操作得到的将会是边缘p.f.或者边缘p.d.f.\n那么我们怎么得到边缘p.f.或者边缘p.d.f呢？\n Theorem If $X$ and $Y$ have a discrete joint distribution for thich the joint p.f. is $f$,then the marginal p.f. $f_1$ of $X$ is $$ f_1(x)=\\sum_{\\text{All } y}f(x,y). $$ Similarly,the marginal p.f. $f_2$ of $Y$ is $$ f_2(y)=\\sum_{\\text{All } x}f(x,y). $$\n 这个定理可以用来计算一个二维离散随机变量的联合分布如何计算出两个离散变量分别的离散分布，而书上给出的证明也过于感性化，作者通过一副和我们上一篇类似的图来说明 $f_1(x)$ 是在联合分布概率函数中，某一个变量不变（$x$） 而另一个变量取和的这种方式，来得到的边缘分布（x的一个一维分布概率函数）。如果从最初的关于概率的定义出发，我们也能得到类似的结论，二维随机变量对应的可以能是一个试验的二维结果，二维结果就对应了两个维度的可能性，而这个二维联合p.f.就是描述这个二维结果发生的概率的，当一个维度的可能性被消除（也就是二维变一维：$(x,y)\\to x$）那么就应该把同一个 $x$ 下的所有不同的 $y$ 都加起来，因为在二维联合分布情况下 $(x_i,y_k)$ 和 $(x_i,y_l)$ 是不相关的（$k\\neq l$）所以可以进行加法。\n连续分布边缘概率密度函数 Continuous 接着我们看连续情况下的：\n Theorem If X and Y have a continuous joint distribution with joint p.d.f $f$ then the marginal p.d.f. f_1 of X is $$ f_1(x)=\\int_{-\\infty}^{\\infty}f(x,y)dy \\text{ for }-\\infty\u0026lt;x\u0026lt;\\infty $$ Similarly,the marginal p.d.f.f_2 of Y is $$ f_2(y)=\\int_{-\\infty}^{\\infty}f(x,y)dx \\text{ for }-\\infty\u0026lt;y\u0026lt;\\infty $$\n 这个证明方法有点意思，用到了c.d.f.的定义和c.d.f.的边缘分布的定义，我们假设这个二维连续联合分布的p.d.f是$f(x,y)$ 那么根据c.d.f.的定义$F(x,y)$ 为下面的式子： $$ F(x,y)=\\int^{x}{-\\infty}\\int^{y}{-\\infty}f(x,y)dydx $$ 没问题，当我们想得到x的边缘c.d.f.，我们根据定理，要把$y\\to \\infty$ $$ F(x)=\\int^{x}{-\\infty}\\int^{\\infty}{-\\infty}f(x,y)dydx $$ F(x)和$f(x)$ 是导数与反导数的关系，所以，F(x) 求导可以得到： $$ f(x)=\\frac{dF(x)}{dx}=\\frac{d\\int^{x}{-\\infty}\\int^{\\infty}{-\\infty}f(x,y)dydx}{dx}\\ \\text{set :}\\int^{x}{-\\infty}[\\int^{\\infty}{-\\infty}f(x,y)dy]dx=\\int^{x}{-\\infty}g(x)dx\\ \\text{so }f(x)=g(x)=\\int^{\\infty}{-\\infty}f(x,y)dy $$ Q.E.D\n证明过程用到了单变量c.d.f.和p.d.f.之间的关系，上面提到的定理3.4.5（莫名其妙的编号来自原书），以及微积分基本定理，这个证明过程如上所述，简单粗暴（与书本给出的证明不太一样，如有问题请及时指出，谢谢）。\n有了上面这些个公理，使用过程多半就变成了春计算，公理背后的逻辑就是上面我们的两个证明，一个图解，一个分析，虽然不太喜欢做计算，但是我们还是写个例子吧： 假设一个二维联合分布满足： $$ f(x,y)= \\begin{cases} \\frac{21}{4}x^2y\u0026amp;\\text{for }x^2\\leq y\\leq 1\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ 求 $x$ 的边缘p.d.f. $$ f_1(x)=\\int^{\\infty}{-\\infty}f(x,y)dy=\\int^{1}{x^2}\\frac{21}{4}x^2ydy=(\\frac{21}{8})x^2(1-x^4) $$\n混合分布 Mixed  Theorem Let f be the joint p.f./p.d.f. of X and Y,with X discrete and Y continuous.Then the marginal p.f. of X is : $$ f_1(x)=Pr(X=x)=\\int^{\\infty}{-\\infty}f(x,y)dy \\quad\\text{for all }x $$ and the marginal p.d.f. of Y is: $$ f_2(y)=\\sum{x}f(x,y) \\text{ for }-\\infty\u0026lt;y\u0026lt;\\infty $$\n 证明就是前两个的集合版本。例子也不再废话了，也是上面两个例子的结合，我们要进入下面这个比较重要的主题了，关于随机变量的独立性。\n随机变量的独立 Independent Random Variables 我们前面研究过事件的独立性，事件独立不是不相关，而是其概率满足一定关系，我们称之为独立，也就是事件发生与否不会影响另一个事件，我们称之为相互独立，而我们通过随机变量把样本空间（也包括事件）过渡到实数，那么这些实数之间的概率相互独立是怎么回事呢？ 对于离散随机变量，其独立性和事件的独立性非常相似：\n$$ Pr((x,y))=Pr(x)Pr(y) $$\n但是对于连续变量，这个就有问题了，因为连续变量的单一位置的概率是0，中已经做了明确的介绍，所以我们要有一片区域代替一个点。 先来看一个总体的定义：\n Definition Independent Random Variables.It is said that two random variables $X$ and $Y$ are independent if,for every two sets $A$ and $B$ of real numbers such that ${x\\in A}$ and ${Y\\in B }$ are events, $$ Pr(X\\in A \\text{ and } Y \\in B)=Pr(X\\in A)Pr(Y\\in B) $$\n 从事件的角度去看这个定义很明确了我们假设事件 $E={X\\in A}$ 事件 $F={Y\\in B}$ 这两个事件独立的条件是当且仅当： $$ Pr(E\\cap F)=Pr(E)Pr(F) $$\n上面的定义并没有规定A和B的选择方法，那么我们可以规定如下的规则： $$ Pr(X\\in{X\u0026lt;x})\\ Pr(Y\\in{Y\u0026lt;y})\\ \\text{so we have:}\\ Pr(X\\leq x\\text{ and }Y\\leq y)=Pr(X\\leq x)Pr(Y\\leq y) $$\n哈哈哈，看出来上面藏了个谁了么？没有？仔细看？还没有？再仔细看。\n判定随机变量独立方法 1  Theorem Let the joint c.d.f. of $X$ and $Y$ be $F$ let the marginal c.d.f. of $X$ be $F_1$ and let the marginal c.d.f. of $Y$ be $F_2$ Then $X$ and $Y$ are independent if and only if ,for all real numbers $x$ and $y$ , $F(x,y)=F_1(x)F_2(y)$\n 怎么样，加上这个定理是不是明显一点了呢？ 于是我们得到了一个确定随机变量是否独立的方法，随机变量独立的充分必要条件就是其c.d.f.满足乘法关系，怎么来的？随机变量独立的定义在上面规定的。\n判定随机变量独立方法 2 上面的定理很简单，不需要解读，我们继续看下面的定理通过joint p.d.f., joint p.f. 或者是joint p.f./p.d.f. 确定随机变量独立：\n Theorem Suppose that $X$ and $Y$ are random varibales that have a joint p.f.,p.d.f. or p.f./p.d.f. $f$ ,Then $X$ and $Y$ will be independent if and only if $f$ can be represented in the following form for $-\\infty\u0026lt;x\u0026lt;\\infty$ and $-\\infty\u0026lt;y\u0026lt;\\infty$ : $$ f(x,y)=h_1(x)h_2(y) $$\n 这个定理给出了用pdf或者pf或者混合pf和pdf确定随机变量独立的方法，我们这里证明最复杂的情况，一个是离散的随机变量x，一个是连续的随机变量y，证明过程分为两部分，\n  \u0026ldquo;if\u0026rdquo; part: $$ \\text{ hold: } f(x,y)=h_1(x)h_2(y) \\ \\text{marginal p.d.f. of }x\\text{ : }f_1(x)=\\int^{\\infty}{-\\infty}h_1(x)h_2(y)dy=c_1h_1(x) $$ $c_1$ 是一个非负函数积分的结果，所以我们保证其是非负数，同时我们确定其有限，并且不是0，那么我们就能得到： $$ h_1(x)=\\frac{f_1(x)}{c_1} $$ 与此相似，我们用求和的方法求离散的边缘分布： $$ f_2(y)=\\sum{x}f(x,y)=\\sum_{\\text{All }x}h_1(x)h_2(y)=h_2(y)\\sum_{x}\\frac{1}{c_1}f_1(x)=\\frac{1}{c}h_2(y) $$  我们分别求出了两个随机变量的概率密度函数和概率函数（这两个函数具有任一性，也就是可以代表整个pdf或者pf族），所以我们可以得到结论： $$ f(x,y)=\\frac{f_1(x)}{c_1} c_1 f_2(y)=f_1(x)f_2(y) $$\n结合我们关于随机变量的定义(mixed 版本)： $$ Pr(X\\in A \\text{ and } Y \\in B)=\\sum_{X \\in A}\\int_Bf(x,y)dy\\ =\\int_B\\sum_{X\\in A}f_1(x)f_2(y)dy=\\sum_{X\\in A}f_1(x)\\int_Bf_2(y)dy $$ 所以根据定义，随机变量独立！\n  \u0026ldquo;only if\u0026rdquo; part: we assume that X and Y are independent: $$ Pr(X\\in A \\text{ and } Y \\in B)\\ =\\sum_{X\\in A}f_1(x)\\int_Bf_2(y)dy\\ =\\int_B\\sum_{X\\in A}f_1(x)f_2(y)dy $$ 经过上面的分解，我们就能得到两个随机变量的概率 $$ Pr(A)=h_1(x)=\\sum_{X\\in A}f_1(x)\\ Pr(B)=h_2(y)=\\int_Bf_2(y)dy $$ 这部分证明过程用到了上一篇混合随机变量的联合概率密度函数( $Pr(X\\in A \\text{ and } Y \\in B)=\\sum_{X\\in A}f_1(x)\\int_Bf_2(y)dy$ ) 然后通过微积分的基本过程得到了结论  Q.E.D\n 所以我们在平时思考的时候，当我们考虑两个随机变量是否独立的时候，只要思考当一个概率密度函数（概率函数）从未知变成已知的时候时，另一个是否受到影响，反之亦然。\n举个例子： 假设两个连续变量满足如下分布 $$ f(x,y)= \\begin{cases} kx^2y^2\u0026amp;\\text{for }x^2+y^2\\leq 1\\ 0\u0026amp;\\text{otherwise} \\end{cases} $$ X,Y是否独立。 明显这个是不独立的，为啥，我们发现，$f_1(0.9)\\neq 0$ 而 $f_2(0.9)\\neq 0$ 但是 $f_1(0.9)f_2(0.9)=0$ 所以我们可以确定其并不独立。\n我们看出这个例子的定义域是个圆心在原点，半径为1的圆，那么这个定义域和独立与否是否有关系呢？ 于是我们引出下面的定理：\n判定随机变量独立方法 3  Theorem Let $X$ and $Y$ have a continuous joint distribution .Suppose that ${(x,y):f(x,y)\u0026gt;0}$ is a rectangular region $R$ (possibly unbounded) with sides (if any) parallel to the coordinate axes.Then X and Y are independent if and only if $f(x,y)=h_1(x)h_2(y)$ holds for all $(x,y)\\in \\Re^2$\n 这个定理对于上面例题是个很好的总结，当随机变量范围不是一个边和数轴平行的矩形时（有无边界无所谓，也就是开区间还是闭区间无所谓）没有可能独立，肯定相关，只有当其实矩形的时候，才有可能是独立的，是矩形，同时满足上面的给出的判断定理，就能确定是否独立了。 这个可以看做一个判定方法，也可以说是2的一个扩展\n判定随机变量独立方法 4 对于形式上就是分离的函数，其随机变量独立： $$ f(x,y)=e^xy\\quad \\text{ for } -\\infty\u0026lt;x\u0026lt;0 \\text{ and }0\u0026lt;y\u0026lt;1 $$ 这个x和y的函数可以一眼就分离开了，同时定义域是矩形，而且积分是1（我自己编的例子，有问题请留言） 从定义上证明也很好证明，我们的定义是： $$ Pr(X\\in A \\text{ and } Y \\in B)=Pr(X\\in A)Pr(Y\\in B) $$\n那么我们就把事件A设置成 $Pr(A)=e^x$ 事件B设置成 $Pr(B)=y$ 这就明显了， $$ Pr(X\\in {X\u0026lt;e^x} \\text{ and } Y \\in {Y\u0026lt;y})=e^xy $$\n总结 本文知识点主要在随机变量的独立上，边缘概率比较直观，后面的条件概率和边缘概率有点相似，但是更复杂更有用一些，我们年后继续，祝大家新年快乐！ 待续。。。。。\n","permalink":"https://go.face2ai.com/math/math-probability-3-5-marginal-distributions.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文承接上文，对于二维联合分布，如何求出二维变量中一个变量的一个分布，也就是标题所说的边缘分布；以及对独立随机变量的讨论。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Marginal p.f.,Marginal p.d.f.,Independent\u003c/p\u003e","title":"【概率论】3-5:边缘分布(Marginal Distribution)"},{"content":"Abstract: 本文主要介绍双变量的分布情况，以及其中的一些有用的性质 Keywords: Discrete Joint Distribution,Continuous Joint Distribution,Mixed Bivariate Distribution,Bivariate Cumulative Distribution Functions\n二维分布 今天的废话还是想劝诫自己也劝诫那些看过我的博客和希望学有所成的人，学习也好，开发也好，投资也好，最忌急功近利，想一年翻十倍，有可能，但是概率小到报表，到后面学到具体的一些分布的时候，就知道这个方差有多大，人生需要经历，经历了就明白了，感谢周围所有的人，帮助过我们的，让我们受到人们，他们做的所有都在促进我们成长，一步一个脚印，慢就是稳，稳就是快。 本文我们介绍两个变量的分布，数学的两大分支，理论数学和应用数学，这两者没有明确的界限，不管什么理论，只有应用了以后才能推动社会进步，同时存进新的理论形成，所以，两者没有什么主次，都是重要的，概率论的应用性极强，我们不断学习，知识逐渐变得复杂，这个目的不是为了复杂而复杂，而是为了接近真相，我们生活中很少简单单变量的事件，出了扔硬币，甚至中国国粹打麻将扔骰子都是一次扔两个，那么就有了两个随机变量组合的问题，当然你也可以发明一个一边扔骰子一边扔硬币的游戏。 我们不断地复杂模型，就是为了去更好的描述实际情况，而简单的模型所能模拟的情况，多半是我们自己为了附会模型而创造的。\n联合分布 Joint Distribution 对于双变量，我们有下面这些组合： $$ \\text{Bivariate}= \\begin{cases} Discrete \u0026amp; \\text{Discrete,Discrete}\\ Continuous \u0026amp; \\text{Continuous,Continuous}\\ Hybrid \u0026amp; \\text{Discrete,Continuous} \\end{cases} $$\n学习概率从一开始我们就是按照先离散后连续的方式逐步进行，像上台阶一样，那么我们二维随机变量也从离散开始。\n Definition Joint/Bivariate Distribution:Let $X$ and $Y$ be random varibales.The joint distribution or bivariate distribution of $X$ and $Y$ is the collection of all probabilities of the form $Pr[(X,Y)\\in C]$ for all sets $C$ of pairs of real numbers such that ${(X,Y)\\in C}$ is an event\n 再次回忆随机变量（函数），和样本空间（集合）之间的关系，随机变量是一个函数，把样本空间上的样本点映射到实数，那么如果有两个样本空间，那么这两个集合的笛卡尔积将会组成一个新的样本空间，这个新的样本空间产生的随机变量以及随机变量的分布就是联合分布\n离散联合分布 Discrete Joint Distribution  Definition Joint Distribution:Let X and Y be random variables,and consider the ordered pair(X,Y).If there are only finitely or at most countably many different possible values (x,y) for the pair (X,Y),then we say that x and Y have a discrete joint distribution\n 这里面又提到了可数的这个概念，可数无限的，和有限的样本空间通过笛卡尔积得到的新的有限的，或者可数的样本空间就是原始两个样本空间的联合分布。\n举个🌰 ： 扔一个均匀的骰子和一个均匀的硬币，将他们的结果组合起来会有下面这些结果组合： 硬币分为head 和tail，骰子是1~6点，其联合分布的结果如下表：\n    Head Tail     1 $\\frac{1}{12}$ $\\frac{1}{12}$   2 $\\frac{1}{12}$ $\\frac{1}{12}$   3 $\\frac{1}{12}$ $\\frac{1}{12}$   4 $\\frac{1}{12}$ $\\frac{1}{12}$   5 $\\frac{1}{12}$ $\\frac{1}{12}$   6 $\\frac{1}{12}$ $\\frac{1}{12}$     Theorem: Suppose that two random variable X and Y each have a discrete distribution.Then X and Y have a discrete joint distribution\n 证明主要分为两部分\n 如果两个样本空间有限，那么其笛卡尔积有限，这个是集合论中已经明确的结论了 如果两个样本空间可数无限，那么气笛卡尔积也是可数无限的，这个相关证明也在集合论中，这里不再证明   Definition Joint Probability Function,p.f. The joint probability function,or the joint p.f. of X and Y is defined as the function f such that for every point (x,y) in xy-plane: $$ f(x,y)=Pr(X=x\\text{ and } Y=y) $$\n 这个定义是定义联合概率函数，对应一维随机变量里的概率函数，这个函数输入二维随机变量，产生一个实数概率值：$X\\times Y\\to \\Re$\n Theorem Let $X$ and $Y$ have a discrete joint distribution.If $(x,y)$ is not one of the possible values of the pair $(X,Y)$ the $f(x,y)=0$ .Also, $$ \\sum_{\\text{All }(x,y)}f(x,y) $$ Finally,for each set $C$ of ordered pairs $$ Pr[(X,Y)\\in C]=\\sum_{(x,y)\\in C}f(x,y) $$\n 接着我们定义的是离散联合分布，以及通过新的规则 $(X,Y)\\in C$ 产生一个新的分布。\n还是上面第一个例子，如果我们把$C={c:x=\\text{Head and } y\\leq 3}$，那么这个新的联合事件的概率 ： $$ Pr[(X,Y)\\in C]=\\frac{1}{12}+\\frac{1}{12}+\\frac{1}{12}=\\frac{1}{4} $$\n连续联合分布 Continuous Joint Distribution 进一步，我们就来到了两个连续随机变量的联合分布上了，相对于离散随机变量，连续随机变量在联合这个问题上难度将提升非常大。 先来个🌰 ： 还是水费和电费的例子，我们之前解决是用几何的方法解决的，现在我们用分析的方式解决，回一下题干，“我们有一家工厂，每个月用水是在 $[10,110]$ 吨，用电在 $[100,1100]$ 千瓦时，并且是完全随机的均匀的，那么我们可以计算用水在 $[20,40]$ 吨，用电在 $[600,800]$ 千瓦时的概率是多少？”\n用二重积分的方法，我们能求出联合分布面积，结合其概率密度分布$f(x,y)=\\frac{1}{100000}$\n$$ Pr[(X,Y)\\in C]=\\int_C\\int \\frac{1}{100000}dx dy $$ 这个只是算是微积分的一个简单应用，如果不知道为啥这么做，可以去看一下多重积分的基础知识就自然明白了，但是这个公式却可以延伸成为联合分布的定义。\n Definition Continuous Joint Distribution/Joint p.d.f./Support:Two random varibales X and Y have a continuous joint distribution if there exists a nonnegative function f defined over the entire xy-plane such that for every subset C of the plane, $$ Pr[(X,Y)\\in C]=\\int_C\\int f(x,y)dx dy $$ if the integral exists.The function $f$ is called the joint probability density function(abbreviated joint p.d.f)of $X$ and $Y$.The closure of the set ${(x,y):f(x,y)\u0026gt;0}$ is called the support of (the distribution of) $(X,Y)$\n 关于连续随机变量联合分布的定义，和前面的连续随机变量第一非常像，这个定义只是告诉我们了外观，什么样的形式，但最核心的部分也就是那个函数$f$ 并没有告诉我们怎么得到的，是否能通过两个随机变量自有的分布计算得出，也就是我们假设连续随机变量X 有一个$g_1(x)$ 的连续分布函数，连续随机变量Y 有一个$g_2(y)$ 的连续分布函数，我们怎么从$g_1$和$g_2$ 得到$f$ ，他们之间具体有没有实质性的关系，我们留作一个思考。\n其实单随机变量也没有给出其pdf的求法，而是直接制定了存在的某个连续函数，其要求满足一些性质，但是我们的联合pdf，应该在后续给出以下类似的性质。\n A joint p.d.f. must satisfy the following two conditions: $$ f(x,y)\\geq 0 \\text{ for } -\\infty \u0026lt;x\u0026lt;\\infty \\text{ and } -\\infty \u0026lt;y\u0026lt;\\infty $$ and $$ \\int^{\\infty}{-\\infty}\\int^{\\infty}{-\\infty}f(x,y)dxdy=1 $$\n 所有满足上面这个性质的二元函数都能当联合pdf，其对应的某些 $X$ 和 $Y$ 概率分布。\n由于 $X$ 和 $Y$ pdf不唯一，所以联合起来的pdf也不唯一。\n联合连续分布的pdf的积分性质：\n Theorem For every contimuous joint distribution on the xy-plane,the following two statements hold:\n  Every individual point,and every infinite sequence of points,in the xy-plane has pribability 0. Let $f$ be a continuous function of one real variable defined on a(possibly unbounded) interval(a,b).The sets ${(x,y):y=f(x),a\u0026lt;x\u0026lt;b}$ and ${(x,y):x=f(y),a\u0026lt;y\u0026lt;b}$ have probability 0  直观的解释，就是继承自单连续随机变量的某个点的概率是0，延伸到二维空间，一个点的概率是0，每一个无限的点的序列概率也是0，二维空间上的一维函数对应的概率也是零，可以理解为二维分布上一条线（对应一维分布上的一个点）概率是0.\n对于一条线，两个变量有一个被消去了，也就是从二重积分上来说微分部分是0，所以结果是0，从图形上来看，一个面的体积是0 。\n混合二维分布 Mixed Bivariate Distribution 混合分布，一个连续的，一个离散的，结合了上述两种的全部特点，当然也有一些特例，我们先举个例子说明这种情况存在：比如我们选择特定的五个城市的气温情况，随机选择一个城市，是离散的，而城市温度可以看做一个连续的随机变量，那么这个试验就是一个混合的分布。\n Definition Joint p.f/p.d.f: Let $X$ and $Y$ be random variables such that $X$ is discrete and $Y$ is continuous.Suppose that there is a function $f(x,y)$ define on the xy-plane such that,for every pair A and B of subsets of the real numbers $$ Pr(X\\in A \\text{ and } Y \\in B)=\\int_B\\sum_{x\\in A}f(x,y)dy $$ if the integral exists.Then the function $f$ is called the joint p.f./p.d.f of $X$ and $Y$\n 没错我们关注的焦点应该是那个联合分布函数（现在他既是连续又是离散的）只要积分存在，那么这个函数就是X 和Y的一个混合二维分布。 同样要满足下面这个雷打不动的性质，以满足柯氏公理: $$ \\int^{\\infty}{-\\infty}\\sum^{\\infty}{i=1}f(x_i,y)dy=1 $$\n相对于离散的双重求和，连续分布的二重积分，混合分布的积分加求和应该是自然的。 当然我们可能想到可不可以交换求和和积分顺序，当然没什么问题(只是积分范围需要重新确定一下，这个问题属于微积分，计算的技巧比较强，但是还是要强调一下)： $$ (X,Y)\\in C\\ \\text{we set } C_x={y:(x,y)\\in C}\\ \\text{then } Pr((X,Y)\\in C)=\\sum_{\\text{All } x}\\int_{C_x}f(x,y)dy $$\n纯计算问题，在做二重积分的时候，比较麻烦的地方就是积分界的代换。 举个🌰 ： 一个联合分布： $$ f(x,p)=p^x(1-p)^{1-x}\\text{ for } x=0,1 \\text{ and } 0\u0026lt;p\u0026lt;1 $$ 计算$Pr(X\\leq 0 \\text{ and } P\\leq \\frac{1}{2})$ 解： $$ Pr(X\\leq 0 \\text{ and } P\\leq \\frac{1}{2})=\\int_0^{\\frac{1}{2}}(1-p)dp\\ =-\\frac{1}{2}[(1-\\frac{1}{2})^2-(1-0)^2]=\\frac{3}{8} $$\n二维累积分布函数 Bivariate Cumulative Distribution Functions 因为我们前面研究的离散分布，描述离散分布的工具就是一个概率函数，扩展到二维联合分布就是变成了一个二维的表，研究连续分布，工具是pdf，我们也研究过了，但是另一个能同时描述离散和连续分布的cdf还没研究其联合形式。\n Definition: Joint(Cumulative) Distribution Function/c.d.f. The joint distribution function or joint cumulative distribution function (joint c.d.f) of two random variables X and Y is defined as the function F such that for all values of x and y( $-\\infty\u0026lt;x\u0026lt;\\infty$ and $-\\infty\u0026lt;y\u0026lt;\\infty$) $$ F(x,y)=Pr(X\\leq x \\text{ and } Y\\leq y ) $$\n 定义好像没什么稀奇的，就是一维的扩展版，但是有些性质变得复杂了，比如求一个区间的概率： $$ Pr(a\u0026lt;x\\leq b \\text{ and } c\u0026lt;Y\\leq d)\\ =Pr(a\u0026lt;X\\leq b \\text{ and } y\\leq d) -Pr(a\u0026lt;x\\leq b \\text{ and } y\\leq c)\\ =[Pr(X\\leq b \\text{ and } Y \\leq d)-Pr(X\\leq a \\text{ and } Y \\leq d)]-\\ [Pr(X\\leq b \\text{ and } Y \\leq c)-Pr(X\\leq a \\text{ and } Y \\leq c)]\\ =F(b,d)-F(a,d)-F(b,c)+F(a,c) $$ 这个过程可能画个平面图也能看的清楚些，用概率定义时候的定理也可以：  Theorem Let $X$ and $Y$ have a joint c.d.f. $F$.The c.d.f. $F_1$ of just the single random variable $X$ can be derived from the joint c.d.f. $F$ as $F_1(x)=lim_{y\\to \\infty}F(x,y)$.Similarly,the c.d.f. $F_2$ of $Y$ equals $F_2(y)=lim_{x\\to \\infty}F(x,y)$ ,for $0\u0026lt;y\\leq \\infty$\n 这个定理解决的问题是我们上面提到的关于如何把联合pdf拆分成单个变量的pdf，这个定理帮我们解决的是个相关的问题，就是把cdf拆成两个变量分别的cdf，具体做法就是对于二维联合cdf，对其中一个变量取极限得到的就是另一个变量的cdf。 证明如下，我们只证明其中一个，另一个将不证自明：  离散情况下： $$ \\text{Let }\\ B_0={X\\leq x \\text{ and } Y\\leq 0} \\ B_1={X\\leq x \\text{ and }n-1\u0026lt;y\\leq n}\\text{ ,for } n=1,2\\dots\\ A_m=\\bigcup^{m}{n=0}B_n\\text{ ,for }m=1,2,\\dots\\ $$ 我们可以确定:${X\\leq x}=\\bigcup^{\\infty}{n=-0}B_n$ ，并且 $A_m={X\\leq x \\text{ and } Y\\leq m} \\text{ for } m=1,2,\\dots$ 这样我们有 $Pr(A_m)=F(x,m) \\text{ for each } m$  那么我们就有： $$ F_1(x)=Pr(X\\leq x)=Pr(\\bigcup^{\\infty}{n=1}B_n)\\ =\\sum^{\\infty}{n=0}Pr(B_n)=lim_{m\\to \\infty} Pr(A_m)\\ =lim_{m\\to \\infty}F(x,m)=lim_{y\\to \\infty}Pr(A_m) $$ 注意加法原理成立的必然条件B之间相互不想交，切所有B的集合是全集。 2. 连续情况下： 这个证明就简单多了和上述证明基本一致，只要把求和编程就积分就可以证明连续情况下的结论\n最后说一下joint cdf和joint pdf的关系，微积分关系😆 $$ F(x,y)=\\int^y_{-\\infty}\\int^x_{-\\infty}f(r,s)drds\\ f(x,y)=\\frac{\\partial^2F(x,y)}{\\partial x\\partial y} $$\n总结 我们从单变量逐步向更复杂更贴近现实应用的双变量联合分布靠近，得出了双变量联合分布的一些性质和工具，明天继续从联合分布进一步演化内容，待续。。\n","permalink":"https://go.face2ai.com/math/math-probability-3-4-bivariate-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍双变量的分布情况，以及其中的一些有用的性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Discrete Joint Distribution,Continuous Joint Distribution,Mixed Bivariate Distribution,Bivariate Cumulative Distribution Functions\u003c/p\u003e","title":"【概率论】3-4:二维分布(Bivariate Distribution)"},{"content":"Abstract: 本文介绍描述随机变量分布的另一种工具，累积分布函数，CDF Keywords: Cumulative Distribution Function，Quantial\n累积分布函数 有一位时事评论人士曾经说过里根总统是一位非常耿直的总统，这位伟大的总统有一个很简单的评判人的观点，他认为一个人以前的做法和以后的做法将会非常相似，比如你是曾经是一个罪犯，那么你以后很有可能还会犯罪。佛说放下屠刀立地成佛，文人说浪子回头金不换，但是能达到这种境界的“坏人”不多，更多情况的是民间谚语：“狗改不了吃屎”。\n累积分布函数的定义和基本性质 Definition and Basic Properties 当我们从试验和事件通过随机变量数学化以后，所有数学性质都是围绕随机变量展开的，其中比较关键的就是随机变量到概率的映射，离散分布（离散随机变量）和连续分布（连续随机变量）我们前两篇已经讨论过了，而且描述这两种形式的随机变量的方法也不同，离散分布通过概率函数从随机变量得到概率，连续分布通过概率密度函数结合积分来得到概率，并且概率函数和概率密度函数都有一些自己的性质，可以帮助我们分析问题。我们这节的目的是找出一个可以同时用于离散分布和连续分布的工具，来指示随机变量和概率间的关系。\n Definition (Cumulative) Distribution Function :The distribution function or cumulative distribution function(abbreviated c.d.f) $F$ of a random variable $X$ is the function: $$ F(x)=Pr(X\\leq x)\\quad \\text{for} \\quad -\\infty\u0026lt;x\u0026lt;+\\infty $$\n 分布函数或者叫做累积分布函数，定义就是上式，还记得概率分布的定义么？没错$Pr(X\\in C)=Pr({s:X(s)\\in C})$ 这个描述中当把$X(s)\\in C$ 中C这个关系定义成 $X\\leq x$ 那么我们就有了分布函数，或者累积概率分布函数。 我们说过分布对于随机变量的研究非常重要，而一个函数被叫做分布函数，那么这个函数对随机变量甚至整个概率体系的数学化都是最基本的基础。 来个🌰 : 伯努利的c.d.f.,最简单的伯努利的分布是 $Pr(1)=p$ ,$Pr(0)=1-p$ ,那么其概率分布根据上面的定义是： $$ F(x)= \\begin{cases} 0 \u0026amp;\\text{for} \u0026amp;x\u0026lt;0\\ 1-p \u0026amp;\\text{for}\u0026amp; 0\\leq x\u0026lt;1\\ 1 \u0026amp;\\text{for}\u0026amp;x\\geq 1 \\end{cases} $$\n这个过程没啥解释的当变量小于0的时候概率是0，大于等于0，小于等于1的时候只有$Pr(0)=1-p$ 在这个范围内，最后当$x\\geq 1$ 的时候其对应的是 $Pr(0)+Pr(1)$ 。\n接下来介绍c.d.f的三条重要性质，我们把事件千辛万苦的搞成数字，为的就是用各种工具来研究他们之间的关系，这些性质就是帮助我们的工具。\n性质1：不减性 Nondecreasing  The function $F(x)$ is nondecreasing as $x$ increases;that is,if $x_1\u0026lt;x_2$ then $F(x_1)\\leq F(x_2)$\n 我们学过微积分应该知道非减函数的定义，当在定义域中$x_1\\leq x_2$ 时 $f(x_1)\\leq f(x_2)$\n如果从集合的角度我们也可以证明，那么我们将回归到概率的公理和基本性质 证明： $$ \\text{for }x_1\\leq x_2\\ {x:x\u0026lt;x_1}\\subset {x:x\u0026lt;x_2} $$ 根据1-1中的T4可以得到： $$ Pr({x:x\u0026lt;x_1})\\leq Pr( {x:x\u0026lt;x_2}) $$ Q.E.D.\n性质2：有限 Limits  Limits at $\\pm\\infty$ $lim_{x\\to -\\infty}F(x)=0$ and $lim_{x\\to \\infty}F(x)=1$\n 两种极限下c.d.f的表现，证明可以借用上面P1的过程， $$ \\text{for }x_1\\leq x_2\\ {x:x\u0026lt;x_1}\\subset {x:x\u0026lt;x_2} $$\n只要能求出 $Pr({x:x\u0026lt;x_1})=1 \\quad \\text{for } x_1=+\\infty$\n以及\n$Pr({x:x\u0026lt;x_2})=0 \\quad \\text{for } x_2=-\\infty$\n只要能证明上述两个结果那么就能得到结论 c.d.f不需要有连续性，如图： 根据函数的连续性定义，当左右极限和函数值相等时连续， $$ F(x^{-})=lim_{y\\to x,y\u0026lt;x}F(y)\\ F(x^{+})=lim_{y\\to x,y\u0026gt;x}F(y) $$\n在cdf的间断点上，只可以有右极限。\n性质3：向右连续 Continuity from the Right  Continuity from the Right.A c.d.f. is always continuous from the right: that is,$F(x)=F(x^{+})$ at every point $x$\n 这个证明后续补上，需要数学分析中极限的证明（遗留问题）\n从分布函数中确定概率 Determining Probabilities from the Distribution Function 那么如何从分布函数中找到某个点（离散下）或者区间（连续下）的概率呢？\nT1： $Pr(X\u0026gt;x) = 1 - F(x)$  Theorem: For every value $x$, $$ Pr(X\u0026gt;x) = 1 - F(x) $$ QED 证明显而易见，依据定义 $F(x)=Pr(X\\leq x)$ 又因为 ${x:X\\leq x}$ 和${x:X \u0026gt; x}$ 是互补事件，所以依据1-1必然有 $Pr(X\u0026gt;x) = 1 - F(x)$\n T2:$Pr(x_1\u0026lt;X\\leq x_2)=F(x_2)-F(x_1)$  Theorem :For all value $x_1$ and $x_2$ such that $x_1\u0026lt;x_2$ : $$ Pr(x_1\u0026lt;X\\leq x_2)=F(x_2)-F(x_1) $$\n 这个证明也是相对简单，用到了上面T1的证明过程： 证明： 三个集合 $A={X:X\\leq x_1 }$、$B={X: x_1\u0026lt;X\\leq x_2 }$ 、 $C={X:X\\leq x_2 }$ $$ B=A^{c}\\cap C $$ 根据1-1中的T6： $$ Pr(B)=Pr(C)-Pr(A\\cap C)\\ \\text{for: }A=A\\cap C\\ Pr(B)=Pr(C)-Pr(A)\\ Pr(x_1\u0026lt;X\\leq x_2)=F(x_2)-F(x_1) $$ QED\n证明过程用到了前面1-1的一条定理，证明相对简单。\nT3:$Pr(X\u0026lt;x)=F(x^{-})$  Theorem For each value $x$ $$ Pr(X\u0026lt;x)=F(x^{-}) $$ 这个证明和上面的P3的证明一样用到了数学分析中极限的证明，后面写数学分析的时候回来补上\n 这个证明后续补上（遗留问题）\nT4:$Pr(X=x)=F(x)-F(x^{-})$  Theorem For every value $x$ $$ Pr(X=x)=F(x)-F(x^{-}) $$ 证明: 根据概率定义明显有以下 $$ Pr(X=x)=Pr(X\\leq x)-Pr(X\u0026lt;x) $$ 根据cdf的定义 $$ F(x)=Pr(X\\leq x) $$ 那么根据这个定理，图中的 $Pr(x_1)=z_1-z_0$\n 离散分布的累积函数 The c.d.f. of a Discrete Distribution 对于离散分布，cdf可以通过定义得到，而其函数形状应该是阶梯状的，而且离散分布的cdf有以下几点性质（设 $f(x)$ 是离散随机变量的概率函数）：\n $F(x)$ will have a jump of magnitude $f(x_i)$ at each possible valuee $x_i$ of $X$ $F(x)$ will be constant between every pair of successive jump The distribution of a discrete random variable $X$ can be represented equally well by either the p.f. or the c.d.f. of $X$  上面这几点性质主要是描述离散cdf在函数图像的表现。\n连续分布的累积函数 The c.d.f. of a Continuous Distribution 连续分布的c.d.f.\n Theorem Let X have a continuous distribution, and let f(x) and F(x) denote its p.d.f and the c.d.f. respectively.Then $F$ is continuous at every $x$: $$ F(x)=\\int^{x}_{-\\infty}f(t)dt $$ and $$ \\frac{dF(x)}{dx}=f(x) $$ at all x such that f is continuous.\n 进入连续模式下，我们更多开始关注p.d.f和c.d.f这类函数的函数性质，极限，导数，积分这些是了解函数的最基本的手段，根据cdf的定义和微积分基本定理，上述表达成立一点都不意外，很和谐流畅的数学表达。 证明过程主要使用cdf的定义，和pdf的定义以及微积分的意义，这里就不写了，太简单了。\n分位数函数 The Quantile Function 研究了cdf的函数性质，那么对于这种单调的函数，其必有反函数，那么cdf的反函数是啥？有啥用途么？有很大用处。\n Definition Quantiles/Percentiles: Let $X$ be a random variable with c.d.f. $F$ .For each $p$ strictly between $0$ and $1$ ,define $F^{-1}(p)$ to be the smallest value $x$ such that $F(x)\\geq p$ .Then $F^{-1}(p)$ is called the $p$ quantile of $X$ or the $100p$ percentile of $X$ .The function $F^{-1}$ defined here on the open interval $(0,1)$ is called the quantile function of $X$\n 定义的简单解释，对于一般情况，我们假定cdf严格递增（可以通过水平检测）我们观察cdf的图像可以知道其横轴是随机变量的所在轴，纵轴是$F(x)$ 对应的值，$[0,1]$ 那么如果我们去其反函数，那么自变量变成了 $[0,1]$ ，而对应的值域就是随机变量，当我们选定任意 $1\\geq p\\geq 0$ 时通过反函数$F^{-1}（p）$ 得到的是随机变量的值，这个随机变量含义是，获得概率和为 $p$ 的随机变量上限。 对于特殊情况，不能通过水平检验的，我们规定，其反函数的下: 所以我们规定当出现这种情况是，我们选择较小的随机变量值$x_1$ 虽然我们前面说数学应该更多使用分析的方法，但是这里使用了大量的图片是为了让对函数图像没有概念的同学可以直观的感受下，如果从分析的方法直接分析值域和定义域的关系能得到相同的答案。 我们的quantiles 是cdf的衍生物，所以他和cdf一样，只依赖与分布，分布一旦确定cdf、quantiles 就唯一确定了。\n中位数 Median quartiles  Definition Median/Quantiles.The $\\frac{1}{2}$ quantile or the 50th percentile of a distribution is called its median.The $\\frac{1}{4}$ quantile or 25th percentile is the lower quartile.The $\\frac{3}{4}$ quantile or 75th percentile is called upper quartile.\n 三个比较特殊的位置，分别是25% 50% 75%，其中50%是最特殊的。\n总结 本文主要介绍另一个非常重要的描述分布的工具，分布函数（累计分布函数）c.d.f，用好这些工具可以帮我们更顺利的完成后面的工作\n","permalink":"https://go.face2ai.com/math/math-probability-3-3-cumulative-distribution-function.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍描述随机变量分布的另一种工具，累积分布函数，CDF\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Cumulative Distribution Function，Quantial\u003c/p\u003e","title":"【概率论】3-3:累积分布函数(Cumulative Distribution Function)"},{"content":"Abstract: 本文继续上文的离散随机变量的观点，将部分试验模型转换到连续随机变量的观点上。 Keywords: Continuous Random Variable，Continuous Distributions，Probability Desity Function，Uniform Distributions on Intervals\n连续分布 今天的废话想说说关于转行，之前去看中医（别问我信不信中医，学过概率的人都应该有自己的模型去评估一件事）那位老师真的是白发苍苍，很有医者风范，甚是慈祥有条不紊，在询问病情的过程中无意中聊到现在各行业的发展，老人家说，现在有些人两年三年就换个行业，还说叫什么转型，这么换来换去的什么东西能做好？一边说甚至有点义愤填膺的感觉。 其实我之前也有这种感觉，为啥这个人先是云计算，然后又去智能穿戴了，后来又智能家居，然后3D打印，后来就人工智能了，问问没准最近可能在研究区块链了，更让人感慨的是，他们研究个三五个月就敢自称专家了，如果有个博士学位，或者大公司平台背书，那整个过程能更加自然流畅。 我一直在对自己身边的人宣传要做自己喜欢的事。但是如果你喜欢的事经常变，而且是啥新鲜变成啥，那我只能说甚是与时俱进。 为啥我要回到数学，一篇一篇的写博客，做练习，我觉得我之前也太激进了，就像一个三岁小孩就急着上战场，手枪还用不好呢，就想用M416，太扯淡了，拿着枪照镜子觉得很有自信，当真要你死我活的时候，估计会死的很难看，当然不是最惨的，旁边那个两岁的拿着AK跟某组织正在申请高新技术企业呢！\n概率密度函数 The Probability Density Function 继续我们的概率论，首先我们回忆一下，我们首先从试验开始，然后得到样本空间，每个试验结果都是一个样本点，对应一个概率，试验结果的个数是离散的，确定的，比如扔硬币，扔骰子，随后我们将这些结果通过一个函数映射到一个随机变量，使其更加数学化，随机变量是个函数，有时候也表示函数值，为了研究这个函数值对应的概率，我们又提出了distribution这个概念。今天我们将随机变量这个坑补齐，因为我们研究离散的随机变量时，其定义域和值域都有洞，我们今天把洞补齐。 学过数学分析，或者是研究过实数系的同学都知道，自然数，整数，有理数（比例数）这些数都是不连续的，因为有洞，这些洞比如 $\\sqrt{2}$ 这个位置，就是有理数没办法表示，后来我们为了补全整根数轴，我们得到了实数系，这个没有洞的数，那么我们今天就是想找个试验，能把实验结果映射到一段连续的实数上（可以是有界的也可以是无界的），然后研究他的性质。 在研究连续随机变量前，我们可以引进一个关于面积的模型，其实书上很早之前就有，一般的概率论书上一般在古典概率部分就会提出这个面积模型。 举个🌰 ： 我们有一家工厂，每个月用水是在 $[10,110]$ 吨，用电在 $[100,1100]$ 千瓦时，并且是完全随机的均匀的，那么我们可以计算用水在 $[20,40]$ 吨，用电在 $[600,800]$ 千瓦时的概率是多少？ 这里我们可以画一个图： 从直觉上来说，中间这小块的面积与完整面积的比例就应该是上面描述的事件的概率。 从频率的角度上来说也是，因为每个点出现的可能性相同。那么一个区域出现的可能性就是这个区域的积分，积分在几何的表示是面积，那么我们用面积比来近似概率应该没有问题，那么，我们如果是这个思想继续展开，一个点的概率是多少呢？点没有面积，所以每个点的概率是0，而我们之前认为事件的概率是0，表明事件不可能发生，但是在这个例子里面，任何一个点都有可能发生，矛盾了，那么我们的模型有问题？还是前面的公理有问题？我们接着往下看。 我们发现我们认为事件概率是0的事件不可能发生的事件是离散的，也就是事件数量是有限的，而我们这个例子的事件个数是无限多的，那么我们要从连续的角度重新思考事件，随机变量和对应的概率的问题了。\n连续分布，连续随机变量 Continuous Distribution \u0026amp; Continuous Random Variable  Definition Continuous Distribution/Random Variable: We say that a random variable $X$ has a continuous distribution variable if there exists a nonnegative function $f$ ,defined on the real line,such taht for every interval of real nombers (bounded or unbounded),the probability that $X$ takes a value in the interval is the integral of $f$ over the interval\n 我们定义下连续分布和连续随机变量（依然是个函数），定义连续随机变量（函数），在实数轴上，每一个区间（有界或者无界）对应的概率是这个区间上 $f$ 的积分。\n$$ Pr(a\\leq X\\leq b)=\\int^b_af(x)dx $$\n上面这个定义和上面的例子相互对应，所以，上面的例子的解法没有问题，那么为什么单点概率会是0？因为当事件的随机变量产生的结果是连续的，那么无数个事件对应无数个连续的随即变量值，为了满足公理 Kolmogorov Axiom 3 这些事件的概率和必然是1，那么，只有通过积分，才能把无数个小到零的数字，加到1，所以连续随机变量的值，描述要用到积分。又因为单点积分没有定义（用黎曼积分没办法算），所以我们可以理解，上面的定义是很有道理的。计算一个区间内的随机变量的积分，比如 $[x-\\epsilon,x+\\epsilon]\\text{ for }(\\epsilon\u0026gt;0)$ 上的积分，来近似表示x处的概率，当 $\\epsilon \\to 0$ 可以完成这个近似。 总结起来就是：连续事件有无穷多个随机变量值，那么单个随机变量值变得毫无意义（无限趋近于0），只有用（某区间上的）无数多个随机变量的概率的积分( $\u0026gt;0$ )，与完整样本空间上的随机变量概率的积分和（1）的比例来描述这部分(区间上的)的概率。\n那么上面这个表达的一个突出结果就是 $f$ 将会是非常重要的，这个$f$ 有点像离散随机变量的概率函数，但是操作起来又有很大差别。\n概率密度函数的定义 Definition Probability Density Function/p.d.f/Support  Definition Probability Density Function/p.d.f/Support:If $X$ has a continuous distribution ,the function $f$ described in definition Continuous Distribution/Random Variable is called the probability density function(abbreviated p.d.f)of $X$.The closure of the set ${x:f(x)\u0026gt;0}$ is called the support of (the distribution of) $X$\n 上面关于连续随机变量的定义中 $f$ 的定义并不是随意的，而且这个 $f$ 可以算是一节大佬了，因为后面很多连续随机变量都要用它表达，被称为概率密度函数，过程简称pdf，这个函数能够表征一个连续随机变量的分布请款个，与离散情况下的概率函数功能相似，而且性质也类似，每个连续试验可以对应不同的pdf（可以移植上文中概率函数的证明方法），并且我们能够得到一个关于support(支撑) 的定义，连续随机变量所在的闭区间必须要能保证pdf大于0（因为有等于0的情况），这个区间就是一个support。\n图解的话就是下面的图了： 图中所有有定义的区间（绿色）的积分的和是1，指定区间的积分是映射到这个区间上的所有事件之一发生的概率。红色开区间是上面定义的support。上图描述了一个随机变量在两个分开的区间上的pdf。\n所以pdf就是连续随机变量的一种描述方式，后面还有别的描述方式，但是pdf特殊就特殊在，连续随机变量是用它来定义的。\npdf需要满足以下两点性质： 性质一： $$ f(x)\\geq 0 \\quad \\text{for all } x $$ 性质二： $$ \\int^\\infty_{-\\infty}f(x)=1 $$ 性质一和性质二都是为了满足柯氏公理而产生的，大于等于0表示不可能出现负数的概率，积分等于保证公理三成立。\n还是要强调，连续随机变量的概率密度函数（p.d.f.）中概率等于0，事件不可能发生。\n概率（密度）函数的不唯一性 Nonuniqueness of the p.d.f. 和前面所说的概率函数不“唯一”一样，pdf也不唯一，一个原因是关于随机变量（函数）的选择问题，还有很重要的一点，由于单点pdf对于整体而言其概率是0，那么其pdf值可以为任意值，而保持任意包含此点的区间上的pdf积分值不变（也就是概率不变）。 这个pdf和下面这个pdf等效，但是却不一致：\n所以每个连续随机变量都有无穷个pdf，我们通常会选取最自然的那个，而且尽量不要用这些存在间断点的pdf，因为从函数研究的角度来讲，这个将会很麻烦，我们把事件过渡到实数域的目的就是为了利用数学工具，比如函数，积分，微分等，如果还人为的引入障碍就有点不明智了，但是如果引入间断点可以更好的表述模型，那么也未尝不可。\n还有一个要注意下，我们说过几种分布了，不如二项分布，伯努利分布，均匀分布，这些分布里面的分布，与连续分布，离散分布不是一回事儿： 区间上的均匀分布 Uniform Distributions on Intervals 我们扩展一下上一篇中的均匀分布，从离散扩展到连续，我们学习概率分布都是从均匀分布开始的，说明其具有非常简单的性质，当然，就是扔骰子嘛！\n Definition Uniform Distribution on an interval:Let $a$ and $b$ be two given real numbers such that $a\u0026lt;b$.Let $X$ be a random variable such that it is known that $a\\leq X \\leq b$ and ,for every subinterval of $[a,b]$ ,the probability that $X$ will belong to that subinterval is proportional to the length of that subinterval .We then say that the random variable X has the uniform distribution on the interval $[a,b]$\n 翻译：一个区间上 $[a,b]$ 上任意一个区间上的概率（pdf的积分）等于这段区间的长度与整个定义区间的比例，那么这个分布就叫做在 $[a,b]$ 上的均匀分布（有些细节没有添加，可以从上面英文的定义找到）\n所以根据上述定义，我们可以猜测也可以计算出，下面这个pdf是 $[a,b]$ 上的均匀分布的一个pdf。\n Theorem Uniform Distribution p.d.f. If X has the uniform distribution on an interval $[a,b]$,then the p.d.f. of X is:\n $$ f(x)= \\begin{cases} \\frac{1}{b-a} \\quad \\text{for} a\\leq x\\leq b\\ 0\\quad\\quad \\text{otherwise} \\end{cases} $$\n证明思路就是首先证明在所有可能的随机变量上其积分是1，这个结论是显而易见的。因为在区间外的任意区间上的积分都要是0，又可知在上述式子中， $$ \\frac{\\int^{x_2}{x_1}f(x)dx}{\\int^{b}{a}f(x)dx}=\\frac{|x_2-x_1|}{b-a}\\quad \\text{for:} x_2\u0026gt;x_1 $$ 恒成立,那么上述的p.d.f是均匀分布的p.d.f.之一。 QED\n请注意：p.d.f对应值没有实际意义，这点与概率函数有着非常大的差异，p.d.f的值可以大于1，甚至可以是 $+\\infty$ ，因为上面我们也看到了，pdf的区间积分才是表达概率的，所以概率的一些表要性质不需要pdf表现出来，而是需要pdf的区间积分表现出来。 举个例子： 计算下面pdf的概率：\n$$ f(x)= \\begin{cases} \\frac{x}{8}\\quad \\text{for} 0\u0026lt;x\u0026lt;4\\ 0\\quad \\quad \\text{otherwise} \\end{cases} $$\n计算 $Pr(1\\leq X \\leq 2)$ 和 $Pr(X\u0026gt;2)$ :\n$$ Pr(1\\leq X\\leq 2)=\\int^2_1\\frac{1}{8}xdx=\\frac{3}{16} $$\n以及\n$$ Pr(X\u0026gt;2)=\\int^4_2\\frac{1}{8}xdx=\\frac{3}{4} $$\n混合分布 Mixed Distribution 混合分布这里简单介绍一下，当一个试验被二维随机变量映射时，将会产生一个二维的随机变量值这个值有两个维度，这两个维度相互之间可以有影响也可以没有影响，但是他们连续与否是完全独立的，也就是说两个输出值，其中一个是连续的，另一个离散的，这完全没问题，而具体的计算也可以分别参考离散和连续随机变量。\n总结 本文主要讲如何用pdf来描述一个随机变量（值）的连续分布，后面我们继续研究连续情况下的分布表述，以及其他一些工具。\n","permalink":"https://go.face2ai.com/math/math-probability-3-2-continuous-distribution.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文继续上文的离散随机变量的观点，将部分试验模型转换到连续随机变量的观点上。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Continuous Random Variable，Continuous Distributions，Probability Desity Function，Uniform Distributions on Intervals\u003c/p\u003e","title":"【概率论】3-2:连续分布(Continuous Distributions)"},{"content":"Abstract: 本文主要介绍随机变量的引入，离散分布的介绍以及离散均匀分布，二项分布的基本原理 Keywords: Random Variables，Discrete Distributions，Uniform Distributions on Integers，Binomial Distributions\n随机变量和分布 目前阶段，每天研究数学，数学和技术的最基本差别是数学基本不能马上变现，而技术不一样，学个java或者php你可以在三到五个月内找到工作，三到五个月微积分计算都学不透彻，更别说用这个挣钱了，所以学数学基本没办法看到短期结果，但有没有用我就不说了，因为有人觉得有用有人觉得没用，我已经用我的行动站队了，而且我也不想劝别人跟自己站一队。\n关于别人的建议，我觉得自己肯定干不出任何事，听取别人意见也是很重要的，孔圣人的境界：三个人就有一个是他老师。这句话从概率的角度分析有没有道理？有，我们可以简单分析一下，假设这三个人的属于最常规的人，他们之间的知识互相独立，并假设每个人有 $\\frac{1}{n}$ 概率的知识是可以教给我们的，那么三个人互相独立，三个人中找到一个可学习的知识的概率就是 $\\frac{1}{n} \\times 3=\\frac{3}{n}$ ，看起来还不错，那么我们继续分析，每个事件（可以被学到的知识点）拥有相等的概率，也是我们下面要离散均匀分布，从频率派的角度，我们三个人每讨论n件事才能学到3个知识，假设讨论一个问题的时间恒定为t，那么我们学会三个知识点的大概用时是 $\\frac{nt}{3}$ 的时间；下一种情况，如果我们和一个在我们相同圈子，而且比较资深的专家探讨呢？假设其有 $\\frac{1}{m}$ 的概率知识可以启发我们，那么我们学会一个知识点的时间是 $mt$ 如果这位资深专家的 $\\frac{1}{m}\\geq \\frac{3}{n}$ 的话 $mt\\leq\\frac{nt}{3}$ 就可以节约我们的时间，比如机器学习，我们和 Geoff Hinton 教授讨论，Prof. Hinton 的 $\\frac{1}{m}$ 应该会远远远远大于 隔壁大婶，二舅妈和三姨的 $\\frac{3}{n}$ 简陋的例子，牛人的一封邮件比一般人的三天三夜的长谈还有营养，从感觉上也是这样的。 所以当我不在接受你的建议并保持沉默的时候，不是我们很高傲，可能是 $\\frac{1}{n}$ 太小了。\n随机变量 Random Variables 上面的应用题是简单的概率分析，简单到可能有点漏洞百出，但要准确建模，我觉得我再来一百年也搞不定，但是我们应该有一个比较直观的感受，我们在此之前研究的概率，都是基于事件的，而事件是样本空间的子集，样本空间是试验结果的集合，也就是说，我们研究的最原始的变量是结果，这个结果可以是下不下雨，打不打雷，扔硬币的结果，等等，在最初研究赌博天气预报可能这些可以接受，那时候概率可能还不太属于数学，或者只能算是数数的过程，但是随着研究深入，概率论转移到数学范围，我们就必须数学化，数学化的第一步就是把这些实验结果数字化，所以我们引出了随机变量。\n随机变量的定义 Definition of a Random Variable 先来个例子压压惊🌰 ： 一个硬币扔十次，可能会出现$s={HTTTHHTTHT}$ 的结果序列，如果我们把序列当做实验结果，我们定义出现H的次数为这个结果的数字化描述，那么存在一个函数$X(s)=4$ 。有这样的函数么？答案是没问题，数学分析上的函数定义与此完全一致，从一个集合$S$ （样本空间）映射到另一个集合 $\\Re$ （实数）。\n Definition Random Variables:Let S be the sample space for an experiment.A real-valued fuction that is defined on S is call a random variable\n 上面这句话从中文的语法来讲很有意思，等效的表达“这只黑白花的猫叫做狗”（郭德纲的一个段子：“我家狗叫猫咪，猫咪叫鹦鹉，鹦鹉叫兔子，兔子叫儿子，儿子叫于谦”），一个定义在某试验样本空间S上的一个实值函数叫做随机变量。提取句子主干“函数叫变量”，没错就是这个意思，这个意思一定要掌握，不然你没办法从样本空间中真实的实验结果过渡到数学模型下的研究。 “这个定义是个概率论非常关键的转折，从此我们从赌博走向了数学的康庄大道” 上面的例子还可以得到另一个随机变量，就是$Y(s)=6$ 表示T出现的次数。\n随机变量的分布 The Distribution of a Random Variable 我们刚才定义了随机变量这个叫变量的函数。那么他的输出对应着之前的样本空间，因此输出也要对应一个概率。但是当函数出现多对一（函数的一种状态，多个不同的输入产生同一个输出）的时候，这个概率描述和前面就有不同了。我们下面说到随机变量，可能有时候把关注集中在他（随机变量这个函数）的输出上，也就是产生那个实数值。 那么这个随机变量（函数）的结果（实数值）可能是之前某些样本点的和，我们可以在实数轴上定义一个区间$C$ 并产生一个事件$X \\in C$ (这里的X就是指随机变量的输出值$X(s)\\in C$ )，我们就有了一个新的随机变量版本的概率: $$ Pr(X\\in C)=Pr({s:X(s)\\in C}) $$\n这个数学描述很清晰了，从分析的角度看，已经不需要再解释什么了，但是下面我们还是画个图来直观角度看看。\n Definition Ditribution :Let X be a random variable.The distribution of X is the collection of all probabilities of the form $Pr(X\\in C)$ for all sets C of real numbers such that ${X\\in C}$ is an event\n 中文翻译，对于某个事件C，满足 ${X(s)\\in C}$ 中的所有s的的概率的和是这个事件C的概率，所有可能的C的概率组成的一个集合叫做一个distribution。 是不是有点复杂，没错，因为这涉及到一个函数，之后又对函数结果进行了一个“分割”而这个分割又对应一个概率的集合，这个概率集合就是distribution。\n举个例子： 还是上面连续扔十个正常的硬币，按顺序记录结果，那么一共有$2^{10}$ 种不同的结果（样本点 $s$ ），那么我们定义随机变量$X(s)$ 为s中H面的个数，那么可能的X的输出值就是 ${0,1,\\dots ,10 }$ 那么 ${0,1,\\dots ,10 }$ 对应的所有概率就是X的一个种概率分布： $$ Pr(X=x)=\\sum_{s\\in{X(s)=x}}Pr(s)=\\begin{pmatrix}10\\x\\end{pmatrix}\\frac{1}{2^{10}} \\quad for\\quad x=1,2\\dots, 10 $$\n疑惑，很多人可能不懂为啥有个 $X=x$ ，其实我也一直不太懂这里，知道我在这本书上看到了随机变量是函数，其含义就是 $X(s)=x$ ，缩写会方便会的人告诉扩展，同时也给初学者设置了一定的障碍。\n注意：从这开始，我们将不再故意区分随机变量到底是函数还是函数的输出值，全靠自己理解了哦\n离散分布 Discrete Distribution 我们继续上面的话题，从前面文章的研究事件的概率转移到研究随机变量的概率，随机变量（的输出结果）其实也可以称之为事件，并且随机变量（的输出结果）也可以组合出新的事件，所以一个随机变量可以产生无数个分布（无数种关于C的划分），我们先研究下，当随机变量是离散的时候怎么办。\n Definition Discrete Distribution/Random Variable:We say that a random variable $X$ has a discrete distribution or that $X$ is a discrete random variable if $X$ can take only a finite number k of different values $x_1,\\dots , x_k$ or at most an infinite sequence of different values $x_1,x_2\\dots$\n 上面对离散分布，离散随机变量的定义就是随机变量（函数）输出的结果应该是有限的序列，或者可数无限的序列，可数无限的比较抽象（可数的序列就是可以和自然数产生对应的一个序列）举个例子可能是更好的，比如随机变量的的值可以是$1,2,3\\dots$ 这种也可以是 $\\frac{1}{2},\\frac{1}{3},\\frac{1}{4}\\dots$ 这种无限序列。\n下面我们定义概率函数，主要针对离散随机变量：\n Definition Probability Function/p.f./Support:If a random variable $X$ has a discrete distribution the probability function (abbreviated p.f.)of $X$ is defined as the function $f$ such that for every real number $x$ , $$ f(x)=Pr(X=x) $$ the closure of the set ${x:f(x)\u0026gt;0}$ is called the support of (the distribution of)$X$\n 还是上面的套路，只是给 $Pr(X=x)$ 换了个简单的名字，把它称为概率函数。概率函数只针对离散随机变量有效。\n举个例子，扔一个不正常的硬币，出现正面的概率是0.6，出现反面的概率是0.4，那么我们的试验结果是 ${正面,背面}$ ，我们设一个随机变量 $X$ 为： $$ X(s)=\\begin{cases} 1\u0026amp; \\text{s为正面}\\ 0\u0026amp; \\text{s为反面} \\end{cases} $$\n那么对应的概率就是： $$ Pr(X=1)=Pr(s为正面)=0.6\\ Pr(X=0)=Pr(s为反面)=0.4 $$\n根据概率函数的定义： $$ f(x)=\\begin{cases} 0.6\u0026amp; \\text{x=1}\\ 0.4\u0026amp; \\text{x=0} \\end{cases} $$\n所以我们的通过概率函数获得概率的过程是：\n 试验-\u0026gt;样本空间-\u0026gt;随机变量-\u0026gt;概率\n 下面介绍几个关于概率函数的定理：\n Let $X$ be a discrete random variable with p.f. $f$ ,If $x$ is not one of the possible values of $X$ ,then $f(x)=0$ also if the sequence $x_1,x_2,\\dots$ include all the possible values of X then $\\sum^{\\infty}_{i=1}f(x_i)=1$\n 注意这是一个关于概率函数的定理，和概率分布有些不同的是，概率函数 $Pr(X=x)$ 而概率分布式 $Pr(X\\in C)$ 这两个有关系，但是是完全不同的两个概念。上述定理说到的是当一个x不在随机变量中的时候，其概率是0，这点很明显，因为原始样本空间就没有与之对应的自变量，也就没有概率与其对应。 接着说的是所有可能的随机变量对应的概率和是1，这个与样本空间中1-1的T2一致。\n注意下，我们上面强调了，分布和概率函数的不同，但是之间有关系，没错，$Pr(X=x)$ 不同于 $Pr(X\\in C)$ 但是当 $C$ 被设置为$X=x$ ，这种条件下，两种情况等价，也就是说概率分布其实“是但不限于”概率函数的。 概率函数是随机变量的一种分布表示。随机变量可以有无数种概率分布，关键点在于“事件”的划分\n Theorem If $X$ has a discrete distribution,the probability of each subset $C$ of the real line can be determined from the relation: $$ Pr(X\\in C)=\\sum_{x_i\\in C}f(x_i) $$\n 我们下面用两张图来梳理其中关系： 先来个小的： 这是详细的： 样本空间是起始点，然后向左产生了事件，概率，向右产生了随机变量，概率函数，离散分布。\n有一些离散分布很出名，很基础，因为后续的很多能很好模拟实际过程的概率模型都是根据他们演化出来的\n伯努利分布 Bernoulli Distributions 伯努利分布也叫0-1分布，只有两个随机变量可以选，0和1，各对应于一个概率，扔硬币就是伯努利分布。\n Bernoulli Distribution/Random Variable:A random variable Z that takes only values 0 and 1 with $Pr(Z=1)=p$ has the Bernoulli distribution with parameter $p$ .We also say that $Z$ is a Bernoulli random variable with parameter $p$\n 在描述概率分布的时候，一定要把参数说全，不然这句话就是一句有问题的话，比如上面的$p$ ，如果说一个xx分布的随机变量，就是指其概率函数的那个概率分布（ $Pr(x\\in{X=x})$ ）\n扔不正常的硬币的例子，X就是一个$p=0.6$ 的Bernoulli分布。当忽略p,直接说X是个Bernoulli分布没有任何意义。\n均匀分布 Uniform Distributions(Discrete)  Definition Uniform Distribution on Integers:Let $a\\leq b$ be integers.Suppose that the value of a random variable $X$ is equally likely to be each of the integers $a,\\dots ,b$ .Then we say that $X$ has the uniform distribution on the integers $a,\\dots,b$\n  Theorem If X has the uniform distribution on the integers $a,\\dots ,b$ ,the p.f. of X is : $$ f(x)=\\begin{cases} \\frac{1}{b-a+1} \u0026amp;\\quad \\text{for $x=a,\\dots,b$}\\ 0\u0026amp; \\text{otherwise} \\end{cases} $$\n 这个定理是根据定义每个随机变量概率相同，以及总概率是1计算出来的每个点的概率的定理\n随机变量是一个整数并在一个区间范围内 $[a,b]$ ，并且每个随机变量拥有相同的概率，那么这个分布就是在 $[a,b]$ 内$p=\\frac{1}{b-a+1}$ 的均匀分布。\n均匀分布也很常见，比如扔一个均匀的骰子，骰子上的数目对应相应的数字，一点对应1，二点对应2\u0026hellip;\u0026hellip;.六点对应6，那么他就是一个在$[1,6]$ 内$p=\\frac{1}{6}$ 的均匀分布。\n二项分布 Binormial Distributions 二项分布是从伯努利那里演化过来的，可以模拟反复丢硬币的那个过程。 我们上面引入随机变量的地方说到了扔十次硬币的例子，我们现在假设扔的硬币不均匀，其得到正面的概率是 $\\frac{3}{5}$ ,得到反面的概率是 $\\frac{2}{5}$ 并且我们通过随机变量 $X(s)$ ，把正面对应于1，反面对应于2，假设反复进行 $N$ 次试验，那么我们想要知道出现 $m(m\\leq N)$ 次正面的概率是多少。 分析：前后两次丢硬币互不影响，所以每步之间独立，分步试验对应于我们的乘法法则，事件 $s_1$ “前 $m$ 次全是正面”的概率: $$ Pr(s_1)=p^{m}(1-p)^{N-m} $$ 这是极其特殊的一种情况，通过分析，我们只需选m个正面出现的位置，剩下的就是反面出现的位置，并且每种排列，其概率都是上面求出的$Pr(s_1)$ ，根据计数原理，这是个组合问题，N中without replacement的选取m个，共有 $$ \\begin{pmatrix}N\\m\\end{pmatrix} $$ 种，那么根据1-1中的T2加法原理，得到m个出现的概率是： $$ Pr(X=m)=\\begin{pmatrix}N\\m\\end{pmatrix}p^{m}(1-p)^{N-m} $$\n那么当我们把m改成一个变量的话，那么我们有: $$ Pr(X=x)=\\begin{pmatrix}N\\x\\end{pmatrix}p^{x}(1-p)^{N-x} $$\nX的p.f. 是： $$ f(x)=\\begin{cases} \\begin{pmatrix}N\\x\\end{pmatrix}p^{x}(1-p)^{N-x} \u0026amp;\\quad \\text{for $x=0,1,\\dots,N$}\\ 0\u0026amp; \\text{otherwise} \\end{cases} $$\n Definition Binomial Distribution/Random Variable:The distribution represeanted by the p.f. $$ f(x)=\\begin{cases} \\begin{pmatrix}N\\x\\end{pmatrix}p^{x}(1-p)^{N-x} \u0026amp;\\quad \\text{for $x=0,1,\\dots,N$}\\ 0\u0026amp; \\text{otherwise} \\end{cases} $$ is called the binomial distribution with parameters $n$ and $p$ .A random variable with this distribution is said to be a binomial random variable with parameters $n$ and $p$\n 这个定义开始逐渐的不说显示中的情况了，直接来了个形状如上式的分布就是二项分布。 二项分布就是每步独立的伯努利分布的复合实验结果。\n总结 总结，我们从事件进入到数字化的时代，从此概率论进入数学时代，今天好冷，打了这么多字手都冻僵了，明天继续\n","permalink":"https://go.face2ai.com/math/math-probability-3-1-random-variables-and-discrete-distributions.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍随机变量的引入，离散分布的介绍以及离散均匀分布，二项分布的基本原理\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Random Variables，Discrete Distributions，Uniform Distributions on Integers，Binomial Distributions\u003c/p\u003e","title":"【概率论】3-1:随机变量和分布(Random Variables and Discrete Distributions)"},{"content":"Abstract: 本文是关于Bayes\u0026rsquo; Theorem 的介绍性知识 Keywords: Bayes\u0026rsquo; Theorem，Law of total Probability\n贝叶斯定理 今天的废话可能有点正经，就是关于Bayes\u0026rsquo; Theorem的相关往事，做人脸识别的都会知道一个叫做联合Bayes的分类器，没错，我们当时做的时候也是第一个做了这个算法，当然，我没有自己去研究实践，最后结果不太理想，当然我不是说我去做结果就能好多少，其实投资也好做项目也好，失败向来不可怕，各种各样的死法在开始之前都有些预期，但是我们有一个期待，就是自己有个主观概率觉得我有90%可能会成功，前提是xx个项目能够顺利如期成功，但是当xx项目进展严重受阻，你就会重新评估成功概率，这个过程就可以用Bayes定理去建模；这个过程其实不可怕，最可怕的是你不知道你现在遇到了什么问题，你的小伙伴告诉你成功了，但其实根本没成功，有时候告诉你没成功，但是其实已经成功了，当一些事件结果变得不可信，那么你的评估将会变得毫无意义。 失败不可怕，可怕的是你不可控，说赚了三块钱，结果赔了五块，说赔了五块，结果赔了八块。\n贝叶斯定理 Bayes\u0026rsquo; Theorem 上面一个经典案例中包含了两个Bayes相关的经验，首先是一个具体的算法，用于分类，另一个是实现这个算法的过程产生的故事，也可以用贝叶斯公式来建模。 贝叶斯公式在概率论和统计中处于非常核心的部分，所以才有贝叶斯学派这种派系（不知道在我们社会主义伟大祖国，搞个这种学派是不是违法的）对立学派是频率派，这两派具体怎么样我们先不去深入区分，因为我们目前研究的境界还不需要站队，而就算有一天你需要站队了，这种站队可能出于利益和权利的考虑也要少很多，而是更多的源自自己觉得哪一派更接近真理。\n 频率学派的观点主要是认为所有事件的概率都不确定，但是可以用频率来近似得到。 贝叶斯派则认为所有事件的概率都是确定的，只是你的先验知识不足，所以算不出来而已。  大概可以这么理解，我们不再纠结上面这个关于上帝存不存在的问题，回到数学本身。\n例如上图，来自全概率公式那一篇：当我们面对一个试验，在所有的partition中（ $B_1\\dots B_n$ ）必然有一个将要发生，但不确定发生哪一个，我们可以观察到另一个事件A，如果已知$Pr(A|B_i)$ ，那么当我们确定A事件的发生概率的时候，就可以翻过来推到每个partition发生的概率有多大$Pr(B_i|A)$ 上面这个是简单的逻辑分析，可以大概知道我们为啥要Bayes公式。\n从分析上通过前面的条件概率公式可以非常轻松的得到贝叶斯公式： $$ Pr(A\\cap B)=Pr(A|B)Pr(B)=Pr(B|A)Pr(A)\\ Pr(A|B)=\\frac{Pr(B|A)Pr(A)}{Pr(B)} \\quad where \\quad Pr(B)\\neq 0\\ or\\ Pr(B|A)=\\frac{Pr(A|B)Pr(B)}{Pr(A)} \\quad where \\quad Pr(A)\\neq 0 $$\n这个就是最简单的贝叶斯公式推到，同时可以用全概率公式进行代换，就能得到更专业的贝叶斯公式。 举个例子🌰 ：\n 还是前面的随机拿螺丝，两个箱子A和B，我们随机选取一个箱子从里面拿螺丝(箱子有相等的概率被选中，所以 $Pr(A)=\\frac{1}{2}$ )，已知A中有长螺丝6个短螺丝4个，B箱子有长螺丝3个短螺丝7个，设长螺丝为事件L短螺丝为事件S，那么可以有 $$ Pr(L|A)=\\frac{6}{10}\\ Pr(S|A)=\\frac{4}{10}\\ Pr(L|B)=\\frac{3}{10}\\ Pr(S|B)=\\frac{7}{10}\\ Pr(L)=Pr(L|A)Pr(A)+Pr(L|B)Pr(B)=\\frac{6}{10}\\times \\frac{1}{2}+\\frac{3}{10}\\times \\frac{1}{2}=0.45\\ Pr(S)=Pr(S|A)Pr(A)+Pr(S|B)Pr(B)=\\frac{4}{10}\\times \\frac{1}{2}+\\frac{7}{10}\\times \\frac{1}{2}=0.55 $$ 现在我们已知拿出了一个长的螺丝，那么问从A箱子拿出来的概率是多少，从B箱子拿出来的概率是多少。 $$ Pr(A|L)=\\frac{Pr(L|A)Pr(A)}{Pr(L)}=0.3/0.45=0.667\\ Pr(B|L)=\\frac{Pr(L|B)Pr(B)}{Pr(L)}=0.15/0.45=0.333 $$\n得出结论是A的可能性更高，因为A中长螺丝更多，这与我们的感觉上是一致的。\n  Theorem Bayes\u0026rsquo; Theorem: Let the events $B_1\\dots B_k$ from a partition of the space S such that $Pr(B_j)\u0026gt;0$ for $j=1,\\dots ,k$ and let A be an event such that $Pr(A)\u0026gt;0$ , Then for $i=1,\\dots,k$ :\n $$ Pr(B_i|A)=\\frac{Pr(B_i)Pr(A|B_i)}{Pr(A)}=\\frac{Pr(B_i)Pr(A|B_i)}{\\sum_{j=1}^{k}Pr(B_j)Pr(A|B_j)} $$\n这个就是普通的Bayes公式结合上全概率公式得出的一个比较实用的版本，证明这里就不详细写出来了，因为通过条件概率和全概率公式就能得出上面的结论，那么如此简单的一个公式为什么如此核心呢？ 我们从哲理中分析一下，首先我们看 $Pr(B_i)$ 从本文第一张图片上可以看出来，这个划分从客观上说是是对完整样本空间的一个分别估计，这个过程与A是否发生无关，但当试验被观察到事件A发生了，我们的样本空间实际上发生了改变，不在是完整的S，因为$A^c$ 不可能发生了，这种情况下，我们将这个分割重新计算： 这个图是从上面的那幅图啃出来的，所以有点不整齐，这个整个过程就是贝叶斯要刻画的，样本空间的改变导致了整个划分的概率发生了不确定的变化，以前大的可能现在变得小到没有，现在大的可能之前非常小。\n一般情况下，我们在实验开始之前已经知道了样本空间，以及它的“划分”，如果观察到某个划分发生了，我们可以相应的变更我们所关注的事件A，这是一般上的从原因推出结果，典型的概率论过程，已知原理推结果。\n实际上我们不可能，也没能力知道这个划分，但是结果我们只要观察可能定能看到，于是我们通过结果A来反过来推这个划分是什么样子的，那么这个过程是个典型的数理统计过程，已知结果推原理，也叫贝叶斯统计。\n条件贝叶斯定理 Conditional Version of Bayes\u0026rsquo;s Theorem 所有概率公式定理都可以改装成条件版本，Bayes\u0026rsquo;s Theorem也不例外，当然和原始公式基本一致：\n$$ Pr(B_i|A\\cap C)=\\frac{Pr(B_i|C)Pr(A|B_i\\cap C)}{\\sum^k_{j=1}Pr(B_j|C)Pr(A|B_j\\cap C)} $$\n对于扩展到条件概率，我们一般的做法就是给所有项的条件上都并上一个附加的事件。\n先验概率，后验概率 Prior and Posterior Probabilities 我记得大学的时候学概率论的时候，第一次学贝叶斯公式，那时候老师跟我们说贝叶斯的每一项都有一个名字，先验，后验，似然值什么的，那时候没心思学习数学理论，考试的时候只能去背一下，结果一直也不知道啥意思，后来干活的时候，也遇到过Prior filter 和posterior filter，当时只知道一个前置滤波器，一个是后置滤波器，当看DeGroot的书的时候，看到Prior Probability和Posterior Probability才知道原来这个先验概率和后验概率是先于试验和后于试验的意思，很明显，等号前面的那个是要求的，肯定是试验后要计算的，肯定是后验概率，分母上的$Pr(B_i)$ 这个划分明显是试验之前知道的，故而叫做先验概率。\n多步后验概率计算 Computation of Posterior Probabilities in More Than One Stage 这部分可以看做是个应用但是很有实际意义： 两个硬币，一个是正常的,A,两面不同 ${H,T}$ ，另一个是特制的B，两面相同都是 ${H}$ ，在我们不知道拿出了哪个硬币的前提下，丢硬币，第一次出现了$H_1$ 是个H，下标表示试验步数，那么我们可以使用贝叶斯计算：\n$$ Pr(A|H_1)=\\frac{Pr(H_1|A)Pr(A)}{Pr(H_1|A)Pr(A)+Pr(H_1|B)Pr(B)}=\\frac{\\frac{1}{2}\\times \\frac{1}{2}}{\\frac{3}{4}}=\\frac{1}{3}\\ Pr(B|H_1)=\\frac{Pr(H_1|B)Pr(B)}{Pr(H_1|A)Pr(A)+Pr(H_1|B)Pr(B)}=\\frac{1\\times \\frac{1}{2}}{\\frac{3}{4}}=\\frac{2}{3} $$\n这是第一步的结果，那么如果我们继续丢同一个硬币，观察结果，我们得到了$H_2$ 还是一个H，那么我们将得到条件版本的贝叶斯用法： $$ Pr(A|H_1\\cap H_2)\\ =\\frac{Pr(H_2|A\\cap H_1)Pr(A\\cap H_1)}{Pr(H_2|A\\cap H_1)Pr(A\\cap H_1)+Pr(H_1|B\\cap H_1)Pr(B\\cap H_1)}\\ =\\frac{Pr(H_2|A\\cap H_1)Pr(A|H_1)Pr(H_1)}{Pr(H_2|A\\cap H_1)Pr(A|H_1)Pr(H_1)+Pr(H_1|B\\cap H_1)Pr(B| H_1)Pr(H_1)}\\ =\\frac{Pr(H_2|A\\cap H_1)Pr(A|H_1)}{Pr(H_2|A\\cap H_1)Pr(A|H_1)+Pr(H_1|B\\cap H_1)Pr(B| H_1)} $$ 上述过程对于B是一样的，通过条件概率的公式，可以化简得到上述公式，通过对比，我们发现两步的差别就是第二步的Prior 是第一步的posterior，如果是多步的试验，可以完全满足这个规律上一步的posterior用作下一步的prior 这种和我们现实中的各种推理完全符合，我们通过推理分析，修改了我们最初的估计，并实现在估计更接近事实，这就是个进化过程。 计算得出我们第二步的结果： $$ Pr(A|H_1\\cap H_2)=\\frac{1}{5}\\ Pr(B|H_1\\cap H_2)=\\frac{4}{5} $$ 这是符合我们的感觉的，出现H越多，A的可能性就越低。\n条件独立事件 Conditionally Independent Events 概要观察上面这个丢硬币的例子，$H_2$ 的出现和 $H_1$ 的出现应该是相互独立的，但是结合上面的计算我们发现当$H_1$ 发生的时候，$H_2$的不确定性被影响了，所以在A和B不确定时，并不独立，但是当我们确定是一个正常的硬币，也就是A确定发生的时候，$H_1$ 和 $H_2$ 独立。 而且我们观察上面的计算过程，如果我们继续算下去，当前步骤的先验概率（上一步中的后验概率）会逐渐收敛，而不论初始值是什么:\nN = 10 P_a_=0.5 P_b_=0.5 for i in range(N): P_a_hi_=(P_a_*0.5)/(P_a_*0.5+P_b_*1.0) P_b_hi_=(P_b_*1)/(P_a_*0.5+P_b_*1.0) P_a_=P_a_hi_ P_b_=P_b_hi_ print \u0026#34;$Pr(A|\\\\bigcap^\u0026#34;+ `i` + \u0026#34; H_i\u0026#34; + \u0026#34;)=\u0026#34; + \u0026#34;%0.6f\u0026#34; %P_a_ + \\ \u0026#34;;Pr(B|\\\\bigcap^\u0026#34;+ `i` + \u0026#34; H_i\u0026#34; + \u0026#34;)=\u0026#34; + \u0026#34;%0.6f\u0026#34; %P_b_ +\u0026#34;$\u0026#34; 跑了下结果：\n 当$Pr(A)=0.1$ 和 $Pr(B)=0.9$ 时： $Pr(A|\\bigcap^0 H_i)=0.052632;Pr(B|\\bigcap^0 H_i)=0.947368$ $Pr(A|\\bigcap^1 H_i)=0.027027;Pr(B|\\bigcap^1 H_i)=0.972973$ $Pr(A|\\bigcap^2 H_i)=0.013699;Pr(B|\\bigcap^2 H_i)=0.986301$ $Pr(A|\\bigcap^3 H_i)=0.006897;Pr(B|\\bigcap^3 H_i)=0.993103$ $Pr(A|\\bigcap^4 H_i)=0.003460;Pr(B|\\bigcap^4 H_i)=0.996540$ $Pr(A|\\bigcap^5 H_i)=0.001733;Pr(B|\\bigcap^5 H_i)=0.998267$ $Pr(A|\\bigcap^6 H_i)=0.000867;Pr(B|\\bigcap^6 H_i)=0.999133$ $Pr(A|\\bigcap^7 H_i)=0.000434;Pr(B|\\bigcap^7 H_i)=0.999566$ $Pr(A|\\bigcap^8 H_i)=0.000217;Pr(B|\\bigcap^8 H_i)=0.999783$ $Pr(A|\\bigcap^9 H_i)=0.000108;Pr(B|\\bigcap^9 H_i)=0.999892$\n 当$Pr(A)=0.5$ 和 $Pr(B)=0.5$ 时：\n$Pr(A|\\bigcap^0 H_i)=0.333333;Pr(B|\\bigcap^0 H_i)=0.666667$ $Pr(A|\\bigcap^1 H_i)=0.200000;Pr(B|\\bigcap^1 H_i)=0.800000$ $Pr(A|\\bigcap^2 H_i)=0.111111;Pr(B|\\bigcap^2 H_i)=0.888889$ $Pr(A|\\bigcap^3 H_i)=0.058824;Pr(B|\\bigcap^3 H_i)=0.941176$ $Pr(A|\\bigcap^4 H_i)=0.030303;Pr(B|\\bigcap^4 H_i)=0.969697$ $Pr(A|\\bigcap^5 H_i)=0.015385;Pr(B|\\bigcap^5 H_i)=0.984615$ $Pr(A|\\bigcap^6 H_i)=0.007752;Pr(B|\\bigcap^6 H_i)=0.992248$ $Pr(A|\\bigcap^7 H_i)=0.003891;Pr(B|\\bigcap^7 H_i)=0.996109$ $Pr(A|\\bigcap^8 H_i)=0.001949;Pr(B|\\bigcap^8 H_i)=0.998051$ $Pr(A|\\bigcap^9 H_i)=0.000976;Pr(B|\\bigcap^9 H_i)=0.999024$\n 当$Pr(A)=0.9$ 和 $Pr(B)=0.1$ 时： $Pr(A|\\bigcap^0 H_i)=0.818182;Pr(B|\\bigcap^0 H_i)=0.181818$ $Pr(A|\\bigcap^1 H_i)=0.692308;Pr(B|\\bigcap^1 H_i)=0.307692$ $Pr(A|\\bigcap^2 H_i)=0.529412;Pr(B|\\bigcap^2 H_i)=0.470588$ $Pr(A|\\bigcap^3 H_i)=0.360000;Pr(B|\\bigcap^3 H_i)=0.640000$ $Pr(A|\\bigcap^4 H_i)=0.219512;Pr(B|\\bigcap^4 H_i)=0.780488$ $Pr(A|\\bigcap^5 H_i)=0.123288;Pr(B|\\bigcap^5 H_i)=0.876712$ $Pr(A|\\bigcap^6 H_i)=0.065693;Pr(B|\\bigcap^6 H_i)=0.934307$ $Pr(A|\\bigcap^7 H_i)=0.033962;Pr(B|\\bigcap^7 H_i)=0.966038$ $Pr(A|\\bigcap^8 H_i)=0.017274;Pr(B|\\bigcap^8 H_i)=0.982726$ $Pr(A|\\bigcap^9 H_i)=0.008712;Pr(B|\\bigcap^9 H_i)=0.991288$\n 当次数足够多的时候，会收敛到一个常数值，为什么呢？我们可以去研究一下他的递推公式 $$ x_i=\\frac{ax_{i-1}}{ax_{i-1}+b(1-x_{i-1})} $$ 没算出来具体怎么收敛，数分还没学好，如果后面会了回来算一下。\n补充个例子：\n某种病毒在人口中的携带率是$Pr(pv)=0.03$ (pv,nv分别代表携带病毒和不携带病毒)检测的时候中会出现一下情况（有机器学习经验同学会对这个非常熟悉，包括True Positive (TP)，False Positive (FP)，False Negtive (FN），True Negtive (TP)）： $$ Pr(P|pv)=0.99\\ Pr(P|nv)=0.05\\ Pr(N|pv)=0.01\\ Pr(N|nv)=0.95\\ $$ 当假定某人被检测其携带病毒，那么此人携带病毒的概率是多大？\n 我们没文化，根据实验说明，我们有99% 的概率携带病毒，如果病毒致死律很高，这个概率足以让你准备后事。 如果我们有文化，考虑下先验知识，病毒本身携带率那么低，当我们被检出携带病毒，那么我们结合携带率和实验准确度，可以得出：$Pr(pv|P)=\\frac{Pr(P|pv)Pr(pv)}{Pr(P|pv)Pr(pv)+Pr(P|nv)Pr(nv)}=\\frac{0.03\\times 0.99}{0.03\\times 0.99+0.97\\times 0.05}=0.380$ ,这个看起来还可以抢救一下  这种实验的关键是提高准确率也就是0.99需要更高比如0.9999999，这是后检测结果更接近真实结果，0.99会产生大量虚报，这个在机器学习上非常有用！\n总结 引用陈希孺先生的一句话总结：“概率思维是人们正确观察事物必备的文化修养，这样说并不过分”。 有文化真好。 ","permalink":"https://go.face2ai.com/math/math-probability-2-3-bayes-teorem.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是关于Bayes\u0026rsquo; Theorem 的介绍性知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Bayes\u0026rsquo; Theorem，Law of total Probability\u003c/p\u003e","title":"【概率论】2-3:贝叶斯定理(Bayes' Theorem)"},{"content":"Abstract: 本文介绍事件的独立，以及派生出来的多事件独立，以及条件独立 Keywords: Independent Events，Independent of Several Events，Coditionally Independent Events\n独立事件 今天想说的废话就是，其实什么行业想做的好都要靠知识，做软件是这样，硬件也是，说相声是，其他行业也是，刚开始可能觉得经验丰富的人可以很顺利的做好任何我们觉得很难的事，过了三五年发现我们自己也可以很顺利的完成了，我们就变得优秀了么？其实不是，时间成本决定了你可以在不太努力的情况下成长，但是这种成长速度不足以让你成为行业顶级人物，当然不是所有人都想做顶级，想做顶级，必须强迫自己去做一些舒适区以外的事情。 从根本上思考理解自己做的事，同时有计划的训练才是野蛮生长的最快途径。 美国人Noel Tichy提出的理论，图里的3个区可以表示为你想学习的事物的等级:\n不要停留在舒适区，也不要好高骛远的直接去Panic Zone，那样会死的很惨，多去学习区，这样才能不断的进步。\n独立事件 Independent Events 上一篇我们讲到条件概率，这一篇其实是和上一篇同样的研究两个事件间的关系的，条件概率最朴素的想法就是当一个事件发生了，其对我们关心的事件是否发生有没有影响，有影响与否这都是个条件概率，如果没有影响，我们进一步把这两个事件称之为独立的。 独立与互斥，对立等意义都不相同，独立性是概率论的重要基础关系，在独立事件上建立的概率论理论已经很完备了，但是对于非独立事件相关的研究开在继续完善中，很多情况下我们都是假设，或者近似一些事件独立的，来是模型更加简单准确。 我们上一篇中说的条件概率，假定我们已知事件B发生，那么事件A发生的概率在此条件下发生的概率是$Pr(A|B)$ 如果我们进一步说，B事件的发生与否对于A事件来说没有什么影响，那么我们就有$Pr(A|B)=Pr(A)$ 上文中我们有一个很重要的乘法定义（条件概率定义引出的） $$ Pr(A|B)=\\frac{Pr(A\\cap B)}{Pr(B)}\\quad where \\quad Pr(B)\u0026gt;0\\ Pr(A\\cap B)=Pr(A|B)Pr(B) \\quad where \\quad Pr(B)\u0026gt;0 $$ 那么当我们带入$Pr(A|B)=Pr(A)$ 可以得到一个让我们一直使用到去世的关系： $$ Pr(A\\cap B)=Pr(A)Pr(B) $$ 以上关系成立的充分必要条件是，事件A和事件B相互独立。\n两个事件独立 Independence of Two Events  Definition Independent Events.Two events A and B are independent if $$ Pr(A\\cap B)=Pr(A)Pr(B) $$\n 定义是说如果事件A和事件B满足上述关系，那么他们独立，注意，对于数学定义来讲，其条件会完全在定义的条件中体现，如果没有，那么就说明没限制。\n举个🌰 ： 两个机器甲和乙，之间没有影响，独立工作，事件A表示甲工作三小时后坏掉$Pr(A)=\\frac{1}{4}$，事件B表示乙工作山小时后坏掉$Pr(B)=\\frac{1}{3}$，那么，当甲乙同时开始工作三个小时，那么他俩一起坏掉的概率是多少？\n分析：甲乙之间独立，互不影响，所以从原理上满足组成复合事件时互相独立，那么$Pr(A\\cap B)=Pr(A)Pr(B)=\\frac{1}{4}\\times \\frac{1}{3}=\\frac{1}{12}$\n互斥事件和独立事件 Exclusive Events and independent Events 说一个我之前初学概率时的一个思维误区: 我一直以为独立在集合上的反映就是不想交，，像这样： 这是一种很自然的想法，当题设的条件中用到独立，互不影响地进进行两个实验的时候，我们很自然的把他们理解为两个样本空间，所以打死不会有交集，但事实是我们关心的是这两个试验的一个复合结果产生的样本空间。\n但事实是这是个明显错误的，当A和B事件作为同一个试验的事件，那么当B发生的时候严重影响A的概率，因为当B发生的时候，A不可能发生，根据1-1中的T1，我们可以确定当$Pr(B)\\geq 0$ 的时候 $Pr(A|B)=\\frac{Pr(A\\cap B)}{Pr(B)}=\\frac{Pr(\\emptyset)}{Pr(B)}=0$ 这是个对立事件，不是独立事件。\n分析独立事件的定义，我们可以看出独立并不等于不想交，从频率角度的解释是相交部分的面积占事件B的比例，刚好等于事件A面积对整个样本空间的面积比例:\n用图像的理解，A的面积与整体S的面积比等于灰色与B面积的比。\n更深入的说，试验甲可能会产生一个包含10个事件 $A_1\\dots A_{10}$ 的样本空间，试验乙可能会产生一个包含5个事件 $B_1\\dots A_5$ 的样本空间，于是我们组合出来$10\\times 5=50$ 个复合事件的样本空间（笛卡尔积，或者使用事件的乘法原理），我们关心的是这50个复合事件中的某个事件的概率；这时才是考虑组成复合事件的基本事件独立与否的时候，而不应该是脱离了复合事件直接分开考虑$A_i$ 和 $B_i$ 之间的独立性。 前一篇说过图像只能帮我们初步的感受其大概的意思，分析学方法才能帮助我们从逻辑的角度深入研究，分析方法就是看定义的公式，如果$Pr(A)\\neq 0$ 和 $Pr(B)\\neq 0$ ，那么交集的概率必然不是0，所以A与B必然有交集。\n补集之间的独立 Independence of Complements 如果我们说两个事件独立了，其相交的事件的概率和这两个事件的关系会被确定，同样我们能得到其补集间的关系：\n Theorem If two events A and B are independent,then the events $A$ and $B^c$ are also independent.\n 怎么证明呢，书上写的比较跳跃，我来补充下： $$ A=(A\\cap B^c)\\cup(A\\cap B)\\ Pr(A)=Pr(A\\cap B^c)+Pr(A\\cap B)\\ Pr(A\\cap B)=Pr(A)Pr(B)\\ Pr(A\\cap B^c)=Pr(A)-Pr(A)Pr(B)=Pr(A)(1-Pr(B))=Pr(A)Pr(B^c) $$ Q.E.D 上面第一步是集合论知识，第二步是[1-1]中的T2，接着第四步是[1-1]中的T3，证毕。\n根据上面的结论，当事件$A$ 和$B$ 独立的时候，那么$A$ 和$B^c$ 独立，继续推导$A^c$ 和$B^c$ 独立。\n多事件独立 Independent of Several Events 把两个事件扩展到多个事件稍微有点复杂，从原理上讲，这些独立的事件里，某一个或者某几个发生并不影响其他事件发生的概率，那么他们就是独立的\n Definition Mutually Independent Events The k events $A_1,\\dots,A_k$ are independent or mutually independent if for every subset $A_{i_1},\\dots,A_{i_j}$ of j of these events($j=2,3,\\dots,k$) $$ Pr(A_{i_1}\\cap \\dots \\cap A_{i_j} )=Pr(A_{i_1})\\dots Pr(A_{i_j}) $$\n 这个定义其实看起来有点费力，尤其是当出现两重下标的时候，公式显得复杂很多，但是如果看过分析类的书就可以用一种很简单的方式解释，那就是对于下标 $i_j$ 我们可以把它看成一个 $i(j):N\\to N$的映射，映射从 $[1,k]$ 映射到 $[1,k]$ 可以出现多对一，这样就会产生一个子集 ，而上面的表示可以产生一个集合的全部子集，可以用排列的方式计算一共有多少。\n用中文表示一下就是一组事件独立，当其全部子集内的事件都是独立的，那么这组事件相互独立，还是个递归定义，三个事件的组合要通过两个事件的独立验证，四个事件独立要用三个事件和两个事件的独立来验证。\n例如：\n 当 $$ Pr(A\\cap B)=Pr(A)Pr(B)\\ Pr(A\\cap C)=Pr(A)Pr(C)\\ Pr(B\\cap C)=Pr(B)Pr(C)\\ Pr(A\\cap B\\cap C)=Pr(A)Pr(B)Pr(C) $$ 必须满足上述四个关系，才能能保证三个事件独立。\n但是\n $Pr(A\\cap B\\cap C)=Pr(A)Pr(B)Pr(C)$ 不能保证三个事件独立 这三个也不行 $$ Pr(A\\cap B)=Pr(A)Pr(B)\\ Pr(A\\cap C)=Pr(A)Pr(C)\\ Pr(B\\cap C)=Pr(B)Pr(C)\\ $$  下面的这个例子，说明这个情况： 在扔两个硬币的样本空间$S={HH,HT,TH,TT}$下有以下几个事件：\n①$A={HH,HT}$\n②$B={HH,TH}$\n③$C={HH,TT}$\n那么$A\\cap B \\cap C=A\\cap B=B\\cap C=A\\cap C={HH}$ $$ Pr(A\\cap B)=Pr(B\\cap C)=Pr(A\\cap C)=Pr(A\\cap B \\cap C)=\\frac{1}{4} $$ 同时满足 $$ Pr(A\\cap B)=Pr(A)Pr(B)\\ Pr(A\\cap C)=Pr(A)Pr(C)\\ Pr(B\\cap C)=Pr(B)Pr(C)\\ $$ 但是不满足$Pr(A\\cap B\\cap C)=Pr(A)Pr(B)Pr(C)$ 所以三个事件相互不独立。\n 上面例子也好定理也好，都是要强调完整的子集和的集合。\n独立和条件概率 Independence and Conditional Probability 补充个多事件独立的条件概率的定义：\n Theorem Let $A_1,A_2,\\dots,A_k$ be events such that $Pr(A_1\\cap \\dots \\cap A_k)\u0026gt;0$ .Then $A_1\\dots A_m$ and $A_1\\dots A_l$ of$A_1\\dots A_k$ we have $$ Pr(A_{i_1}\\cap \\dots \\cap A_{i_m}|A_{j_1}\\cap \\dots \\cap A_{j_l})=Pr(A_{i_1}\\cap \\dots \\cap A_{i_m}) $$ 这个解释是两个事件的独立的条件理解的扩展，这里不再过多解释啦。\n 条件独立事件 Coditionally Independent Events 条件概率上一篇就说到了，我们可以给任何事件的概率加上条件，只是有些可以不加，或者省略，而且条件概率的所有性质，定理，公理都与概率一致（因为所有概率都是条件的）。那我们就顺着这个思路，个独立事件加上条件，称之为条件独立。条件独立更加复杂，当然也更加通用。但其定义方法及其简单，我们直接对多个事件的定义下手，两个事件的条件独立被包含在其中：\n Theorem Conditional Independence We say that event $A_1\\dots A_k$ are conditional independent given B if,for every subcollection $A_{i_1},\\dots ,A_{i_j}$ of $j$ of these events ($j=2,3,\\dots,k$) $$ Pr(A_{i_1}\\cap \\dots \\cap A_{i_j}|B)=Pr(A_{i_1}|B)\\dots Pr(A_{i_j}|B) $$\n 这个定义的解释和上面多事件的解释基本一致，一组事件在条件B下独立的条件是他的所有子集的组合都是在条件B下独立的。\n对于两个事件的条件独立，我们有下面这个定理\n Theorem Suppose that $A_1,A_2$ and $B$ are events such that $Pr(A_1\\cap B)\u0026gt;0$ Then $A_1$ and $A_2$ are conditional independent given $B$ if and only if $Pr(A_2|A_1\\cap B)=Pr(A_2|B)$\n 证明过程不复杂： $$ Pr(A_1\\cap A_2 \\cap B)=Pr(A_1\\cap A_2|B)Pr(B)=Pr(A_2|A_1\\cap B)Pr(A_1|B)Pr(B)\\ then: \\quad Pr(A_1\\cap A_2|B)=Pr(A_2|A_1\\cap B)Pr(A_1|B)\\ for: \\quad Pr(A_1\\cap A_2|B)=Pr(A_1|B)Pr(A_2|B)\\ so: \\quad Pr(A_1|B)Pr(A_2|B)=Pr(A_2|A_1\\cap B)Pr(A_1|B)\\ Pr(A_2|A_1\\cap B)=Pr(A_2|B) $$\n总结 总结就一句话，掌握了这一句就掌握了这一篇的所有，什么是独立，互不影响就是独立\n","permalink":"https://go.face2ai.com/math/math-probability-2-2-independent-events.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍事件的独立，以及派生出来的多事件独立，以及条件独立\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Independent Events，Independent of Several Events，Coditionally Independent Events\u003c/p\u003e","title":"【概率论】2-2:独立事件(Independent Events)"},{"content":"Abstract: 本文介绍条件概率的定义及相关知识，提出全概率公式 Keywords: Conditional Probability，Multiplication Rule，Partitions ，Law of total Probability\n条件概率 关于学习看过不同人的很多说法，大部分来自“知乎（分享你新编的故事）”，虽然有些不太可信，但是有一些确实有点道理，比如有人问是否应该着重训练微积分的计算能力，其实这个问题我个人也有过，原因就是经过高考的人有点不太能把握“什么程度才能叫做一项知识学会了”，因为就算你对一个知识点非常透彻，但是还是有人能通过这个知识点创造出你完全不会解的题，于是我们就开始怀疑自己到底到底学会没学会，那微积分有必要反复做题么？有一个答案的大概意思是，反复训练微积分，就像反复训练加减法一样，过了小学，这个训练就没有必要进行了，这个似乎很有道理。 那么我们准备做机器学习的同学们有必要反复训练自己的基础算法么？比如线性回归啊，感知机啊什么的么？开始的时候当然很有必要，但是当你进入一定阶段了，就要研究其背后的分析方法的原理了，所以，会用算法，理解算法，分析算法是完全不同的层次，就像小学，大学还有博士阶段一样。所以，打好基础，学好数学，才能读博士.当然现在如火如荼的环境下，小学生就业也非常乐观。 今天继续概率论的讨论，本来想把事件的独立也添加到本文，但是后来分析了一下，内容太多容易乱，所以本文只介绍条件概率，还是那句话，好就不能快，多就不能省，虽然感觉时间紧迫，但是也要一步一步来。\n条件概率的定义 The Definition of Conditional Probability 概率本身用途有限，为什么？首先从我们的知识图上可以看到微积分，线性代数，概率论本身处于数学基础，基础很少被直接应用于平时的问题中，而概率更是如此，除了算算彩票，骰子，基本没有什么场景能完全满足概率的“条件”，但是作为基础，统计和随机过程则是非常实际的应用方法，概率对于统计的一个作用就是：当知道某个特定的事件被观察到的时候，通过概率论的知识，可以更新某些事件的概率。 比如我们观察一个试验，事件A是我们关心的，试验结束后，我们得到的关于A是否发生的信息是事件B发生了，那么此时事件A的概率被称为条件B发生时A的条件概率。\n Definition Conditional Probability: Sippose that we learn that an event B has occurred and that we wish to compute the probability of another event A taking into account that probability of the event A given that the event B has occurred and is denoted $Pr(A|B)$ . If $Pr(B)\u0026gt;0$ ,we compute this probability as: $$ Pr(A|B)=\\frac{Pr(A\\cap B)}{Pr(B)} $$ ps:The conditional probability $Pr(A|B)$ is not defined if $Pr(B)=0$\n 翻译一下，当我们知道事件B已经发生并且我们希望计算另外一个事件A的时候考虑当B发生时A的概率，那就是条件概率 $Pr(A|B)$ 。当$Pr(B)=0$ 时，条件概率无定义\n但是我们应该注意到，所有事件的概率都是条件的，比如试验：扔一个均匀的硬币，事件A表示正面，那么概率$Pr(A)$ 的条件就是\u0026quot;扔一个均匀硬币\u0026quot;，如果把扔一个均匀的硬币这个事件看做事件B，那么出现正面的概率是$Pr(A|B)$ ，这里事件B是肯定发生的，所以作为条件给出，如果事件B不是肯定发生，那么就要用我们下面给出的计算法则计算正面出现的概率了。 以上是从Subjective Interpretation的角度定义的，但是从Frequency Interpretation的角度，我们可以这么理解，一个重复$N$ 次的试验事件B发生的$N_B$次数与试验次数的比例近似于$Pr(B)=\\frac{N_B}{N}$ 事件A和事件B同时发生的次数$N_{A\\cap B}$ 与试验次数$N$ 的比例近似于$Pr(A\\cap B)=\\frac{N_{A\\cap B}}{N}$，所以当B事件发生时，A事件也发生的概率接近于比例：$\\frac{N_{A\\cap B}}{N_B}=\\frac{\\frac{N_{A\\cap B}}{N}}{\\frac{N_B}{N}}=\\frac{Pr(A\\cap B)}{Pr(B)}$\n通过上面的几个表达式可以了解关于频率观点下的条件概率的计算方法。\n 举个例子： 条件描述：扔两个六面体骰子，每个面出现概率相等，两个骰子互不影响。 事件的概率：那么当我们知道其结果的和是奇数的条件下，其和小于8的事件T的概率是多少？ 分析：首先我们通过条件知道这两个骰子出现每个数字概率相等为 $\\frac{1}{6}$ 那么就可以分析出所有结果了：     event Probability     2 $\\frac{1}{36}$   3 $\\frac{2}{36}$   4 $\\frac{3}{36}$   5 $\\frac{4}{36}$   6 $\\frac{5}{36}$   7 $\\frac{6}{36}$   8 $\\frac{5}{36}$   9 $\\frac{4}{36}$   10 $\\frac{3}{36}$   11 $\\frac{2}{36}$   12 $\\frac{1}{36}$    那么： $$ Pr(A\\cap B)=\\frac{2}{36}+\\frac{4}{36}+\\frac{6}{36}=\\frac{12}{36}=\\frac{1}{3}\\ Pr(B)=\\frac{2}{36}+\\frac{4}{36}+\\frac{6}{36}+\\frac{4}{36}+\\frac{2}{36}=\\frac{1}{2} $$ Hence: $$ Pr(A|B)=\\frac{Pr(A\\cap B)}{Pr(B)}=\\frac{2}{3} $$ 2. 再举个例子，两个箱子，装着不同的螺丝，箱子A装着长螺丝7个和短螺丝3个，B装长螺丝6个短螺丝4个，这两个箱子被随机分给我们，如果我们有 $\\frac{1}{3}$ 的概率被分到箱子A，$\\frac{2}{3}$ 的概率被分到箱子B，那么当我们已知被分到A箱子的时候，我们拿出一个长螺丝的概率是多少？\n这道题看起来很复杂，当然求问题的解很简单，$\\frac{Pr(N_L\\cap A)}{Pr(A)}=\\frac{7}{7+3}$ ，但是这道明显是个分步的试验，首先得到什么样的箱子是个未知的随机事件，然后从箱子里拿螺丝还是随机事件，但是当我们已知是哪个箱子了以后，拿螺丝的过程将会被很容易的描述\n在进入下一部分之前我想简单分析下什么时候可能条件概率，第一种就是一个试验，结果被划分成不同的事件，试验结束后，可以观察到某个事件B发生了，这是我们会更新我们在试验前计算的事件A的概率，称之为条件概率$Pr(A|B)$ ；第二种是分步的试验，第二步的试验产生的事件A可能根据第一步的不同而不同，所以当第一步产生了事件B，那么事件A的概率将会被重新计算为$Pr(A|B)$ 其值等于 $\\frac{Pr(A\\cap B)}{Pr(B)}$ 的结果。 同时我们观察这个关系式可以得出$Pr(B)\\leq 1$所以$Pr(A|B)\\leq Pr(A\\cap B)$，这个关系式是确定的，但是$Pr(A|B)\\leq \\geq Pr(A)$ 之间的大小不确定。\n乘法原则 The Multiplication Rule 乘法原则在之前的计数方法里有提到过，就是分步试验每步有不同的结果，那么组合起来其满足乘法关系，这里的乘法法则是通过上面条件概率的定义得出的：\n Definition Multiplication Rule for Conditional Probability: $$ if \\quad Pr(B)\u0026gt;0:\\quad Pr(A\\cap B)=Pr(B)Pr(A|B)\\ if \\quad Pr(A)\u0026gt;0:\\quad Pr(A\\cap B)=Pr(A)Pr(B|A) $$\n 这两个式子是上面条件概率公式的变形，但是却有了不同含义，我们把它定义为条件概率的乘法原则\n Definition Multiplication Rule for Conditional Probability:Suppose that $A_1,A_2,A_3\\dots A_n$ are events such that $Pr(A_1\\cap A_2\\cap A_3\\dots \\cap A_{n-1})\u0026gt;0$ then $$ Pr(A_1\\cap A_2\\cap A_3\\dots \\cap A_n)=\\ Pr(A_1)Pr(A_2|A_1)Pr(A_3|A_1\\cap A_2)\\dots Pr(A_n|A_1\\cap A_2 \\cap A_3 \\cap \\dots \\cap A_{n-1}) $$\n 这个公式的证明很容易，把等号右边的式子前两个结合，得到一个事件并的事件，把它设为一个新事件$C_i$并进行替换和迭代，就能根据上面的$Pr(A\\cap C_i)=Pr(C_i)Pr(A|C_i)$ 把全部整合，最后得到等号左边的结果。\n举个例子来使用上面的公式，如果一个盒子里有r个红色球，b个蓝色球，其中r和b均大于2，那么我们每次随机取出一个球，without replacement，那么我们取出4个球，其排列是\u0026quot;红，蓝，红，蓝\u0026quot;的概率是多少呢？ 分析，首先取出球的概率是会相互影响的，因为是without replacement，除了第一个，后面的球的概率都会因为上一次的取出而改变，所以我们假设取出序列为\u0026quot;红，蓝，红，蓝\u0026quot;的事件为$R_1\\cap B_2\\cap R_3\\cap B_4$ 那么 $$ Pr(R_1\\cap B_2\\cap R_3\\cap B_4)=Pr(R_1)Pr(B_2|R_1)Pr(R_3|R_1\\cap B_2)Pr(B_4|R_1\\cap B_2\\cap R_3)\\ =\\frac{r}{r+b}\\frac{b}{r+b-1}\\frac{r-1}{r+b-2}\\frac{b-1}{r+b-3} $$ 这里主要的一个关键点就是分步试验的后面步骤受到前面步骤的影响，所以最后的结果是用条件概率给出的乘法关系。 条件概率的性质和普通概率的性质一样，因为我们开篇的时候说过所有的概率都是条件概率，只不过有些条件是规定的必然出现的，我们就把这个条件省略掉当成已知试验条件，不用考虑进行计算。 为了验证上面这句话，我们给出下面这个定理:\n Suppose that $A_1,A_2,A_3\\dots A_n,B$ are events such that $Pr(B)\u0026gt;0$ and $Pr(A_1\\cap A_2\\cap A_3 \\dots A_{n-1}|B)\u0026gt;0$ then: $$ Pr(A_1\\cap A_2\\cap \\dots A_n|B)=\\ Pr(A_1|B)Pr(A_2|A_1\\cap B)Pr(A_3|A_2\\cap A_1\\cap B)\\dots Pr(A_n|A_{n-1} \\cap \\dots \\cap A_2\\cap A_1\\cap B) $$\n 上面这个过程的证明和上面乘法原理的证明一样，就是通过等号右边每两个结合运用乘法原理，能够得到和等号左边一样的结果。 我们只要掌握一个规律就可以，那就是，条件概率和普通的概率一样，加上某个条件时，其计算方法和不加这个条件时候计算方法一致。\n条件概率与分割，全概率公式 Conditional Probability and Partition - Law of total Probability 在1-1的T3中，我们介绍了当一个样本空间被划分成两部分的时候，概率的计算方法，那么如果我们把切分继续下去，也就是一个样本空间我们把它切成k块不相交的子空间时，那么响应的计算会有什么变换呢？\n Definition partition Let S denote the sample space of some experiment,and consider k events $B_1 \\dots B_k$ in S such that $B_1 \\dots B_k$ are disjoint and $\\bigcup^k_{i=1}B_i=S$ It is said that these events from a partition of S\n 翻译下，意思是说把样本空间打碎成k个disjointed的事件，这些事件可以组合成S，那么打碎的过程叫做partition\n一般来说，当一个碎片发生时，整个试验的不确定性将会降低，因为其结果空间变得小了，但并不意味着这个碎片上含有的事件的概率会升高。\n根据上面打碎原理，我们可以得出下面的全概率公式，\n Theorem Law of total probability:Suppose that the events $B_1 \\dots B_k$ from a partition of the space S and $Pr(B_j)\u0026gt;0$ for $j=1,\\dots ,k$ Then ,for every event A in S: $$ Pr(A)=\\sum^k_{j=1}Pr(B_j)Pr(A|B_j) $$\n 上述公式为全概率公式，将一个样本空间分割成k个不相交的小空间，然后每个空间上有事件A的一部分在整个空间上的概率为$Pr(A\\cap B_j)=Pr(A|B_j)Pr(B_j)$ 把他们都加起来就是完整的事件A的概率了。 全概率公式可以通过乘法原理和partition的定义获得，当然也可以画图证明，通过集合论的知识也可以。\n①画图： 圆圈是内是A，各分块内都有A的一部分，$B_i\\cap A$，那么所有的部分加起来就是完整的A，通过下面集合论也可以完整的解释\n②集合论： $$ A=(B_1\\cap A)\\cup(B_1\\cap A)\\cup\\dots \\cup(B_k\\cap A) $$ 并且 $(B_j\\cap A)$ 之间是disjointed ，所以 $$ Pr(A)=\\sum^k_{j=1}Pr(B_j\\cap A)\\ if \\quad Pr(B_j)\u0026gt;0 (j=1\\dots k)\\quad then \\quad Pr(B_j\\cap A)=Pr(B_j)Pr(A|B_j) $$ 就可以得到上述的全概率公式了。\n同样全概率公式也有条件版本： $$ Pr(A|C)=\\sum^k_{j=1}Pr(B_j|C)Pr(A|B_j\\cap C) $$ 通过画图可以简单的了解一下最简单的情况： 怎么样很机制吧，给完整的样本空间S再加个套，这样$Pr(S)\\neq 1$ 而是一个小于1的概率了，这种情况下产生了一个条件概率版本的全概率公式。 如果从分析的方法看： $$ A\\cap C=(B_1\\cap A \\cap C)\\cup(B_1\\cap A \\cap C)\\cup\\dots \\cup(B_k\\cap A \\cap C) $$ 并且 $(B_j\\cap A|C)$ 之间是disjointed ，所以 $$ Pr(A| C)=\\sum^k_{j=1}Pr(B_j\\cap A | C)=\\sum^k_{j=1}\\frac{Pr(B_j\\cap A \\cap C)}{Pr(C)}\\ if \\quad Pr(B_j)\u0026gt;0 (j=1\\dots k)\\quad then \\quad Pr(B_j\\cap A|C)=Pr(B_j)Pr(A|B_j\\cap C) $$ 证明过程和上述完全一致，这里就不再描述了。\n扩展试验 Augmented Experiment  Definition Augmented Experiment: If desired,any experiment can be augmented to include the potential or hypothetical observation of as much additional information as we would find useful to help us calculate any probabilities that we desire\n 上面这个定义是告诉我们所有的试验如果需要都能通过潜在或者假想的条件将其变成条件概率的形式，如果有利于计算，我们可以这样进行扩展。\n总结 果然一个条件概率就写了一天，如果加上独立事件肯定没办法写好，明天继续。。\n","permalink":"https://go.face2ai.com/math/math-probability-2-1-conditional-probability.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍条件概率的定义及相关知识，提出全概率公式\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Conditional Probability，Multiplication Rule，Partitions\n，Law of total Probability\u003c/p\u003e","title":"【概率论】2-1:条件概率(Conditional Probability)"},{"content":"Abstract: 本文主要介绍事件的并集对应的概率计算，以及一个补充的概率小知识，怎么用统计骗人 Keywords: Union of two Events，Union of Finite Number of Events，Statical Swindles\n事件的的并集 废话还是说说数学吧，学数学真的看不到立竿见影的事，相比学个C++、TensorFlow，这些更有成就感，毕竟写了就有结果可以看，数学学习的结果就是，你可能只会做两道题，没办法直接让你升值加薪，但是凡事都有因果，通过这几个月简单的学习，我发现身边的很多事都能用数学解释，比如今天要写的，如果我早些学习可能可以避免很多不必要的损失，而且通过学数学分析，可以通过一个人的语言来判断这个人的逻辑，进而判断这个人的性格，这是心理学的内容了，我不懂心理学，但是很感兴趣，如果有机会去研究下心理学，毕竟也跟人工智能强相关。 本篇介绍两个小知识点，关于事件的并集的概率求法，以及一些概率的日常应用\n两个事件的并 Union of Two Events 在前面1-1概率定义中的T7给出了两个相交的事件的并集的概率计算方法： $$Pr(A\\cup B)=Pr(A)+Pr(B)-Pr(A\\cap B)$$ 详细的证明在1-1中也有给出，这个公式在本文中将会进一步展开，把其延展到无数项，但是在开始之前我们还是来复习下这个定理，事件是试验结果的集合，集合的基本运算就是交，并，补，补集和概率的对应我们在1-1中的T3就是最基础的补集的概率计算，剩下就是交集和并集的计算了，T7给出了两个集合并集的概率计算公式，并给出了分析的证明方法，之前看书和上课老师都是给我们画个Venn图没然后说 $Pr(A\\cup B)$ 是 $Pr(A)+Pr(B)$ 但是重复加了一遍 $Pr(A\\cap B)$ 所以要减去。 提到Venn图说一下，就是关于理解数学，到底是用图形化的可视化的方法好，还是分析法好，这个没办法一棍子打死，大学之前老师们都喜欢用画图的方法教大家理解概念知识点，原因是高中，初中，知识点极其少，更多是的各种拐弯的习题，所以为了加深大家的理解，画个图，直观，而且更容易被人接受，但是到了大学以后，画图就不再合适了，因为知识点变多，而且有很多没办法用二维三维的图来解释，所以，分析的方法到了大学以后是更有用的，拿机器学习的例子来说，做可视化是一个方向，但是这个方向的结果大多是为了展示给一些没有背景的人来看的，业内人士多半关注参数。\n这里说了一大堆话的目的就是说明，分析到后面越来越有用，所以数学分析是数学系的开蒙课程。\n有限个事件的并 Union of Finite Number of Events 多个事件的并集，就是对上面“分析”理论的一个很好的诠释，当事件数量超过五个，Venn图马上就乱掉了，我们这里省去三个事件的并集的概率计算，直接进行更高难度的有限个事件的并集：\n Theorem For every events $A_1,\\dots,A_n$ , $$ Pr(\\bigcup^n_{i=1}A_i)=\\sum^n_{i=1}Pr(A_i)-\\sum_{i\u0026lt;j}Pr(A_i\\cap A_j)\\ +\\sum_{i\u0026lt;j\u0026lt;k}(A_i\\cap A_j \\cap A_k)-\\sum_{i\u0026lt;j\u0026lt;k\u0026lt;l}(A_i\\cap A_j\\cap A_k \\cap A_l)+\\ \\dots + (-1)^{n+1}Pr(A_1\\cap A_2 \\cap \\dots \\cap A_n) $$\n 给出了个公式，证明过程其实就是一个分析过程，所以证明需要用数学语言来完成，而不是画个图放在那，那么我们来分析这个问题，首先这个公式的变量n是个自然数，那么最基础的方法就是归纳法。\n 当$n=1$ 时，显然是成立的，其与1-1中的T7相等。 设当 $n=m$ 时成立 $$Pr(\\bigcup^m_{i=1}A_i)=\\sum^m_{i=1}Pr(A_i)-\\sum_{i\u0026lt;j}Pr(A_i\\cap A_j)\\ +\\sum_{i\u0026lt;j\u0026lt;k}(A_i\\cap A_j \\cap A_k)-\\sum_{i\u0026lt;j\u0026lt;k\u0026lt;l}(A_i\\cap A_j\\cap A_k \\cap A_l)+\\ \\dots + (-1)^{m+1}Pr(A_1\\cap A_2 \\cap \\dots \\cap A_m)$$ 当 $n=m+1$ 时，我们套用$Pr(A\\cup B)=Pr(A)+Pr(B)-Pr(A\\cap B)$ 公式，   其中 $\\bigcup^m_{i=1}A_i$ 为$A$，$A_{m+1}$ 为 $B$ 那么，$A\\cup B=\\bigcup^{m+1}_{i=1}A_i$ 最关键的是 $A\\cap B=\\bigcup^{m}{i=1}(A_i\\cap A{m+1})$ ，可以根据集合论的工时得到，可以看到 $Pr(B)-Pr(A\\cap B)$ 有： $$ Pr(B)-Pr(A\\cap B)=Pr(A_{m+1})-\\sum^m_{i=1}Pr(A_i\\cap A_{m+1})+\\sum_{i\u0026lt;j}Pr(A_i\\cap A_j \\cap A_{m+1})\\ -\\sum_{i\u0026lt;j\u0026lt;k}(A_i\\cap A_j \\cap A_k \\cap A_{m+1})+\\sum_{i\u0026lt;j\u0026lt;k\u0026lt;l}(A_i\\cap A_j\\cap A_k \\cap A_l\\cap A_{m+1})-\\ \\dots + (-1)^{m+2}Pr(A_1\\cap A_2 \\cap \\dots \\cap A_{m+1})) $$ 最关键的是，当$n=m+1$ 时，$Pr(\\bigcup^{m+1}{i=1}A_i)-Pr(\\bigcup^{m}{i=1}A_i)$ 和上面的表达式一致(计算过程太复杂，所以，这里省略) Q.E.D  上述证明比较粗糙，大家可以自己计算下，对于有限时间的并集的概率计算大致的意思就是加多了减，减多了再加，直观的，可以通过画三个集合的Venn图来观察，分析的，就是上述的大致过程。\n匹配问题 Matching Problem 上面是严格的数学证明，下面我们来分析一个简单但是有趣的应用，其中用到了多事件并集的概率计算，matching problem，配对或者叫做匹配游戏。 描述下问题，假设我们已有一个n个不同的符号的序列，我们来自己随便排列这n个符号的顺序（我们不知道这已有的排列顺序），如果我们排列的符号序列对应位置上的符号和已有符号能够对应上，就叫做一个match，那么当n变化的时候，match的概率$p_{n}$ 怎么描述呢？\n 分析：\n  假设第i个字母matching的事件为$A_i$，其概率$Pr(A_i)=\\frac{1}{n}$， 如果有1个match(k=1):$\\sum^{n}_{i=1}Pr(A_i)=n\\cdot \\frac{1}{n}=\\frac{1}{1!}$ 如果有2个match(k=2):$\\sum^{n}_{i\u0026lt;j}Pr(A_i\\cap A_j)=\\begin{pmatrix}n\\2\\end{pmatrix}\\cdot \\frac{1}{n(n-1)}=\\frac{1}{2!}$ 如果有3个match(k=3):$\\sum^{n}_{i\u0026lt;j\u0026lt;m}Pr(A_i\\cap A_j\\cap A_3)=\\begin{pmatrix}n\\3\\end{pmatrix}\\cdot \\frac{1}{n(n-1)(n-2)}=\\frac{1}{3!}$ $\\vdots$ 如果有k个match: $\\begin{pmatrix}n\\k\\end{pmatrix}\\cdot\\frac{n!}{(n-k)!}=\\frac{1}{k!}$ 所以，根据我们上面证明多事件并集得到的公式$p_n=\\frac{1}{1!}-\\frac{1}{2!}+\\frac{1}{3!}-\\frac{1}{4!}\\dots +(-1){n+1}\\frac{1}{n!}$ 当$lim_{n\\to \\infty}p_n=\\frac{1}{1!}-\\frac{1}{2!}+\\frac{1}{3!}-\\frac{1}{4!}\\dots +(-1){n+1}\\frac{1}{n!}=1-\\frac{1}{e}\\approx 0.63212$  也就是说当n无限多的时候，匹配成功的概率将会收敛到0.63212。我们可以发现当n=7的时候，写个小程序计算下结果：\na=20 result=1.0 factorial=1 for i in range(2,a,1): factorial=factorial * (i ) result+=(-1)**(i+1)*(1.0/(factorial)) print \u0026#39;n=\u0026#39;+`i`+\u0026#39;:\u0026#39;+`result` n=2:0.5 n=3:0.6666666666666666 n=4:0.625 n=5:0.6333333333333333 n=6:0.6319444444444444 n=7:0.6321428571428571 n=8:0.6321180555555556 n=9:0.632120811287478 n=10:0.6321205357142857 n=11:0.6321205607663941 n=12:0.6321205586787184 n=13:0.6321205588393088 n=14:0.6321205588278381 n=15:0.6321205588286029 n=16:0.6321205588285551 n=17:0.6321205588285579 n=18:0.6321205588285578 n=19:0.6321205588285578 Process finished with exit code 0 仔细观察发现当n=7的时候，就已经开始收敛了，也就是说，后面再怎么增加n也不会影响其概率了。\n概率欺诈 Statistical Swindles 接下来这些话题都是属于知识联系实际的科普小软文，有事的同学可以先行离开了。\n统计滥用 Misleading Using of Statistics 马克吐温说过：“世界上的谎言有三种，谎言，无耻的谎言，统计”，“你可以通过统计证明任何事情。” 我们学习概率的另一个重要用途就是日常中不会被报纸新闻上的统计信息欺骗，通过已有知识的判断真实的情况。比如经常说的平均工资，人均收入，这些都是统计概念，相比对其真实性大家都有所怀疑。\n完美预测 Perfect Forecasts 神预测，如果一个投资公司每周一给你推送一只股票，这个股票本周大涨50%，第二周又给你推送了另一只，结果又是大涨50%，第三周第四周，周周如此，你是不是会觉得，卧槽，发财的机会来了，卖房卖地卖媳妇也要进去赚一笔，但是实际投资公司不是神，只是耍了你而已。 他们的做法是什么呢？ 首先我们假定其推送的股票涨50%的概率是$Pr=\\frac{1}{n}$，那么他们想保证k周有人连续正确，他们只需要发给$n^k$ 个人就可以了\n 第一周，发送给所有人 第二周，发给第一周正确地人，正确的人数大概在$n^{k-1}$ 左右 第三周，发给第二周正确地人，正确的人数大概在$n^{k-2}$ 左右 第四周，发给第二周正确地人，正确的人数大概在$n^{k-3}$ 左右 $\\vdots$ 第k周，发给第k-1周正确地人，正确的人数大概在$1$左右  怎么样，如果他连续发给你一年都是正确的，可以保证，这个人有内幕消息，估计那样他根本不用发给你，他自己可以卖车卖房卖媳妇，而没必要从你身上赚取服务费了。 完美预测，需要的要么是人多，要么就是概率高一些。\n保证胜利 Guaranteed Winner 保证胜利，看球赛，有公司这么搞，给你推荐获胜球队，你可以去买彩票或者赌球，推荐对的收取一定服务费，不对不收费，你看起来这事没问题，稳赚不赔，但是，从概率的角度分析，他并不能提高你的获奖概率，而且当你获奖时他要收取服务费，当他没有内幕消息的时候，这个完全是骗人的，而这些公司完全没有任何风险，按照一定的比例给不同的人，推送不同的获胜方，这样他肯定会有收入（肯定会有正确的），具体怎么将收入最大化，那就是优化问题，\n买彩票 Improving Your Lottery Chances 买彩票，不知道大家有没有买过，我买过，我发现每期都有两个连续的数字，我们来分析下这种现象是否有概率依据。 假设我们的彩票是40个数字选6个，without replacement，各个数字间没有影响，那么这个模型就是个见得组合，那么一共有： $$ \\begin{pmatrix}40\\6\\end{pmatrix}=3,838,380 $$ 种组合，一等奖，全部命中的概率是 $\\frac{1}{3,838,380}$ 那么出现至少两个连续的号码呢？结果是大约有0.577的概率，怎么算？你可以先算出所有号都不连续的组合方式，然后用1-1中的T3，就可以得到一个大概的数字，因为这个存在一个边界的问题，所以还是要小心一点。\n总结 学习知识，就不会被人骗了，哈哈哈哈，明天继续。。\n","permalink":"https://go.face2ai.com/math/math-probability-1-4-union-of-event.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍事件的并集对应的概率计算，以及一个补充的概率小知识，怎么用统计骗人\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Union of two Events，Union of Finite Number of Events，Statical Swindles\u003c/p\u003e","title":"【概率论】1-4:事件的的并集(Union of Events and Statical Swindles)"},{"content":"Abstract: 本文主要介绍组合的相关知识，以及引出的二项式系数，多项式系数 Keywords: Combination，Binormial Coeffcient，Multinormial Coeffcient\n组合 苏东坡有诗云：“可使食无肉，不可居无竹。无肉令人瘦，无竹令人俗。人瘦尚可肥，俗士不可医。” 穷可以通过任意一种方法变富，但是没文化真的没办法短时间变富。从概率论的角度来说，如果我们有文化，我们可以通过买彩票变富，比如买32个数选6个，我们的中奖概率是$P_{32,6}$ ,我们有这么大的概率一夜变富，但是目前科技还没有能一夜变得才高八斗的方式概率能超过$P_{32,6}$ 的，所以，我一直跟周围的人说，钱很好赚，但是有文化这个事真的特别难，钱对于人来说个附属品，可以随时增减，但是文化思想是属性，不好改。\n组合方法 Combinatorial Methods 我们的废话中还复习了下上一节的知识，彩票分两种，一种是有序排列，一种是无序排列，比如开奖过程中Without Replacement的方式抽取，先后出来的号码是 ${4,1,5,3,8}$ ，那么第一种中奖方式是：“你的五个号码必须与上述号码一致，并且，顺序必须一致！如果你的号码是 ${1,4,3,8,5}$ ，那么恭喜你，一分钱都没中！”；第二种方式是：“号码部分顺序，只要你的数字是 ${1,3,4,5,8}$ 这个集合就可以”，第一种方式就是我们昨天说的排列问题，第二种就是我们今天要说的组合，两种计数方法的共同前提就是Without Replacement。如果With Replacement，那么情况将会完全不同。\n组合 Combination 重新描述下问题，回到最初的设计，我们把试验的所有结果看做一个有限的集合，也就是所说的有限的样本空间，多步without replacement的过程中，每一步之间没有影响，如果每次抽取的结果严格要求与步骤保持对应，就是所说的排列问题，如果我们只关心结果的元素而不关心顺序，那么我们就要用到组合（如果不太明白这段话，可以参考上面的例子，或者下面的例子）\n Definition Combination:Consider a set with $n$ elements.Each subset of size $k$ chosen from this set is called a combination of $n$ elements taken $k$ at a time.We denote the number of distinct such combinations by the symbol $C_{n,k}$\n 为什么说一次取 $k$ 个呢？因为一次拿出来就没有先后顺序了。所以当我们从一个有 ${1,2,3}$ 的集合里面拿出两个元素， Permutation的结果是：\n$$ { (1,2),(1,3),(2,1),(2,3),(3,1),(3,2) } $$\n而Combination的结果是：\n$$ { {1,2},{1,3},{2,3} } $$ 注意看我用的括号，一个是小括号，而一个大括号，小括号的(1,2)和(2,1)是不同的，但是大括号的{1,2}和{2,1}却不做区分，这个过程明确了，那么我们就要思考接下来的问题了，如何计算不考虑顺序的数量。\n数学有个非常有意思的地方就是，当你在不断地学习时，相当于你在不断的扩展你的工具包，而且这些工具包没有重要与次要，只是看你需要哪个，你可以用各种已经证明过得定理来证明你当前的问题，我们上一篇学到的Permutation就可以马上拿来计算Combination，而且，我们也可以用其他的定理来计算Combination。\n继续解决问题，如果我们倒着看，先看Combination我们假设 $A={1,2,3}$ 是一个集合S（包含 $n$ 个元素）的子集，这是一个combination的结果，那么它对应的S的Permutation结果就是：A集合中三个元素的Permutation，那么也就是说：\n$$ C_{n,3}*P_{3,3}=P_{n,3} $$\n很明显我们可以把3一般化成 $k(k\\leq n)$ 那么:\n$$ C_{n,k}*P_{k,k}=P_{n,k}\\ C_{n,k}=\\frac{P_{n,k}}{P_{k,k}}\\ C_{n,k}=\\frac{n!}{k!(n-k)!} $$ 再次说明一下，我们这两篇所讲的都是计数方法，不涉及到随机，不管是Permutation和Combination都是数数的过程，其结果是数字（一般来说都是自然数），对试验，以及试验结果没有任何影响，一旦试验条件确定，这个数字唯一确定，不会有任何更改。 重复强调其不影响随机的主要原因是，我上高中的时候老师给我们讲过二项式，而那时我理解的是在二项式中随意挑数字，所以有随机过程在里面，其实完全没有，这两节跟随机一毛钱关系都没有，所以下面我们开始研究二项式\n二项式系数 Binormial Coeffcient 二项式： $$ (x+y)^n $$ 其中 $n\u0026gt;0$ 为常整数，其展开形式就是二项式展开，每一项的系数就是二项式系数，举个🌰： $$ (x+y)^2=x^2+2xy+y^2 $$ 这个是比较简单的二项式（最简单的是：x+y），初中就学过，对应的n=2，二项式对应的系数： $x^2:1$ $xy:2$ $y^2:1$\n那么这个和我们的Combination有什么关系呢？ 我们展开二项式看一下： $$ (x+y)^n=\\underbrace{(x+y)(x+y)\\dots (x+y)}_{n} $$\n一共n个元素组成了多项式（，那么当我们想要得到项 $x^ky^{n-k}$ 的时候，我们要做的是在n个式子中选k个来使用x，而不需要关心y因为k个x一旦选中，y自然是剩下的n-k个式子的y，来看个图，就豁然开朗了： 我们来看 $x^1y^{n-1}$ ，最简单的是我们选中第一项的x 做为 $x^1$，他会和剩下的所有项的y相乘，但是不会和自己的y相乘，这样，我们还可以选剩下所有项中的x作为 $x^1$ ,一共有$C_{n,1}$ 种选择方法。\n接着我们看 $x^2y^{n-2}$ : 这个就复杂一点，首先黄色问好是想说当我们选中第一项，第二项的x来作为 $x^2$ 中的x的时候，我们要不要把他们中y放进 $y^{n-2}$ ,答案当然是不能，这两项的结合是 $xy$ 而不是 $x^2$ 这样，我们就可以继续上面的思路了，一共n个项，选出两项的方法就是$C_{n,2}$\n于是我们得出了我们的主题，二项式对应的系数就是Combination的结果：\n对于正整数 $n$ ，小于等于n的正整数 $k$ : $$ (x+y)^n=\\sum^n_{k=0}\\begin{pmatrix}n\\k\\end{pmatrix} x^ky^{n-k} $$\n这就是二项式定理，如果上面的解释真的看不懂，那个笔，自己分析下3次的二项式，就基本明白了。 接着这个定理也比较有用，而且很直观： $$ \\begin{pmatrix}n\\k\\end{pmatrix}=\\begin{pmatrix}n\\n-k\\end{pmatrix} $$ 想要证明？很容易： $$ (x+y)^n=(y+x)^n=\\underbrace{(y+x)(y+x)\\dots (y+x)}_{n} $$\n 计算第 k项（ $y^kx^{n-k}$ ）的系数，可以直接套用二项式定理，可以得到 $\\begin{pmatrix}n\\k\\end{pmatrix}$ ,如果按照x+y的顺序呢？就是选择n-k项（$x^{n-k}y^k$）于是得到 $\\begin{pmatrix}n\\n-k\\end{pmatrix}$ ，很明显，相等。 Q.E.D 从计算的角度： $$ C_{n,k}=\\frac{n!}{k!(n-k)!}\\ C_{n,n-k}=\\frac{n!}{(n-k))!(n-(n-k))!}=\\frac{n!}{k!(n-k)!} $$ Q.E.D  举个特别的例子，上面说的都是without replacement的方式进行的，如果是with replacement的呢？\n 可以有两组，每组n种颜色的球，每组取一个一共有多少Combination？ 假设第二个球与第一个球颜色不同，那么我们有$C_{n,2}$ 种取法，那么如果颜色可以相同呢？当然是再加上n被，因为之前被排除掉的n再补回来： $$ n+\\begin{pmatrix}n\\2\\end{pmatrix} $$ 原始例子是关于基因的，讲出来可能大家不太懂，需要等位基因，表现型等比较专业的知识背景（哈哈，我是学生物医学工程的）。 二项式解决了，能不能应用到多项式呢？试试吧。\n 多项式系数 Multinormial Coeffcient 首先假设另一种试验，上面我们注意到定义说一次取出$k$个，那么如果我们分步进行抽取呢？两步理论上互不影响，分两步分别取$k_1$,$k_2$个，根据乘法原理和上面的Combination，我们可以确定分两步，取$k_1$,$k_2$ 并且 $(k_1+k_2\\leq n)$ 会有： $$ \\begin{pmatrix}n\\k_1\\end{pmatrix}\\begin{pmatrix}n-k_1\\k_2\\end{pmatrix}=\\frac{n!}{k_1!(n-k_1)!}\\frac{(n-k_1)!}{k_2!(n-k_1-k_2)!}\\ =\\frac{n!}{k_1!k_2!(n-k_1-k_2)!}=\\frac{n!}{k_1!k_2!k_3!} \\quad where \\quad (k_3=n-k_1-k_2) $$ 上面的$k_3$ 是为了整洁写上去的，同时如果我们分三步，$k_1$,$k_2$,$k_3$ 并且三步之后把所有的取完，那么$k_3$ 就有意义了。 如果我们把n分成k份（随意分）并且分成k步执行，那么定义： $$ \\begin{pmatrix}n\\n_1，n_2，\\dots ，n_k\\end{pmatrix}=\\frac{n!}{n_1!n_2!\\dots n_k!} $$ 称之为多项式系数，文字上的意义就是分多次取，每次同时取$n_i$ 个，without replacement，直到最后把所有取完，一共的结果的个数。\n多项式定理： $$ (x_1+x_2+\\dots+x_k)^n=\\sum\\begin{pmatrix}n\\n_1，n_2，\\dots ，n_k\\end{pmatrix}x_1^{n_1}x_2^{n_2}\\dots x_k^{n_k}\\ n_1+n_2+\\dots+n_k=n $$ 上述$n_i$ 都是非负整数，多项式系数也可以用二项式的那种图来解释，但是过于麻烦，这里就不再举例了，二项式可以看做多项式系数的退化版本，因为从算式上可以清楚的看出： $$ \\begin{pmatrix}n\\k，n-k\\end{pmatrix}=\\begin{pmatrix}n\\k\\end{pmatrix} $$\n总结 完成另一种计数方法Combination的总结，这两篇给的例子不多，但是每个都很经典，有时候看懂例题更重要，因为概率以例子为主，待续。。\n","permalink":"https://go.face2ai.com/math/math-probability-1-3-combinatorial-methods.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍组合的相关知识，以及引出的二项式系数，多项式系数\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Combination，Binormial Coeffcient，Multinormial Coeffcient\u003c/p\u003e","title":"【概率论】1-3:组合(Combinatorial Methods)"},{"content":"Abstract: 本文主要介绍有限样本空间下的古典概率问题，以及其中包含的计数方法，排列的基本思想 Keywords: Counting Methods，Combinatorial Methods，Multiplication，Permutations，Stirling\u0026rsquo;s Formula\n计数方法 最近这几篇博客感觉每一篇知识点都有点多，概率这个东西更讲究应用，所以决定本文分成两篇写，这两篇只讲原理下后面讲案例，这样对比这看可能更好，一般的教材的通用做法是讲一个知识点，然后给出丰富的例子让我们来理解知识点，但是这样知识点之间衔接就被例题打断了，而先讲理论再来例题，理论又容易记不住，所以这是个两难的选择，但是我还是更倾向于先把理论用通俗的话讲明白，然后再举例子，这样我会感觉到踏实一点，也能检验自己理论是否真的搞明白了。 另外我对碎片化的学习表示非常反对，所谓任务导向的学习，什么不会学什么，这样做的好处是多快好省，这是我们dang经济建设的口号，“这个口号本身很矛盾，多就不能快，好就不能省，一分钟洗一万个土豆，你敢吃么？给我一千块钱盖房子，我只能用纸壳报纸给你盖”（语出自袁腾飞老师）,多快好省的问题就在于许多问题不会很快暴露，但是会陆陆续续的一直干扰你。 本文中的部分内容高中就有涉及，所以这篇博客写的关键字有点多，这里只强调与后续概率相关的知识，如果有遗漏大家可以自己补习。\n有限样本空间 Finite Sample Space 我们书接上回，上回书我们说到试验的所有输出组成的集合，我们称之为样本空间，当样本空间中的样本点的个数是有限的时候，我们就称之为有限样本空间，那么我们的问题来了，如果我们已知试验的条件，怎么确定结果是否有限个，如果是有限个，那么怎么确定个数呢？也就是怎么数一下结果呢？于是就有了下面的一些列计数方法。\n古典概率 Classical Interpretation 我们在第一篇introduction里面提到过Classical Interpretation，古典观点的概率，本篇中我们主要的出发点与古典概率一致，古典概率假设：“试验中，每一个样本点的概率相等”，那么从这个出发点看，结合上文中概率的性质T3,假设样本空间有n个样本点，把每个样本点看做一个事件，这样n个事件都互不相关，n个事件的概率和是1，那么每个事件的概率是 $\\frac{1}{n}$ 这个概念说起来可能比较拗口，各种条件之间看起来可能比较复杂，但是举个例子就非常简单了，扔骰子： 扔一个均匀的六面体骰子，那么样本空间是 ${1,2,3,4,5,6}$ 所有点数出现的概率相等，那么$Pr(x=1)=\\frac{1}{6}$ 、 $Pr(x=2)=\\frac{1}{6}$ 、$Pr(x=3)=\\frac{1}{6}$ 、$Pr(x=4)=\\frac{1}{6}$ 、 $Pr(x=5)=\\frac{1}{6}$ 、$Pr(x=6)=\\frac{1}{6}$ 这个结论比较能够满足我们的直观感觉，条件中“均匀的骰子”就表明样本空间中每个样本点等可能出现。 现在我们已经基本理解了古典概率的基本思路了，但是思考下，我们扔的一个骰子，能够很轻松的确定结果集的所有元素，那么如果我们扔两个骰子，或者扔更多的骰子呢？数数可能有点困难了，所以我们引出了下面的课题，计数方法。\n计数方法 Counting Methods 计数方法的根本目的就是为了计算试验结果的个数，与试验出现的结果无关，也不影响任何试验的随机性，计数只是通过条件给出的前提，在试验前或者试验后，计数结果不变。 总结下就是一句话：计数不影响试验的结果，计数方法在概率中的最主要的应用就是为了计算试验结果的个数。\n乘法原理 Multiplication Rule 乘法原理是最基础的一种计算结果的方法，首先我们感受下实际的用途，先后扔两个骰子，将结果排列成一个有序的序列，比如第一个骰子是6，第二个骰子是4，那么结果我们记做 $(6,4)$ 那么所有可能出现的结果： $$ {(1,1),(1,2),(1,3),(1,4),(1,5),(1,6),\\ (2,1),(2,2),(2,3),(2,4),(2,5),(2,6),\\ (3,1),(3,2),(3,3),(3,4),(3,5),(3,6),\\ (4,1),(4,2),(4,3),(4,4),(4,5),(4,6),\\ (5,1),(5,2),(5,3),(5,4),(5,5),(5,6),\\ (6,1),(6,2),(6,3),(6,4),(6,5),(6,6)} $$ 一共36种结果，36，很神奇的数字，因为每个骰子有6种结果，两个骰子，难道是$6^2$ 的关系？我们先不给出结论，我们来观察试验过程，首先我们扔第一个骰子，可能出现以下结果： 下面我们扔第二个骰子：\n看出来了吧，第一个骰子能扔出来6种结果，每一种结果后第二个骰子还能扔出6种结果，那么两个骰子的结果总数就是 $6\\times 6$ 种结果。 同样扔两个硬币就会得到$2\\times 2$ 种结果，分别是： $$ {(HT),(HH),(TH),(TT)} $$\n Definition: Multiplication Rule for Two-Part Experiments, (i)The Experiments is performed two parts (ii)The first part of the experiment has $m$ possible outcomes $x_1,x_2,\\dots,x_m$ ,and regardless of which one of these outcomes $x_i$ occurs,the second part of the experiment has $n$ possible outcomes $y_1,y_2\\dots,y_n$ the sample space $S$ contains excatly $mn$ outcomes\n 对于可以分成两步的试验，如果第一步试验有$m$ 种结果，第二步试验有 $n$ 种结果，那么整个试验共有$mn$ 种结果。\n接下来的扩展把上述两步试验扩展成更一般的试验，也就是多步试验，那么整体试验的结果是每一步结果数量的连续乘积。\n Definition:Multiplication Rule.Suppose that an experiment has k parts($k\\geq2$),that the $i$th part of the experiment can have $n_i$ possible outcomes($i=1,\\dots,k$),and that all of the outcomes in each part can occur regardless of which specific outcomes have occurred in the other parts.Then the sample space $S$ of the experiment will contain all vectors of the form $(u_1,\\dots,u_k)$ where $u_i$ is one of the $n_i$ possible outcomes of part $i(i=1,\\dots,k)$ .The total number of these vectors in $S$ will be equal to the product $n_1n_2\\dots n_k$\n 上面这段英文定义就是乘法原理的多阶段版本，也是通用版本，这里值得指出的就是重点的一句，各个阶段的试验互不影响，这个是非常关键的前提，如果不满足这个条件，乘法原理不成立。\n排列 Permutations 接下来就是排列了，排列怎么来的？从乘法原理来的呀，我们在看下面三种情况，陈希孺老师称下面这个叫“坛子模型”，这个模型虽然简单，但是是构成概率论的基础：\n  一个不透明的箱子，里面三个球，除了每个球颜色不同其他都相同（假设红R绿G蓝B三种颜色），也就是说如果靠触觉没办法区分，那么我们不看从箱子里拿球出来，每个球被拿出的可能性相同，并且，我们每次拿出球以后记录了颜色以后，再放回去，然后重复上述过程，这样我们重复三次，可能的结果： $${ RRR,RRG,RRB,RGR,RGG,RGB,RBR,RBG,RBB,\\ GRR,GRG,GRB,GGR,GGG,GGB,GBR,GBG,GBB,\\ BRR,BRG,BRB,BGR,BGG,BGB,BBR,BBG,BBB }$$ 根据实验的各步骤之间互不影响，所以乘法原理成立，结果有27种($3\\times3\\times3$) 这个重复的过程有个关键的步骤就是，每次取出球记录颜色后再放回去，这一步很重要。\n  如果我们取出球以后不放回去呢？那么我们可能得到的结果就是： $$ { RGB,RBG,GRB,GBR,BRG,BGR } $$ 解释下，第一步我们有三种可能的结果，同时第一个球不会放回，第二步受到第一步的影响，只可能有两种结果，同时第二个球不会被放回，那么第三步只有最后一个球，没有选择，只能是他，那么，这个不放回的过程，共有$3\\times2\\times1$ 种结果，比上面的模型1，要少很多种情况，\n  我们简化上面2中步骤，把三步减到两步，那么根据2，我们共有$3\\times2$种，虽然结果都是6，但是，效果是不同的，例如如果我们有四个球，不放回的取两个 $4\\times3$ 共12种情况，如果是全取出 $4\\times3\\times2\\times1$ 种情况。\n  如此便可引出组合的概念\n Definition: Permutations. Suppose that a set has $n$ elements.Suppose that an experiment consisits of selecting $k$ of the elements one at a time without replacement.Let each outcome consist of the $k$ elements in the order selected.Each such outcome is called a permutation of n elements taken $k$ at a time.We denote the number of distinct such permutations by the symbol $P_{n,k}$\n 定义的翻译，有n个球，取k个出来，一次取一个，不放回，并且取出的顺序被严格记录，不允许顺序被打乱。这个就是排列。\n Theorem Number of Permutations. The number of permutations of $n$ elements taken $k$ at a time is $P_{n,k}=n(n-1)\\dots(n-k+1)$\n 上述为排列的数字结果，总结出下面的表示方法，提出一个阶乘的概念,阶乘的具体概念可以自己查一查，注意，我们约定$0!=1$,注意，这是个约定。\n Theorem Permutations:The number of distinct orderings of $k$ items selected without replacement from a collectioin of $n$ different items $(0\\leq k\\leq n)$ is: $$ P_{n,k}=\\frac{n!}{(n-k)!} $$\n 小结一下上面关于排列的知识，最主要的关键字有两个，一个是“with replacement放回”与“without replacement不放回”，另一个就是顺序，取出结果的顺序被严格记录不允许颠倒顺序。\n生日问题 Birthday Problem 问题描述：“假设一年365天（不考虑闰年）k个人，至少有两个出生在同一天（只考虑月和日，不考虑年）的概率是多少？” 秉着越困难的题越短的思路，这个题可能要考虑不少，所以我们要仔细考虑下， 首先k一定不能大于365，不然肯定有同一天出生的人，那么概率是1没有讨论价值，那么我们可以利用前面集合的概念，假设样本空间S包含 $(365^k)$ 种出生组合（这是个with replacement）的结果，事件A是至少两个人出生在同一天，那么在全集S上的补集$A^c$ 的意义就是所有k个人都不同一天出生，那么根据排列的公式就有$P_{365,k}$ 种情况，那么根据每天等概率的前提下，$Pr(A^c)=\\frac{P_{365,k}}{365^k}$ ，根据1-1：Definition of Probability中的T3 $$ Pr(A)=1-Pr(A^c)=1-\\frac{P_{365,k}}{365^k}=1-\\frac{365!}{(365-k)!365^k} $$\n我们可以带入几个k值来计算下概率：\n   $k$ $Pr(A)$ $k$ $Pr(A)$     5 0.027 25 0.569   10 0.117 30 0.706   15 0.253 40 0.891   20 0.411 50 0.970   22 0.476 60 0.994   23 0.507      可见当有60个人的时候，这些人中有至少两个同一天过生日的概率是0.994，这就是很著名的生日问题，虽然很简单，但是很有趣，当然我们也可以不用补集的方法，直接正面解决，但是可能有点困难，大家可以自己试试。\n斯特林公式 Stirling\u0026rsquo;s Formula Stirling\u0026rsquo;s Formula,斯特林公式，主要为了解决的问题是，在$P_{n,k}=\\frac{n!}{(n-k)!}$ 的计算过程中，当n比较大的而k不太大的时候，这个计算变得很麻烦，因为数字太大了！什么？没有概念？那我下面写几个不太大的数字直观的给你看下： $$ P_{10,2}=\\frac{10!}{(10-2)!}=10\\times9=90\\ P_{100,2}=\\frac{10!}{(10-2)!}=10\\times9=9,900\\ P_{1000,2}=\\frac{10!}{(10-2)!}=10\\times9=999,000\\ P_{10000,2}=\\frac{10!}{(10-2)!}=10\\times9=99,990,000 $$ 这里我是口算的，所以让$k=2$ 如果让$k=5$ ，那么计算将会非常麻烦，但是我们发现一个有用的关系： $$ \\frac{n!}{a_n}=e^{ln(n!)-ln(a_n)} $$\n但是$ln(n!)$ 也太大了，没办法直接计算，于是Stirling\u0026rsquo;s Formula 被提出：\n Theorem Stirling\u0026rsquo;s Formula .Let: $$ set:;s_n=\\frac{1}{2}ln(2\\pi)+(n+\\frac{1}{2})ln(n)-n\\ then:;lim_{n\\to \\infty}|s_n-ln(n!)|=0\\ or:;lim_{n\\to \\infty}\\frac{(2\\pi)^{1/2}n^{n+1/2}e^{-n}}{n!}=1 $$\n 证明过程可以被简单的写成 $ln(n!)\\sim\\frac{1}{2}ln(2\\pi)+(n+\\frac{1}{2})ln(n)-n$ 即 $e^{ln(n!)}\\sim e^{\\frac{1}{2}ln(2\\pi)+(n+\\frac{1}{2})ln(n)-n}$ 也就是证明$n!\\sim (2\\pi)^{1/2}n^{n+\\frac{1}{2}}e^{-n}$ 成立： 证明： 略（后面补上）\n总结 本来想多写点知识点，结果发现，多就不能快，那我们明天继续，另外stirling 公式后面再给出证明，此处先略过，待续。。\n","permalink":"https://go.face2ai.com/math/math-probability-1-2-counting-methods.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍有限样本空间下的古典概率问题，以及其中包含的计数方法，排列的基本思想\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Counting Methods，Combinatorial Methods，Multiplication，Permutations，Stirling\u0026rsquo;s Formula\u003c/p\u003e","title":"【概率论】1-2:计数方法(Counting Methods)"},{"content":"Abstract: 本文介绍样本空间，公理化的概率的定义，以及概率的性质 Keywords: Sample Space，Finite Sample Space，Kolmogorov axioms(Probability Axioms)，Definition of Probability，Properties of Probability，Bonferroni Inequality\n概率定义 开篇提示:基本定理的证明都要用到集合论的知识，所以前面的集合论博客一定要先看哦！\n做基础难的另一个原因是看不到结果，研究算法也好做应用也好，起码能写个程序观察下结果，虽然不知道为啥有结果但是能看着点东西总觉得自己在进步，但是天天做数学题真是看不出啥进展，没有感官上的刺激容易让人失去动力。\n样本空间 Sample Space 继续上文中的讨论，我们在上一篇文章中说到了试验的outcome，并且对其进行约定必须是完全已知的，并把它当做集合来看，于是我们引入一个新的名词来命名这个包含所有结果的集合\u0026ndash;样本空间(Sample Space)\n Definition: The collection of all possible outcomes of an experiment is called the sample space of experiment\n 有一个神奇的事情就是陈希孺老师的《概率论与数理统计》中并没有在概率论部分提出样本空间这个概念，而是在数理统计部分提出的样本的概念，不知道老师为何如此安排，但别的入门书籍都是在前面就给出样本空间的定义，所以我们可以先接受这个概念。 举个🌰 : 掷一个六面的骰子，可以预期的结果无非就是123456点。 $$ {1,2,3,4,5,6} $$ 那么这个描述中，试验就是“掷一个六面的骰子”，试验可能的outcome的集合就是上面的集合，所以我们说这个试验的样本空间就是上面描述的集合。\n样本点 Point 试验的每一个outcome可以成为一个样本点(Point或者Element),所以事件event还可以看成是样本点的集合（之前一篇说的是样本空间的子集），这里概念是一致的，全部的样本点组成了样本空间，部分样本点组成了事件，和容易判定的一个关系。\n有限样本空间 Finite Sample Space 有限的样本空间，首先我们应该去看一下集合论的博客，在那里面我们介绍了集合部分的内容，而且在后面我会把《陶哲轩实分析》中集合的提出和证明加入进去，里面有有关有限集合和无限集合的讨论。\n试验可能出现无限结果的可能，比如某位置x测温度的试验，其可以描述成一个函数， $$ T=f(x) $$ 这明显是一个把位置映射到实数的函数，那么结果T就是个连续（实际上不可能连续，因为测试工具不可能精确到趋近于0，这句话如果不太理解没关系，这涉及到分析学中的实数连续性的东西），说白话，理想情况下（温度计没误差，可以精确到无限位）就是试验结果有无穷多，那么这个结果集合是无限的。\n当我们把样本空间看做集合，首先我们肯定不研究上面这个连续的例子，这个太复杂了，我们还是来点简单的，自然从无限多个结果的试验转换到有限个实验结果的实验上，比如扔骰子。 只要是扔有限个正常骰子，其结果都是有限的，比如扔一个，其结果是： $$ {1,2,3,4,5,6} $$ 扔两个： $$ X_1={1,2,3,4,5,6}\\ X_2={1,2,3,4,5,6}\\ then:\\ Y_2={(x_1,x_2)|x_1\\in X_1 ,x_2 \\in X_2} or\\ Y_2=X_1\\times X_2 $$ 最后这个表达式 $X_1\\times X_2$ 表示为笛卡尔积，有限集合的笛卡尔积是有限的，所以，扔有限个骰子的结果是完全确定的，也就是这类试验是Finite Sample Space的。\n概率是什么 What is the Probability ? 概率在上一篇中我们更多的用可能性来替代，事件(event)有可能性，那么我们进一步,每个事件具有概率。 下面我们通过Kolmogorov Axiom 柯氏公理来定义概率，需要解释的是，公理不同于定理，公理是不证自明的，也就是说公理不需要证明，他可以明确的告诉你，我就是对的，公理也是近代数学的基础，数学分析主要研究这套理论，（广告：后面会写数学分析相关的博客）。\n柯氏公理 1 Kolmogorov Axiom 1 对于任意事件A： $$ Pr(A) \\geq 0 $$ 公理1：任何事件的概率都是非负的\n柯氏公理 2 Kolmogorov Axiom 2 公理2，如果对于某实验X的事件S，必然发生，我们说S的概率： $$ Pr(S)=1 $$ 必然要发生的事件的概率是1 比如试验：我们有3个红色的球，我们随意选一个出来，选出是红球的概率，那么事件“选出红球”必然发生，那么他的概率就是1.\n不相交事件 Disjointed Events 已经两条公理了，再有一条就大功告成了，但是在这之前必须插播一条关于不相交事件的说明，我们前面反复说事件就是样本点的集合，那么就涉及到集合相交的问题，如果两个事件包含多于一个相同的样本点，那么他们相交，否则不相交。\n如果两个不相交的事件A，B对应的概率是$Pr(A)$ 、$Pr(B)$，那么 ${A发生 or B发生}$ 这个事件C的概率，我们可以很自然的认为是A的概率加上B的概率，即 $Pr(C)=Pr(A)+Pr(B)$\n上面这条假设可谓是基石一样存在，进一步扩展就是变成多个不向交的事件，无限多个不相交的事件，同样假设成立。 于是根据这个假设，可以提出第三个公理\n柯氏公理 3 Kolmogorov Axiom 3 对于无限不向交事件序列$A_1,A_2,A_3,\\dots$ 那么： $$ Pr(\\bigcup^\\infty_{i=1}A_i)=\\sum^\\infty_{i=1}Pr(A_i) $$\n公理3相对复杂一点点，我们来举个🌰说明下，概率论的逻辑性没有数分和线性代数那么强，但是例子性非常强，多举例子才能更容易理解： 扔骰子的例子，我们扔一个均匀的标准的，六面体骰子，我们定义事件A是得到点{1，2}，定义事件B是得到点{3，4}，定义事件C是得到点数{5}，定义事件D是得到{6}，可见这四个事件是完全不向交的，于是那么我们可以定义一个并集，事件$S=A\\cup B \\cup C$ 那么我们可以计算出$Pr(S)=Pr(A)+Pr(B)+Pr(C)$\n概率的定义 Definition of Probability  Definition: A probability measure ,or simply a probability,on a sample space S is a specification numbers Pr(A) for event A that satisfy of Axioms 1,2 and 3 定义，概率描述，或者概率，在一个样本空间S上，对于事件A，是一个特别的数字Pr(A)，其满足三条公理。 这句话有点拗口，但是我们可以利用中文来分析下句子成分，主语\u0026quot;概率描述或者概率\u0026quot;，谓语“是”，宾语“数字”，宾语的定语“对于事件A”，“满足三条公理”，状语“在样本空间上”，那么这个套路就很清晰了：\n 时间：不详 地点：样本空间上 人物：概率 事件：对于某个事件进行可能性评估 经过：如果满足三条公理 结果：可以得到完备的概率定义 概率的性质 Properties of Probability 根据三条公理可以引申出不少性质，有点像线性代数中行列式的提出，概率的提出也是通过先定义性质，再引出实体（公理也是性质）,然后再得出其他性质，下面我们介绍一些列Theorem：\nT1: $Pr(\\emptyset) = 0$ 直观的来看这条定理，空集对应的事件N中包含0个样本点，所以不会发生这样的事件，所以可能性是不可能，是0，但是我们要用公理来证明定理，这样才能体现数学的公理化，这个定理的证明比较简单，可以用两种方法证明，第二种是DeGroot给出的证明，第一种是Tony的证法：\n方法1（此方法有错误！你能找到哪里出了问题么？）：\n①设事件A对应空集合，根据公理1，设其概率是 $$ A=\\emptyset\\ Pr(A)\\geq0 $$\n②我们再设一个事件S，其包含全部样本点，那么这个事件就变成了必然事件，根据公理2，其概率 $$Pr(S)=1$$\n③且根据集合论 $$S \\cap A=\\emptyset$$ 所以S和A是不相关的\n④根据集合论 $$S \\cup A=S$$\n⑤根据公理下面即将要证明的T2可以得到： $$ Pr(S)=Pr(S\\cup A)=Pr(S)+Pr(A)=1 $$ 又因为②，可得到 $$ Pr(A)=0 $$ Q.E.D\n没错⑤存在严重的逻辑问题，因为T2中用到了T1的结论，所以产生了相互证明的逻辑问题！注意，这种问题经常发生！：\n 方法2： 设一个无限序列$A_0,A_1,A_2,\\dots$ 并且对于任意$A_i=\\emptyset$,那么根据概率论可知他们都是不相交的 $\\emptyset \\cap \\emptyset=\\emptyset$ ，根据公理3: $$ Pr(\\emptyset)=Pr(\\bigcup^\\infty_{i=0}A_i)=\\sum^\\infty_{i=0}Pr(A_i)=\\sum^\\infty_{i=0}Pr(\\emptyset) $$ 所以可以得到 $$ Pr(A)=0 $$ Q.E.D\nT2: $Pr(\\bigcup^n_{i=1}A_i)=\\sum^n_{i=1}Pr(A_i)$ T2是对公理3的一个退化版本，也可以叫加法原理，无限个不相交事件退化成有限个 证明: 我们可以假设当$m\u0026gt;n$ 时，$A_m=\\emptyset$ 所以： $$ Pr(\\bigcup^n_{i=1}A_i)=Pr(\\bigcup^\\infty_{i=1}A_i)\\ =Pr(\\bigcup^n_{i=1}A_i\\cup\\bigcup^\\infty_{i=n+1}A_i)\\ =Pr(\\bigcup^n_{i=1}A_i)+Pr(\\bigcup^\\infty_{i=n+1}A_i)\\ =Pr(\\bigcup^n_{i=1}A_i)+0\\ =\\sum^n_{i=1}(Pr(A_i)) $$ Q.E.D\nT3: $Pr(A^c)=1-Pr(A)$ 接下来就是更进一步的推到了，一般理论的发展就是，现有少量的公理，然后推出比较基础的使用广泛的性质，然后进一步推出更特殊的更专业的性质。\n证明： 假设样本空间全集为S，根据公理2， $Pr(S)=1$ 那么 $$ A^c\\cap A=\\emptyset\\ A^c\\cup A=S\\ Pr(A\\cup A^c)=Pr(A)+Pr(A^c)=Pr(S)=1\\ Pr(A)=1-Pr(A^c)\\ $$ Q.E.D\nT4: If $A\\subset B$ then $Pr(A)\\leq Pr(B)$ 基本定理的证明都要用到集合论的知识，所以前面的集合论博客一定要先看哦！\n①$A^c\\cap B\\neq \\emptyset ,so,Pr(A^c\\cap B)\u0026gt;0$\n②$B=A\\cup(A^c\\cap B)$\n③$A\\cap(A^c\\cap B)=\\emptyset$\n④$Pr(B)=Pr(A)+Pr(A^c\\cap B)$\n⑤$Pr(B)\u0026gt;Pr(A)$ Q.E.D\nT5: $0\\leq Pr(A)\\leq 1$ 对于全集S，$A\\subset S$ 且 $Pr(S)=1$ 所以根据T5，$Pr(A)\\leq Pr(S)$ 再结合公理1，就可得到T5的结论 Q.E.D\nT6: $Pr(A\\cap B^c)=Pr(A)-Pr(A\\cap B)$ 根据T2\n① $(A\\cap B)\\cap(A\\cap B^c)=\\emptyset$\n② $(A\\cap B)\\cup(A\\cap B^c)=A$\n③ $Pr(A)=Pr(A\\cap B)+Pr(A\\cap B^c)$\n得出： $Pr(A\\cap B^c)=Pr(A)-Pr(A\\cap B)$\nQ.E.D\nT7: $Pr(A\\cup B)=Pr(A)+Pr(B)-Pr(A\\cap B)$ 根据集合论中的结论：\n① $A\\cup B=B\\cup(A\\cap B^c)$\n② $B\\cap(A\\cap B^c)=\\emptyset$\n所以 ③ $Pr(A\\cup B)=Pr(B)+Pr(A\\cap B^c)$\n根据T6: ④ $Pr(A\\cup B)=Pr(A)+Pr(B)-Pr(A\\cap B)$ Q.E.D\nT8: Bonferroni Inequality 对于所有的事件 $A_1,A_2,\\dots,A_n$ $$ Pr(\\bigcup^n_{i=1}A_i)\\leq \\sum^n_{i=1}Pr(A_i) \\ Pr(\\bigcap^n_{i=1}A_i)\\geq 1-\\sum^n_{i=1}Pr(A^c_i) $$ 书中并没有给出Bonferroni不等式的证明，但是感觉证明也并不难，\n第一个不等式是说，当存在事件相关的时候，其和的概率会比其相加的小，T7给出了证明\n？第二个不等式 $$ 1=\\sum^n_{i=1}Pr(A^c_i)\\ Pr(\\bigcap^n_{i=1}A_i)+\\sum^n_{i=1}Pr(A^c_i)\\geq 1 $$ 第二个不等式可能有点问题，因为题设并没说明白A是否能够构成样本空间的全集，或者我可能理解有问题，这个先画个问好吧。\n$Pr(a)=0$ 某个事件为概率为0并不意味着这件事永远不会发生，比如在连续的情况下，每个点的概率都是0，只有面积才有意义（这个后面会更详细的叙述）\n总结 这篇入门知识总结相当全面，而且是从分析的角度进行入门，数学的美感完全让我忘记了饥饿，我的湿炒牛河都变成干炒了，明天继续。。\n","permalink":"https://go.face2ai.com/math/math-probability-1-1-definition-of-probability.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍样本空间，公理化的概率的定义，以及概率的性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Sample Space，Finite Sample Space，Kolmogorov axioms(Probability Axioms)，Definition of Probability，Properties of Probability，Bonferroni Inequality\u003c/p\u003e","title":"【概率论】1-1:概率定义(Definition of Probability)"},{"content":"Abstract: 本文主要介绍概率的基本概念和观点，主要为了说明概率是什么，同时给出比较重要的试验和事件的解释说明。\nKeywords: Probability，the Frequency Interpretation，the Classical Interpretation，the Subjective Interpretation，the Axiomatic Interpretation，Experiments，Events\n概率论介绍 虽然我很冷静，但外面确实一片歌舞升平，各种AI应用如雨后春笋，各路AI大神伪大神如跳梁小丑各种雷人的说法也是层出不穷，有些行业确实要看谁的跑的快，但是有些确实需要看谁跑的更远，当方向明确的时候需要跑得快，方向不明的时候需要的是探路。\n概率是什么 世界上有确定一定要发生的事么？有，按照目前人类能理解的自然，有些公理是确定要发生的，所以公理是不证自明的，比如生老病死，这些事对于生物来说是一定会发生的，但是更多事是不太确定的，今天下不下雨？明天有没有雾霾？后天拆不拆迁？ 这里插播一条有点跑题的话，忘了哪本书曾经说过，人类的文明发展的任务主线是理解自然，比如我们最开始的宗教，哲学，以及最近的科学，所有的这些都是在解释自然现象，近代科学发展迅速的原因是因为科学，尤其是数学物理的发展，一些列结果表明科学可以更准确的描述和预测自然结果，没错概率论的任务很贴近上述的描述。 观察我们周围不确定事远远多于确定的事，当我们严格的定义了一件事的条件后（不存在模棱两可的词语），考虑这个条件产生结果的时候，这些结果一般是多个中的一个，虽然完全无法确定哪结果个会发生，扔硬币，扔骰子，但这些结果范围完全确定，概率论能帮我们确定么？不能，但是概率论能帮我们描述这个结果集，也就是某个结果出现的可能性是多少（用数字来描述发生的可能性，0~1，越接近0表示发生的可能性越小，越接近1表示发生的可能性越大）。\n试验与事件 在我们深入研究概率是什么之前，我们先学习概率论从头到尾都要用的两个概念，试验和事件，从我们传统教育的角度来讲，这两个概念比本文其他讨论的知识都重要，因为这个是考点，当然我认为其他的知识同样重要，这两个概念要从始至终的跟随我们，但是其他知识告诉我们的是关于这个学科的整体思路，同样重要。\n试验 Experiments  Definition:An experiment is any process,real or hypothetical,in which the possible outcomes can be identified ahead of time\n 试验可以使假设的也可以是实际的，但是他们的结果范围必须要已知。\n最简单的real例子就是丢硬币，结果可能是正面，反面，或者立着；hypothetical的例子就是有一个不能立起来的硬币，丢硬币的结果就是“正、反面”，不存在“立着”的结果。 概率中实际的试验更有意义，因为我们的目的就是研究自然中实际存在的问题，当然有时候需要用hypothetical来建模实际的试验。\n事件，随机事件，偶然事件 Events  An event is a well-defined set of possible outcomes of the experiment\n 事件是个集合！事件是个集合！事件是个集合！ 事件相对于试验来说后面用到的更多，当我们描述一个过程的时候试验是描述条件，对应的我们知道结果集，事件的就是被完备定义的结果集合的子集。 解释一下：我们定义或者已知了试验，并对试验结果了如指掌，我们知道并且确定实验结果组成的集合X，那么事件就是X的子集x，数学描述： $$x={x|x\\in X,P(x)=1}$$ 其中P就是我们的well-define的抽象写法，可以看成是函数P: $$ P(x)= \\begin{cases} 0\u0026amp; \\text{x 不满足well-defined的定义 }\\ 1\u0026amp; \\text{x 满足well-defined的定义 } \\end{cases} $$\n概率只有针对事件的时候才有意义 举几个🌰 ：\n 扔硬币10次，出现五次正面的概率是多少？ Thomas Jefferson出生在1741的概率是多少？  分析：\n 例1中事件是出现五次正面，outcome集合是可以出现1次2次\u0026hellip;10次 例2中的时间是出生在1741年，outcome集合是可以出生于人类出现那年~2019年  注意，event可以是空集哦\n必然事件 肯定会发生的，扔骰子，点数小于7的事件是肯定发生的，也即必然事件是所有outcome的全集\n不可能事件 不可能事件对应必然事件，那就是其是个空集，比如扔骰子结果是-1点数的这个事件，或者结果既是基数又是偶数这个事件。\n试验与事件 这两个概念之间还有一些其他的内容，这里补充一下，试验有的时候是人工的，比如扔硬币，但有时候是不参与的，比如下雨，如果我们不参与试验只是记录试验结果，这类试验可以叫做“观察”，在概率论中可能没啥区别，但是在数理统计里面有区别。 上面例子2中我们不能明确的确定全部可能的outcome（我们不知道人类哪年出现的），但是结果范围不会超出某个范围，后面随机变量出现的时候就是把这范围进一步的用数学抽象，处理起来就更方便了。\n关于概率的几种观点 对于概率的定义，目前没有明确的一种定义被证明是真理而且他定义是谬论，而且这个问题的讨论也不是我们目前这个阶段要考虑的，很多事物我们并不知道其本身是什么，但我们却在研究事物之间的关系。以下几种观点可以从不同角度学派来定义概率\n频率派解释 The Frequency Interpretation 频率派认为：事件的概率就是经过大量重复试验后，此事件出现的次数与试验总次数的比，也就是事件出现的频率，这个定义非常模糊，比如大量试验，多大算大？一万次？一亿次？而且当我们用一万次试验定义某个事件概率后，又进行了一万次试验很有可能结果不一，也就是按照一个定义做了两次，结果自相矛盾，那么这个定义是定义不成功的，但是这两个结果应该非常接近，或者说他们很接近真理了。 频率派和统计派是一个派。\n古典派解释 The Classical Interpretation 古典概率派：认为假设所有结果等可能性出现，然后每个结果出现的概率就是结果的个数分之一。 这个定义有点搞笑的是里面出现了可能性这个字，而我们要定义的就是可能性，换种说法让他们定义人的话，他们会说：有一种是人的生物叫做人。这逻辑，神了吧 而且等可能性这个事情本身就不可能发生，比如扔硬币，你觉得可能出现正反面的可能性一致，但其实考虑到硬币两面不同的样式，以及重心的微弱变化，其结果不可能是完全等可能的。 但是这个古典派定义是很有用在于我们后面要反复研究丢硬币，扔骰子来引出各种其他的概念\n主观概率 The Subjective Interpretation 主观概率，这个更有意思了，是我们猜一个事件的概率，比如我们猜测一下一会儿下雨的可能性，张三可能是个老人家，他在这里住了一辈子，他估计下雨的可能性是70%因为他很了解这个地方的天气，李四刚来本地，他说下雨的可能性是60%，他不熟悉本地天气，但是他根据他的人生经验给出了一个概率，在问一个刚说话的小朋友一会儿下雨的概率是多少，他说10%，没人信，他根本不知道10%什么意思，但是这也是他主观的概率。 事实上主观概率对于这个实验的概率计算没啥帮助，但是并不是完全没有用的，我们可以通过这个概率来研究这些人的某些性质，比如民调，那些万恶的不自由没人权的资本主义国家大选的时候不就经常搞个民调，研究下大家觉得谁更有可能当总统。\n公理化概率 The Axiomatic Interpretation 数学公理化是近代数学的主要发展方向，所以概率这么大的学科肯定也要进行公理化。 伟大的前苏联数学家柯尔莫哥洛夫通过极为简单的若干个公理作为基础，建立了概率论的宏伟大厦。 后面我们关于概率的定义会引用柯氏公理，但是我们要知道柯氏公理的意义在于他用数学手段严格了数学化的概率论。\n我们并不能说上述四个角度的优劣，频率派，古典派这些我们都要进行一定研究，能够准确的解决自然问题的都是好的，所以在没有严格证伪的前提下，都要进行研究。\n如何研究概率论 其实这个扩展到告诉我们如何进行科学研究。 对于任何学科我们应该仔细区分理论的三个方面，这三个方面相辅相成，抛开任何一个都没办法进行研究，或者研究完全会跑偏。\n形式逻辑的内容 公理化的数学只论及无定义事物见的关系，最简单的就是几何中点和线的关系，我们并不知道点是什么，也不知道线的定义是什么，但是我们可以研究这两个对象见得关系，点和线在这里是无定义的概念，我们更关系其间的关系，比如两个点确定一条直线，这是一个“规则的描述”，不同的公理系统能产生不同的规则，这样研究不同的问题就有更方便贴切的工具了，这就是公理化数学的魅力，你可以自己建立自己的体系，当然你要足够厉害。 国际象棋的例子也在于我们可以不知道每个棋子的本质是什么，甚至我们可以给他们不同的名字，不叫皇后，马什么的，改名叫a，b。。。照样可以按照规则来完成棋局，规则才是主要的，甚至我们没棋盘都能照样玩。\n直观的背景 对于几何和物理，我们有非常直观的观察对象，比如对于重力，我们能观察苹果落地，对一件事很熟悉了之后我们的感官并不会感到迷茫，虽然路边的老人不懂概率论，但是你问他有多大可能性下雨的时候，他能用语言给你描述个大概，这里有部分是经验，有部分是直觉，当对某个领域非常熟悉后，直觉可能更可贵，虽然没办法用公理解释证明。 概率刚出现的时候，其结论与人们固有思维甚至统治集团的思想宣传都有冲突，但是百年之后，我们看到书上提到可能性的时候完全不会排斥，而是很轻松的就能理解其要表达的信息。\n应用 应用是所有科学的最终目的，数学是抽象的工具，当要研究同一个自然现象的时候可以有无数多种抽象的数学模型：“数学理论的应用方式不依赖于事先形成的意见，他是一种有目的技术，依赖于经验，而且随经验改变” 再深入就是哲学了，我们必须剥离哲学和公理化的数学以及经验之间分别来研究，不然会跑偏。\n以上三个方面是所有研究的基础思想，公理逻辑是重要的工具和原料，背景，经验灵感给了我们设计图纸，应用是我们的目的，这样捋顺一下，豁然开朗不？\n概率论的核心问题 以下部分是概率论现在研究以及以后都要研究的重点，也是概率论最核心的问题！\n 已知一个试验的所有输出及其概率，我们要确定一个方法来计算某个事件的概率。 当得到附加的信息后如何调整修正当前事件的概率  总结 这是我写过的最详细的一篇介绍性文章，写了很多知识，尤其是后面如何研究概率论，对其他科目也有极大帮助，我们算是正式开启概率论了，明天继续。。。\n","permalink":"https://go.face2ai.com/math/math-probability-1-0-introduction.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍概率的基本概念和观点，主要为了说明概率是什么，同时给出比较重要的试验和事件的解释说明。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKeywords:\u003c/strong\u003e Probability，the Frequency Interpretation，the Classical Interpretation，the Subjective Interpretation，the Axiomatic Interpretation，Experiments，Events\u003c/p\u003e","title":"【概率论】1-0：概率论介绍"},{"content":"Abstract: 本文通过分析opencv的Mat类的计算过程，来研究OpenCV的数据结构以及算法过程 Keywords: OpenCV，矩阵计算\n矩阵计算过程 2017年最后一篇博客，\u0026ldquo;2017年是不平凡的一年\u0026hellip;.\u0026quot;，今年下半年重新开始不定期更新博客，写了三四十篇，觉得还可以，求知欲可能是所有欲望里面最正面的，沉浸在求知的过程中的人内心更平静，因为当你知道周围的所有你生活的环境，人，事物，财富都将因时间化为云烟的时候，就跟他们没什么可以争论的了；留下点知识，或者思想，才是我们来到这个世界上的永不磨灭的印记，就像欧拉的《无穷分析引论》200年后依然影响了我，但是我已经不知道他们当时的首富是谁，他们的市长又是谁； 我这些烂博客肯定不能跟欧拉的著作相提并论，但是留下一些总比什么都没有好，而且如果我们继续努力，惊世神作也不是完全没可能的。\nOpenCV OpenCV我2011年左右就开始了解到了，当时也是大学时候要做个小竞赛；那时之前看了钢铁侠，视觉心灵冲击大到影响了我的职业生涯，Jarves这种强人工智能暂且不说，《钢铁侠1》中的全息影像加手势交互给了我特别大的灵感，于是我做了个识别手指个数的小作品，参加了影响我一生的-西安电子科技大学生命科学技术学院星火杯科技竞赛，然后喜闻乐见的我得了个参与奖，当时用到opencv就是读取摄像头图像，自己编了一个算法在确定手指个数，现在看肯定naive但是当时完全是啥也不懂的情况下，没人指导，也没向谁请教，自己安装OpenCV，学51单片机和PC通信，焊电路板，然后还写了个MFC的界面，做到最后可以在竞赛现场演示，但是结果就是，参与奖，第一名是51单片机的光电对管的小应用，第二名是能吹灰的黑板擦（黑板擦加上一个小风扇）。。 跟高手过招，才能显示出身份，总跟下三路的对手pk，赢了没意义，输了更恶心。 后来尝试写图像处理库的时候，OpenCV更是帮我验证了很多算法，但是我一直有个疑惑OpenCV究竟是用什么办法处理一个矩阵中的不同数据类型的，典型的，我们有int ,char,float类型的数据，但是Mat类中指向数据的data 的指针是个uchar* 类型，也就是说Mat是个通用的，但是在算法实践中要把data转换成特定的类型，或者每种类型分别实现算法，那这样就工作量太大了，而且都是重复工作，而且我们并不知道OpenCV中矩阵这个类，和矩阵计算这些是怎么分块的，学习一下别人的分类模式可以帮助我们后面要做的PineNut提供很大的参考意义。\nDebug OpenCV 想要知道OpenCV是怎么计算的，最简单的方法就单步跟踪，如果不太明白单步追踪，下面的文章可以先不看，先去练习下写程序，但是单步调试的一个问题就是必须要有debug版本的OpenCV，OpenCV是开源的，cmake编译的时候设置成debug版本就可以了，debug版本与release版本的区别，这里也省略，因为跟我们关系也不是很大。 然后我们写了一小段应用调用OpenCV的加法：\n#include \u0026lt;iostream\u0026gt;#include \u0026lt;opencv2/opencv.hpp\u0026gt;using namespace cv; int main() { std::cout\u0026lt;\u0026lt;CV_VERSION\u0026lt;\u0026lt;std::endl; uchar A_8u[3][3] = { { 10, 5, 3 }, { 6, 4, 7 }, { 1, 0, 9 } }; uchar B_8u[3][3] = { { 1, 3, 8 }, { 7, 5, 4 }, { 10, 6, 0 } }; Mat A_m_8u(3, 3, CV_8S, A_8u); Mat B_m_8u(3, 3, CV_8S, B_8u); Mat C_m_8u; C_m_8u=A_m_8u + B_m_8u; std::cout\u0026lt;\u0026lt;C_m_8u\u0026lt;\u0026lt;std::endl; return 0; } 这段程序主要是定义了两个Mat，然后使用+号对两个Mat进行相加，首先我们知道在C中是不能加两个结构体的，+一般只针对内置类型的数字字符等数据，结构体这种加法要自己定义函数，所以这个加号肯定是重载过的，也可以把它看做一个函数。 那么我们设置断点，进入加号\n果然根据我们的猜测，他进入了一个叫做MatExpr的类的加号运算符重载函数，并返回了一个MatExpr对象\nMatExpr operator + (const Mat\u0026amp; a, const Mat\u0026amp; b) { MatExpr e; MatOp_AddEx::makeExpr(e, a, b, 1, 1); return e; } 接下来加法操作返回了，神奇的是这个加法操作没有执行任何循环或者加的动作，而是只返回了一个对象，也就是说这个对象中记载了要做的计算等到要执行的时候再执行，这样做有点类似tensorflow中的机制。 分析：重载的MatExpr符号操作（+）并没有执行任何计算操作，他只是记录了要做的的事，等到最后再做，这样做的好处可以减少一些不必要的计算，比如你这里有个A+B但是结果没有赋值给其他对象，那么这样的操作如果编译器没有处理掉的话是会浪费很多计算量，这种只记录不计算的方法就能避免，直到不得不计算的时候。 接着我们返回到main.cpp中，我们就得到了第一张图中序号①和②，先记录①的加法，然后在遇到②这种赋值符号的时候进行计算，但是要注意，这个赋值符号有讲究的，不是Mat赋值给Mat，而是MatExpr赋值给Mat 的时候，也就是说MatExpr我们把他叫做矩阵计算也好或者我们可以把它想象成矩阵的组合，里面有一张完整的计算图，当要把它赋值给Mat的时候进行计算，得到最终结果，这样在矩阵运算作为参数传递给函数的时候，这个方式也是可行的。 于是我们来到了MatExpr的赋值号（ = ）重载函数：\ninline Mat\u0026amp; Mat::operator = (const MatExpr\u0026amp; e) { e.op-\u0026gt;assign(e, *this); return *this; } 出现了一个新的对象op。这个对象的类是MatOp类 进入assign函数发现\nvoid MatOp_AddEx::assign(const MatExpr\u0026amp; e, Mat\u0026amp; m, int _type) const { Mat temp, \u0026amp;dst = _type == -1 || e.a.type() == _type ? m : temp; if( e.b.data ) { if( e.s == Scalar() || !e.s.isReal() ) { if( e.alpha == 1 ) { if( e.beta == 1 ) cv::add(e.a, e.b, dst);//执行此函数  else if( e.beta == -1 ) cv::subtract(e.a, e.b, dst); else cv::scaleAdd(e.b, e.beta, e.a, dst); } else if( e.beta == 1 ) { if( e.alpha == -1 ) cv::subtract(e.b, e.a, dst); else cv::scaleAdd(e.a, e.alpha, e.b, dst); } else cv::addWeighted(e.a, e.alpha, e.b, e.beta, 0, dst); if( !e.s.isReal() ) cv::add(dst, e.s, dst); } else cv::addWeighted(e.a, e.alpha, e.b, e.beta, e.s[0], dst); } else if( e.s.isReal() \u0026amp;\u0026amp; (dst.data != m.data || fabs(e.alpha) != 1)) { e.a.convertTo(m, _type, e.alpha, e.s[0]); return; } else if( e.alpha == 1 ) cv::add(e.a, e.s, dst); else if( e.alpha == -1 ) cv::subtract(e.s, e.a, dst); else { e.a.convertTo(dst, e.a.type(), e.alpha); cv::add(dst, e.s, dst); } if( dst.data != m.data ) dst.convertTo(m, m.type()); } op并不是MatOp类而是MatOp_AddEx类（应该是有继承关系的），所以这里使用了多态，那么这个类家族中应该还有：MatOp_Cmp，MatOp_T。。。等各种各样的继承（没错，这两个根本不是我猜的，是我去源代码里面扒出来的），这也就是说，矩阵运算（operation）下有很多具体的类，具体的实现。 我们继续走，发现会执行cv::add那句（加注释那句），所以我们继续跳转到这个函数：\nvoid cv::add( InputArray src1, InputArray src2, OutputArray dst, InputArray mask, int dtype ) { CV_INSTRUMENT_REGION() arithm_op(src1, src2, dst, mask, dtype, getAddTab(), false, 0, OCL_OP_ADD ); } 于是我们来到了一个全局函数，这个函数的输入输出是一个新的类，InputArrary，OutputArray,有过opencv编程经验的人都应该知道，最好是写过C版本的opencv的同学都见过这个类，好多老函数都是用这个参数类型的，而且这个参数类型可以兼容CvMat，IplImage等好多类，所以这个输入输出类应该是一个兼容性设置，或者只读只写这类保护性作用的。然后接着一个干什么不知道的宏，然后进入到了arithm_op的函数，arithm_op有一个参数是个函数,\nstatic BinaryFuncC* getAddTab() { static BinaryFuncC addTab[] = { (BinaryFuncC)GET_OPTIMIZED(cv::hal::add8u), (BinaryFuncC)GET_OPTIMIZED(cv::hal::add8s), (BinaryFuncC)GET_OPTIMIZED(cv::hal::add16u), (BinaryFuncC)GET_OPTIMIZED(cv::hal::add16s), (BinaryFuncC)GET_OPTIMIZED(cv::hal::add32s), (BinaryFuncC)GET_OPTIMIZED(cv::hal::add32f), (BinaryFuncC)cv::hal::add64f, 0 }; return addTab; } BinaryFuncC是个函数指针，这个函数定义一个数组，不同的元素对应着不同的数据类型·其中宏定义：\n#define GET_OPTIMIZED(func) (func) 这种宏没有操作意义，但是可以增加代码的可读性，而且不影响源代码的编译质量，这样这个函数返回了一个函数指针，这个指针指向一个针对不同数据类型的函数族。 接下来进入了一个很长的，但是也是本过程最核心的部分：\nstatic void arithm_op(InputArray _src1, InputArray _src2, OutputArray _dst, InputArray _mask, int dtype, BinaryFuncC* tab, bool muldiv=false, void* usrdata=0, int oclop=-1 ) { const _InputArray *psrc1 = \u0026amp;_src1, *psrc2 = \u0026amp;_src2; int kind1 = psrc1-\u0026gt;kind(), kind2 = psrc2-\u0026gt;kind(); bool haveMask = !_mask.empty(); bool reallocate = false; int type1 = psrc1-\u0026gt;type(), depth1 = CV_MAT_DEPTH(type1), cn = CV_MAT_CN(type1); int type2 = psrc2-\u0026gt;type(), depth2 = CV_MAT_DEPTH(type2), cn2 = CV_MAT_CN(type2); int wtype, dims1 = psrc1-\u0026gt;dims(), dims2 = psrc2-\u0026gt;dims(); Size sz1 = dims1 \u0026lt;= 2 ? psrc1-\u0026gt;size() : Size(); Size sz2 = dims2 \u0026lt;= 2 ? psrc2-\u0026gt;size() : Size(); #ifdef HAVE_OPENCL  bool use_opencl = OCL_PERFORMANCE_CHECK(_dst.isUMat()) \u0026amp;\u0026amp; dims1 \u0026lt;= 2 \u0026amp;\u0026amp; dims2 \u0026lt;= 2; #endif  bool src1Scalar = checkScalar(*psrc1, type2, kind1, kind2); bool src2Scalar = checkScalar(*psrc2, type1, kind2, kind1); if( (kind1 == kind2 || cn == 1) \u0026amp;\u0026amp; sz1 == sz2 \u0026amp;\u0026amp; dims1 \u0026lt;= 2 \u0026amp;\u0026amp; dims2 \u0026lt;= 2 \u0026amp;\u0026amp; type1 == type2 \u0026amp;\u0026amp; !haveMask \u0026amp;\u0026amp; ((!_dst.fixedType() \u0026amp;\u0026amp; (dtype \u0026lt; 0 || CV_MAT_DEPTH(dtype) == depth1)) || (_dst.fixedType() \u0026amp;\u0026amp; _dst.type() == type1)) \u0026amp;\u0026amp; ((src1Scalar \u0026amp;\u0026amp; src2Scalar) || (!src1Scalar \u0026amp;\u0026amp; !src2Scalar)) ) { _dst.createSameSize(*psrc1, type1); CV_OCL_RUN(use_opencl, ocl_arithm_op(*psrc1, *psrc2, _dst, _mask, (!usrdata ? type1 : std::max(depth1, CV_32F)), usrdata, oclop, false)) Mat src1 = psrc1-\u0026gt;getMat(), src2 = psrc2-\u0026gt;getMat(), dst = _dst.getMat(); Size sz = getContinuousSize(src1, src2, dst, src1.channels()); tab[depth1](src1.ptr(), src1.step, src2.ptr(), src2.step, dst.ptr(), dst.step, sz.width, sz.height, usrdata); return; } ... 我们只截取了我们要用到的部分，后面还有一段，但是我们没用到，这个函数进行了一些类类型，规模等的判断然后调用了tab数组（指向函数的数组）中的函数，并输入了正确的参数，其中prt()这个函数值得注意,然后我们进入了下一个函数：\nvoid add8s( const schar* src1, size_t step1, const schar* src2, size_t step2, schar* dst, size_t step, int width, int height, void* ) { CALL_HAL(add8s, cv_hal_add8s, src1, step1, src2, step2, dst, step, width, height) vBinOp\u0026lt;schar, cv::OpAdd\u0026lt;schar\u0026gt;, IF_SIMD(VAdd\u0026lt;schar\u0026gt;)\u0026gt;(src1, step1, src2, step2, dst, step, width, height); } 检查是否有HAL加速，发现没有，然后记者执行一个模板函数：\nvBinOp\u0026lt;schar, cv::OpAdd\u0026lt;schar\u0026gt;, IF_SIMD(VAdd\u0026lt;schar\u0026gt;)\u0026gt;(src1, step1, src2, step2, dst, step, width, height); 进入这个函数：\ntemplate\u0026lt;typename T, class Op, class VOp\u0026gt; void vBinOp(const T* src1, size_t step1, const T* src2, size_t step2, T* dst, size_t step, int width, int height) { #if CV_SSE2 || CV_NEON  VOp vop; #endif  Op op; for( ; height--; src1 = (const T *)((const uchar *)src1 + step1), src2 = (const T *)((const uchar *)src2 + step2), dst = (T *)((uchar *)dst + step) ) { int x = 0; #if CV_NEON || CV_SSE2 #if CV_AVX2  if( USE_AVX2 ) { for( ; x \u0026lt;= width - 32/(int)sizeof(T); x += 32/sizeof(T) ) { typename VLoadStore256\u0026lt;T\u0026gt;::reg_type r0 = VLoadStore256\u0026lt;T\u0026gt;::load(src1 + x); r0 = vop(r0, VLoadStore256\u0026lt;T\u0026gt;::load(src2 + x)); VLoadStore256\u0026lt;T\u0026gt;::store(dst + x, r0); } } #else #if CV_SSE2  if( USE_SSE2 ) { #endif // CV_SSE2  for( ; x \u0026lt;= width - 32/(int)sizeof(T); x += 32/sizeof(T) ) { typename VLoadStore128\u0026lt;T\u0026gt;::reg_type r0 = VLoadStore128\u0026lt;T\u0026gt;::load(src1 + x ); typename VLoadStore128\u0026lt;T\u0026gt;::reg_type r1 = VLoadStore128\u0026lt;T\u0026gt;::load(src1 + x + 16/sizeof(T)); r0 = vop(r0, VLoadStore128\u0026lt;T\u0026gt;::load(src2 + x )); r1 = vop(r1, VLoadStore128\u0026lt;T\u0026gt;::load(src2 + x + 16/sizeof(T))); VLoadStore128\u0026lt;T\u0026gt;::store(dst + x , r0); VLoadStore128\u0026lt;T\u0026gt;::store(dst + x + 16/sizeof(T), r1); } #if CV_SSE2  } #endif // CV_SSE2 #endif // CV_AVX2 #endif // CV_NEON || CV_SSE2  #if CV_AVX2  // nothing #elif CV_SSE2  if( USE_SSE2 ) { for( ; x \u0026lt;= width - 8/(int)sizeof(T); x += 8/sizeof(T) ) { typename VLoadStore64\u0026lt;T\u0026gt;::reg_type r = VLoadStore64\u0026lt;T\u0026gt;::load(src1 + x); r = vop(r, VLoadStore64\u0026lt;T\u0026gt;::load(src2 + x)); VLoadStore64\u0026lt;T\u0026gt;::store(dst + x, r); } } #endif  #if CV_ENABLE_UNROLLED  for( ; x \u0026lt;= width - 4; x += 4 ) { T v0 = op(src1[x], src2[x]); T v1 = op(src1[x+1], src2[x+1]); dst[x] = v0; dst[x+1] = v1; v0 = op(src1[x+2], src2[x+2]); v1 = op(src1[x+3], src2[x+3]); dst[x+2] = v0; dst[x+3] = v1; } #endif  for( ; x \u0026lt; width; x++ ) dst[x] = op(src1[x], src2[x]); } } 哈哈哈哈，终于见到循环了没错这个就是最终的计算过程，为了找他我们也是煞费苦心了，最后这个函数里有很多加速的方式，SSE等这些指令集都是用来加速的，我们可以研究下怎么写，但还是优先优化算法，模板函数，这个就可以兼容所有的数据类型了，而且这个函数兼容所有的操作(op),循环里是展开写的，为了减少循环次数来减少其条件判断所带来的额外计算。\n总结 这个就是简单的跟踪了下加法计算的全过程，下一篇就通过这些计算找出opencv 整个Mat计算所依靠的体系（各个类的作用，和设计目的） 我们通过下面这个表来看一下他们的调用关系，我们把MatExpr先暂时叫做计算图（类似于TensorFlow）\n   文件名 类 函数名 返回值 作用     matop.cpp MatExpr operator + MatExpr 生成计算图   matop.cpp MatOp_AddEx makeExpr void MatOp的派生，为了完成不同的运算，MatOp派生了很多类   mat.inl.hpp Mat operator = Mat\u0026amp; 重载，确定何时计算   matop.cpp MatOp_AddEx assign void 执行计算图的计算   arithm.cpp  cv::add void 通用加法计算，可以接受各种类型参数（Mat,CvMat,IplImage）   arithm.cpp  arithm_op void    arithm.cpp  add8s void 区别输入数据类型，调用vBinOp   arithm_core.hpp  vBinOp void 模板函数，完成最终计算    下一篇我们深入研究各个类\n原文地址：https://www.face2ai.com/Other-OpenCV-Mat过程分析转载请标明出处\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-opencv-mat%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文通过分析opencv的Mat类的计算过程，来研究OpenCV的数据结构以及算法过程\n\u003cstrong\u003eKeywords:\u003c/strong\u003e OpenCV，矩阵计算\u003c/p\u003e","title":"【OpenCV源代码分析】矩阵计算过程"},{"content":"Abstract: 本文以线性变换的角度重新理解矩阵变换的原理，以对角化和SVD作为主要的案例 Keywords: Diagonalization，Pseudoinverse\n对角化和伪逆 傻子不是生出来的，是教出来的，如果一个人从小没人教他如何看问题如何思考，或者他自己也不愿意去思考，别人说什么他都相信，那么这个人将会变成一个温和的劳动者，这个道理我们老一辈革命家们都明白，枪杆子笔杆子就可万世而为君，看个头条都能热血沸腾的人统治成本极低。\n对角化和伪逆(Diagonalization and the Pseudoinverse) 首先我们要回顾下，并且强调下昨天讲的内容，就是线性变换对应的矩阵，对于不同空间相互变换，知道空间是不能确定矩阵的，还要确定基和相互关系，光知道基也没用，比如求导和求积分的例子告诉我们，必须要知道他们之间的计算关系，或者叫做原始空的基向量线性变换到目标空间后的向量是啥才能确定矩阵A(上一篇的🌰 1)。 接下来的换基运算是在同一个空间下进行的，也就是线性变换后空间不变，空间不变以为这矩阵的Rank不发生变化，想要进行换基操作就要知道新的基和原来的基都分别是什么，然后就能确定出矩阵了。 手工图片，简约而不简单，哈哈，$T_1$就是个典型的夸空间的变换，已知两组基 $\\vec{v_n}$ 和 $\\vec{w_m}$ 并且要明确的知道 $T(\\vec{v_n})$ 是啥，这样才能确定$T_1$ 对应的变换矩阵A是啥。\n但是对于$T_2$ 就没有后面的要求了，只要知道两组基 $\\vec{v_n}$ 和 $\\vec{w_n}$ 就可以确定出$T_2$ 对应的变换矩阵A了；\n上面是一个简单的回顾，也是上一篇的高度概括，主要在求变换矩阵上，每个矩阵都对应一个变换，如果已知输入空间，那么根据这个矩阵可以得到一个线性变换的输出空间，我们所有使用到矩阵乘法的地方都可以看成线性变换的过程，当然其中一部分也可以当做换基操作，我们下面的主要研究集中在换基上，也就是同一个空间下的形式转换。\n本课程最前面有一句话，就是我们天天变成三角矩阵也好对角矩阵也好，就是为了让矩阵形式变得简单同时，暴露出矩阵的性质（类似于大数据挖掘），而这些所有对角化，消元操作对应的都是矩阵乘法，也即是说，我们可以通过换基来使矩阵变得漂亮（上一篇说过），我们今天就看看怎么通过换基让矩阵变得简洁漂亮。\n因为变换太多了，我们找两个厉害的，对角化和SVD来讲解 在讲解之前我们有必要对矩阵和向量相乘进行下分析，下面假设A可逆： $$ w=A\\vec{v} $$ 这里面我们规定 $\\vec{v}$ 的基是标准基，就是$I$ ,那么 $w$ 的基就是 $A^{-1}$ ，为什么，下面我们开动想象，我们说向量有大小有方向，所以我们用一个有序的数字串来表示一个向量，但是这里隐含的是在坐标系是标准坐标系的情况下，那么我们一旦更换坐标系，变换的是这串数字，向量本身的方向大小不变，也就是说指向南方的长度为1的向量，可以有不同的无数组有序数字串表示，也就是基于不同的基。\n所以上面的v到w的线性变换就是换基，根据我们前面的例子，可以得知这矩阵A线性变换后的基是$A^{-1}$ 的列向量，如果我们继续我们的变换也就是 $$ w\u0026rsquo;=Bw=BA\\vec{v} $$\n这个连续的线性变换的过程是$\\vec{v}$作为输入先通过A到输出中，也就是w，然后以这个输出作为输入，经过B得到B的输出空间。注意先A后B，在乘法中表现为 $BA$ 。\n注意上面我们并没有左边的矩阵，也就是没有等效矩阵，接下来我们引入C，$C=BA$ 那么我们就有： $$ w\u0026rsquo;=Bw=BA\\vec{v}=C\\vec{v}\\ C\\vec{v}=BA\\vec{v} $$ 如果我们更严谨一些把省略掉的标准基补充进来的话 $\\vec{v}=I\\dot{v}$ 是最正确的写法，我们这里不需要默认 $\\dot{v}$ 的基是什么，他现在就是一个没有基的数组了（也不是向量），但是单独说v没有意义，必须$Iv$ 才有意义，那么上面的式子将变成： $$ CI\\dot{v}=BAI\\dot{v}\\ CI=BAI $$ 我们这里研究一下$BAI$，我们首先得到的是标准基的输出（通过与$I$相乘）,然后我们得到A的输出（通过与 $A$ 相乘）,最后我们得到B的输出（通过与 $B$ 相乘），上述所有的输入都是上一步的输出。 所以上面的整个过程可以分解成下面这幅图： 整个换基过程就是这样被拆解的，但是面对不同的拆解方式，我们可能得到非常标量的A，这个是我们想要的，也就是C被分解了，分解出了漂亮的A，我们对矩阵的所有操作基本都是在干这个事，所以这些事都算换基操作，整个过程就是当标准基输入到C中我们可以得到输出，这个过程的等效过程是先输入漂亮的A然后得到的输出再输入到B最后得到的输出和C的输出一致。\n要说明的是上面的箭头方向对应一个矩阵，如果这个矩阵可逆那么这个箭头就可以反过来对应的矩阵也就变成了原矩阵的逆。\n相似矩阵 :$A$ and $S^{-1}AS$ and $W^{-1}AW$ 还是我们上面$C=BA$ 的分解过程，那么把它变形到其他矩阵分解形式，对角化或者相似矩阵和上面的比较相似，举个🌰 $$ B=M^{-1}AM\\ $$ 如果我们已知B，那么如果A很漂亮，这个换基过程就很值得了 首先从标准基换到 $M^{-1}$ 为基的空间（首先要从又开始上面说为啥了，为啥是$M^{-1}$ 前面有说过，$M^{-1}$ 的每一列都是基，因为M可逆所以必然线性无关）得到的输出作为输入，输入到漂亮A，然后的输出是漂亮A所对应的基，然后再通过$M^{-1}$ 回到以M的每列为基的形式，这个过程出来的矩阵被定义为相似的，当M对应的是特征向量，那么漂亮A改名 $\\Lambda$ 这个矩阵足够漂亮到所有人都喜欢他. 不同的M可以得到不同的漂亮A，下面我们看看🌰\n eg. Project onto the line $y=-x$ that goes from northwest to southeast. The vector $(1,0)$ projects to $(.5,.5)$ ,the projection of $(0,1)$ is $(-.5,.5)$\n  按照标准做法，选取标准基，我们可以得到线性变换矩阵 $A=\\begin{bmatrix}.5\u0026amp;-.5\\newline -.5\u0026amp;.5\\end{bmatrix}$ 。 如果我们想要对角矩阵，需要先求出特征向量矩阵 $S=\\begin{bmatrix}1\u0026amp;1\\newline-1\u0026amp;1\\end{bmatrix}$ 对应的求出在 $S^{-1}$ 基下的线性变换矩阵 $\\Lambda=\\begin{bmatrix}1\u0026amp;0\\0\u0026amp;0\\end{bmatrix}$ 果然很漂亮然后把输出再变换到$S$ 为基的形式就回到了上面的A 求相似和2类似这里不再赘述  上面的过程对于线性变换$T$ 对应矩阵A，我们可以通过一系列的换基操作在其他基下找到等效的但是比A漂亮的矩阵，再通过换基操作还原到A基形式下，这个过程就是$IAI=W^{-1}XW$ 的各种特例。\n奇异值分解(SVD) 与上面的最大不同，SVD输入输出来自两个空间，我们上面说在同一空间下的线性变换都是换基操作，这里展示不同空间的换基操作，神奇的SVD，对于不同空间的线性变换矩阵，一般为长方形的（参考求导和微分的例子，在上一篇），假设矩阵A可以完成一个跨空间的线性变换，那么当我们改变两个空间的基的时候，这个时候，奇迹就发生了，当我们把原始空间X(dim=n)的基从标准基变成$AA^T$特征向量，得到一个换基后的输入，然后我们再把原始出Y(dim=m)换基到成$A^TA$特征向量，作为目标矩阵的输入和输出，那么我们会得到一个漂亮的对角矩阵（对角元素可能是0） 上面这个求解 $\\Sigma$ 的过程并不是一根筋，而是两头堵，根据紫色线的方向求解的，因为U和V都是正交矩阵，所以其逆和转置一样。\n伪逆(The Pseudoinverse) 伪逆就是不是真的逆，并不是所有矩阵都有逆，但是所有矩阵都有伪逆，当矩阵是可逆矩阵的时候，伪逆和逆相等，我们前面学四个子空间的时候学过一幅图： 这幅图我们当时有详细讲解，假设矩阵是 $m\\times n$ 的那么他的行空间和Nullspace加起来肯定是$\\Re^n$ ，文中已证明，这个也是线性代数的基本定理，那么 $\\Re^n$ 中的任何向量都能被分解成rowspace和nullspace中基的线性组合，所以线性映射后得到将是列空间中的一个向量加上0，所以当nullspace不止零向量的时候不可逆，因为nullspace不能被反射，但是这里有个有趣的就是$Ax_r=b$ 可以被反射得到输入向量的rowspace的分量$x_r$，这个反射矩阵就是Pseudoinverse。 从换基的角度，继续SVD，我们看到上图我们走的紫色路线，那么我们如果想反着走： 红线对应的就是A的伪逆（如果A是可逆的也可能是逆），我们采取和求$Sigma$ 同样的两头堵占略，根据紫色的大粗箭头的方向进行换基，能够求到 $\\Sigma_r^{-1}$ （只有r个有效成分），然后我们跟着红色箭头的方向就能得到A的伪逆了$A^{+}$ :\n$$ \\Sigma_r^{-1}= \\begin{bmatrix} \\sigma_1^{-1}\u0026amp;\u0026amp;\\ \u0026amp;\\ddots\u0026amp;\\ \u0026amp;\u0026amp;\\sigma_r^{-1} \\end{bmatrix}\\ \\Sigma^{+}= \\begin{bmatrix} \\sigma_1^{-1}\u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\ \u0026amp;\\ddots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;\\sigma_r^{-1}\u0026amp;\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;\u0026amp;0\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp;\\ddots\u0026amp;\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;0 \\end{bmatrix} $$\n然后\n$$ \\Sigma^{+}\\Sigma = \\begin{bmatrix} 1\u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\ \u0026amp;\\ddots\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;1\u0026amp;\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;\u0026amp;0\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp;\\ddots\u0026amp;\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;0 \\end{bmatrix} $$\n然后顺势就能求出$A^{+}$ 了整个过程如此，不在计算验证。把书上反射的图也贴上，大家对比来看： 然后总结下我们学过的所有的变换的换基角度： Conclusion 至此，线性代数的基础理论部分算是完结了，我们接下来可能介绍线性代数的一些相关应用，也有可能增加复数矩阵部分，但是不作为这个系列的内容了，所以我们线性代数今天正式毕业！撒花，啦啦啦，但是后面还有更精彩的系列，敬请关注！\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-7-3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文以线性变换的角度重新理解矩阵变换的原理，以对角化和SVD作为主要的案例\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Diagonalization，Pseudoinverse\u003c/p\u003e","title":"【线性代数】7-3:对角化和伪逆(Diagonalization and the Pseudoinverse)"},{"content":"Abstract: 本篇有点长，内容及其丰富，包括线性变换的矩阵形式以及相关例子（导数和积分），然后详细的讲解了下怎么构造矩阵，也就是矩阵的来源，之后是矩阵相乘的原理，基的变换，最后一波大应用，小波变换和离散傅里叶变换 Keywords: Matrix,Matrix for the Derivate,Matrix for the Integral,Construction of the Matrix,$AB$ Match $TS$,Multiplication,Change of Basis Matrix,Wavelet Transform,Fourier Transform(DFT)\n线性变化的矩阵 线性代数过了今天可能就剩下下一篇的一点剩下的基础理论了，从开始写到现在，已经三个月了，速度确实太慢了；而且没什么人看，但是我觉得我敢自称会线性代数了，当然考试的话可能还得不了几分，但起码我能说出来一些很关键的知识，下一步就是机器学习最关键也是我之前完全没学会的概率了，概率和数理统计对于机器学习可能更重要一些，所以后面的博客继续更新概率论，矩阵分析可能要提上日程了，但是目前不确定什么时候写。 注意：下文中线性变换和线性组合是有区别的，请区分对待\n线性变化的矩阵 如果我们不去回想第一张的矩阵乘法，矩阵向量相乘，我们只从上一篇的思路继续，当时我们假定线性变换$T$ 对$v_1 \\in \\Re^n$ 的变换结果是 $w_1 \\in \\Re^m$ ，如果$w_1 \\neq v_1$，那么就是空间发生了变换，我们假定存在矩阵A满足这个变换，也就是 $T(v_1)=Av_1=w_1$ 那么矩阵规模是 $m\\times n$ 的，等等，如果$v_1$ 所在的空间V和 $w_1$ 所在的空间W 已经确定知道，那么能确定矩阵$A$么？答案是不确定的，也就是说输入空间输出空间即便确定了，我们也不能肯定之间的对应关系，那么还需要什么条件呢？答案是空间的基向量，我们知道基向量可以确定出整个空间（子空间）但是已知空间，却可以对应无数组各种各样的基向量，所以同样的空间，不同的基应该对应着不同的线性变换矩阵$A$ 。 线性代数的另一个重要任务就是通过找到最完美的基来得到最完美的矩阵 $A$ 。 下面我们研究一下基，我们假设空间V有n个线性独立的向量组成的一组基 $\\vec{v_1},\\vec{v_2},\\dots ,\\vec{v_n}$ ，那么空间内任一向量均可表示为 $\\vec{v}=c_1\\vec{v_1}+c_2\\vec{v_2}+\\dots +c_n\\vec{v_n}$\n Key idea of this section： Suppose we know $T(\\vec{v_1}),\\dots,T(\\vec{v_n})$ for the basis vectors $v_1,\\dots,v_n$ Then linearity produces $T(\\vec{v})$ for every other input vector $v$\n 翻译一下，也就是我们知道，空间中的任一向量都是通过基向量的线性组合出来的，经过线性变换$T$ 会得到新空间的一组向量然后进行线性组合就能得到结果： $$ T(\\vec{v})=T(c_1\\vec{v_1}+c_2\\vec{v_2}+\\dots +c_n\\vec{v_n})=c_1T(\\vec{v_1})+c_2T(\\vec{v_2})+\\dots +c_nT(\\vec{v_n}) $$ 也就是线性组合的线性变换等于线性变换后的线性组合，由于上式的表示的是输入空间的任一向量，所以从输入空间到输出空间的映射就此完成，有点拗口但是看上面的公式一目了然，彪悍的逻辑，不需要解释。 这里会举一个🌰 ，这个🌰 很重要，虽然简单，但是值得拥有：\n eg1. suppose $v_1=(1,0),transforms,T(v_1)=(2,3,4)$ and $v_2=(0,1),transforms,T(v_2)=(5,5,5)$ If $T$ is linear from $\\Re^2$ and $\\Re^3$ then the \u0026ldquo;Standard matrix \u0026quot; is 3 by 2,Those output $T(v_1)$ and T(v_2) go into the column $A=\\begin{bmatrix}2\u0026amp;5\\3\u0026amp;5\\4\u0026amp;5\\end{bmatrix}$ so $v=(1,1)$ transformation $T(v)=\\begin{bmatrix}2\u0026amp;5\\3\u0026amp;5\\4\u0026amp;5\\end{bmatrix}\\begin{bmatrix}1\\1\\end{bmatrix}=\\begin{bmatrix}7\\8\\9\\end{bmatrix}$\n 这个例子的简单在于先给出v的基是标准基，也就是我们平时说坐标张嘴就来的坐标系下的，单位正交基，写成矩阵就是$I$ 的那组基，我们前面从来没说过这个事，也就是说默认都是他，当然我们现在要研究他了，所以必须强调，而且这个例子很轻松的就给出了矩阵 $A$ 也是因为这个标准基的特殊性质带来的方便，那么当我们把基变了以后会得到什么样的结果呢？ 下面我们看另一个例子，这个🌰可以引出线性代数基本定理（Fundamental Theorem）\n eg2. 函数$1,x,x^2,x^3$ 的导数是$0,1,2x,3x^2$ 这些是求导这个线性变换$T$ 的四个因素，输入输出都是函数，更关键的因素是求导是线性的： $$ \\frac{d(cv+dw)}{dt}=c\\frac{dv}{dt}+d\\frac{dw}{dt} $$ 所以我们可以根据以上两个因素求得多项式$4+x+x^2+x^3$的导数 $$ \\frac{d(4+x+x^2+x^3)}{dx}=1+2x+3x^2 $$\n 这个例子厉害就厉害在了T，这个不在局限于乘加计算，很多计算都可以满足线性的要求，所以之前说求导是线性的，我是拒绝的，包括后面的积分老师说他是线性的，我也是拒绝的。继续分析这个例子，输入空间是个n维的向量，当然你也可以叫他函数，不过老爷子（prof. Strang）说我认为他是个向量，所以我们就按照向量来，如果我们观察输入输出维度我们发现，输出的维度都要小于输入维度，比如输入最高是4次，那么输出最高是3次，也就是说，输出是输入的一个subspace，少一维，例如输入空间$m=4$ 那么 输出空间是$n=3$ 所以根据🌰 1 或者前面基本公式，都能确定矩阵A是个$n\\times m$的 输出空间$n=3$ 那么矩阵$rank(A)=3$ 维度是3 什么情况下输出是 $\\vec{0}$ 呢，根据导数性质，我们知道常数的导数是0 ，也就是我们只能保留第一项（基为1的项可以有，其他$x,x^2,x^3,\\dots$ 必须不存在）那么也就是说Nullspace应该是$V_{null}=(a,0,0,0,\\dots)$ 维度是1 那么我们就能得出线性代数基本定理了:\n dimension(nullspace)+dimension(columnspace)=n\n or\n dimension of range(columnspace)+dimension of kernel(nullspace)=dimension of input inputspace\n 接下来还是🌰 今天的例子怎么这么多呢，因为以前🌰 也多但是我试图用文字描述了例子中的关键点，但是这篇必须要有🌰，不然会非常抽象。\n eg3. 积分是求导的逆运算(微积分的基本定理Fundamental theorem)，我们先用符号 $T^{-1}$来表示积分计算，我们还没有说他是线性变换，所以我们继续我们的例子，我们有： $$ \\int_0^x1dx=x\\ \\int_0^xxdx=\\frac{1}{2}x^2\\ \\int_0^xx^2dx=\\frac{1}{3}x^3\\ \\vdots $$ 这个是其中的一个因素，另一个重要因素，积分也是线性的，不信？ $$ \\int_0^xc_1g(t)+c_2f(t)dt= c_1\\int_0^x g(t)dt+c_2\\int_0^x f(t)dt $$ 信不信，不信去看calculus的教材吧，包您满意，接下来我们就进行上面🌰 2 的逆过程，已知$w=B+Cx+Dx^2$ 那么$T^{-1}(w)=Bx+\\frac{1}{2}Cx^2+\\frac{1}{3}Dx^3+E$ 可见积分计算相当于把🌰 2中的W空间重新变换回了V空间，所以$T^{-1}$ 是个$4\\times3$ 的矩阵\n 例子一直观的通过一个代数计算过程告诉我们输入空间到输出空间线性变换过程跟一个矩阵有关系，跟基有关系，并且矩阵的尺寸和输入输出维度有关系，例子二和例子三则是告诉我们线性变换可以存在逆运算，下面要研究的就是求导和积分这两个变换的矩阵形式是什么样子的。\n微分积分的矩阵形式 上面我们从函数的角度研究了一下多项式的求导和积分，在上面老爷子一直强调他把多项式看成向量，也就是变量 $x^n,n=0,1,2,3\\dots$ 作为基，分别分析了求导和积分后的dimension的变化和相互的关系，我们接下来研究的是如何把求导过程矩阵化，值得强调的是，我们从此开始只研究基，对于基前面的系数（上述求导和求积分的例子中多项式的系数）只需作为验证，我们对基进行变换，后带入系数验证结果，既有理论的严谨又有直观的感受。\n 对于求导，我们从输入空间 $V=(1,x,x^2,x^3)$ 求导后输出向量的空间基是 $W=(1,x,x^2)$ 对应的是 $3\\times 4$ 的矩阵  $$ A= \\begin{bmatrix} 0\u0026amp;1\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;2\u0026amp;0\\ 0\u0026amp;0\u0026amp;0\u0026amp;3 \\end{bmatrix} $$ 我们并不知道求A的方法或者一般过程，上面🌰 1 中通过标准基的线性变换得到的矩阵对于标准基适用，但在这里也不太好用，所以我们不知道A是怎么来的，那么我们来看看他是怎么没的，如果我们输入向量是$A+Bx+Cx^2+Dx^3$ 也就是向量 $v=(A,B,C,D)$ 线性变换 $T(v)$ 可以写成下面的方式: $$ T(v)=Av= \\begin{bmatrix} 0\u0026amp;1\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;2\u0026amp;0\\ 0\u0026amp;0\u0026amp;0\u0026amp;3 \\end{bmatrix} \\begin{bmatrix}A\\B\\C\\D\\end{bmatrix}= \\begin{bmatrix}B\\2C\\3D\\end{bmatrix} $$ 如果我们用微积分的方法直接求导呢： $$ \\frac{d(A+Bx+Cx^2+Dx^3)}{dx}=B+2Cx+3Dx^2 $$ 可见与矩阵方法一致，证明了A的正确性。 与此同理，积分我们就不在详细叙述了，我们还是把积分称为 $T^{-1}$ 对应的矩阵 $A^{-1}$ 输入空间基 $(1,x,x^2)$ 输出空间基 $(1,x,x^2,x^3)$ 那么： $$ A^{-1}= \\begin{bmatrix} 0\u0026amp;0\u0026amp;0\\ 1\u0026amp;0\u0026amp;0\\ 0\u0026amp;\\frac{1}{2}\u0026amp;0\\ 0\u0026amp;0\u0026amp;\\frac{1}{3} \\end{bmatrix} $$ 对于输入$v=(B,C,D)$ 我们可以得到 $A^{-1}v=\\begin{bmatrix}0\\B\\\\frac{1}{2}C\\\\frac{1}{3}D\\end{bmatrix}$ 接下来我们要分析下，我们把这个线性变换矩阵称为$A^{-1}$ 的原因是因为他的线性变换与上面的求导是互为逆过程的，但是我们知道只有方阵有逆运算，这种长的矩阵不会有逆的，但是如果我们计算$AA^{-1}$ 会发现其结果是 $\\begin{bmatrix}1\u0026amp;0\u0026amp;0\\0\u0026amp;1\u0026amp;0\\0\u0026amp;0\u0026amp;1\\end{bmatrix}$ 这就很有灵性了，两个非方阵相乘结果是单位矩阵，但是按照逆矩阵的要求 $A^{-1}A$ 却等于 $\\begin{bmatrix}0\u0026amp;0\u0026amp;0\u0026amp;0\\0\u0026amp;1\u0026amp;0\u0026amp;0\\0\u0026amp;0\u0026amp;1\u0026amp;0\\0\u0026amp;0\u0026amp;0\u0026amp;1\\end{bmatrix}$ 不是单位矩阵，但是对角线上只有0和1。\n从微积分的角度继续思考如果一个函数先求导再求积分(对应的矩阵乘法是$A^{-1}A$，问我为什么，如果输入是$v$ 的话，输出$w=A^{-1}Av$ A首先对v进行线性变换，所以应该从右往左算)，丢失的信息是常数项，因为求导后常数项是0，求积分后变成未知常数，那么也就是有一位是有损失的，所以对应的就是第一列的那个0就是损失的常数，相反的，先求积分再求导就不会有数据损失，也就是常数项也完好无损的回来了，所以$AA^{-1}=I$ 至此上面的整个变换过程从数字上和逻辑上是没有问题的。 那么对于非方阵如果存在$AA^{-1}=I$ 这种情况，我们称之为 one-side inverse，就是一边的逆，另一边不是逆。\n小结下上面这两个矩阵，这两个矩阵告诉我们的内容就是，兄弟，线性变换好像真的和矩阵一一对应呢，而且我们主要研究的是基，这个基可以是向量，也可以是像$x^n,n=0,1,2,3\\dots$ 这样的因子，所以我们接下来就研究下如何通过已知输入输出空间的基来求矩阵A。\n构造矩阵（Construction of the Matrix） 现在我们就要研究最核心的部分了，构造矩阵，如何为任意一个线性变换构造一个矩阵A，通过A来连接输入空间V（dim=n）和输出空间W(dim=m)，方式是$\\vec{w}=A\\vec{v}$ ,所以矩阵规模与输入空间输出空间对应为$m\\times n$ 下面我们可能要改道了，因为老爷子和《linear algebra done right》都是用了下面这套方法来确定矩阵的： 我们选定输入空间基 $\\vec{v_1},\\vec{v_2},\\dots ,\\vec{v_n}$ ,输出空间基 $\\vec{w_1},\\vec{w_2},\\dots ,\\vec{w_m}$ 我们来分析，V和W各代表一个空间，当$v_i$ 经过T后将会得到W空间一个向量，这个向量是基么？不一定，必须强调一下，这个问题刚刚想明白，我一直在思考如何让$v_i$变成$w_j$，但是这个向量既然是W中的一个向量，就可以用$w_j$ 的线性组合得出来 线性变换后的结果： $$ T(v_i)=a_{1i}w_1+\\dots + a_{mi}w_m $$ 这个不要通过代数的方式看，你就从空间的角度思考，因为如果按照代数的思维就凌乱了，所以我们通过上面的这个式子就能得到一个矩阵，这个矩阵每列对应一个输入空间的基，每行对应一个输出空间的基，矩阵的元素就是上面的a们： $$ \\begin{array}{lc} \\mbox{}\u0026amp; \\begin{array}{cc}\\vec{v_1}\u0026amp;\\dots\u0026amp; \\vec{v_n}\\end{array}\\ \\begin{array}{c}\\vec{w_1}\\\\vdots\\\\vec{w_m}\\end{array}\u0026amp; \\left[\\begin{array}{cc} a_{11}\u0026amp;\\dots\u0026amp;a_{1n}\\ \\vdots\u0026amp;\\ddots\u0026amp;\\vdots\\ a_{m1}\u0026amp;\\dots\u0026amp;a_{mn} \\end{array}\\right] \\end{array} $$\n这个矩阵就是我们梦寐以求的A，当然上面的过程并没有说明为啥排列好了就是A这个说明白，但是我们回忆，我们最初通过方程组定义矩阵不也是没证明么？为啥，因为定义不需要证明啊，我们证明定理，定义只是定理公理结合后的一个新的名字，或者说我们如果没学前面的所有课程，这个A就可以当做矩阵的定义，所有的矩阵都对应一个线性变换，任何一个线性变换都对应一个矩阵（在输入输出空间的基确定以后，不同的基产生不同的矩阵）。\n上面这个矩阵定义非常的重要，因为他是矩阵和线性变换的纽带，必须再强调一遍，线性变换是空间到空间的变换，对于线性变换我们只能知道规模，也就是只能确定$m\\times n$ 但是我们不知道其中的具体内容，只有当输入输出空间的基确定了（包括顺序，不同的顺序相当于不同的基），才能确定A。 举个🌰 ，还是上面求导的🌰 如果我们交换一下多项式中各项的位置，$Bx+Cx^2+Dx^3+A$ 从代数的角度看这个没有变化，但是老爷子说要把它看成向量，那就麻烦了，新的输入空间V $(x,x^2,x^3,1)$ 输出空间W的基不变 $(1,x,x^2)$ 那么我们根据定义来求一下A，首先是第一列 $$ T(v_1)=a_{11}\\cdot 1+a_{21}\\cdot x+a_{31}\\cdot x^2\\ \\frac{dx}{dx}=1\\ so:\\ T(v_1)=1\\cdot 1+0\\cdot x+0\\cdot x^2 $$ 所以对应的，矩阵的第一列就是 $\\begin{bmatrix}1\\0\\0\\end{bmatrix}$ 迭代下去得到其他元素 $$ \\begin{bmatrix} 1\u0026amp;0\u0026amp;0\u0026amp;0\\ 0\u0026amp;2\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;3\u0026amp;0 \\end{bmatrix} $$\n可见当基发生了变化的时候矩阵随之发生变化，但规模不变，仔细观察发现是顺序改变了。 求导和积分的🌰给我们了下面这些信息：\n 线性变换在微积分，微分方程和线性代数中都有应用 空间很重要 线性变换（根据基）都对应一个矩阵并且我们学会了如何求这个矩阵  $AB$ 乘积匹配 $TS$ 变换 这个section我们会从线性变换的角度讲解矩阵乘法，我们前面是通过线性组合，然后把矩阵与多个向量相乘合并起来形成了矩阵乘法，但是我们如果从线性变换角度来看，那么矩阵乘法一切顺理成章。 如果我们考虑两个线性变换$TS$，和对应的两个矩阵$AB$ 如果矩阵$B$ 将输入空间V线性变换到U那么我们得到$u=S(v)=Bv$，我们继续把u变换到空间W，$w=T(S(v))=T(u)=Au=ABv$ 可见连续的线性变换对应的是矩阵相乘并且规模上是正确的。 我们把这个过程拆分开来对应矩阵乘法： 在输入空间中，我们只研究一个基$v_1$，他对应的在S中的向量是$b_{11}s_1+\\dots b_{m1}s_m$ 接下来进一步线性变换， 基$s_1$ 对应的是$a_{11}t_1+\\dots a_{q1}t_q$ 基$s_m$ 对应的是$a_{1m}t_1+\\dots a_{qm}t_q$ 代入后能得到： $$ (a_{11}t_1+\\dots a_{q1}t_q)b_{11} \\\\vdots\\ +(a_{1m}t_1+\\dots a_{qm}t_q)b_{m1} $$ 竖着看，这就是A中各行，和B中的一列相乘得到的是AB的第一列$T(s_1)$ ，确实这个地方有点抽象，但是结合 $$ \\begin{array}{lc} \\mbox{}\u0026amp; \\begin{array}{cc}\\vec{v_1}\u0026amp;\\dots\u0026amp; \\vec{v_n}\\end{array}\\ \\begin{array}{c}\\vec{w_1}\\\\vdots\\\\vec{w_m}\\end{array}\u0026amp; \\left[\\begin{array}{cc} a_{11}\u0026amp;\\dots\u0026amp;a_{1n}\\ \\vdots\u0026amp;\\ddots\u0026amp;\\vdots\\ a_{m1}\u0026amp;\\dots\u0026amp;a_{mn} \\end{array}\\right] \\end{array} $$ 来看的话连续的线性变换就是多个矩阵乘法，并且前一个矩阵的输入空间维度（列的数量）要和后面一个矩阵的输出维度一致（行的数量），所以矩阵乘法的基本规模要求就是这样确定的。\n这里举个例子，旋转矩阵R，假设旋转了 $\\theta$ 角度，那么 $$ R= \\begin{bmatrix} cos(\\theta)\u0026amp;-sin(\\theta)\\ sin(\\theta)\u0026amp;cos(\\theta) \\end{bmatrix} $$ 如果旋转两次呢，会得到对应的角度 $2\\theta$ 么 $$ RR=\\begin{bmatrix} cos(\\theta)\u0026amp;-sin(\\theta)\\ sin(\\theta)\u0026amp;cos(\\theta) \\end{bmatrix} \\begin{bmatrix} cos(\\theta)\u0026amp;-sin(\\theta)\\ sin(\\theta)\u0026amp;cos(\\theta) \\end{bmatrix}\\ \\begin{bmatrix} cos^2(\\theta)-sin^2(\\theta)\u0026amp;-2sin(\\theta)cos(\\theta)\\ 2sin(\\theta)cos(\\theta)\u0026amp;-sin^2(\\theta)+cos^2(\\theta) \\end{bmatrix}\\ =\\begin{bmatrix} cos(2\\theta)\u0026amp;-sin(2\\theta)\\ sin(2\\theta)\u0026amp;cos(2\\theta) \\end{bmatrix} $$ 计算验证表示没有问题。\n本征变换和换基矩阵 我们前面反复说明线性变换是空间到空间的，那么我们这个section就来研究一下，如果空间不变，也就是说输入输出空间一致，那么会发生什么，从最表面的来说，应该是方阵，因为输入输出维数相同，不考虑子空间，只考虑完整的空间，那么这个矩阵必然是方阵，所以最特殊的线性变换就是自己到自己，也就是单位矩阵 Identity矩阵，这个矩阵不会对空间以及基做任何变换。 上面说的最简单的情况就是基不变空间不变，那么如果空间不变但是我们想要换组基呢？那么我们就按照上面介绍的一般方法做就能求出矩阵M，但是这个M被称为换基矩阵。\n举个🌰 ： 输入基 $\\vec{v_1}=\\begin{bmatrix}3\\7\\end{bmatrix},\\vec{v_2}=\\begin{bmatrix}2\\5\\end{bmatrix}$输出基是 $\\vec{w_1}=\\begin{bmatrix}1\\0\\end{bmatrix},\\vec{w_2}=\\begin{bmatrix}0\\1\\end{bmatrix}$ 那么对应的矩阵中：$\\vec{v_1}=a_{11}\\vec{w_1}+a_{21}\\vec{w_2}$ 这样就可以解得$a_{11}=3,a_{21}=7$,同样的过程解得$a_{12}=2,a_{22}=5$ ，综合上述换基矩阵A: $$ A=\\begin{bmatrix}3\u0026amp;2\\7\u0026amp;5\\end{bmatrix} $$ 同样的过程如果我们想把矩阵重新换基回到原始基，那么就是上面的逆过程：\n举第二个🌰 ： 输入基 $\\vec{v_1}=\\begin{bmatrix}1\\0\\end{bmatrix},\\vec{v_2}=\\begin{bmatrix}0\\1\\end{bmatrix}$ 输出基是 $\\vec{w_1}=\\begin{bmatrix}3\\7\\end{bmatrix},\\vec{w_2}=\\begin{bmatrix}2\\5\\end{bmatrix}$ 那么对应的矩阵中：$\\vec{v_1}=a_{11}\\vec{w_1}+a_{21}\\vec{w_2}$ 这样就可以解得$a_{11}=5,a_{21}=-7$,同样的过程解得$a_{12}=-2,a_{22}=5$ ，综合上述换基矩阵A: $$ B=\\begin{bmatrix}3\u0026amp;-2\\newline -7\u0026amp;5\\end{bmatrix} $$\n可以看出$AB=BA=I$ 互逆的两个换基操作得到对应的互逆矩阵，互逆的线性变换可以保证one-side 逆（上面非方阵的结论）。 换基矩阵在同一空间下的基的互相转换。\n小波变换 = 换成小波基 标题小波变换，等于换成小波基，这里对小波的介绍并不多只是介绍了最简单的haar小波基的简单介绍，haar小波基主要用于数据压缩，压缩图像等数据，原理和前面的SVD压缩图像差不多，只是那个基是$U$ 和$V^T$ 中的特征向量的乘积，不同的奇异值代表这个向量在原始数据中的重要性，小波基与他不同点在于小波基的基是确定，压缩过程基本一致，无损的换基操作，然后丢弃影响小的部分（有损）完成压缩，然后对压缩后的数据进行逆变换恢复重建原始数据，压缩前后数据肯定是有损失的，那么这项研究的目的就是高压缩比例，小的数据损失。\n傅里叶变换 = 换成傅里叶基 离散傅里叶变换包含虚数部分，他的基是： $$ F= \\begin{bmatrix} 1\u0026amp;1\u0026amp;1\u0026amp;1\\ 1\u0026amp;i\u0026amp;i^2\u0026amp;i^3\\ 1\u0026amp;i^2\u0026amp;i^4\u0026amp;i^6\\ 1\u0026amp;i^3\u0026amp;i^6\u0026amp;i^9\\ \\end{bmatrix} $$ 这里举了两个应用的例子，说的都不是很详细，只是让我们记住，线性代数在很多地方非常有用，不然人家怎么称得上基础。\nConclusion 本文历时24小时终于完成，可能稍微有点点混乱，但是最精彩的部分是构造线性变换矩阵那部分，如何通过空间中的向量和基的关系来表示变换前后的关系，下一篇简答介绍下线性变换在伪逆和对角化中的意义，然后基础理论基本上就算讲完了，后面一波应用等着我们呢，应该可以快些了。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-7-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本篇有点长，内容及其丰富，包括线性变换的矩阵形式以及相关例子（导数和积分），然后详细的讲解了下怎么构造矩阵，也就是矩阵的来源，之后是矩阵相乘的原理，基的变换，最后一波大应用，小波变换和离散傅里叶变换\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Matrix,Matrix for the Derivate,Matrix for the Integral,Construction of the Matrix,$AB$ Match $TS$,Multiplication,Change of Basis Matrix,Wavelet Transform,Fourier Transform(DFT)\u003c/p\u003e","title":"【线性代数】7-2:线性变化的矩阵(The Matrix of a Linear Transformation)"},{"content":"Abstract: 本篇介绍线性代数的另一个角度，就是线性变换思想 Keywords: Linear Transformation，Linear Combination，Kernel，Range\n线性变换思想 线性代数到上一篇可以说已经成体系了，不能说自己精通，起码了解了大概的讨论，接下来这三篇第7章的博客是从另一个角度入门线性代数，所以从现在开始，你可以忘记前面学的所有的切入方法，也就是线性代数提出的方法，我们之前的完整套路依靠的是对线性方程组求解的过程引出后面一系列的操作，但是，线性代数为什么叫线性代数，而不叫线性方程组呢？那么本章就是解释，我觉得这个角度切入提出完整的理论更通顺，但是问题就是这么切入没有通过方程组来的那么简单，所以这个角度适合下一步提高，线性方程组适合入门，但是殊途同归，最后都能得到完整的线性代数知识框架。\n线性变换思想(The Idea of a Linear Transformation) 我们第一章应该反复强调线性组合，就是一个乘加计算满足线性，当时一直很困惑为啥叫线性组合，并且当时的博客一直强调线性组合是线性代数的重要理论基础，有了线性组合才有了后面的所有。但是线性组合和线性变换有所区别：线性组合是线性变换的一种，但是线性变换并不一定就是乘加计算（线性组合）。从Caculus的角度来看，如果把某个线性组合看成一个函数，那么这个函数 $T$ 满足 ： $$ T(x+y)=T(x)+T(y)\\ T(\\alpha x)=\\alpha T(x) $$ 或者 $$ T(\\alpha x + \\beta y)=\\alpha T(x) + \\beta T(y) $$ 这两种表达方式都是在Caculus中对函数线性的表达，如果对于线性代数，我们应该把输入输出转换到对应的向量空间，$T$ 不在表示一个number to number的映射，而是一个vector to vector的映射：\n A Transformation T assigns an output $T(v)$ to each input vector $v$ in $V$. The transformation is linear if it meets these requirement for all $\\vec{v}$ and $\\vec{w}$: $$ T(v+w)=T(v)+T(w)\\ T(cv)=cT(v) $$ or take the two equation into one: $$ T(cv+dw)=cT(v)+dT(w) $$\n 接下来正经的做法是举例子，但是我想让大家都冷静冷静，当年张三丰教张无忌太极剑法的时候，就问张无忌忘了多少了？张教主说忘了一半了，又过了一会儿，张教主说全忘了，于是张真人说甚好，去揍他吧。哈哈，这段话说的基本上就是一个知识理解的必要过程，如果这段知识，你的师父或者教材让你很流畅的读下来，你肯定啥都记不住，因为一切太通常，你回觉得一切都那么理所当然，而蹩脚的老师讲的课会让你听了之后印象深刻，因为每个地方都有不懂的，你就会思考所以印象深刻，这种印象深刻的最直接后果就是等你忘了以后就真的忘了，而大师讲的课，你可能忘得更快，但是当你再看第二遍的时候整个知识树瞬间全部连接起来，融汇贯通，印象最深刻的一个坑爹老师是高中的一个生物老师，说实话，不客气的说那个老师真的是高中遇到最垃圾的老师，当时高中我还是个积极上进的好学生，每天还在思考上清华还是上北大呢，结果这个老师让我第一次知道什么是垃圾的老师，上课念课本不说，完全没有自己的体系，然后每天的方式就是提问不会的要被罚写（每个概念20遍，然后我用五只笔捆绑在一起，然后瞬间完成），卧槽，我当时都震惊了，还有这种教课方式，好在后来可爱的海燕老师来了，这个货就滚蛋了，不然我估计我连西电都考不上。 怎么样，现在还能想起来矩阵怎么来的么？如果还能想起来上面那么一大段废话就白说了，赶紧忘了矩阵，我们现在没有矩阵，只有线性变换。 举个🌰 ： 如果$T(v)=v$ 那么，我们就有了一个极为特殊的例子，也就是说输入空间的向量和输出向量一致，那么输入空间V和输出空间W也就是一致的，这里引出了输入输出空间。 举另外一个🌰 ： $$ T(v)=Av+u_0 $$ 我们先忽略那个A，告诉自己这不是矩阵不是矩阵不是矩阵，这就是个参数，大写的参数，。 那么 $$ T(v+w)=Av+Aw+u_0 \\neq Av+u_0+Aw+u_0=T(v)+T(w) $$ 很明显这不是一个线性变换，这是一个线性变换$Av$ 然后sift了一下(linear-plus-shift)，我们管这个叫做放射变换（Affine）在图像处理里面经常用到。\n线到线，三角到三角(Lines to Lines,Triangles to Triangles) 尘归尘土归土，经过线性变换，直线依旧是直线，这就是这个section要说的，也就是一个例子，然后这个例子表明为啥叫做线性变换，直线经过变换依旧是直线。 如何证明直线经过线性变换还是直线呢，我们知道两个点可以确定一个直线，那么我们假设点$a(x_1,y_1)$ 在直线上， 点$b(x_2,y_2)$ 在直线上，那么直线上的中点，可以表示为$c(\\frac{1}{2}x_1+ \\frac{1}{2}x_2,\\frac{1}{2}y_1+ \\frac{1}{2}y_2)$ 也就是说对于两个点确定的直线上的中点经过线性变换，如果依然是这两个点的中点，那么直线经过线性变换还是直线： $$ c\u0026rsquo;=\\frac{1}{2}T(a)+\\frac{1}{2}T(b)=T(\\frac{1}{2}a+\\frac{1}{2}b)=T(c) $$\n这就证明了直线上两个给定点的中间点经过线性变换还是变换后的直线的中间点，也就是说$c$经过变换后还是$a,b$ 的中点，迭代下去可以证明$a,c$的中间点$c\u0026rsquo;$, $a,c\u0026rsquo;$ 的中间点$c\u0026rsquo;\u0026rsquo;$ 以此迭代下去最终得到极限情况下，直线经过线性变换后依旧是直线。\n整理下上面的凌乱的过程，我们为了证明直线经过线性变换后还是直线，那么就证明直线上两点之间的点，经过线性变换后还是这两个点（变换后的）的中点，那么递归的证明下去，整个直线依旧是直线，就这样了。 不太懂的看看图吧： 图上把三角形也画出来了，道理跟上面直线的一样，线性变换后点到对应的点上去，直线还是直线，所以三角形也能保持还是三角形。只是形状和方向发生了变化。\n线性变换的扩展，对于多个向量: $$ u=c_1v_1+c_2v_2+\\dots + c_nv_n\\ T(u)=c_1T(v_1)+c_2T(v_2)+ \\dots + c_nT(v_n) $$\n对于线性变换，也有一套独立的术语，比如对于一个线性变换 $T(v)=Av$ (A不是矩阵，不是矩阵不是矩阵)，那么$T(v)=0$ 成立的 $v$ 叫做\u0026quot;Kernel\u0026quot; ；所有的输出$T(v)$ 被称作\u0026quot;range\u0026quot; 。对应的就是nullspace和columnspace不过我们只叫他们kernel或者range，因为我们还没给出矩阵的定义，下篇才会给出。\n平面线性变换(Linear Transformation of the Plane) 这里还是一个大🌰 ；但是看起来比较可爱，证明过程上面证明直线的时候已经证明了，但是这里有个小问题，就是他又用矩阵了，我这篇中极力的避免矩阵，因为我们还没证明线性变换跟矩阵什么关系，但是我们不妨假设矩阵 $A$ 对向量空间 $V$ 的变换是线性的，所以我们的小房子就要变形了： Conclusion 总结就是今天要把线性变换和后面矩阵引出的部分写完，然后我就可以高高兴兴的搞概率论了，线性代数的基础部分也就算是毕业了，待续。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-7-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本篇介绍线性代数的另一个角度，就是线性变换思想\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Linear Transformation，Linear Combination，Kernel，Range\u003c/p\u003e","title":"【线性代数】7-1:线性变换思想(The Idea of a Linear Transformation)"},{"content":"Abstract: 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 Keywords: Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors\nSVD分解 今天的废话关于学习知识，最近看到一种说法，我觉的非常的形象，有个大神（是谁我忘了），他说已知的知识像一个圆圈，而自己能感受的未知就是紧邻圆圈，圆外部的区域，当你知道的知识越来越多，圆圈不断扩大，圆周也随之扩大，所以你会越来越发现自己无知，那么就会更努力的去学习，所以越有知识的人越谦逊，尤其是对待知识上，尊重知识，探索未知领域是人类文明存在的根本动力。 ## 奇异值分解 (Singular Value Decomposition) SVD，熟悉的名字，如果不学习线性代数，直接机器学习，可能最先接触的就是SVD，所以我记得在写上个系列的博客的时候（CSDN，图像处理算法）就说到过SVD，当时还调侃了下百度，每次搜SVD出来的都是一把枪（报告政府，这个枪是穿越火线里面的，没超过1.7J） 这张分解图是我无意中发现的，ak47的发明人说过，如果一把枪，零件已经精简到最少了，那么这个才是精品，类似的意思上篇博客也说过，矩阵变换到最简单的形式，能够体现出其最重要的性质。 SVD，奇异值分解，与QR，LU，\\(S\\Lambda S^{-1}\\) 等变换类似，其经过变换后会得到一个结构特异性质非凡的矩阵，SVD分解的结果和形式与对角化都非常相似，只是在形式和思路上更复杂，或者说如果说Jordan 是矩阵的对角化的扩展，因为有些矩阵特征向量不完全，那么SVD也是对角化的扩展，因为有些矩阵并不是方的。 所以SVD也是对角化，并且拥有比 \\(A=S\\Lambda S^{-1}\\) 更完美的性质，但却是也复杂了一些，\\(A=S\\Lambda S^{-1}\\) 有以下几个问题，需要完善： 1. S中特征向量一般不是正交的，除非A是对称矩阵 2. A并不是总有足够的特征值，这个是Jordan解决的问题，多个特征值相等，其对应于一个特征向量的时候，Jordan可以写成一块一块的对角矩阵 3. A必须是方的方的方的\nSingular Vectors作为eigenvectors 的替代品，可以完美解决上述问题，但是作为代价，我们的计算过程会变得复杂，并且Singular Vectors有两组，\\(u\\) 和 \\(v\\)\n\\(u\\) 对应的是\\(AA^T\\) 的特征向量，因为 \\(AA^T\\) 对称，所以 \\(u\\) 们可以选择相互正交的一组。 同理 \\(v\\) 对应 \\(A^TA\\) 的特征向量，因为\\(A^TA\\) 对称，所以 \\(v\\) 们也可以选择相互正交的一组。 这里注意是选择，因为你也可以选择不正交的，但是不正交的可能就会很麻烦了。\n铺垫的差不多 ，然后我们有下面的这条重要性质，为什么会成立后面有证明，现在就告诉你SVD究竟是个啥子鬼： \\[ Av_1=\\sigma_1u_1\\\\ Av_2=\\sigma_2u_2\\\\ \\vdots\\\\ Av_n=\\sigma_nu_n\\\\ \\]\n\\(v_1,\\dots,v_n\\) 是\\(A^TA\\) 的特征向量，所以 \\(v\\) 是矩阵A的Row Space \\(u_1,\\dots,u_n\\) 是\\(AA^T\\) 的特征向量，所以 \\(u\\) 是矩阵A的Column Space \\(\\sigma_1,\\dots,\\sigma_n\\) 全部为正数，称为矩阵A的奇异值。\n然后下面我们把 \\(u\\) 和 \\(v\\) 组合成矩阵 \\(U\\) 和 \\(V\\) ,那么根据对称矩阵的性质，\\(U^TU=I\\) 同理 \\(V^TV=I\\) 那么接下来我们来组合一下：\n\\[ AV=U\\Sigma \\\\ A \\begin{bmatrix} \u0026amp;\u0026amp;\\\\ v_1\u0026amp;\\dots\u0026amp;v_r\\\\ \u0026amp;\u0026amp; \\end{bmatrix}= \\begin{bmatrix} \u0026amp;\u0026amp;\\\\ u_1\u0026amp;\\dots\u0026amp;u_r\\\\ \u0026amp;\u0026amp; \\end{bmatrix} \\begin{bmatrix} \\sigma_1\u0026amp;\u0026amp;\\\\ \u0026amp;\\ddots\u0026amp;\\\\ \u0026amp;\u0026amp;\\sigma_r \\end{bmatrix} \\]\n矩阵形式就是这样喽，没什么解释的，就是上面计算的组合形式，但是注意这里有个很重要的参数，\\(r\\) 没错，就是矩阵的rank，这里rank表示了矩阵A的Singular Values的数量，所以上面计算从规模上是： \\[ (m\\times n)(n\\times r)=(m\\times r)(r\\times r)\\\\ m\\times r=m\\times r \\] 从矩阵相乘的规模上也能看出等式没有问题，但是这个r有的问题，可以肯定的是，有效的Singular vector有r组，但是这样与原始矩阵形状差的有点多，那么就补一补，虽然补的都是没用的，但是也算是整齐划一了，首先 \\(\\Sigma\\) 中缺少的只能补0 ，所以对应的V就只能补A的Nullspace了，因为这样 \\(AV\\) 的补充部分是0,同理，为了配合V，U添加的是left nullspace，并且这些添加的无用值也要选择orthonormal的，以保证\\(U^TU=I\\) 和\\(V^TV=I\\)。\n其实这里隐藏了一个重要的知识点，就是四个空间的那课，矩阵的rowspace和nullspace正交column space与left nullspace正交，而V本来是A的行空间正交基，那么添加的一定是Nullspace中的正交基，以保证矩阵正交，所以完美结合，（如果忘了四个空间点击查看）\n所以更一般化的表示： \\[ AV=U\\Sigma \\\\ A \\begin{bmatrix} \u0026amp;\u0026amp;\\\\ v_1\u0026amp;\\dots\u0026amp;v_n\\\\ \u0026amp;\u0026amp; \\end{bmatrix}= \\begin{bmatrix} \u0026amp;\u0026amp;\\\\ u_1\u0026amp;\\dots\u0026amp;u_m\\\\ \u0026amp;\u0026amp; \\end{bmatrix} \\begin{bmatrix} \\sigma_1\u0026amp;\u0026amp;\u0026amp;\\\\ \u0026amp;\\ddots\u0026amp;\u0026amp;\\\\ \u0026amp;\u0026amp;\\sigma_r\u0026amp;\\\\ \u0026amp;\u0026amp;\u0026amp; \\end{bmatrix} \\] 规模上是，注意 \\(\\Sigma\\) 不是方阵： \\[ (m\\times n)(n\\times n)=(m\\times m)(m\\times n)\\\\ m\\times n=m\\times n \\] \\(\\Sigma\\) 被填充成立 \\(m\\times n\\) 通过在矩阵中加入0来实现，新的矩阵U和V依旧满足 \\(V^TV=I\\)以及 \\(U^TU=I\\)\n那么我们的A就可以分解了 \\[ AV=U\\Sigma\\\\ for:\\;VV^T=I\\\\ so:\\\\ A=U\\Sigma V^T\\\\ SVD\\,\\,\\, is:\\\\ A=u_1\\sigma_1 v_1^T+\\dots+u_r\\sigma_r v_r^T \\] 其中\\(u\\) 是\\(m\\times 1\\) 的 \\(v^T\\) 是 \\(1\\times n\\) 的，所以A是 \\(m\\times n\\) 的没有问题，并且所有 \\(u_i\\sigma_r v_i^T\\) d的rank都是1，这就是Sigular Values Decomposition了，这里反复的验证规模的原因是因为A不是方阵，所以，在做乘法的时候要非常小心矩阵规模。那个小的只有r个有用值的SVD我们叫他reduced SVD（其实我觉得这个更有实际意义，毕竟这里面才有最重要的信息，新增的那些最后奇异值都是0了，也就没有啥作用了）可以表示为: \\[ A=U_r\\Sigma_r V_r^T \\] 写了这么多，我们到现在还不知道Singular是怎么计算出来的，那么我们先给出结论，后面继续证明： \\[ \\sigma_i^2=\\lambda_i \\] 其中\\(\\lambda_i\\) 是\\(A^TA\\) 和\\(AA^T\\) 的特征值。 那么要问\\(A^TA\\) 和\\(AA^T\\) 拥有相同的特征值，为什么？ 这个我真没想明白怎么证明，所以这个地方算个坑，会了再回来填\n然后我们得到Singular Values后，我们把他们按照从大到小的顺序排列，然后写成上面SVD的形式： \\[ \\sigma_1 \\geq \\sigma_2 \\geq \\sigma_3 \\dots \\geq \\sigma_n \\]\n下面举个小🌰 ： 什么时候SVD和对角化相等？ 当A是半正定或者正定矩阵的时候，\\(S=U\\) 并且 \\(S^T=V^T\\) 此时 \\(\\Lambda=\\Sigma\\) ,因为正定矩阵特征值为正，而且是对称的，所以 \\(U=V=Q\\)\n下面介绍一个应用，大应用，为什么我会把这个应用写出来呢？因为他和图像有关，所以我们可以简单实践一下，还是从感性上认识一下SVD，然后再理论上完整的证明一下。\n图像压缩 (Image Compression) 在介绍SVD之前，我们先来分析一个问题：图像的存储空间，一个图像假如是512x512的大小，灰度图像，每个像素占一个字节的话，这张图片那么会占据硬盘262144个字节，也就260多k个字节，如果按照23帧每秒，十秒钟大概要60兆，一分钟大概3.6G，在想想这个尺寸这么小，我们看的一般都是720P的，这样算的话硬盘根本不够用，一个东京热以后就再也不能一本道了，所以必须要压缩一下子，怎么压缩呢，这里介绍下JPEG的一个大致思路，就是矩阵分解： \\[ A=u_1\\sigma_1 v_1^T+\\dots+u_r\\sigma_r v_r^T\\\\ A=\\sigma_1 u_1 v_1^T+\\dots+\\sigma_r u_r v_r^T\\\\ A=\\sigma_1 S_1+\\dots+\\sigma_r S_r\\\\ where:\\,S_i=u_i v_i^T \\] 这样就是按照singular的大小，给所有singular vector排序，奇异值越大证明这个奇异值向量对原始数据影响越大，所以这两奇异向量组成的这个基就越重要，当然要提前加进去，所以我们如果选择一部分奇异值和奇异向量，比如\\(N=100\\) 也就是200个奇异向量和100个奇异值，那么一共是\\(200\\times 512+100=102500\\) 比原来的260k减少了一半，如果用的更少那么减少的更多，当然图像质量也就有所损失了，写了个python程序，可以观察一下： 使用多组S来还原原始数据，使用S越多还原度越高，但需要的存储空间就越大，S就是上面\\(u_iv_i\\)的结果，可以看做图像的一个切片，视频中第一幅为原图，第二幅为若干张S相加得到的结果，最后一张是他们之间的差，我们可以暂时理解为压缩误差。\n视频演示地址http://player.youku.com/embed/XMzE5NTIyNjQ4MA==\n代码\nimport cv2 import numpy as np from numpy import linalg import copy N=500 image=cv2.imread(\u0026#39;lena.jpg\u0026#39;,0) cv2.imshow(\u0026#39;src\u0026#39;,image) U,Sigma,V=linalg.svd(image) sigma_size=len(Sigma) image_s=[] waittime=0 for i in range(N):   if i\u0026gt;sigma_size:  break  image_s.append(U[:,i].reshape(len(U[:,i]),1)*V[i]*Sigma[i])  if i\u0026gt;0:  image_s[i]+=image_s[i-1]  print \u0026#39;total: \u0026#39;,i,\u0026#39; Singular Value\u0026#39;  show_image=copy.deepcopy(np.uint8(image_s[i]))  font = cv2.FONT_HERSHEY_TRIPLEX  cv2.putText(show_image, `i`, (10, 500), font, 4, (255, 255, 0), 1, False)  cv2.imshow(\u0026#39;Compression\u0026#39;,show_image)  cv2.imshow(\u0026#39;Different\u0026#39;,np.uint8(image-image_s[i]))  cv2.waitKey(waittime)  waittime=100 上面公式中每个S都是一个rank=1的矩阵，如果两个这样的矩阵相加，rank就变成了2，n个相加就是rank=n，所以SVD分解后再组合有点像切片，每一片都是有规律的，同样的道理，SVD换成小波，那就有了JPEG2000，小波的基矩阵也是满足一些特殊性质的，后面可能会将，但是不确定，所以数据压缩可以理解为最关键的一步就是两个向量相乘，能够得到一个矩阵，这个矩阵组成了基础的片，然后多个片加权求和就得到了还原数据。 SVD的应用应该非常多这里就写了一个，比较重要直观的，下面还是要继续研究原理。\n基和SVD(The Bases and the SVD) 书中并没有给出严格的证明和推到过程，老爷子还是走的启发式套路，我们来从基开始看，假设一个矩阵A是个2x2的矩阵，这样比较好计算，并且A是非奇异矩阵，也就是A可逆，rank=2，span成整个 \\(\\Re^2\\) 空间，那么我们应该可以找到两个向量正交的单位向量 \\(v_1,v_2\\) 满足 \\(u_1=\\frac{Av_1}{|Av_1|},u_2=\\frac{Av_2}{|Av_2|}\\) 使得\\(u_2\\)和\\(u_1\\) 正交，并且是单位向量，那么就有： \\[ A \\begin{bmatrix}v_1\u0026amp;v_2\\end{bmatrix}= \\begin{bmatrix}Av_1\u0026amp;Av_2\\end{bmatrix}= \\begin{bmatrix}|Av_1|\\cdot u_1\u0026amp;|Av_2|\\cdot u_2\\end{bmatrix}= \\begin{bmatrix}u_1\u0026amp;u_2\\end{bmatrix} \\begin{bmatrix}|Av_1|\u0026amp;\\\\\u0026amp;|Av_2|\\end{bmatrix}\\\\ AV=U\\Sigma \\] 总结下，这个构造基的过程是瞄准了目标去的，也就是说目标就是类似于构造 \\(Ax=\\alpha y\\) 形状的一种形式，但是x和y要满足不同的关系，如果相等那么就是特征值，如果不相等就可以构造出多组相互正交形成奇异值，并且通过上面的构造过程，我们可以得知奇异值等于缩放u到单位向量的缩放比例 \\(|Av|\\) 其实可以用一种更直观的方法解答SVD的存在： 假设对任意矩阵A存在分解 \\(A=U\\Sigma V^T\\) 并且其中\\(U^TU=I\\) \\(V^TV=I\\) 那么我们要证明U和V的存在即可： \\[ A^TA=(U\\Sigma V^T)^T(U\\Sigma V^T)\\\\ A^TA=V\\Sigma (U^TU) \\Sigma V^T\\\\ A^TA=V\\Sigma^2 V^T \\] 虽然A不是对称的，但因为\\(A^TA\\) 是对称矩阵，存在正交矩阵Q使得 \\(A^TA=Q\\Lambda Q^T\\)，那么这时候的V就是 \\(A^TA\\) 的Q这个是没问题的，至于 \\(\\lambda_i=\\sigma_i^2 \\geq 0\\) 成立的原因是 \\(A^TA\\) 是个正定矩阵（A中各列线性无关），或者半正定矩阵（A中各列线性相关），所以其特征值 \\(\\lambda\\) 必然非负数，所以根号后能得到奇异值，根据\\(Av_i=\\sigma u_i\\) 可以求出剩下的 \\(u_i\\) ,当然这是理论上的方法，实际上的数值计算过程中可以避免 \\(A^TA\\) 这种大规模矩阵乘法。 回忆一下正定矩阵关于椭圆的那个例子 一个2x2正定矩阵对应二维空间一个椭圆（或者圆），其正交特征矩阵Q矩阵是对椭圆轴的旋转，特征值矩阵 \\(\\Lambda\\) 是对轴的拉伸，那么我们的SVD有同样的功效，而且有过之无不及，思考： 作为\\(A^TA\\) 的正交特征矩阵\\(V\\)也是一个旋转矩阵，旋转的是圆的轴，\\(V^T\\) 当然就是反方向旋转，\\(\\Sigma\\) 是对图形的拉伸，圆的拉成长的，接着 \\(AA^T\\) 的正交特征矩阵也是旋转，整个过程如下图：\n所以一个2x2的可逆矩阵，对一个圆的操作就是先拉伸，然后旋转。图中来自Cliff Long and Tom Herm\n上面讲解了如何求V，同样的道理也可以求U， \\[ AA^T=(U\\Sigma V^T)(U\\Sigma V^T)^T\\\\ AA^T=(U\\Sigma V^T)(V\\Sigma U)\\\\ AA^T=U\\Sigma^2 U^T \\] U是\\(AA^T\\) 的正交特征矩阵，也就是说同一个 \\(\\Sigma^2\\) 既是 \\(AA^T\\) 又是 \\(A^TA\\) 的特征值，所以上面那个疑问也得到了证明。 我们重新梳理一下这个证明过程，我们首先假设结论成立，来找到使结论成立的条件，也就是V和U矩阵，结果很理想的，我们找到了，所以原来结论成立，成立的条件就是我们刚找到的这两个矩阵（如果算上奇异值，可以说是三个矩阵），思考过程可以通过2x2构造那里来推出简单情况下，来验证我们的结论，然后再推广到任意矩阵。 至此SVD的基本来路我们已经算是摸着门了，所以可以深入开发下V和U矩阵，这两个矩阵是方阵，那么组成这些矩阵的向量门也是很有来历的，总结个表格，前面在第一部分有说过，就是reduced SVD那部分，这里在啰嗦一边：\n  Number in which Matrix(column) in which Subspace    r V rowspcae of A  n-r V nullspace of A  r U column space of A  m-r U nullspace of \\(A^T\\)    前r列对应的是 \\(A^TA\\) 和 \\(AA^T\\) 的特征向量，因为其间的正交关系，所以SVD是没有什么问题的。 严格的证明： \\[ A^TAv_i=\\sigma_i^2v_i \\] 这个是可以作为条件使用的，其成立的必然原因是 \\(A^TA\\) 是正定或半正定矩阵，所以必然存在n个大于等于0的实数特征值，这样可以得到\\(v_i,\\sigma_i\\) ，在这个条件下我们目标是证明：存在\\(u_i\\) 使得\\(Av_i=\\sigma_i u_i\\) 成立\n对条件两边同时乘上 \\(v_i^T\\) 后： \\[ v_i^TA^TAv_i=\\sigma_i^2v_i^Tv_i\\\\ ||Av_i||^2=\\sigma_i^2 so:\\\\ ||Av_i||=\\sigma_i\\\\ \\] 对条件两边同时乘A得到： \\[ AA^TAv_i=\\sigma_i^2Av_i\\\\ set\\,unit\\,vector:\\;u_i=\\frac{Av_i}{\\sigma}\\\\ AA^Tu_i=\\sigma_i^2 u_i\\\\ \\] 上面两个过程及其精妙，尤其是下面这个，用\\(u_i=\\frac{Av_i}{\\sigma}\\) 进行置换后，得到\\(u_i\\) 是\\(AA^T\\) 的特征向量，然后得出结论，存在 \\(u_i=\\frac{Av_i}{\\sigma}\\) 使得命题成立，而且u就是\\(AA^T\\)的特征向量，并且相互正交： 下面证明u相互正交: \\[ u_i^Tu_j=(Av_i)^T(Av_j)=v_i^T(A^TAv_j)=v_i^T(\\sigma^2v_j)=0 \\] QED\n然后老爷子在书上发话了，这是这本书最高潮的部分了，也是基础理论的最后一步了，因为他用到了所有的前面的理论： 1. 四个子空间的维度（我们上面那个表） 2. 正交 3. 正交基来对角化矩阵A 4. 最后得到SVD \\(A=U\\Sigma V^T\\)\n于是我写了一天这篇博客，比之前看书收获了更多，也可能有写纰漏，难免的因为水平有限，而且这个是比较精髓的部分，自然难度也比较大\n搜索网络(Searching the Web) 这里讲了个应用，但是我实在不想写了，已经精疲力尽了，所以我打算后面选几个应用写，这个作为候选，这里略 ## Conclusion 线性代数的高潮算是来了，但是后面还有一个比较有意思的主题，也是很常用的，叫做线性变换，也可以作为切入线性代数的一个切入点，我们这个系列的博客是从线性方程组开始的，当然也可以通过线性变换引出矩阵，我们下一篇来讲解这个，待续。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-7.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍SVD，奇异值分解，应该可以算是本章最后的高潮部分了，也是在机器学习中我们最常用的一种变换，我们经常需要求矩阵的特征值特征向量，比如联合贝叶斯，PCA等常规操作，本文还有两个线性代数的应用，在图像压缩上，以及互联网搜索上。 \u003cstrong\u003eKeywords:\u003c/strong\u003e Singular Value Decomposition,JPEG2000,Eigenvalues,Eigenvectors\u003c/p\u003e","title":"【线性代数】6-7:SVD分解(Singular Value Decomposition-SVD)"},{"content":"Abstract: 本文主要介绍根据矩阵对角化以及特征值引出的相似矩阵的性质和特点 Keywords: Similar Matrices,Jordan Form,Eigenvalues,Eigenvectors\n相似矩阵 特征值特征向量这一章是线性代数的高潮部分，可以说是高潮迭起，这部分相比四个子空间部分可能逻辑性更强一点，需要前后联通，只看一部分肯定要掉坑，所以这几篇写的都非常多，今天的相似矩阵是对角化引出的另一个重要分支，但是篇幅不大。 在研究这一章的时候总感觉Prof. Strang写的很细致，可以很容易的帮你知道什么是什么但是如果想了解点深入的背后的东西，由于篇幅限制（可以看出老先生有意的控制篇幅，并没有长的长的短的短，所有章节长度基本相同）没有深入讨论，也可能线性代数的introduction仅限于这些，更深入的话可能就是另一门课了，所以后续可能出个矩阵论或者矩阵分析类的系列博客。\n相似矩阵(Similar Matrices) Similar相似，但又不同，如果说某两件事物相似，那么必然有相似点，也就是这两件事物的某一属性，或者某几个属性一致，那么如果说两个矩阵相似，有可能是形状，比如上三角矩阵，对角矩阵，这些矩阵都有相同的属性，我们这里定义矩阵相似\u0026ndash;拥有相同的特征值。\n本章我们研究的主要内容是矩阵的对角化，对角化的前提是有足够的特征向量，也就是说如果某个矩阵特征向量不足，那么就没办法产生特征向量矩阵$S$ 那么我们就不研究他们了，😁我们这里不研究特征向量不够的情况，同样，本文中我们也是研究有足够特征向量的矩阵，如果矩阵$A$ invertible,那么可逆矩阵$M$ 构成的$MAM^{-1}$ 的新矩阵是 $A$ 的相似矩阵，因为$M$ 的多样性，所以相似矩阵不是唯一的相反的，应该是一群（family）。\n Definition: Let $M$ be any invertible matrix.Then $B=M^{-1}AM$ is similar to $A$\n 因为我上面剧透了，相似矩阵是有相同特征值的矩阵，但是如果我们不知道这个性质，我们来观察下面这个过程,假设矩阵 $M$ 可逆： $$ B=M^{-1}AM\\ A=MBM^{-1}\\ set:;N=M^{-1}\\ then:;N^{-1}=M\\ A=N^{-1}BN $$\n要说的是 $B=M^{-1}AM$ 和 $A=N^{-1}BN$ 无论是长相和性质都极为相似，那么根据定义A相似与B，B也相似与A，所以相似是个可逆的，即可以反过来的。 对角化后的 $\\Lambda$ h和原始矩阵A也是相似的，当$M=S$ 的时候，使得相似矩阵B变成了 $\\Lambda$\n线性代数和微分方程关系紧密（线性代数是基础学科，所以跟基本谁都有关系），比如对于微分方程 $\\frac{du}{dt}=Au$ ,我们对变量 $u$ 进行代换 $u=Mv$ 其中M是可逆的常数矩阵： $$ \\frac{du}{dt}=Au\\ \\frac{dMv}{dt}=AMv\\ M\\frac{dv}{dt}=AMv\\ \\frac{dv}{dt}=M^{-1}AMv $$\n通过代换u和v，我们得到了A的一个相似矩阵，也可以通过将M变成S，这样得到的系数矩阵就是对角矩阵，其实前面有一篇专门讲微分方程的课我们略过了，实际上特征值是可以反应出系统是逐渐偏向增加还是减少的（这个地方听不懂没关系，因为出来的太突然了，没有一点点防备，你就这样出现。。）所以相似矩阵必然有一样的特征值，这样才能进行代换后，不影响系统原始的变化方式，那么，又是那句话，相似矩阵家族的特征值是相同的不然不相似。\n下面的关键就是证明特征值没变,假设矩阵A是可对角化的矩阵。\n$$ A=S_A\\Lambda S_A^{-1}\\ B=M^{-1}AM\\ B=M^{-1}S_A\\Lambda S_A^{-1}M\\ B=(M^{-1}S_A)\\Lambda (M^{-1}S_A)^{-1}\\ S_B=M^{-1}S_A\\ B=S_B\\Lambda S_B^{-1} $$\n可见B和A的特征值是不变的，然后特征向量矩阵要乘以系数矩阵 $M^{-1}$ ，所以下面这个过程是等价的： $$ B=M^{-1}AM\\ Bx=\\lambda x\\ M^{-1}AMx=\\lambda x\\ AMx=M\\lambda x\\ AMx=\\lambda (Mx)\\ set:; y=Mx\\ Ay=\\lambda y $$\n又一次证明了相似矩阵的特征值不变，同时特征向量变化等于 $Mx$ 就是原始特征值乘以了系数矩阵。这里面还有一个隐藏的坑，就是特征值相等的时候，如果矩阵A有n个相等的特征值 $\\lambda$，那么B重也有n个相等的特征值 $\\lambda$ 。这种情况有些困难，下面开始举🌰 ： $$ A=\\begin{bmatrix}0.5\u0026amp;0.5\\0.5\u0026amp;0.5\\end{bmatrix}\\ S_A=\\begin{bmatrix}1\u0026amp;1\\newline -1\u0026amp;1\\end{bmatrix}\\ S_AAS_A^{-1}=\\begin{bmatrix}1\u0026amp;0\\0\u0026amp;0\\end{bmatrix}\\ Chose:\\ M=\\begin{bmatrix}1\u0026amp;0\\1\u0026amp;-2\\end{bmatrix}\\ then:\\ B=M^{-1}AM=\\begin{bmatrix}1\u0026amp;-1\\0\u0026amp;0\\end{bmatrix}\\ S_B=\\begin{bmatrix}1\u0026amp;1\\0\u0026amp;1\\end{bmatrix}\\ S_B^{-1}=\\begin{bmatrix}1\u0026amp;-1\\0\u0026amp;1\\end{bmatrix}\\ S_BBS_B^{-1}=\\begin{bmatrix}1\u0026amp;0\\0\u0026amp;0\\end{bmatrix} $$\n可以通过简单的计算得到与证明中一致的结论，本例子和书中有写区别，用octave进行了验证，可以放心使用\n下面再举一个比较困难的🌰就是两个特征值相等的情况。 $$ A=\\begin{bmatrix}0\u0026amp;1\\0\u0026amp;0\\end{bmatrix}\\ \\lambda_1=0\\ \\lambda_2=0\\ $$ 这样的话两个特征值都为0，那么特征向量就是A的0空间，又因为矩阵的rank是1，也就是说nullspace的dimension是1，所以两个特征值对应一个个方向上的特征向量,$S=\\begin{bmatrix}1\u0026amp;1\\0\u0026amp;0\\end{bmatrix}$ 不可逆， A不能对角化，但是我们依然可以找到可逆矩阵$M=\\begin{bmatrix}a\u0026amp;b\\c\u0026amp;d\\end{bmatrix}$ 他的逆是$M^{-1}=\\frac{1}{ad-bc}\\begin{bmatrix}d\u0026amp;-b\\newline -c\u0026amp;a\\end{bmatrix}$ ，那么我们就能得出一个通用的框架，满足$B=M^{-1}AM$ 的所有矩阵与A相似，虽然A没有足够的特征向量（文章开始说不研究的，但是还是被带进坑了）\n$$ B=\\frac{1}{ad-bc}\\begin{bmatrix}d\u0026amp;-b\\newline -c\u0026amp;a\\end{bmatrix}\\begin{bmatrix}0\u0026amp;1\\0\u0026amp;0\\end{bmatrix}\\begin{bmatrix}a\u0026amp;b\\c\u0026amp;d\\end{bmatrix}=\\begin{bmatrix}cd\u0026amp;d^2\\newline -c^2\u0026amp;-cd\\end{bmatrix} $$ 满足上述公式的所有B（必须可逆，也就是行列式要不等于0）都是A的相似矩阵，当然这个特殊的例子也要强调一下：除了 $\\begin{bmatrix}0\u0026amp;0\\0\u0026amp;0\\end{bmatrix}$ ，这个家伙比较讨厌也不可逆，所以我们把它排除了。观察上面B，发现trace是0，行列式也是0，满足检验。 B是A的一个通用版本，但是可以看出没办搞成对角矩阵，A已经是最接近对角矩阵的了，我们把A称作是这个family的Godfather，或者学术一点叫做 Jordan Form，后面我们会大概的介绍Jordan form更多内容要看进阶篇了。\n这是一个比较特殊的例子，特征向量不够的情况，但是不论够不够，相似矩阵总有下面这些性质\n   Not changed by M Changed by M     Eigenvalues Eigenvectors   Trace and determinant Nullspace   Rank Column space   Number of independent eigenvectors Row space   Jordan form Left nullspace    Sigular values    一条一条分析，首先特征值不变，我们已经论证过了，同时也知道特征向量发生了变化，特征值不变以为着Trace和determinant的不变（因为他们和特征值之间有数值关系，所以特征值不变这两个可能不变）；Nullspace发生了变化，因为Nullspace是特征值为0的时候的特征向量span的space，如果特征向量发生变化，那么Nullspace肯定也发生了变化，这里有点疑问，如果M是identity呢？ ；Rank是不变的，因为如果特征值不变，0特征值的个数就不变，这样nullspace的维度就不变，那么rank不变；与nullspace同理，非零特征值对应的特征向量发生改变，其span的column space就会发生改变；independent的特征向量的数量不会发生变化，因为对应的子空间的维度不变，所以线性独立的特征向量（也就是子空间的一组基的数量）不会发生变化；row space发生变化，因为行空间的基都乘以了M；Jordan不变，因为不改变相似性，在整个家族中Jordan是最接近对角矩阵的，所以不会发生改变；left nullspace随着行空间改变而被改变；Singular values会改变，这个下一篇会讲到，这里大家还不太明确奇异值是什么。\n乔丹形式的例子(Examples of the Jordan form) Jordan Form上文说是最接近对角矩阵的矩阵，这句话有个问题，就是如何评价什么“接近”，也就是没有一个统一的标准，来计算一个矩阵和对角矩阵的差别有多大。所以我们在这段和下一段找找根据，这段主要是🌰 ： Jordan Matrix J: $$ J=\\begin{bmatrix}5\u0026amp;1\u0026amp;0\\0\u0026amp;5\u0026amp;1\\0\u0026amp;0\u0026amp;5\\end{bmatrix}\\ then:\\ J-5I=\\begin{bmatrix}0\u0026amp;1\u0026amp;0\\0\u0026amp;0\u0026amp;1\\0\u0026amp;0\u0026amp;0\\end{bmatrix} $$ 所有与J相似的矩阵B都有三个一样的特征值5，并且 $B-5I$ 的rank不变（是2），也就是nullspace是1，这样的矩阵B与J相似（这里有个问题，就是在确定特征值相等后，还有反复确认rank和nullspace，这个也是这里第一次见，后面可能有更具体的解释）\n Jordan Theorem :$J^T$ 和 $J$ 相似\n $$ J^T=M^{-1}JM= \\begin{bmatrix}\u0026amp;\u0026amp;1\\\u0026amp;1\u0026amp;\\1\u0026amp;\u0026amp;\\end{bmatrix} \\begin{bmatrix}5\u0026amp;0\u0026amp;0\\1\u0026amp;5\u0026amp;0\\0\u0026amp;1\u0026amp;5\\end{bmatrix} \\begin{bmatrix}\u0026amp;\u0026amp;1\\\u0026amp;1\u0026amp;\\1\u0026amp;\u0026amp;\\end{bmatrix} $$ 数值上支持了理论，但是Jordan Theorem具体内容还应该具体的证明，例子只是从表面上让大家知道说的是什么，但本身没有证明真伪的意义。\n另一个例子，这个例子是微分方程，与上面提到的一样，这里不再具体描述了，但是要记住： 对变量 $u$ 进行代换 $u=Mv$ 其中M是可逆的常数矩阵： $$ \\frac{du}{dt}=Au\\ \\frac{dMv}{dt}=AMv\\ M\\frac{dv}{dt}=AMv\\ \\frac{dv}{dt}=M^{-1}AMv $$\n这里是对输入，或者是初始状态进行了变换，下一章将对基进行变化（也就是一个动坐标值，一个动坐标轴）。\n乔丹形式(The Jordan Form) 接下来详细的介绍下Jorda Form也就是Jordan形式，没有详细的推到过程，因为对Jordan形式的严格证明书中就没有，也不是线性代数初级部分的内容，我们要知道的就是Jordan的具体形式，以及部分用途，Jordan存在的最大意义就是解决不能对角化的矩阵在需要对角化的时候的引发的问题，也就是Jordan Form 是更广义的对角化，而之前我们讲的拥有足够特征向量的方阵的对角化只是Jordan Form的一种特例而已，Jordan是更广泛的方式。\n【宏观】Jordan Form： $$ M^{-1}AM=\\begin{bmatrix} J_1\u0026amp;\u0026amp;\\ \u0026amp;\\ddots\u0026amp;\\ \u0026amp;\u0026amp;J_s \\end{bmatrix}=J $$ 【微观】其中$J_i$ 是一个块： $$ J_i=\\begin{bmatrix} \\lambda_i\u0026amp;1\u0026amp;\u0026amp;\u0026amp;\\ \u0026amp;\\ddots \u0026amp; \\ddots\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp; \\ddots\u0026amp;1\\ \u0026amp;\u0026amp; \u0026amp;\\lambda_i \\end{bmatrix} $$ 中文描述，对于整体，就是一些Jordan块在矩阵的对角线上（还记得矩阵分块不？就是这个矩阵块），但是每个块里也有特殊的规定，就是对角线上是一个特征值，注意是一个，然后剩下的，每个特征值头顶上的元素是1，就这样。 每个Jordan块上有一个特征值，对应一个特征向量。\n在判断矩阵相似上我们也有了另一种描述矩阵similar中如果A和B相似，他们共享（share）一个Jordan形式，或者说这个定义才是最优版本。 Jordan形式在微分方程，求矩阵高次幂的时候都非常有用，但在实际计算中Jordan Form一般不被使用，因为计算量大，我们希望使用计算量最小的求解方法，A slight change对A就可以把A对角化。\n经典名言：\n Proved or not,you have caught the central idea of similarity-to make A as simple as possible while preserving its esstntial properties\n 上面这个可以当做线性代数的至理名言： 当一个矩阵被搞得非常非常简单的时候，这个简单的矩阵将保持着最基础的性质\nConclusion 本文主要介绍了Similar矩阵，距离最后的高潮只差一下了，大家努力吧，明天SVD\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-6.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍根据矩阵对角化以及特征值引出的相似矩阵的性质和特点\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Similar Matrices,Jordan Form,Eigenvalues,Eigenvectors\u003c/p\u003e","title":"【线性代数】6-6:相似矩阵(Similar Matrices)"},{"content":"Abstract: 关于正定矩阵的相关知识总结，正定矩阵在数学中的一个应用 Keywords: Positive Definite Matrices,Symmetric Matrices,Eigenvalues,Eigenvectors\n正定矩阵 正定矩阵(Positive Definite Matrices) 正定矩阵，对这个矩阵印象深刻，知道学了这节以后，才知道，正定矩阵就是\u0026quot;Positive Definite Matrices-正的确定矩阵\u0026quot;，这个翻译也是耿直，\n Positive Definite Matrices 定义为，对称矩阵，并且所有特征值全部大于0\n 那么我们第一个大问题就是如何确定一个矩阵是不是正定矩阵呢，求特征值肯定是根本方法，定义都说了，对称矩阵，特征值大于0，求出所有特征值，那么自然明朗了，但是有时候我们只需要知道是不是正定矩阵，而不需要知道特征值，这样的话计算代价有点大，我们需要找点别的招数，来避免求特征值。 接下来我们的目标是：\n 找到能快速判断对称矩阵的特征值都是正数 正定矩阵的重要应用  $2 \\times 2$ 矩阵 对于 $A=\\begin{bmatrix}a\u0026amp;b\\b\u0026amp;c\\end{bmatrix}$ 什么情况下特征值是正的呢？因为只有两个特征值，那么我们考虑第一个条件：\n 两个正数相乘，结果是正数  但是如果两个复数相乘也是复数，所以引入第二个条件： 2. 两个正数相加，结果是正数\n那么如果满足，两个特征值相加是正数，相乘也是正数，那么肯定特征值全部都是正数。 又因为我们已知特征值相乘等于行列式，相加等于trace，那么： $$ a+c\u0026gt;0\\ and:\\ ac-b^2\u0026gt;0 $$ 就是$2 \\times 2$ symmetric Matrices 正定的充分必要条件啦，当然还可以做个简化$ac-b^2\u0026gt;0$ 中如果 $a\u0026gt;0$ 和上面的式子等效： $$ a\u0026gt;0\\ and:\\ ac-b^2\u0026gt;0 $$ 注意关联词哦是and两个必须同时发作，那么这个条件才算是成功了.举两个计算的🌰。 $$ A_1=\\begin{bmatrix}1\u0026amp;2\\2\u0026amp;1\\end{bmatrix};;ac-b^2=1-4\u0026lt;0\\ A_2\\begin{bmatrix}1\u0026amp;-2\\newline -2\u0026amp;6\\end{bmatrix};;ac-b^2=6-4\u0026gt;0;;a=1\u0026gt;0 $$ 结论 $A_1$ 不是正定的 $A_2$ 是正定的\n不定矩阵，正定矩阵，负定矩阵(Indefinite,Positive Definite and Negative Definite) 有正定就应该有负定，零正定（这个没有，0被分配到大于等于0的行列叫做半正定）或者不定矩阵。\n Positive Definite：对称矩阵特征值全部为正数 Negative Definite：对称矩阵特征值全部为负数 Indefinite： 特征值有正有负，不定矩阵 Positive Semidefinite：对称矩阵，特征值大于等于0  正定也好不正定也好，我们可以现在想一想在空间上的操作到底是个什么鬼： 学习对称矩阵，我们一直在研究的是对角化，对角化作为一种分解方式和LU，QR等有着相同的身份，但是从书籍上的讲解篇幅和教授描述，都要比LU，QR更具体更深入，所以，对角化应该更有市场，我们接下来就从对角化来说说正定矩阵A: $$ A=S\\Lambda S^{-1}\\ Q=S\\ A=Q\\Lambda Q^{-1} $$ 这个是上上上一篇讲述的，但是上一篇有一个惊天的秘密可以足够清晰的帮我们研究正定矩阵，也就是当对称矩阵A和一个向量相乘的时候，从空间的角度来讲，这个向量被投影到了A的列空间，但A的列空间和特征向量的空间不一致（特征向量可能比A的列向量维度大，因为当A存在0特征值的时候，相当于给自己降维了，也就是A是全体特征向量的子空间），那么现在: $$ u=c_1q_1+c_2q_2+\\dots +c_nq_n=Qc\\ A=Q\\Lambda Q^{-1}\\ Au=Q\\Lambda Q^{-1}Qc=Q\\Lambda (Q^{-1}Q)c=Q\\Lambda (Q^{-1}Q)c=Q\\Lambda c\\ Q\\Lambda c= \\begin{bmatrix}\u0026amp;\u0026amp;\\q_1\u0026amp;\\dots \u0026amp;q_n\\\u0026amp;\u0026amp;\\end{bmatrix} \\begin{bmatrix}\\lambda_1\u0026amp;\u0026amp;\\\u0026amp;\\ddots \u0026amp;\\\u0026amp;\u0026amp;\\lambda_n\\end{bmatrix} \\begin{bmatrix}c_1\\\\vdots \\c_n\\end{bmatrix}= c_1\\lambda_1 q_1+\\dots +c_n\\lambda_n q_n $$ 看到什么了？老铁们，一个向量乘以一个对称矩阵，相当于把这个向量以特征向量们为基，然后长度伸缩特征值倍，不改变方向且没有0的是正定矩阵，有0的是半正定，这时候有些方向上的长度被消灭了，也就是说向量分解到特征向量基空间少一维，也就是A并不是满空间的，也就是singular矩阵，和前面的0正好对应，如果有正有负，那就是瞎搞了，哈哈哈哈。 Negative Definite， Indefinite的🌰就不说了，后面会有一个section专门讲Positive Semidefinite。\n能量基的定义 (Energy-based Definition) 能量based的定义，能量一般都是正的，所以这个和正定矩阵有关系也不那么令人惊讶，从源头看$Ax=\\lambda x$ 是一个矩阵的特征值和特征向量，如果我们在这两边同时乘上点什么： $$ x^TAx=x^T\\lambda x=\\lambda x^Tx\\ x^Tx=|x|^2 \\geq 0 $$ 特征向量都是非零向量所以上面等号在特征向量前提下永不成立，那么也就是只要保证 $x^TAx\u0026gt;0$ 那么就有 $\\lambda \u0026gt;0$ 得出矩阵可能是正定的，然后检验所有特征向量，这个工作量也有点大，根据其他一些应用，这里提出一个新的定义，关于能量的$x^TAx$ 表示一个系统的能量，其必须大于0.也就是说对于一个矩阵，其能量为正，这个矩阵定义为正定矩阵。 上面这个是定义正定矩阵的一个方法，也就是说了一个有点实际意义的定义，而不是上来拿枪指着你，说，“小子，特征值大于0的对称矩阵就是正定矩阵，说不行打死你”，所以这种定义在应用中出现的时候，要想到正定矩阵就好。 上一篇有一块介绍pivot和Eigenvalue之间的关系，他们的符号是相同的，也可以测试$x^TAx$是正是负。\n Definition: A is positive definite if $x^TAx\u0026gt;0$ for every nonzero vector x:\n $$ x^TAx= \\begin{bmatrix}x\u0026amp;y\\end{bmatrix} \\begin{bmatrix}a\u0026amp;b\\b\u0026amp;c\\end{bmatrix} \\begin{bmatrix}x\\y\\end{bmatrix}=ax^2+2bxy+cy^2\u0026gt;0 $$\n这个就是能量观点下定义的完整写法，如果理解了这个过程也就对这个形式没什么困惑了，我记得我之前是老师说，来，把这个公式记住。。。 $ax^2+2bxy+cy^2$ 是一个碗的形状，当然xy都不是零的时候。\n根据能量观点可以得到正定矩阵的可加性，也就是规模相同的两个正定矩阵，相加也是正定的，证明过程如下，假设其中A，B规模相同，切都是正定的： $$ C=A+B\\ x^TAx\u0026gt;0\\ x^TBx\u0026gt;0\\ x^T(Ax+Bx)=x^T(A+B)x=x^TCx\u0026gt;0 $$ QED\n$R^TR$ 这是一种新的判断正定的方法，通过上面能量的观点$x^TAx$ 延伸，出如果能把A分解成一个矩阵和矩阵的转置相乘的形式，那么就能得到 $(Rx)^T(Rx)$ 的形式 那么这个形式下，如果$Rx\\neq 0$ 必然其结果大于0（$Rx$ 是一个向量，所以 $(Rx)^T(Rx)$ 就变成了 $|Rx|$ 是一个长度）那么如果我们假设必然存在R，那么R要满足什么条件呢？ $Rx\\neq 0$ 就是条件，对于所有非0向量，也就是R的Nullspace只有0向量，也就是R的列必须相互独立,这也是唯一的条件，如果矩阵能分解成$A=R^TR$ 的形式，R的列线性独立，那么A正定。 反过来也是正确的，如果矩阵R各列线性独立，那么 $A=R^TR$ A是正定矩阵。\n正定矩阵\u0026quot;五项原则\u0026quot; 总结下我们判断正定的几种方法，他们互相等效，一个成立便可以推出其他所有：\n All n pivots are Positive All n upper left determinants are positive All n Eigenvalues are Positive $x^TAx$ is positive except at $x=0$ This is the energy-based Definition A equal $R^TR$ for a Matrix R with independent columns  上面的2我们好像没有证明，左上角的行列式的值总和左上角矩阵的pivot有关系，如果第一个行列式为正(也就是1x1的矩阵，即第一个pivot为正)那么第二个矩阵（2x2）的行列式也为正的话，可以得出第二个pivot也是正的，以此类推，就能得到第一条，所有pivot都是正数，这个可以参考行列式文章Permutation 可以有些启发。\n上面的五条基本上可以把线性代数大部分东西全包括进去了，消元，特征值，行列式，子空间，这些都要知道后，才能对上面的五条不存在疑惑，所以直接看本文的同学，不懂往回看。\n从正定返回$R^TR$ 我们回忆一下最初的分解形式$A=LDU$ 分解 当A是正定矩阵的时候，我们可以得到 $U=L^T$ 所以$A=LDL^T$ 其中D是对角矩阵，如果给D开个根号是不是很完美，那么前提是D中所有元素都是正的（也就是所有pivots和Eigenvalue都是正的，这里两个概念等价）正定矩阵满足你的需求，所以就可以分解出来： $$ A=L\\sqrt{D}(\\sqrt{D}L)^T $$ 上面这个是Cholesky Factor，针对正定矩阵的一种分解\n半正定矩阵 (Positive Semidefinite Matrices) 如果特征值中包含0 ，那么 $x^TAx$ (x是特征向量) 有可能是0，也就是没有能量。并且写成 $R^TR$的形式。R总是有线性相关的列，具体原因看上面的证明自明，也就是奇异矩阵必然不是正定矩阵，但是可以使半正定矩阵，这个结论还是很欣慰的，并没有把奇异矩阵一棒子打死。 扩展x为任意变量的时候$x^TAx\\geq 0$ 只有当x为对应于特征值为0的特征向量的时候等号成立。\n一个应用（First Application: The Ellipse) $ax^2+2bxy+cy^2=1$ 学习线性代数，从向量开始，我就有一种感觉就是线性代数当矩阵维度是2或者3的时候，应该是和几何有关系的，也就是我们能画出来的这些形状有关而不仅仅是解方程这么简单，线性变换，对图形的作用应该是比较直观的，所以我们来看书上的🌰，关于特征值，特征向量，以及椭圆的： 两个椭圆，一个倾斜的，一个立正的,两幅图能够看出下面这些信息：\n The tilted ellipse is associated with A. Its equation is $x^TAx=1$ The lined-up ellipse is associated with $\\Lambda$ .Its equation is $x^T\\Lambda x=1$ The rotation matrix that lines up the ellipse is the eigenvector matrix Q  第一个歪的椭圆的方程是： $$ \\begin{bmatrix}x\u0026amp;y\\end{bmatrix} \\begin{bmatrix}5\u0026amp;4\\4\u0026amp;5\\end{bmatrix} \\begin{bmatrix}x\\y\\end{bmatrix}=1\\ A=\\begin{bmatrix}5\u0026amp;4\\4\u0026amp;5\\end{bmatrix}\\ \\lambda_1=9\\ \\lambda_2=1\\ x_1=\\begin{bmatrix}1\\1\\end{bmatrix}\\ x_2=\\begin{bmatrix}1\\newline -1\\end{bmatrix} $$ 分解成 $$ A=Q\\Lambda Q^T= \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}9\u0026amp;0\\0\u0026amp;1\\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} $$ 然后把xy弄进去 $$ \\begin{bmatrix}x\u0026amp;y\\end{bmatrix} Q\\Lambda Q^T \\begin{bmatrix}x\\y\\end{bmatrix}= \\frac{1}{\\sqrt{2}} \\begin{bmatrix}x\u0026amp;y\\end{bmatrix} \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}9\u0026amp;0\\0\u0026amp;1\\end{bmatrix} \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}x\\y\\end{bmatrix}\\ \\frac{1}{\\sqrt{2}} \\begin{bmatrix}x\u0026amp;y\\end{bmatrix} \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}9\u0026amp;0\\0\u0026amp;0\\end{bmatrix} \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}x\\y\\end{bmatrix}\\ + \\frac{1}{\\sqrt{2}} \\begin{bmatrix}x\u0026amp;y\\end{bmatrix} \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}0\u0026amp;0\\0\u0026amp;1\\end{bmatrix} \\frac{1}{\\sqrt{2}} \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;-1\\end{bmatrix} \\begin{bmatrix}x\\y\\end{bmatrix}\\ 9(\\frac{x+y}{\\sqrt{2}})^2+1(\\frac{x-y}{\\sqrt{2}})^2 $$\n最后一步矩阵计算比较长，这里省略了，中间没有trick，我把分解那步写出来也是为了好看，，结论是正确的，观察上面结果，两个系数居然是特征值，厉害不？不是pivot也不是别人，而是特征值，在两个平方内部，是两组向量 $(1,1)$ 和 $(1,-1)$ 这个是特征向量，也是椭圆轴的方向，所以当矩阵扩展到多维的时候（还是正定矩阵），的时候所有特征向量是这个超椭圆的所有轴，这也是为什么叫主轴定理的原因啦，不光是主轴，特征值还能表示在主轴方向上的长度，这个也很值得重视，因为方向和长度是向量的两个组成因素。 进一步，我们看看怎么把歪着的椭圆立起来 $$ X=\\frac{x+y}{\\sqrt{2}}\\ Y=\\frac{x-y}{\\sqrt{2}}\\ for:\\ 9(\\frac{x+y}{\\sqrt{2}})^2+1(\\frac{x-y}{\\sqrt{2}})^2\\ get:\\ 9X^2+Y^2=1 $$ 那么X最大值是 $\\frac{1}{3}$ ,Y 最大值是 $1$ 这两个值的得到方法都是 $\\frac{1}{\\sqrt{\\lambda}}$ 从图上也能看出来，长轴是1对应小的特征值，短轴是$\\frac{1}{3}$ 对应长的特征值。 在xy中，轴的方向沿着A特征向量方向，在XY中是 $\\Lambda$ 的特征向量，也就是坐标轴了，数值上看是这样的，从原理上：\n$$ \\begin{bmatrix}x\u0026amp;y\\end{bmatrix} Q\\Lambda Q^T \\begin{bmatrix}x\\y\\end{bmatrix} =\\begin{bmatrix}X\u0026amp;Y\\end{bmatrix} \\Lambda \\begin{bmatrix}X\\Y\\end{bmatrix}= \\lambda_1X^2+\\lambda_2Y^2=1 $$ xy可以表示一个被线性变换后的坐标表示，当这个坐标被Q投影了以后，就变成了图中正的图形了，可以观察特征值决定了椭圆有多椭，当特征值都是1的时候也就是 $\\Lambda =I$ 的时候，这个图形就是个圆，如果有负数特征值，那就变成双曲线了，如果都是负的。。这个我也不知道是啥了（就有虚数部分了，这个我还不懂）\nConclusion 本文主要讲解正定矩阵，最后的应用是最精彩的部分，前面主要是线性代数知识点的大串讲，可能有些不那么详细，大家可以自己再补充一些，到这里线性代数高潮部分已经完成了一大半，完成了下面就是相似矩阵和svd，线性代数基础部分也就算是快结束了，待续。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-5.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 关于正定矩阵的相关知识总结，正定矩阵在数学中的一个应用\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Positive Definite Matrices,Symmetric Matrices,Eigenvalues,Eigenvectors\u003c/p\u003e","title":"【线性代数】6-5:正定矩阵(Positive Definite Matrices)"},{"content":"Abstract: 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质 Keywords: Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem\n对称矩阵 这几篇在难度上确实要比前面的内容大很多，所以看书理解和总结都变得不那么流畅了，但是慢慢看下来收获还是有很大的，而且我发现不管学的多认真，还是会有遗漏，所以我觉得之前的想法就是一次性把什么什么学会是不可能的，只能学到自己觉得达到自己能发现的最大限度，等到应用之时还是要回来查阅，这样往往会有进一步的更大发现，\n对称矩阵 (Symmetric Matrices) 对称矩阵我们在最早的知识里面就学过 $A^T=A$ 的矩阵叫做对称矩阵，我们也学过投影矩阵,但是当时我们并没有强调过一点就是投影矩阵都是对称的，这个性质今天在这里会有很大的用途。 我们继续说投影矩阵，所谓投影矩阵，就是在和向量 $\\vec{c}$ 相乘的时候，投影到矩阵A的列空间内，那么其中，投影 $p$ 和 原向量 $\\vec{c}$ 的差 $\\vec{e} =\\vec{c}-\\vec{p}$ 与子空间正交。\n举个例子，在三维空间内，A的列空间是一个二维平面那么，A对应的投影矩阵P能够把任何方向的向量投影到平面上，那么如果向量本身属于平面那么 $Px=x$ 显然是不用质疑的（我们之前在投影那篇文章中也讲过） 但是，同志们，看看这个有木有很面熟啊，这个明显就是投影矩阵 $P$ 的特征值和特征向量么？没错，$P$ 有一个平面的特征向量，可以随便选！能选多少个呢、当然是无数个，但是问题又来了，这无数多个并不是独立的，因为一共就二维，选出来三个线性独立的向量都是不可能的，所以这个平面能选出两个线性独立的特征向量，并且对应的特征值都是1，这里有人可能疑惑为啥要选两个，因为我们6-2的时候说过只有特诊向量足够的情况下才能对角化，投影矩阵明显是个3x3的矩阵，那么特征向量也应该有三个呀！我们的子空间是二维的，所以理论上应该有两个特征向量在上面，剩下一维存在一个，那么这一个也能很好找，$\\vec{e}$ 就是 也就是和子空间正交的向量都行 $Px=0x$ 表明 $\\vec{x}$ 和子空间正交，那么这是个特征值为0的特征向量，这样我们又进一步规范一下，选择三个特征向量相互正交，这个也是可以做到的，也就是对于矩阵P我们找到了三个相互正交的特征向量，并且长度缩放到单位长度。\n以上三维投影到二维平面可以通过几何来解释，但为了能让大家从线性空间来理解，就没用几何方法，大家可以自己脑补。\n得出结论，对称矩阵 $P^T=P$ 的特征向量相互正交并且为单位向量。 对称矩阵\u0026quot;It is no exaggeration to say that these are the most important matrices the world will ever see \u0026ndash; in the theory of linear algebra and alos in the applications\u0026quot; 翻译成中文：“对称矩阵是史上最牛B的矩阵，无论在理论还是应用” 这个我们目前还无法考证，还没做过应用呢？不是么，但是我知道PCA中确实用了对称矩阵，SVD等一些列相关技术。 一个矩阵能被如此称赞，不外乎几点原因，首先是其本身拥有较好的性质，其次这个矩阵在自然生活中经常出现，就像正态分布，那么难的公式，却能准确的描述自然届的现象。最后就是如果表现形式简单，那么这个就是非常有用的东西啦。\n下面我们开始探索对称矩阵的性质。 如果一个对称矩阵满足： $$ suppose:\\ A^T=A\\ A=S\\Lambda S^{-1}\\ then:\\ A^T=(S^{-1})^T\\Lambda^T S^T $$ 这种情况下就有下面这种可能了，也就是对应的 $S=(S^{-1})^T$ 注意我们这里说的是可能，并不排除不可能的情况，原文书上用的也是possibly，也就是说我们目前假设: $$ S^T=S^{-1}\\ S^TS=I $$ 这里我们可以预报一下：\n  对称矩阵只有实数特征值 对称矩阵特征向量可以选择正交单位向量 orthonormal   对于 $S^TS=I$ 面熟么？还有印象么？我们认识啊，正交矩阵  $Q^TQ=I$ 矩阵Q中每列之间相互正交，也就是我们对于对称矩阵可以写成： $$ A=Q\\Lambda Q^{-1}=Q\\Lambda Q^T\\ with:\\ Q^{-1}=Q^T $$\n这个就是著名的普定理 \u0026ldquo;Spectral Theorem\u0026rdquo;:\n Every symmetric matrix has the factorization $A=Q\\Lambda Q^T$ with real eigenvalues in $\\Lambda$ and orthonormal eigenvectors in $S=Q$\n 对于所有对称矩阵都能分解成 $A=Q\\Lambda Q^T$ 的形式并且在 $\\Lambda$ 中的所有特征值都是实数，其对应的特征向量是正交单位矩阵，即 $S=Q$\n普定理在数学中很重要，对应的主轴定理 “Principle Axis Theorem”在集合物理中有重要地位。\n这个定理对于从后到前 $A=Q\\Lambda Q^T$ 很好证明，但是对于对称矩阵的特征值都是实数和特征向量相互正交比较难证明，也就是说，对角化时，特征向量矩阵是正交矩阵的时候很容易证明原始矩阵是对称的，但是对称矩阵不太容易证明，可以对角化，并且对角化的特征矩阵 $S$ 是正交矩阵 $Q$\n插播一句，orthonormal，翻译成中文是正交，而且normal，因为特征向量是可以随意缩放的，所以主要强调正交性 我们接下来要做的伟大的证明，分为下面三步来证明：\n 首先举个例子，来展示下特征值都是实数，特征向量orthonormal 当特征值不重复的时候的证明 当特征值重复的时候的证明   我们来先看个🌰： $$ A=\\begin{bmatrix}1\u0026amp;2\\2\u0026amp;4\\end{bmatrix}\\ \\begin{vmatrix}1-\\lambda \u0026amp;2\\2\u0026amp;4-\\lambda\\end{vmatrix}=0\\ \\lambda^2-5\\lambda=0\\ \\lambda_1=0\\ \\lambda_2=5\\ x_1=\\begin{bmatrix}2\\newline -1\\end{bmatrix}\\ x_2=\\begin{bmatrix}1\\newline 2\\end{bmatrix} $$ 可以看出$x_1$ 属于矩阵的nullspace，而 $x_2$ 属于矩阵的column space，但是我们学四个子空间的时候有nullspace和rowspace是正交的，但是这里的 $x_2$ 属于矩阵的column space，为什么呢？因为矩阵实对称的，所以对称矩阵的rowspace和columnspace一致。 $$ Q^{-1}AQ= \\frac{1}{\\sqrt{5}} \\begin{bmatrix}2\u0026amp;1\\newline -1\u0026amp;2\\end{bmatrix} \\begin{bmatrix}1\u0026amp;2\\2\u0026amp;4\\end{bmatrix} \\frac{1}{\\sqrt{5}} \\begin{bmatrix}2\u0026amp;1\\newline -1\u0026amp;2\\end{bmatrix}= \\begin{bmatrix}0\u0026amp;0\\newline 0\u0026amp;5\\end{bmatrix}= \\Lambda $$ 这就是个简单的例子，但是证明全体元素成立不能靠举一个例子来证明，但证明不成立可以靠举个反例来证明不成立。\n 下面证明 所有实数对称矩阵特征值都是实数\n怎么证明一个数是实数呢，只能用点实数的性质，实数的性质不少但是能证明实数是实数的不多，可以想到一个就是实数的共轭是其本身，这也可以说是复数的性质，证明是实数也就是说证明不是复数。 复数的共轭 $$ \\lambda=a+bi\\ \\bar{\\lambda}=a-bi $$\n根据复数的性质，以及向量的基本计算，对于实数矩阵A，满足：\n$$ Ax=\\lambda x\\ \\bar{Ax}=\\bar{\\lambda x}\\ A\\bar{x}=\\bar{\\lambda}\\bar{x}\\ Transpose:\\ \\bar{x}^TA=\\bar{x}^T\\bar{\\lambda}\\ for:\\ \\bar{x}^TAx=\\bar{x}^T\\lambda x\\ \\bar{x}^TAx=\\bar{x}^T\\bar{\\lambda}x\\ and:\\ \\bar{x}^Tx=|x|^2\u0026gt;0\\ so:\\ \\bar{\\lambda}=\\lambda $$\nQED\n整个过程证明了特征值的共轭等于原特征值，故特征值是实数被证明了。 证明思路就是通过构造出 $\\bar{\\lambda}=\\lambda$ 的结构来证明，主要用到了A是实数矩阵的性质，通过转置和乘法等来完成这个过程。 $(A-\\lambda I)x=0$ 可以得出既然特征值是实数，A也是实数，那么x肯定是实数（实数没办法仅通过乘法加法得出复数）\n 下面证明 所有实数对称矩阵特征向量相互正交,当特征值不相等的时候\n$$ suppose:\\ Ax=\\lambda_1x\\ Ay=\\lambda_2y\\ A^T=A\\ (Ax)^T=\\lambda_1x^T\\ (Ax)^Ty=x^TAy=\\lambda_1x^Ty\\ x^TAy=x^T\\lambda_2 y\\ then:\\ \\lambda_1x^Ty=x^T\\lambda_2 y\\ for:\\ \\lambda_1\\neq \\lambda_2\\ so:\\ x^Ty=0 $$ QED\n 插播一个小栗子： $$ A=Q\\Lambda Q^T= \\begin{bmatrix}x_1 \u0026amp;x_2\\end{bmatrix} \\begin{bmatrix}\\lambda_1 \u0026amp;\\\u0026amp;\\lambda_2\\end{bmatrix} \\begin{bmatrix}x_1^T \\x_2^T\\end{bmatrix}=\\lambda_1x_1x_1^T+\\lambda_2x_2x_2^T $$ $A=\\lambda_1x_1x_1^T+\\lambda_2x_2x_2^T$ 把A写成了两个rank=1的矩阵的线性组合，并且这个rank=1的矩阵还是投影矩阵，当然也是对称矩阵，是不是很神奇，这个矩阵分解是比较有意思的，基是矩阵，而且基实对称的rank=1的投影矩阵，是不是很多头衔啊，头衔越多越厉害，不信你去看看大大有多少头衔。这个投影矩阵可以理解为投影到特征向量组成的空间(Eigenspace)\n 其实写到这我有点疑惑了，本来写博客一个是自己总结，另一个是给大家一个参考，但是当我写了上面那个头衔了以后，我发现，如果没有基础或者不从头看起，直接看本文可能会感到疑惑，这也是知识的一个性质，就是连续性和扩展性，那么没有基础，基本都是空中楼阁（这种大牛太多了，基础没用，直接上算法的比比皆是，我以前也是，我现在改邪归正了）\n实数矩阵的复特征值(Complex Eigenvalues of Real Matrix) 本文主要说对称矩阵的特征值，特征向量，对称矩阵的特征值和特征向量一定是实数，我们上面基本都为证明这个结论，那么什么样的矩阵会产生复数特征值特征向量呢？ 如果一个矩阵包含一个复数特征值，那么一定是成对出现的，所谓成对就是如果 $\\lambda=a+bi$ 是一个特征值，那么 $\\bar{\\lambda}=a-bi$ 也一定是A的一个特征值，证明： $$ Ax=\\lambda x\\ A\\bar{x}=\\bar{\\lambda x}=\\bar{\\lambda} \\bar{x} $$ 上面的取共轭操作hexo渲染有点问题，在两个字母中间的$\\bar{\\lambda x}$ 其实是个长的,他画的有点短 我们后面有一章专门介绍复数矩阵，所以这里有点迷糊的不要紧，放过自己，继续看下面。\nEigenvalues 和 Pivots Pivots是我们这章之前主要研究的对象，因为我们主要研究的是矩阵用于方程组的求解，而本章开始Eigenvalues的研究，那么我们的惯性思维就是，既然是一个体系下的知识重点，那么他们有联系么？\n product of pivots = determinant = product of Eigenvalues\n 这个是个特殊的性质，结合前面trace的性质我们知道了特征值和矩阵相关的两个算数性质，但是这个具体的证明我还没学会，包括下面的这个结论，证明我也没看懂。\n The number of positive eigenvalues of $A=A^T$ equals the number of positive Pivots\n 这个结论书上给出了证明，但是说实话，我没明白，所以这个地方必须先留个空白，把书上的东西抄过来没意义\n上述两个结论不会证明，后续补上，此处留坑\n这就是pivots和eigenvalues可以通过上述两个结论产生联系，不过联系应该也就这么多了。\n所有对称矩阵可以对角化(All Symmetric Matrices are Diagonalizable) 接下来我们要证明最后一个结论，就是在有重复特征值的情况下，对称矩阵依然可以被对角化，其实本文第一个例子中就包含相同的特征值，两个 $\\lambda=1$ 虽然如此我们还是在平面中找到了两个相互正交的特征向量，一个特殊的例子没办法证明全部情况，下面我们系统证明一下： 正式的证明之前有个小trick,就是给矩阵对角线上的每个元素加一个扰动 $nc$ 这样所有的特征值不同（具体为啥我也没想明白,有明白人请指教一下）所有有不同的特征向量，当 $c\\to 0$ 得到原始特征值，和一组不同特征向量（这个思路是Prof. Strang写在书上的，他说这个有点不严谨，但是I am sure this is true） 接下来将正规的证明方法 首先用到一个理论：\n Schur\u0026rsquo;s Theorem: Every square matrix factors into $A=QTQ^{-1}$ where T is upper trangular and $\\bar{Q}^T=Q^{-1}$ If A has real eigenvalues the Q and T can be chosen real:$Q^TQ=I$\n 这个定理给出了详细的证明，但比较复杂，我试着简单的证明了一下（如果有不严谨的地方，请各位指出）: pf: 对于一个矩阵A，它代表的是A的列空间的基的矩阵，我们可以把它进行 $QR$ 分解，得到以Q为基（正交基）的同样的子空间，那么 $AQ$ 将还是这个子空间的基，所以我们可以得到： $$ AQ=QT\\ so: A=QTQ^{-1} $$ 上述过程中T是和R类似的上三角矩阵,对于所有方阵成立。 也就是说如果我们把Schur 定理稍微进行变形: $$ A^T=Q^TT^TQ\\ when:\\ A^T=A\\ T^T=T $$ 因为T是三角矩阵，所以当它也是对称矩阵的时候，必然是对角的。QED 以上是我刚发明的简单的证明方法，对于Schur\u0026rsquo;s Theorem 我们需要找的就是是否存在T使得 $AQ=QT$ 成立，其中Q是正交矩阵（研究证明题必须先把目标搞明白，别看了一路都是对的，就是不知道要干嘛，我刚才就犯了这个毛病，看哪句都是真命题，然后迷迷糊糊不知道要证明啥），下面描述下书上的方法： 我们要寻找Q，满足$AQ=QT$ 观察T是上三角矩阵，所以第一列只有一个元素$t_{11}$ 其中 $q_1$ 是Q的第一列,也就是说 $Aq_1=t_{11}q_1$ (这个要是不明白就自己拿笔画个矩阵比划一下子，就知道了)\n$$ \\bar{Q}^T_1AQ_1= \\begin{bmatrix} \\bar{q}^T_1\\ \\vdots \\ \\bar{q}n^T \\end{bmatrix} \\begin{bmatrix} \u0026amp;\u0026amp;\\ Aq_1\u0026amp;\\dots\u0026amp;Aq_n\\ \u0026amp;\u0026amp; \\end{bmatrix}= \\begin{bmatrix} t{11}\u0026amp;\\dots\u0026amp;\\dots\u0026amp;\\dots \\ 0\u0026amp;\u0026amp;\u0026amp; \\ 0\u0026amp;\u0026amp;A_2\u0026amp;\\ 0\u0026amp;\u0026amp;\u0026amp; \\end{bmatrix} $$ 观察上面的整个过程$Aq_1=t_{11}q_1$ 其实是A的特征值和特征向量，那么$q_2\\dots q_n$这些值怎么确定？答案是无所谓，只要找一组和$q_1$ 都正交的基就可以填充出其他部分（因为$t_{12}\\dots t_{1n}$ $q_1\\dots q_n$ 相乘得到A的其他部分，使得等式成立），我们的目的是为了找T，所以第一步我们算是迈出去了，下一步就是找$A_2$ 中的T了，根据假设，我们可以认为 $A_2Q_2=Q_2T_2$ 这里面的矩阵都比上一步少小一号(size=n-1)递归调用上面的证明方法，可以得出$Q_2$ , $t_{22}$ 以及$A_3$ 如此递归下去可以找到所有A的第一个特征向量，然后组合成Q，T $$ Q=Q_1\\begin{bmatrix}1\u0026amp;0\\0\u0026amp;Q_2\\end{bmatrix}\\ T=\\begin{bmatrix}t_{11}\u0026amp;0\\0\u0026amp;T_2\\end{bmatrix}\\ AQ=QT $$ 以上可证存在T使得 $AQ=QT$ 成立，那么如果A是symmetric的，那么: $$ A=QTQ^T\\ A^T=Q^TT^TQ\\ A=A^T\\ so:\\ T=T^T $$ T是上三角矩阵，得出结论，T是对角矩阵 QED 上面这一小段其实在证明Schur定理，因为Schur定理一旦得到证明，那么自然可以得到我们想要的结论，所有对称矩阵可以被对角化，Schur矩阵以复数形式给出，因为我们前面已经证明了对称矩阵的特征值都是实数，所以这里可以用实数表达，当然复数是对于非对称矩阵的，因为非对称实数矩阵可能得到复数特征值和特征向量。\nConclusion 这篇文章扎扎实实写了24小时，而且中间确实有不太清楚的地方，至今没动，所以都高亮标注了，提醒读者也提醒自己要来填坑，schur定理的证明方法还有别的，这个是Prof. Strang 书上的方法，后面如果有新发现继续补充。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-4.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本篇继续线性代数的高潮部分，对称矩阵，以及对称矩阵的一些性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Eigenvalues，Eigenvectors，Symmetric Matrices，Projection Matrices，Spectral Theorem，Principal Axis Theorem\u003c/p\u003e","title":"【线性代数】6-4:对称矩阵(Symmetric Matrices)"},{"content":"Abstract: 本文主要介绍线性代数在微分方程中的应用 Keywords: Eigenvalues,Eigenvectors,Differential Equations\n与最小二乘法那篇一样，由于微分方程将会是后面一项要学习的重点内容，我们这里先不介绍了，就是跳过。。哈哈哈哈。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍线性代数在微分方程中的应用\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Eigenvalues,Eigenvectors,Differential Equations\u003c/p\u003e","title":"【线性代数】6-3:微分方程的应用(Applications to Differential Equations)"},{"content":"Abstract: 矩阵对角化，以及对角化过程中引入的知识，以及对角化的应用 Keywords: Eigenvalues,Eigenvectors,Diagonalizing,Fibonacci Numbers, $A^k$ ,Nondiagonalizable Matrix\n对角化 对角化 对角化一个矩阵，和之前个种各样的分解有一个同样的思路，当矩阵从原始形态通过各种计算性质变形成为各种有规则的，或者在数值上有特殊的性质，这些特殊的形状都可以用在不同问题上，比如LDR分解可以直接求出pivot值，求解方程，QR分解可以是通过变换向量空间的基来使向量某些方面的性质凸显出来。 今天说的对角化就是利用了特征值特征向量的计算性质，通过对 $Ax=\\lambda x$ 进行变形引申得到的。而这个diagonalizing后的矩阵对于矩阵求幂有非常简单的计算。 假设 $n \\times n$ 的矩阵 $A$ 有n个特征向量，那么我们把每个特征向量按照每列一个特征向量的组合方式形成一个矩阵，那么这个矩阵我们称之为 $S$\n$$ AS= A\\begin{bmatrix} \\vdots \u0026amp;\\dots \u0026amp;\\vdots\\ x_1\u0026amp;\\dots \u0026amp;x_n\\ \\vdots \u0026amp;\\dots \u0026amp;\\vdots \\end{bmatrix}= \\begin{bmatrix} \\vdots \u0026amp;\\dots \u0026amp;\\vdots\\ Ax_1\u0026amp;\\dots \u0026amp;Ax_n\\ \\vdots \u0026amp;\\dots \u0026amp;\\vdots \\end{bmatrix}= \\begin{bmatrix} \\vdots \u0026amp;\\dots \u0026amp;\\vdots\\ \\lambda_1 x_1\u0026amp;\\dots \u0026amp;\\lambda_n x_n\\ \\vdots \u0026amp;\\dots \u0026amp;\\vdots \\end{bmatrix}\\ \\begin{bmatrix} \\vdots \u0026amp;\\dots \u0026amp;\\vdots\\ \\lambda_1 x_1\u0026amp;\\dots \u0026amp;\\lambda_n x_n\\ \\vdots \u0026amp;\\dots \u0026amp;\\vdots \\end{bmatrix}= \\begin{bmatrix} \\vdots \u0026amp;\\dots \u0026amp;\\vdots\\ x_1\u0026amp;\\dots \u0026amp;x_n\\ \\vdots \u0026amp;\\dots \u0026amp;\\vdots \\end{bmatrix}\\begin{bmatrix} \\lambda_1 \u0026amp; \u0026amp;\\ \u0026amp;\\ddots \u0026amp;\\ \u0026amp;\u0026amp;\\lambda_n \\end{bmatrix}=S\\Lambda\\ so: AS=S\\Lambda\\ \\Lambda=S^{-1}AS\\ A=S\\Lambda S^{-1} $$ $\\Lambda$ 是 $\\lambda$ 的大写，表示的是对角矩阵，每个元素都是eigenvalue。 如果矩阵A没有n个independence的eigenvector也是无法对角化的，上面的推到过程是属于两头堵的方式，先正向求出 $AS$ 的结果发现其结果和 $S\\Lambda$ 结果一样，所以就得到了 $\\Lambda$ 的表达式，下面我们我们就可以来计算 $A^k$ 了，利用上面推到过程中的最后一步，这个简直非常完美了 $$ A^k=A\\cdot A\\dots A=S \\Lambda S^{-1} S \\Lambda S^{-1} \\cdots S \\Lambda S^{-1}=S \\Lambda \\Lambda \\cdots \\Lambda S^{-1}=S \\Lambda^k S^{-1} $$ 一个矩阵的k次幂等于其对角矩阵的k次幂\u0026ndash; $S \\Lambda^k S^{-1}$ 我们可以回忆下上一篇，我们求过一个矩阵的k次方乘以一个向量 $A^ky$ ,用特征向量来作为 $y$ 的基，然后写成 $$ A^k:\\ suppose: ;C=\\begin{bmatrix}c_1 \u0026amp;\\dots \u0026amp; c_n\\end{bmatrix}\\ y=c_1 x_1+c_2 x_2+\\dots +c_n x_n=SC \\ A^k y=A^k(c_1 x_1+c_2 x_2+\\dots +c_n x_n)\\ =c_1A^kx_1+c_2A^kx_2+\\dots +c_nA^kx_n\\ =c_1\\lambda_1^k x_1+c_2\\lambda_2^k x_2+\\dots + c_n\\lambda_n^k x_n\\ =S\\Lambda^k C $$\n上面这个是回忆上一篇的内容同时通过这篇的内容加以结合，也是 $A^k$ 小节的主要过程，就得到了对角矩阵的一个应用，原理一致，方法不同而已，最终的理论根基都是 $Ax=\\lambda x$这个是本章最核心的方程，没有之一，就是最核心的，而且本章作为线性代数的高潮部分，这个方程也可以称之为线性代数中最重要的方程之一。\n在使用 $\\Lambda$ 之前，我们有几个remark需要强调一下：\n 没有重复特征值的矩阵可以被对角化 特征向量可以任意乘以一个非零常数（长度可以缩放） 对角矩阵中特征值的顺序与S中特征向量的排列顺序对应 有些矩阵没有足够的eigenvalue，所以也没有足够的eigenvector来组成S，这类矩阵不能被对角化。  注意 矩阵对角化和是否可逆没有直接关系\n 可逆表示需要行列式非零，行列式非零对特征值是有影响的，表明特征值不能为0 可对角化是说特征向量必须足够，也就是不能有特征向量缺失，$n \\times n$ 就应该有n个特征向量\n 一个重要的结论：如果有n个不同的eigenvalues，那么就会对应有n个independence 的eigenvectors，那么这个矩阵可以被对角化;也就是说，如果矩阵有n个不同的eigenvalue，那么矩阵可以被对角化 证明 $2\\times 2$ 矩阵的情况 : $$ Suppose :\\ c_1x_1+c_2x_2=0\\ A(c_1x_1+c_2x_2)=0\\ c_1Ax_1+c_2Ax_2=0\\ \\lambda_1 c_1x_1+\\lambda_2 c_2x_2=0\\ \\lambda_1(c_1x_1+c_2x_2)=0\\ so:\\ \\lambda_1 c_1x_1+\\lambda_2 c_2x_2 - \\lambda_1(c_1x_1+c_2x_2)=0\\ (\\lambda_2 - \\lambda_1 )c_2x_2=0\\ for:\\ \\lambda_2 \\neq \\lambda_1 \\ so:\\ c_2=0\\ similarly:\\ c_1=0 $$ 那么一开始的假设中只有 $c_1=0$ 和 $c_2=0$ 满足 $c_1x_1+c_2x_2=0$ 也就是说 $x_1,x_2$ 线性无关 证明可以直接被推广到多维。 这个证明很是巧妙，以至于我也看了半天才明白套路，通过利用特征向量和矩阵相乘得到特征值的性质，以及nullspace的性质来证明，不同的特征值对应的特征向量彼此之间独立。 通过对角化的方法可以轻松得出markov矩阵的性质，一个特征值为1，那么它对应的特征向量在k次幂后是稳定的，另一个小于1的将会被消灭。 那么什么时候$A^k$会自我毁灭，没错，如果矩阵的所有特征值都小于1，那么就毁灭了 $|\\lambda|\u0026lt;1$\n斐波那契数列(Fibonacci Numbers) 斐波那契数列，高中的时候学的生兔子什么的，之前新闻上还说有个女博士用这玩意炒股，赚到翻，然后，C语言刚学了一个月的时候，老师说能把这个做出来说明学的不错了，不过好像确实不太好写，我们来段代码，我们用python来写一下试试：\nN=10 a=1 b=1 print a print b for i in range(N-2): c=a+b a=b b=c print c 输出：\n1 1 2 3 5 8 13 21 34 55 89 144 通过计算机程序可以很快的得到任意项的结果，但是从数学的角度，我们也很关心他的增长率，也就是他是怎么增长的？线性？二次？还是指数增长？ Fibonacci Numbers用递归公式来表示： $$ a_n=a_{n-1}+a_{n-2} ,, where,,n\u0026gt;2 $$ 如果我们把这个递归关系写成矩阵形式，首先，我们需要弄个方阵出来，搞两个向量不太靠谱，没办法对角化，所以我们加一个 $$ \\begin{bmatrix}a_n\\a_{n-1}\\end{bmatrix}= \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;0\\end{bmatrix}\\begin{bmatrix}a_{n-1}\\a_{n-2}\\end{bmatrix} $$ 这个可以叫做矩阵形式的递归，通过 $\\begin{bmatrix}a_{n-1}\\a_{n-2}\\end{bmatrix}$ 来推导出 $\\begin{bmatrix}a_n\\a_{n-1}\\end{bmatrix}$ 递归得出，而中间的系数就比较有趣了，因为我们可以得出 $$ \\begin{bmatrix}a_n\\a_{n-1}\\end{bmatrix}= \\begin{bmatrix}1\u0026amp;1\\1\u0026amp;0\\end{bmatrix}^{n-1}\\begin{bmatrix}a_2\\a_1\\end{bmatrix} , where , n\u0026gt;2 $$ 这样问题就转换到 $A^k$ 的问题上了 $$ A=\\begin{bmatrix}1\u0026amp;1\\1\u0026amp;0\\end{bmatrix}\\ $$ 求A的特征值，特征向量 $$ Ax=\\lambda x\\ det(\\begin{bmatrix}1-\\lambda \u0026amp;1\\1\u0026amp;0-\\lambda\\end{bmatrix})=0\\ \\lambda^2-\\lambda-1=0\\ \\lambda_1=\\frac{1+\\sqrt{5}}{2} \\approx 1.618 \\ \\lambda_2=\\frac{1-\\sqrt{5}}{2} \\approx -.618 \\ x_1=\\begin{bmatrix}\\frac{1+\\sqrt{5}}{2}\\1\\end{bmatrix}\\ x_2=\\begin{bmatrix}\\frac{1-\\sqrt{5}}{2}\\1\\end{bmatrix} $$ Fibonnaci 数列的递归系数矩阵的特征值是黄金分割比！是不是很神奇！也就是说矩阵在k次后 $\\lambda_1$ 将会成为主要的增长系数 $\\lambda_2$ 由于小于1 将会被消灭。 比如100次方后将约等于 $$ \\begin{bmatrix}1\\0\\end{bmatrix}=c_1x_1+c_2x_2= c_1\\begin{bmatrix}\\frac{1+\\sqrt{5}}{2}\\1\\end{bmatrix}+ c_2\\begin{bmatrix}\\frac{1-\\sqrt{5}}{2}\\1\\end{bmatrix}\\ c_1=\\frac{1}{\\sqrt{5}}\\ c_2=-\\frac{1}{\\sqrt{5}}\\ \\begin{bmatrix}a_{100}\\a_{99}\\end{bmatrix}=\\frac{1}{\\sqrt{5}} \\lambda_1^{99}x_1-\\frac{1}{\\sqrt{5}} \\lambda_2^{99}x_2=\\frac{1}{\\sqrt{5}} \\lambda_1^{99}\\begin{bmatrix}\\frac{1+\\sqrt{5}}{2}\\1\\end{bmatrix}\\ a_{100}=\\frac{1}{\\sqrt{5}}(\\frac{1+\\sqrt{5}}{2})^{99}\\frac{1+\\sqrt{5}}{2}=3.534\\times 10^{20} $$ 检验一下： 基本一致\n矩阵求幂 $A^k$ Fibonnaci Numbers是一个典型的差分方程 $u_{k+1}=Au_k$ Solution is $u_k=A^ku_0$ 这就是一个典型的解过程，关键环节就是 $A^k$ ,过程和上面解决问题的关键步骤一般分为三步：\n 分解成以特征向量为基的线性组合 $u_0=Sc$ Multiplies $\\Lambda^k$ $u_k=\\sum c_i(\\lambda_i)^kx_i$ 求解S和 $\\Lambda^k$ 和 $S^{-1}u_0$ 的积  所以: $$ A^ku_0=S\\Lambda^kS^{-1}u_0=S\\Lambda^kc\\ u_k=c_1(\\lambda_1)^kx_1+\\dots + c_n(\\lambda_n)^kx_n $$ 这个就是 $u_k=Au_{k-1}$的解\n所以我们的对于这种迭代关系是线性的差分方程，解法就是通过将初始条件分解成特征向量的线性组成，然后通过特征值的幂和特征向量矩阵的组合，得到解。 transforming to an eigenvector basis 是一种非常经典的做法，比如傅里叶级数都是典型应用。\n非对角化矩阵 并不是所有的矩阵都能对角化的，前面也说了，有些特征值重复或者有些解不存在的时候，那么如何判断是否可以对角化呢？（就像判断是否可逆的那种方法） 我们可以考察两种指标来确定是否能对角化\n Geometric Multiplicity=GM ,计算线性独立的eigenvector 的数量，也就是$A-\\lambda I$ 的nullspace 的维度 Algebra Multiplicity=AM ,计算重复的 $\\lambda$ 主要考察 $det(A-\\lambda I)=0$ 的解  GM和AM保持关系 $GM \\leq AM$ 当$GM \u0026lt; AM$ 时 矩阵不可对角化\n$AB$ 和 $A+B$ 的特征值 特征值是否满足线性呢？不满足，笨方法也能看出来这根本不是一路的： $$ ABx=A\\beta x=\\beta Ax=\\beta \\lambda x\\ $$ 上面明显有问题，看出来问题在哪了么？ 式子$Ax=\\lambda x$当且仅当 x是A的特征向量的时候 所以 $A\\beta x=\\beta Ax=\\beta \\lambda x$ 这一步并不是对于所有矩阵都满足，只有A，B矩阵的特征向量相同的时候才能相等\n Commuting Matrix share eigenvectors ,假设，如果A和B能够被对角化，他们拥有完全相同的特征向量，当且仅当 $AB=BA$\n 证明方法： $$ ABx=A\\beta x=\\beta Ax=\\beta \\lambda x \\ \\beta \\lambda x=\\lambda\\beta x=\\lambda Bx=B\\lambda x=BAx\\ ABx=BAx\\ AB=BA $$\n对于A+B同理，只有当A和B的特征向量一致的时候，才能完成加法。 另外一个重要应用可以参考量子力学，Heisenberg\u0026rsquo;s uncertainty principle\nConclusion 本文主要讲解矩阵对角化，对角化的应用，对角化应该是在应用中最长用到的矩阵处理方法，所以用了一天的时间写了这一篇文章，希望能帮助理解。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 矩阵对角化，以及对角化过程中引入的知识，以及对角化的应用\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Eigenvalues,Eigenvectors,Diagonalizing,Fibonacci Numbers, $A^k$ ,Nondiagonalizable Matrix\u003c/p\u003e","title":"【线性代数】6-2:对角化(Diagonalizing a Matrix)"},{"content":"Abstract: 本文主要介绍集合论中集合的基本运算和运算法则 Keywords: Complement，Union，Intersection，Commutative Law，Associative Law，Distributive laws\n集合操作 这两天写字写的有点多，吐血更新，尤其是线性代数已经接近高潮部分，所以看书时间少了一下，但是感觉看的数量反而没有减少，现在在看概率论的书籍，陈希孺先生的《概率与数理统计》 ，钟开莱先生的《初等概率论》 Morris H.Degroot的他《概率统计》 这三本书后两本是英文，陈希孺先生的是中文，读起来肯定要顺畅一些，但是根据“经验人士透露”这些书都各有长处，所以我决定从一本英文书来下手，陈希孺先生的概率论部分不多，我也先大概多少看了一遍了，接下来是精度了，包括推到里面的所有数学符号，同时在想是看Morris的英文呢？还是看钟开莱先生的英文呢？这两个真的很难选，都是第四版，说明都很畅销，所以我决定扔🎲，如果是小就是钟开莱先生，要是大就Morris，结果跟我想象的差不多，Morris 1到6 ，结果是5 但是集合论这块还是继续用钟开莱先生的书，看完Morris回头再来钟先生\nOperation 我们之前学过很多数字的运算，以及运算法则，下面我们进行扩展，扩展到集合，数字的运算结果是数字，集合的运算结果也是集合，但是要换个名字，比如加法产生和，减法产生差，乘法产生积。\n注意：以下我们要讨论的所有集合的计算都是基于已知固定完整空间 $\\Omega$\nComplement 集合 $A$ 的Complement被写成 $A^c$ 表示： $$ A^c={\\omega | \\omega \\notin A} $$ $A^c$ 表示在空间 $\\Omega$ 中不属于 $A$ 的元素的集合\n几个特殊的例子：\n $\\Omega^c=\\emptyset$ $\\emptyset^c=\\Omega$ $(A^c)^c=A$  Union 并集，就是把两个集合合并到一起，重复元素只保留一份（避免出现重复出现的情况）\n$$A \\cup B={\\omega \\in A , or, \\omega \\in B}$$\n逻辑词汇or表示可以前面的也可以后面的也可以两个同时，C语言，C++,离散数学应该都有讲过\nIntersection 交集，元素必须同时出现在两个集合中： $$A \\cap B={\\omega \\in A , and, \\omega \\in B}$$\n逻辑词汇and，必须满足前后两个条件，同时满足哦。\nCommutative Law \u0026amp; Associative Law $$ A \\cup B=B \\cup A\\ A \\cap B=B \\cap A $$ 这个叫交换律 $$ (A \\cup B)\\cup C= A \\cup (B \\cup C)\\ (A \\cap B)\\cap C= A \\cap (B \\cap C) $$ 这个叫结合律 说了这两个运算律很容让我们联想到加法和乘法，的确是这样，他们确实有些相似之处， $$ \\cup \\leftrightarrow +\\ \\cap \\leftrightarrow \\times $$ 所以上面的连续union或者连续intersection各个操作数都可调换位置。 但是 $$ (A \\cup B )\\cap C \\neq A \\cup (B \\cap C) $$ 解释可以通过Venn 图来看 Venn图怎么画？画圈就行了，相交的地方是交集，合起来是并集，没圈到的地方是complement，这个我看五年级的小孩都在奥数题里面写这个。\nDistributive Law 这个跟前面就有点不同了 $$ (A \\cup B) \\cap C=(A \\cap C) \\cup (B \\cap C) \\dots\\dots (D_1)\\ (A \\cap B) \\cup C=(A \\cup C) \\cap (B \\cup C) \\dots\\dots (D_2) $$ 这两个公式都是正确的么？证明一个公式不正确，只需要找到一个反例就可以但是要证明一个公式正确，就很麻烦了。那么我们还是用五年级奥数的方法来检验一下这两个公式哪些是正确的： 通过venn图可以发现，这两个哥们都是正确的，那么我们之前那个类比就有问题了，也就是合集和并集类似加法和乘法在此处不再合理了，并且其实$A \\cup A =A$ 就已经宣告类比不成立了。 接着钟老师用了一个食物的例子来证明了一下 $D_1$ 这里就不详细说了，这个比较简单，我们下面用数学的方法来证明下 $D_2$ 证明思想就是(I)证明属于左边的元素都属于右边，(II)再证明右边的元素都属于左边 $M \\subset N$ and $M \\supset N$  I:假设元素属于左边，那么它属于A交B或者属于C。 ①如果属于A交B那么 肯定属于A并C也要属于B并C，所以属于右边 $$ suppos:\\ \\omega \\in A \\cap B\\ then:\\ \\omega \\in A \\cup C\\ \\omega \\in B \\cup C $$ ②如果属于C那么肯定也属于右边了 $$ suppos:\\ \\omega \\in C\\ then:\\ \\omega \\in A \\cup C\\ \\omega \\in B \\cup C $$ II假设元素属于右边 ①如果元素属于C，那么肯定属于左边 $$ suppos:\\ \\omega \\in C\\ then:\\ \\omega \\in (A \\cap B) \\cup C $$ ②如果元素不属于C，属于A并C同时也属于B并C，那么元素属于A并且属于B，所以元素属于A并B属于左边 $$ suppos:\\ \\omega \\in A \\cup C\\ \\omega \\in B \\cup C\\ \\omega \\notin C\\ then:\\ \\omega \\in A \\ \\omega \\in B \\ \\omega \\in A \\cap B \\ $$ Q.E.D\nConclusion 今天已经写了两篇博客了，这篇还好比较短，集合作为最基础的数学框架，有很多是从没有文字就开始有了的，过于理论化的东西也不是我们研究的对象，我们要做的就是多画画多理解，好在后面看算法看其他过程的时候心里没有恐惧。 待续。。。\n","permalink":"https://go.face2ai.com/math/math-set-theory-2-operations-with-sets.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍集合论中集合的基本运算和运算法则\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Complement，Union，Intersection，Commutative Law，Associative Law，Distributive laws\u003c/p\u003e","title":"【集合论】2 集合操作"},{"content":"Abstract: 线性代数重点，关于矩阵特征值特征向量的相关知识第一篇文章，简单介绍特征值 Keywords: Eigenvalues，Eigenvectors，Sigular，Markov matrix，Trace，Imaginary Eigenvalues\n特征值介绍 线性代数看了也有一段时间了，做题考试怎么样不知道，但是整个框架现在已经渐渐明朗了起来，但是，我发现Prof. Strang的书写到这里才写了一半，也就是说他用半本书详细的讲述了线性代数整个基础知识体系，那么后半本书呢？分别是：application，numerical linear algebra，和complex vectors and matrix。应用和数值计算部分可以说是非常有用的，我记得上学的时候数值计算是一门单独的选修课，这么说我们的课程设计还是完整的，至于没学会，责任我一个人承担😆。 接着说特征值，作为一个靠着图像处理入门的人，对特征值简直就是不可抗拒，说实话，真的对各种特征都有不必崇敬的心理，当然图像的各种特征值和eigenvalue还是不太一样，但是PCA里面就是Eigenvalue，当时简直是，觉得这个太厉害了，因为不懂所以觉的厉害，就像我90岁的姥姥觉得微信不可思议一样，不可思议源于无知，90岁的老人即使不识字不会用电话也不会被嘲笑，因为环境，历史，他们经历了我们无法了解的苦难，但是在象牙塔里天天王者荣耀，以至于什么都不懂，我觉得就应该感觉到羞愧了，没错，我很羞愧，所以在努力补救。\n特征值介绍 我们之前研究了很多关于 $Ax=b$ 的问题，研究针对一个稳定的问题(系统)，当系统处于动态情况下，我们之前的所有知识就不能继续帮助我们了，而eigenvalue在dynamic problem(WIKIPEDIA)上可以帮助我们解决很多问题（内心os：写到这个我才发现，Stang是从动态问题切入，引出eigenvalue，之前我还没发现，我说怎么一直在研究矩阵的n次方，如果有人问，你有没有好好备课就来写博客，我只能说，每次从新看书都有新发现） 提出问题：$\\frac{d\\vec{u}}{dt}=A\\vec{u}$ 这个微分方程怎么解？解随着时间不断变化，所以消元是不行了，这种问题常见于震荡，衰变等，在这些问题中，我们就要使用eigenvalue和eigenvector的一些厉害的特性了\n$Ax= \\lambda x$ 注意，我们第六章所有矩阵都是方的方的方的。 $Ax=b$ 我们之前已经详细的讲过了，第一种境界，就是方程组，第二种境界是向量的线性组合，第三境界就是子空间，但是问题是，b是个固定值，也就是说，b一旦确定，我们在子空间内就没办法动了，但是我们现在提出一个新的 $Ax=b\u0026rsquo;$ 这个 $b\u0026rsquo;$ 将不再是固定不变的，而且他和变量 $x$ 高度相关，没错 $$ Ax=\\lambda x $$ 这个将是我们现行代数后半段的研究核心，从子空间的角度来观察一下这个式子，首先 $x$ 在 $A$ 的column space 中（因为等号右边都是在A的列空间中的），并且x通过矩阵 $A$ 投影到子空间后，方向不变，只是长度变了，这就是一个很好的性质了，我们想一下之前我们在研究projection的时候研究过，如果想把所有向量都投影到一个固定的方向上 $\\vec{a}$，需要根据这个方向上的向量来确定投影矩阵【机器学习基础之线性代数】4-2:Porjections,这个求出来的投影矩阵对于所有 $\\vec{a}$ 方向上的向量都能投影到其本身；而我们的 $Ax=\\lambda x$与之相反是已知矩阵A，来求一个向量，能被映射到自己所在的方向上，但是不保证其长度不变，长度伸缩 $\\lambda$ 上面和projection类比只是为了说明一个是求ptojection矩阵，一个是求方向向量；一个是自身投影到自身，一个是投影到自身并有缩放。 写个表格吧(这部分不是书里的，是我刚想到的)\n   比较项目 Projection $Ax=\\lambda x$     已知条件 方向向量 $\\vec{a}$ 矩阵 $A$   求解目标 投影矩阵 $P$ 方向向量 $\\vec{x} $   投影后结果 $Pa=a$ $Ax=\\lambda x$   多次投影后 $P^na=a$ $A^nx=\\lambda^nx$    经过上图的对比，可以看出一些问题，尤其是右下角那个式子将会是我们接下来要研究的一个重要表现形式\n$A^n$ 哦，sorry忘了给各位介绍了上面说的 $\\lambda$ 叫特征值，英文名 Eigenvalue（好像不是英文），$x$ 叫特征向量，英文名 Eigenvector（好像也不是英文）。 我们来观察个矩阵的n次方的例子： $$ A=\\begin{bmatrix}.8\u0026amp;.3 \\newline .2\u0026amp;.7\\end{bmatrix}\\ A^2=\\begin{bmatrix}.70\u0026amp;.45 \\newline .30\u0026amp;.55\\end{bmatrix}\\ A^3=\\begin{bmatrix}.650\u0026amp;.525 \\newline .350\u0026amp;.475\\end{bmatrix}\\ \\vdots\\ A^n=\\begin{bmatrix}.6000\u0026amp;.6000 \\newline .4000\u0026amp;.4000\\end{bmatrix}\\ $$ 这个不是我求出来的，我从书上抄过来的，如果有问题别找我，可以看出来，把一个矩阵连续的投影到列空间，经过一段时间后，就会趋于稳定，矩阵就会不在变化，这个结论表面看上去是这样的，但是没有理论证明啊，那么我们就来证明一下，但是首先我们要求一下特征值： $$ Ax=\\lambda x\\ (A-\\lambda I)x=0 $$ 说明一下，按照上式的形式，并且根据规定其中 $x$ 不是0，我们求解特征值的问题就变成求解矩阵$A-\\lambda I$ 了，如果x不是0，那么矩阵就必须是奇异的，也就是说矩阵中列一定是线性相关的，所以null space才会有非0向量，那么奇异矩阵的一个性质就是determinant为0，那么我们的特征值就是： $$ det(A-\\lambda I)=0\\ $$ 行列式求法我们已知，其实这就是个n次方程了，n是矩阵的规模(长或者宽)，那么一般情况下我们会得到n个解，注意这里的解可以是0，也可以重复，也可以是复数，这些情况我们后面讨论，但是每个特征值都对应一个特征向量，带回到 $Ax=\\lambda x$ 就可以得到两个特征向量了。 那么上面的证明就可以这样写了，这里我们假设矩阵A可以得到两个不同方向的特征向量 $x_1,x_2$，具体证明后面给出： $$ Ax_1=\\lambda_1x_1 \\dots \\dots \\dots (1) \\ Ax_2=\\lambda_2x_2 \\dots \\dots \\dots (2) \\ AA=\\begin{bmatrix}A\\cdot col(A)_1 \u0026amp; A\\cdot col(A)_2\\end{bmatrix}\\ a_1=col(A)_1\\ a_2=col(A)_2\\ a_1=p_1x_1+p_2x_2\\ a_2=q_1x_1+q_2x_2\\ $$\n$$ Aa_1=A(p_1x_1+p_2x_2)=p_1Ax_1+p_2Ax_2\\ $$ plug (1) into $Aa_1$: $$ Aa_1=\\lambda_1p_1x_1+\\lambda_2 p_2x_2\\ A^2a_1=\\lambda_1^2p_1x_1+\\lambda_2^2 p_2x_2\\ \\vdots \\ A^na_1=\\lambda_1^np_1x_1+\\lambda_2^n p_2x_2\\ $$ 同理可以得到 $a_2$那么最后 $A^n$ 的结果就是: $$ A^n=\\begin{bmatrix} \\lambda_1^{(n-1)}p_1x_1+\\lambda_2^{(n-1)} p_2x_2 \u0026amp; \\lambda_1^{(n-1)}q_1x_1+\\lambda_2^{(n-1)} q_2x_2 \\end{bmatrix} $$\n数学过程基本就是这个样子了，推到应该还算严谨，思路是把矩阵的列分解到以特征向量为基的表示形式，通过特征向量和矩阵乘积还等于特征向量的这个特征，能够继续迭代进行下去，如下图： 图中的数字是$A=\\begin{bmatrix}.8\u0026amp;.3\\newline .2\u0026amp;.7\\end{bmatrix}$ 作为例子来分解的。 最后还要用到极限的一些知识，首先来看两个特征值，$\\lambda$ 他们要被n次方，所以如果：\n 如果 $\\lambda\u0026gt;1$ 那么n次方后会非常非常大； 如果 $\\lambda=1$ 那么n次方后稳定，还是1 \u0026ndash;steady state； 如果 $\\lambda\u0026lt;1$ 那么n次方后趋近于0 \u0026ndash;decaying state；  Markov matrix $A=\\begin{bmatrix}.8\u0026amp;.3\\newline .2\u0026amp;.7\\end{bmatrix}$ 这种矩阵有个统一的称号，Markov matrix，为什么呢，后面可能挺有说明，因为我曾经在跟踪算法见过Markov链之类的算法，应该是一个Markov，特点:所有列中元素的和是1，这种矩阵就是Markov 矩阵。Markov矩阵中有一个特征值必然为1，问我为啥？后面教你算！\n特征值两个性质 两个性质🤣，特么我竟然想到了三个代表：\n 奇异矩阵的特征值必然包含0 2x2 奇异对称矩阵的特征向量们相互正交  证明一下： 奇异矩阵的特征值，我们回忆下求特征值的过程，其实是求 $(A-\\lambda I)x=0$ 中 $\\lambda$ 的过程，用到的就是矩阵 $(A-\\lambda I)$ 是奇异矩阵，那么如果A已经是奇异矩阵了，那么 $\\lambda$ 是0 必然是解中的一种情况，这就证明了1； 如果是奇异对称的矩阵，那么当同样根据上面所说的，当 $\\lambda=0$ 时，$Ax_1=0$ 表示 $x_1$ 在A的null space中，另外的特征向量 $x_2$ 必然在A的列空间，又因为A对称所以列空间与行空间相同，故而 $x_1$ 与 $x_2$ 正交 ，QED\n越特殊的矩阵，其特征值和特征向量越特殊，这是一个非常有用的性质。后面有一张表，可以总览一下。\n映射矩阵和投影矩阵 $R=\\begin{bmatrix}0\u0026amp;1\\1\u0026amp;0\\end{bmatrix}$ 特征值也比较特殊：$\\pm1$ $$ R=\\begin{bmatrix}0\u0026amp;1\\1\u0026amp;0\\end{bmatrix}=2\\begin{bmatrix}0.5\u0026amp;0.5\\0.5\u0026amp;0.5\\end{bmatrix}-\\begin{bmatrix}1\u0026amp;0\\0\u0026amp;1\\end{bmatrix} $$\n0.5的Markov矩阵P特征值0和1，所以P是个投影矩阵？为什么，一个特征值是1，说明 $Px_2=x$ 这明显就是个投影矩阵，并且Nullspace部分被干掉了（ $Px_1=0$ ） P 的特征向量(1,1),(1,-1) ,R的特征向量也是(1,1),(1,-1)，所以下面的式子成立了 $$R=2P-I\\ Rx=\\lambda_R x\\ Px=\\lambda_P x\\ Ix=x\\ Rx=(2P-I)x\\ \\lambda_R =2 \\lambda_P-1\\ \\lambda_{R1} =2 \\times 1-1=1\\ \\lambda_{R2} =2 \\times 0-1=-1$$ Reflection和Projection如下图： 上图的理解步骤和求解 $A^n$ 的过程类似，首先分解左侧乘数向量到以特征向量为基的一组基，所以经过A的变换后，一个特征向量上的长度($\\lambda=0$)为0，另一个则是1，就是一个分量没了，另一个保持不变，这就是projection的过程，类似的一个长度为-1，另一个不变的就是reflection，如果有不明白的同学可以那笔画一下，也就是上面的图形，也就差不多了。\n通过上面一些列推到可以得出，矩阵shift I ，特征值减1，特征向量不变。或者可以理解为当矩阵们拥有相同的特征向量，特征值满足加法原则。\n下面说另外两个\u0026quot;代表\u0026quot;\n 特征值和特征向量之间有关系，当矩阵之间也有关系的时候，比如R和P $R^2$ 的时候 $\\lambda^2$  上面这些性质也好，计算也好，看起来都是没什么体系的，也就说按照我们的传统思想，首先下定义，然后学公式，然后做计算，最后老师如果还有时间可能给你来两道应用题，但是美国感觉更感性一些，先从外围把一些有意思的东西先写一下，这样看起来的问题就是没有我们上面那个套路那么规范，但是一旦我们把后面的东西学完了，就会发现，这些东西可以连起来的。\n特征值方程 接下来就是“正规操作”的计算了，首先我们需要说一下，我们第六章基本每个计算的第一步都是算$Ax=\\lambda x$ ,我们要研究的就是 $(A-\\lambda)x=0$ 也就是 $(A-\\lambda I)$ 的Nullspace，当我们求出 $\\lambda$ 就可以很轻松的求出对应的eigenvector。前面我们说过为了得到非零eigenvector，我们的目标矩阵Nullspace必须存在非零向量，也就是矩阵必须不可逆，也就是 $(A-\\lambda I)$ 必须是奇异矩阵。那么就是 $det(A-\\lambda I)=0$ 接下来就是解多次方成了，请注意，这里解的方程和我们之前研究的方程组有本质区别，之前研究的 $Ax=b$ 是多元一次方程组，线性，但是我们现在研究的是一元多次方程，所以这个解和前面的解不同，多次方程可能会有重复的解，甚至复数解，这些前面提到一些，但是后面几篇会详细讲解。\n 定义 特征值方程:$det(A-\\lambda)=0$\n 对于$n\\times n$的矩阵determinant形成的多项式最多n次，所以应该共有n个特征值，这些特征值可以重复，每个特征值指向一个特征向量。\n举个计算的例子，我其实在博客里很少举例子，但是这个例子可以帮助我们了解下求eigenvalue的过程： $$ A=\\begin{bmatrix}1\u0026amp;2\\2\u0026amp;4\\end{bmatrix} $$ 矩阵奇异，所以0必然是一个特征值，另一个特征值是5，怎么算出来的，当然后面有些其他方法，我们现在就按照equation of eigenvalue来算就行 $$ A-\\lambda I=\\begin{bmatrix}1-\\lambda\u0026amp;2\\2\u0026amp;4-\\lambda\\end{bmatrix}\\ det(A-\\lambda I)=(1-\\lambda)(4-\\lambda)-4=0\\ \\lambda^2-5\\lambda=0\\ \\lambda_1=0\\ \\lambda_2=5 $$ 接下来求特征向量: $$ (A-0 I)x=0\\ \\begin{bmatrix}1\u0026amp;2\\2\u0026amp;4\\end{bmatrix}x=0\\ x=\\begin{bmatrix}2\\newline -1\\end{bmatrix}\\ $$ 然后是另一个： $$ (A-5 I)x=0\\ \\begin{bmatrix}-4\u0026amp;2\\2\u0026amp;-1\\end{bmatrix}x=0\\ x=\\begin{bmatrix}1\\2\\end{bmatrix}\\ $$ 从求解可以看出，特征向量是可以伸缩的，也就对应于一个特征值，特征向量可以求出来无数个，但是都在一个方向上，根据子空间也能得出，2x2的矩阵有一个pivot那么他的行空间列空间等于rank=1，那么他的列空间，行空间维度是1，所以nullspace也是1，也就是一条直线，也就是特征向量所在空间只有1维，即一条直线QED\n来个大家喜欢的，下面是解题的过程\n Compute $det(A-\\lambda I)$ Find the roots of this polynomial Solve $det(A-\\lambda I)x=0$ to find an eigenvector x  上面说特征向量是在一个方向上所有向量，那么为了统一，我们可以把这些向量归一化，得到单位的特征向量，Matlab里面是这么做的。\nWarning：这里我们说过特征值可能重复，与之对应，特征向量也可能重复，那么就会产生缺维的现象，也就是特征向量没办法span出矩阵的全部维度，所以有些向量没办法用特征向量组成的基来表示，这类矩阵也没办法对角化\n一个悬崖边写了个\u0026quot;warning，前面是悬崖\u0026quot;，程序员都摔死了。\n好消息，坏消息(Good News,Bad News) “坏消息是我们迷路了，以后只能靠吃牛粪为生，好消息是牛粪有的是”\n坏消息是特征值和我们之前讲的消元啊，什么的基本没什么关系，当我们进行elimination的时候特征值改变了，所以pivot一般和特征值也没啥关系（三角矩阵有关系） 好消息是，特征值跟矩阵的其他一些值有关系：\n n个特征值的乘积等于矩阵行列式的值 n个特征值的和等于矩阵的Trace $$ trace=\\sum_{i=1}^{n}a_{ii} $$ 就是矩阵没变换的时候，对角线上的值的和，叫做trace。 上面这两条好消息可以帮助我们在2x2的时候看出来特征值，但是当矩阵维度扩大的时候计算无法避免，但是这两条规则可以帮助我们验算结果是否正确。  上面这些得出一个结论，这一章开始的线性代数已经跟前面没啥关系了，pivot和eigenvalue毫无关系了，而且我们本章的核心 $Ax=\\lambda x$ 都不是线性的\n对角矩阵的特征值就是，对角线上的元素，为什么，因为行列式的其他项都是零呀，自己找个纸写一下，不会留言，我教你。\n复数特征值 复数特征值，多次方程出现复数解非常正常，这里还要举个计算的例子： $$ Q=\\begin{bmatrix}0\u0026amp;-1\\newline 1\u0026amp;0\\end{bmatrix} $$ 根据方程求出$\\lambda_1=-i$ 另一个特征值 $\\lambda_2=i$ 检验$trace=i-i=0$ , $determinant=\\lambda_1\\lambda_2=-1$ 结果正确 计算特征向量: $$ x_1=\\begin{bmatrix}1\\i\\end{bmatrix}\\ x_2=\\begin{bmatrix}i\\1\\end{bmatrix} $$ 纯实数矩阵得到复数特征值特征向量，这事以后会经常出现，但是这个特征值和特征向量也很特殊，并且有些特殊的性质（特殊的特征值对应特殊的性质，这个也是特征值重要的一个原因哦！前面说到过）\n Q是正交矩阵，所以他的所有特征值的绝对值是1,即 $|\\lambda|=1$ Q是反对称的矩阵，所以特征值是纯虚数  解释下对称矩阵 $A=A^T$ 是对称矩阵， $-A=A^T$ 是反对称矩阵 上面这些对应的性质都不是巧合，后面有专门的定理来证明这些巧合\nEigshow in Matlab 这段是讲Matlab中的一个demo ，描述2x2的矩阵的特征值的特征向量一些变化，等我安装个matlab然后录个视频看看效果，如果有补充再来写，所以这段： 略！\nConclusion 这篇写了两天，也算是良心之做了，可能有点啰嗦有点乱，因为水平不到位，不能很好的判断哪些重要哪些次要，所以每一个知识点都反复说，宁缺毋滥，所以各位海涵吧，明天继续。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-6-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 线性代数重点，关于矩阵特征值特征向量的相关知识第一篇文章，简单介绍特征值\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Eigenvalues，Eigenvectors，Sigular，Markov matrix，Trace，Imaginary Eigenvalues\u003c/p\u003e","title":"【线性代数】6-1:特征值介绍(Introduction to Eigenvalues)"},{"content":"Abstract: 集合论基础知识,样本集合 Keywords: Sample set\n样本集合 集合论各种数学教材上都有提到，每次都是，“为了理解xx内容，需要一些集合论的基础知识”，比如数学分析，概率论，拓扑学，这几个分支的基础教材一开始都是讲集合论的，我们画的图也把集合论放在了根部的位置 但是要透彻的研究集合论需要数学专业的大牛们继续，我们是研究计算机的，所以有些东西必须要放掉，所以我们这里讲的集合论内容非常基础，而且可能也不那么专业，没办法，我们只能专业于一点，每个点都专业的，目前大部分人做不到。 说到这就想起来了，我们讲的所有这些数学知识都是为了我们的机器学习算法，人工智能算法研究做基础，所以这些数学知识相当之浅显，只推荐CS专业的学生们参考，我尽量写的通俗准确一些，但可能某些话还是不够准确，希望大家多评论，多探讨。 本篇博客主要内容来自钟开莱先生的《初等概率论》，主要研究路径：集合-性质-基础运算-定律法则-高级计算\nSample sets 这个上来就有点偏概率了，因为标题可以翻译为样本集合，说到样本就跟数理统计直接联系到一起了，但是这都无所谓，接下来全是集合的知识了，首先什么是集合，一个班级有3个人-张三，李四，王二麻子-那么这三个人就构成了一个集合，可以用个大括号括起来： $$ {张三，李四，王二麻子} $$ 是的，上面这就是集合，集合就是一些东西集合在一起，就是集合😆，如果想知道集合的准确数学定义，那么可以自己google一下，其实集合论和概率论非常类似，都是先有了实际应用的定义，后面才有了公理化的定义，我们这篇文章应该说的是朴素集合论，创始人康托尔，于是我想起来了《万万没想到》里的康托尔-伯恩斯坦-施罗德定理，集合论是概率论的重要基础，非常非常重要的基础。 再举个例子，所有素数，这也是个集合，或者一个函数$f(x)$ 的定义域$(a,b)$ 也是一个集合。 那么说完了什么是集合,我们就要研究一下他的性质了：\n 一个东西不能被包含超过一次，比如 ${张三，李四，王二麻子,李四}$ 这就不是一个集合了 我们把集合中的每一个member称为一个point(点)，整个集合叫做space(空间)，为了体现是统计概率类里面的集合，加个修饰词“样本”这样就变成了样本点，和样本空间，这样就更概率了， 常用的表示方法，样本空间 $\\Omega$ 样本点 $\\omega$ ,空集 $\\emptyset$ 表示集合中没有元素，集合S中元素的个数 $|S|$ 是一个非负整数，从0开始哦，因为 $|\\emptyset|=0$ 集合必须被well define，也就是说，我们可以明确的知道一个东西是否属于该集合，不能存在模棱两可的元素，于是，这个地方就可能出现悖论，当元素属于集合的时候我们用 $\\omega \\in S$ 或者反之 $\\omega \\notin S$ 关于上面4中提到的明确定义，这个可能会有人来扯皮，比如我们扔三个骰子，可能出现很多组合，但是有人会跟你扯淡，可能一个扔出去以后你找不到了，就变成两个骰子了，哈哈，这种人你让他开门出去，滚蛋就好了，如果数学问题像这样讨论起来就没意思了。 Well define的方法有两种，一种是穷举-enumerate，或者discribed，比如 $S={1,2,3}$ 这个是个很明确的定义，$4 \\notin S$ ,或者 ${x|0\u0026lt;x\u0026lt;4;and;x\\in \\Re }$ 这两种描述方法都是正确的，但要根据情况使用，比如扔6个骰子的组合结果就能写好长，这时候就要用描述的定义方法了 子集，subset，对应的是superset，如果集合B中元素都在集合A中那么集合B是集合A的subset，A集合是B集合的superset，表示为 $B\\subset A$ 或者 $A\\supset B$ identical，两个集合相等的前提是两个集合中所有元素都相等(集合中顺序一般不被考虑，可以颠倒顺序，因为set这个词本身是一堆的意思，并不是排成一条线，有先有后)相等带来了一个性质 集合A B满足: when $A=B$ we must have $B\\subset A$ and $B\\supset A$ ;两个集合互为子集(超集)时，两个集合identical，这个性质也可以用来证明集合相等，充分必要条件，而且经常被用来证明两个集合相等。  以上就是集合的几点主要性质，研究完性质后，就要研究研究运算了。\n总结 集合基本性质，后面就是运算的相关知识了。\n","permalink":"https://go.face2ai.com/math/math-set-theory-1-sample-sets.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 集合论基础知识,样本集合\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Sample set\u003c/p\u003e","title":"【集合论】 样本集合"},{"content":"Abstract: 关于概率论的整体框架 Keywords: 概率论\n概率论总览 本篇可能都是废话（没有具体知识点），但是先说点废话中的废话，还是吐槽自己之前上学没有好好学习的吧，说实话，概率论和数理统计基本上学学到的知识等于几个名词，我认为当讲授一门课程的时候，作为讲授者，必须对这套知识背后的理论逻辑，以及实际应用有充分的理解和经验，不然没办法把这套完整的数学分支在几十节课的时间让一群完全没有先验知识的人正确的入门，并走上正确的方向，所以入门传道者一定要非常资深，因为如果你入门就走歪了，后面更麻烦，所以我觉得我很幸运，老子还是一张白纸，哈哈哈。 在一个从应用角度，机器学习用处非常多，有一类用户只负责调用工具或者封装好的算法进行一些既定工作，我们把他们定义为强应用用户，他们不关心算法正确性，速度以及结果，只是按照指定的步骤，使用工具；第二类，弱应用用户，负责找出合适问题的算法，并把使用方式告诉第一类用户，这类用户就需要知识或者经验了，可以没有知识，但要有经验，或者没有经验，只有知识，通过大量的实验找出合适的算法；第三类，进入到算法层，理解算法背后的数学逻辑，以及算法失效的情况，以及速度等底层问题，这类人需要较好的数学基础，因为在阅读论文的时候会有很多基础处理方法，作者会略过，所以掌握好基础很重要。 这三类人没有歧视链，大家都是干工作没有高低贵贱，但是如果你是第一类人但讲的都是第三类的话，而且漏洞百出，这样就贻笑大方了。\n概率论 概率论不是数理统计，这两个有明显的区别，概率论是已经知道了内部机制，推算结果，数理统计是通过观察结果反推内部机制，机器学习更倾向于后者，但是概率论是数理统计的基础，所以概率论在我们的big big picture里面是五星的，与微积分线性代数同样重要。 与线性代数和微积分不同，微积分更注重计算，也就是当我们有一个算式的时候可以用各种技巧得出最后的答案，这里的微积分是说初级的，高级的到了分析层面就是另一回事了，那个就是探索真理了，线性代数背后有一套完整的理论体系，而且包含了一些可以应用在实际场景的模型，所以线性代数属于基础与实践的边缘部分，故而在工程里显得尤为有用处。 概率论则更加偏向应用，因为其提出就是为了赌博，概率论的公式都比较复杂，但是每一个公式背后都有非常明显的事物关系，也就是说概率论中的公式能清楚的反应一些事物的本来面目。\n教材 《Probability and Statistics》 M.H.DeGroot 《Probability and Statistics(Fourth Edition)》(Morris H. DeGroot)这本书也很适合入门，书中有大量的例题和课后习题，语言也比较清晰明确，美国教材和中国教材以及俄罗斯前苏联的教材风格迥异，感觉美利坚的教材能让你更有自信，苏联教材能让你怀疑人生，而大部分中国教材能让你考上研究生，哈哈，不黑了\n《概率论与数理统计》陈希孺 条理非常清晰，能很清楚的解释公式背后的理论和应用，作为DeGroot的辅助，来完善整体知识结构。\n《初等概率论》 钟开莱 钟开莱是概率论开天辟地的人之一，而其写的书也是很简单明了，利于入门学习，还有一本《概率论教程》更适合进阶，涉及到了测度论等高级的分析工具。\n《概率导论》 Dimitri P. Bertsekas MIT的教材，比较简单，适合入门，也是作为补充，来辅助我们入门\n博客章节目录  Introduction Conditional Probability Random Variables and Distributions Expectation Speacial Distributions Large Random Samples  总结 先放个图 这个是根据DeGroot的教材总结出的基本知识重点框架。 机器学习相关数学知识可以参考项目,以上图片皆出自此项目： https://github.com/Tony-Tan/MachineLearningMath\n全部文章目录：\n 1.0 概率介绍、试验、事件、公理化的概率 1.1 样本空间、柯氏公理、概率的性质 1.2 古典概率、乘法原理、排列 1.3 组合、二项式定理、多项式定理 1.4 有限事件并的概率、概率欺骗了你 2.1 条件概率、全概率公式 2.2 事件独立、条件独立 2.3 Bayes’ Theorem 3.1 随机变量和离散分布 3.2 连续分布 3.3 Cumulative Distribution Function 3.4 双变量分布 3.5 边缘分布不和独立随机变量 3.6 条件分布 (Part I) 3.6 条件分布 (Part II) 3.7 多变量分布（Part I） 3.7 多变量分布（Part II） 3.8 随机变量的函数 3.9 多随机变量的函数 4.1 随机变量的期望 (Part I) 4.1 随机变量的期望 (Part II) 4.2 期望的性质 4.3 方差 4.4 距 4.5 均值和中值 4.6 协方差和相关性 4.7 条件期望 5.1 分布介绍 5.2 伯努利和二项分布 5.3 超几何分布 5.4 泊松分布 5.5 负二项分布 5.6 正态分布(Part I) 5.6 正态分布(Part II) 5.6 正态分布(Part III) 5.7 Gamma分布(Part I) 5.7 Gamma分布(Part II) 5.8 Beta分布 5.9 多项式分布 5.10 二维正态分布 6.1 大样本介绍 6.2 大数定理 6.3 中心极限定理 6.4 连续性修正  ","permalink":"https://go.face2ai.com/math/math-probability-big-picture.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 关于概率论的整体框架\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 概率论\u003c/p\u003e","title":"【概率论】 概率论总览"},{"content":"Abstract: 本文主要介绍行列式的应用，包括求逆，求面积，求体积，以及叉乘的一些性质 Keywords: Inverses，Cramer\u0026rsquo;s Rule，Volumes，Determinant，Cross Product\n克莱姆法则，逆和体积 废话已经变成每篇的例行公事了，不过我们还是来嘲笑一下Apple这个“垃圾”公司，憋了三五年搞出来个iPhone x，连个双胞胎都识别不出来，我们国内的各大小厂商随便搞个平面摄像头就搞定的简单任务，apple这么大个公司，用了三维图像都搞不出来，被各大网友嘲笑，其实之前好多VC都问我：“你这个识别双胞胎行不行”，我说，“No”，然后大哥语重心长的对我说“别人xx都能识别，你这个技术不到位啊”，以上对话真实存在，而且发生了好多次，后来我们的宣传口号就是\u0026quot;我们的摄像头亩产1亿斤小麦\u0026quot;，哈哈哈。希望业界技术能不断推陈出新，不断再创新高，也祝给为VC投资都有回报，祝那些双胞胎人脸准确率继续攀升。\n克莱姆法则 Cramer应该是行列式研究比较关键的一个人，但绝对不是第一个人，他应该是把行列式单独出来研究的数学家，但是最一开始用行列式解方程的可能是莱布尼兹，所以行列式发明伊始毫无疑问是用来解方程的。Cramer法则也是用来解方程的，顺便也能求个逆什么的。\n$Ax=b$ $Ax=b$ 我们已经研究了有一段时间了，但是我们今天还要继续通过研究旧的知识来得到新的知识，其实数学知识体系应该就是这样的，一开始有几个公理，然后逐渐通过推导，证明， 定义，引申，出来一个完整的数学体系，读陶哲轩的《analysis》和陈希孺的《概率与数理统计》都给人一种这个感觉，很简单的几个公理，能推导出一些列非常惊艳的理论和体系，然后经过我们专家们的努力，变成了各种难度的考试题。 Key Idea: $$ \\begin{bmatrix} \u0026amp;\u0026amp;\\\u0026amp;A\u0026amp;\\\u0026amp;\u0026amp; \\end{bmatrix} \\begin{bmatrix} x_1\u0026amp;0\u0026amp;0\\x_2\u0026amp;1\u0026amp;0\\x_3\u0026amp;0\u0026amp;1 \\end{bmatrix}= \\begin{bmatrix} b_1\u0026amp;a_{12}\u0026amp;a_{13}\\b_2\u0026amp;a_{22}\u0026amp;a_{23}\\b_3\u0026amp;a_{32}\u0026amp;a_{33} \\end{bmatrix}=B_1 $$ 这个大家应该都理解了，如果按照列空间的模式来看，就是 $\\vec{B}$ 被A矩阵射到列空间，然后为了和谐将 $\\begin{bmatrix}0\\1\\0\\end{bmatrix}$ 和 $\\begin{bmatrix}0\\0\\1\\end{bmatrix}$ 陪射到列空间，然后拼起来就是个矩阵乘矩阵等于矩阵了，这样有个非常不错的效果就是 $\\begin{bmatrix}x_1\u0026amp;0\u0026amp;0\\x_2\u0026amp;1\u0026amp;0\\x_3\u0026amp;0\u0026amp;1\\end{bmatrix}$ 这货的行列式是 $x_1$ ,那就可以了，利用行列式的性质，两边去行列式就有了 $$ det(A)det(\\begin{bmatrix}x_1\u0026amp;0\u0026amp;0\\x_2\u0026amp;1\u0026amp;0\\x_3\u0026amp;0\u0026amp;1\\end{bmatrix}) =det(A)x_1\\ =det(\\begin{bmatrix}b_1\u0026amp;a_{12}\u0026amp;a_{13}\\b_2\u0026amp;a_{22}\u0026amp;a_{23}\\b_3\u0026amp;a_{32}\u0026amp;a_{33}\\end{bmatrix}) =det(B_1)\\ x_1=\\frac{det(B_1)}{det(A)} $$ 这样就有了 $x_1$ ; $x_2$ 类似： $$ det(A)det(\\begin{bmatrix}1\u0026amp;x_1\u0026amp;0\\0\u0026amp;x_2\u0026amp;0\\0\u0026amp;x_3\u0026amp;1\\end{bmatrix}) =det(A)x_2\\ =det(\\begin{bmatrix}a_{11}\u0026amp;b_1\u0026amp;a_{13}\\a_{21}\u0026amp;b_2\u0026amp;a_{23}\\a_{31}\u0026amp;b_3\u0026amp;a_{33}\\end{bmatrix}) =det(B_2)\\ x_2=\\frac{det(B_2)}{det(A)} $$ 按照这个方式可以完整的求出 $\\vec{x}$ ,这个就Cramer\u0026rsquo;s Rule的思想描述，前提是det(A)不等于0，完整描述： $$ x_1=\\frac{det(B_1)}{det(A)}\\ x_2=\\frac{det(B_2)}{det(A)}\\ \\vdots\\ x_n=\\frac{det(B_n)}{det(A)} $$ 观察下计算量： $O(det(A))=n!$ , $O(det(B_i))=n!$ (i=1,2\u0026hellip;n) 所以总计算量 $(n+1)!$ 如果n=10，计算量过百万次，并且飞速增长，于是得出结论，别管是计算机还是人，这个法则适合矩阵小的时候，大了不适合。\n$AA^{-1}=I$ 从上面的解方程可以很自然的引申到求逆，稍加改装就行了: $$ \\begin{bmatrix} \u0026amp;\u0026amp;\\\u0026amp;A\u0026amp;\\\u0026amp;\u0026amp; \\end{bmatrix} \\begin{bmatrix} x_{11}\u0026amp;x_{12}\u0026amp;x_{13}\\ x_{21}\u0026amp;x_{22}\u0026amp;x_{23}\\ x_{31}\u0026amp;x_{32}\u0026amp;x_{33} \\end{bmatrix}= \\begin{bmatrix} 1\u0026amp;0\u0026amp;0\\0\u0026amp;1\u0026amp;0\\0\u0026amp;0\u0026amp;1 \\end{bmatrix} $$ 这个改装的不明显？那好我再把它打碎一些： $$ \\begin{bmatrix} \u0026amp;\u0026amp;\\\u0026amp;A\u0026amp;\\\u0026amp;\u0026amp; \\end{bmatrix} \\begin{bmatrix} x_{11}\u0026amp;0\u0026amp;0\\ x_{21}\u0026amp;1\u0026amp;0\\ x_{31}\u0026amp;0\u0026amp;1 \\end{bmatrix}= \\begin{bmatrix} 1\u0026amp;a_{12}\u0026amp;a_{13}\\0\u0026amp;a_{22}\u0026amp;a_{23}\\0\u0026amp;a_{32}\u0026amp;a_{33} \\end{bmatrix}=B_1 $$ 怎么样，这样清晰了吧，我们还是用Cramer法则，得到的 $x_{11}=\\frac{det(I_1)}{det(A)}$ 这个就是Cramer法则的公式的带入，如果没看懂，请回到上一小节，接着我们分析下矩阵 $\\begin{bmatrix}1\u0026amp;a_{12}\u0026amp;a_{13}\\0\u0026amp;a_{22}\u0026amp;a_{23}\\0\u0026amp;a_{32}\u0026amp;a_{33}\\end{bmatrix}=I_1$ 这个矩阵厉害的地方在于他的行列式等于第一个元素1的cofactor，即 $det(B_1)=C_{11}=\\begin{vmatrix}a_{22}\u0026amp;a_{23}\\a_{32}\u0026amp;a_{33}\\end{vmatrix}$，厉害不厉害,那么 $x_{11}=\\frac{det(B_1)}{det(A)}$ 也就是 $x_{11}=\\frac{C_{11}}{det(A)}$ 同理可以推出 $x_{21}=\\frac{C_{12}}{det(A)}$ ,$x_{31}=\\frac{C_{13}}{det(A)}$ 那么这么继续下去（这里跳过了基本步骤，不明白可以回到Cramer 法则，一毛一样的过程），得到了$A^{-1}$ ： $$ A^{-1}{ij}=\\frac{C{ji}}{det(A)} $$ 这个就是矩阵求逆的公式，我记得我当时学的时候这个公式老师写在黑板上，然后就让我们套着这个公式做算数练习了(MDZZ)。。 上面的公式可以反过来证明一下： $$ \\begin{bmatrix} a_{11}\u0026amp;a_{12}\u0026amp;a_{13}\\ a_{21}\u0026amp;a_{22}\u0026amp;a_{23}\\ a_{31}\u0026amp;a_{32}\u0026amp;a_{33} \\end{bmatrix} \\begin{bmatrix} C_{11}\u0026amp;C_{12}\u0026amp;C_{13}\\ C_{21}\u0026amp;C_{22}\u0026amp;C_{23}\\ C_{31}\u0026amp;C_{32}\u0026amp;C_{33} \\end{bmatrix}= \\begin{bmatrix} det(A)\u0026amp;0\u0026amp;0\\ 0\u0026amp;det(A)\u0026amp;0\\ 0\u0026amp;0\u0026amp;det(A) \\end{bmatrix} $$ 这里等于det(A)的点我们就不算了，因为是公式，我们只算为0的点，比如 $\\begin{bmatrix}a_{21}\u0026amp;a_{22}\u0026amp;a_{23}\\end{bmatrix}$ 这个行向量，和 $\\begin{bmatrix}C_{11}\\C_{21}\\C_{31}\\end{bmatrix}$ 相乘 $a_{21}C{11}+a_{22}*C_{12}+a_{23}*C_{13}$ ，因为C也是从矩阵A构建出来的，我们给他来个大还原 $$ \\begin{bmatrix} a_{21}\u0026amp;a_{22}\u0026amp;a_{23}\\ a_{21}\u0026amp;a_{22}\u0026amp;a_{23}\\ a_{31}\u0026amp;a_{32}\u0026amp;a_{33} \\end{bmatrix} $$ 这个矩阵的行列式就是 $a_{21}C{11}+a_{22}*C_{12}+a_{23}*C_{13}$ 很明显更两行相等，就是说行列式值为0 一个不错的例题，如果cofactors都不是0，那么行列式一定非零么？答案当然是否定的，因为行列式最后是对cofactor加权求和的。\n三角形面积 三角行的面积底乘以高除以2，小学版本的面积，升级以后的就是类似于用正弦函数在已知一个角和这个角的两条边的情况下，$S=ab \\cdot sin(\\theta)$ 这个应该算是比较高级的，但是我们分析下这两种常用的公式，里面需要用到的信息是什么？底乘以高，这里明显有两个长度，但是高隐藏了一个角度就是90度这个角至关重要，第二个ab是两条边的长度，和一个角度；也就是说求三角形面积，两个长度和一个角度是必不可少的，或者你也可以知道三个长度，根据余弦定理可以求出角。 回顾下向量，向量有方向，有长度，那么两个向量是不是就能求出三角行的面积？答案是肯定的，用前面的知识也很好求出，但是下面我们将学会一种很有趣的求法，没错，用行列式求面积： 如果一个三角形的三个顶点是 $(x_1,y_1)$ , $(x_2,y_2)$ , $(x_3,y_3)$ 那么三角形的面积是： $$ Area=\\begin{vmatrix} x_1\u0026amp;y_1\u0026amp;1\\x_2\u0026amp;y_2\u0026amp;1\\x_3\u0026amp;y_3\u0026amp;1 \\end{vmatrix} $$ 你可以把它记做公式或者怎么样，但是我觉得没必要，具体的证明？我不想写了，因为证明就是计算行列式，最后结果与其他方法算出来的相同，然后证毕。\n下面的证明是非常有趣的一种方法，没有计算，而是巧妙的利用行列式的性质。 我们定义行列式的时候并不是上来就给力计算公式，而是规定了三个性质，通过这三个基本性质逐步形成了行列式的其他性质以及计算公式，再推广到应用，那么我们可以继续这种方法来把三角形的面积推广到平行四边形，对于平行四边形面积的性质：\n A=I 的平行四边形，面积是1 交换任意两行，面积不变（绝对值），行列式绝对值不变，只是要变号 满足加法和乘法原理，如果一条边乘以2（也就是行列式中的一行），那么面积要乘以2 ，与行列式性质相同，如果变增长一部分相当于延长，在行列式上的直接反应也是增加了一个行列式（另一块的面积），具体如下图  如果不明白A是什么，A是像三角形那个行列式一样，各顶点组合出来的。\n这样的话就能证明行列式就是面积，具体是谁的面积，就是以各行为坐标，圈起来的图形的面积。 比如2x2的矩阵就是平行四边形的以原点为一个顶点，另外两个点是矩阵的行，推广到体积同样成立，\n这里就不再继续描述了，因为大道理就上面的三条性质。\n下面我们要正式回应一下本章开头提出的懵逼场景2(点击查看)，也就是二重积分转换成极坐标的那个带入问题，其实问题的根源在于对于$dA=dxdy$ 是一种简单的形式，或者叫做一个结果，他求的是矩形面积，忽略了其中角度的关系，原始的$dA$ 应该是一个行列式比如对于直角坐标系,对x和y求二重微分 $dx=1dx+0dy$ 以及 $dx=0dx+1dy$ 那么写成矩阵形式就是下面的样子 $$ dA= det(\\begin{bmatrix}dx\u0026amp;0\\0\u0026amp;dy\\end{bmatrix} \\begin{bmatrix} 1\u0026amp;0\\0\u0026amp;1 \\end{bmatrix}) $$ 也就是说我们要把坐标系的基矩阵也跟在后面，而前面是变量之间的微分关系（这个微分关系叫做雅克比矩阵）这个矩阵最后算出来的是一个伸缩比例，什么的伸缩比例？因为你已经把积分的单元改变了，之前那个dA和现在的dA不是一个同一个大小了，而是多了一个比例系数，以及两种不同积分元。 如果我们对积分变量进行代换，原变量为 $\\vec{x}=x_1,x_2,\\dots ,x_n$ 换元后的变量是 $\\vec{y}=y_1,y_2,\\dots ,y_n$ 并且满足 $x=Ay$ 那么 $d\\vec{x}=J(A)d\\vec{y}$ 雅克比矩阵J(A）： $$ J=\\begin{bmatrix} \\frac{\\partial x_1}{y_1}\u0026amp;\\dots\u0026amp;\\frac{\\partial x_1}{y_n}\\ \\frac{\\partial x_2}{y_1}\u0026amp;\\dots\u0026amp;\\frac{\\partial x_2}{y_n}\\ \u0026amp;\\vdots\u0026amp;\\ \\frac{\\partial x_n}{y_1}\u0026amp;\\dots\u0026amp;\\frac{\\partial x_n}{y_n} \\end{bmatrix} $$ 然后根据行列式性质，两边同时行列式，就得到了新的积分元了，雅克比行列式提供了一个伸缩比例 积分元的大小明显发生了变换，伸缩比例就是雅克比行列式的值。\n叉乘 Cross Product与dot Product相对应，其结果是个新的向量而不是一个值，Cross的定义： $$ u \\times v = \\begin{vmatrix} i\u0026amp;j\u0026amp;k\\ u_1\u0026amp;u_2\u0026amp;u_3\\ v_1\u0026amp;v_2\u0026amp;v_3 \\end{vmatrix} $$\n得到的矩阵和u和v都正交，证明？ 直接乘进去就有了\n$$ \\begin{vmatrix} i\u0026amp;j\u0026amp;k\\ u_1\u0026amp;u_2\u0026amp;u_3\\ v_1\u0026amp;v_2\u0026amp;v_3 \\end{vmatrix} =i C_{11}-j C_{12}+k C_{13} $$ 那么进行dot product就有:\n$$ \\langle C_{11},-C_{12},C_{13}\\rangle \\cdot \\langle u_1,u_2,u_3\\rangle = \\begin{vmatrix} u_1\u0026amp;u_2\u0026amp;u_3\\ u_1\u0026amp;u_2\u0026amp;u_3\\ v_1\u0026amp;v_2\u0026amp;v_3 \\end{vmatrix}=0 $$\n同理可以得到v，所以两个向量的cross product与这两个向量组成的平面垂直，方向满足右手定则（这个google下自己比划） 那么总结下Cross的性质\n $u \\times v=- v \\times u$ $u \\cdot (u \\times v)=0$ and so do $v \\cdot (u \\times v)=0$ $u \\times u=0$ 自杀式cross行为，自己cross自己，等于灰飞烟灭  根据上一节和这一节的对比，连个向量的cross等于这两个向量为边的平行四边形面积，也就是说 $$ |u \\times v| =|u||v|sin(\\theta) $$ 方向满足右手定则。\nTriple Product = Determinant = Volumes 推广到三维 $$ (u \\times v)\\cdot w= \\begin{vmatrix} w_1\u0026amp;w_2\u0026amp;w_3\\ u_1\u0026amp;u_2\u0026amp;u_3\\ v_1\u0026amp;v_2\u0026amp;v_3 \\end{vmatrix}=volume $$ 这个就是前面的简单推广，大家可以自己研究一下\n总结 这篇没想到这么长，从早上写到下午，有问题请大家指出。。明天继续。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-5-3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍行列式的应用，包括求逆，求面积，求体积，以及叉乘的一些性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Inverses，Cramer\u0026rsquo;s Rule，Volumes，Determinant，Cross Product\u003c/p\u003e","title":"【线性代数】5-3:克莱姆法则，逆和体积(Cramer's Rule,Inverses,and Volumes)"},{"content":"Abstract: 行列式的几种求法，以及相关的衍生问题 Keywords: Determinants，\u0026lsquo;Pivot Formula\u0026rsquo;，\u0026lsquo;Big Formula\u0026rsquo;，\u0026lsquo;Cofactors Formula\u0026rsquo;，Cofactors，Permutations\n置换和余因子 今天写的是行列式的三种计算方法，瞬间想到了孔乙己的茴香豆的四种写法，一个多少有点文化的人（被老师们解读为迂腐）却被一些没什么文化的人嘲笑挖苦；如果孔乙己是个那个时代的悲剧，那我们自己会不会成为这个时代的悲剧呢？读书无用论，某首富的“北大，清华大不如胆大”论，如果思维继续，结果最后肯定是喜闻乐见\n主值方程 Pivot的方式求行列式的值，Pro. Stang说这是matlab的做法，也就是计算机求行列式一般通过消元后得到Pivot，然后将所有Pivots相乘，得到行列式的值，这里有个主意的地方，我们反复强调，如果不是满rank的话，Pivot必然在某些行或者列里面不存在，那么这个矩阵是奇异矩阵，行列式值为0。 能够支持Pivot的乘积等于行列式的原因是上文关于properties 中Rule5 是消元的主要过程，rule5 告诉我们消元前后行列式的值不变，但是有的时候我们不光要消元还要进行行交换，这个是随机次数的，所以行列式的值等于Pivot乘积的前面正负号不明确，故: $$ det(A)=\\pm p_{11}p_{22}\\dots p_{nn} $$ 从另一个角度讲，如果把消元过程用矩阵方式表达 $PA=LU$ LU分解的矩阵形式，通过rule8 ，就能知道 $$ det(P)det(A)=det(L)det(U)\\ det(P)=\\pm 1\\ det(L)=1\\ det(A)=\\pm det(U) $$ 这样的话，U的对角线是由Pivot组成的，这个就是Pivot Formula的另一个切入点，都能证明行列式的pivot formula的正确性。 Pivot过程就是消元的过程，通过消元，得到行列式的值。 通过相乘的过程我们还能得到一个子矩阵的行列式，比如矩阵$A$的左上角的一块小的矩阵 $A\u0026rsquo;$ 他的行列式等于这个子矩阵覆盖的pivot的值（没有行变换） $$ det(A\u0026rsquo;)=p_{11}p_{22}\\dots p_{kk} \\ if , det(A\u0026rsquo;\u0026rsquo;)=p_{11}p_{22}\\dots p_{k-1k-1}\\ p_{kk}=\\frac{det(A\u0026rsquo;)}{det(A\u0026rsquo;\u0026rsquo;)} $$\n大方程 Pivot 形式的行列式求法计算量小，但是有个问题就是不直观，不是直接得到行列式的值，而是通过一个中间过程-Pivot-的消元过程，那么有没有直接的求法，没错，数学家们就是这么不满足于已有的形式，又发明了个直观，但是超级复杂的式子，这个式子有多少项呢，对于一个$n \\times n$ 的矩阵，big formula 有 $n!$ 项，这下怕了吧，11x11的矩阵有百万项，所以这个方法我觉得就学会3x3一下的矩阵就行。 那么这个big formula是怎么来的呢？我们可以回忆下，行列式是可以被打碎的： $$ det(A)= \\begin{vmatrix} a,b,c\\d,e,f\\g,h,i \\end{vmatrix}= \\begin{vmatrix} a,0,0\\d,e,f\\g,h,i \\end{vmatrix}+ \\begin{vmatrix} 0,b,0\\d,e,f\\g,h,i \\end{vmatrix}+ \\begin{vmatrix} 0,0,c\\d,e,f\\g,h,i \\end{vmatrix} $$ 我有点后悔了，因为选的有点大，所以下面只做分解后的第一项， $\\begin{vmatrix}a,0,0\\d,e,f\\g,h,i\\end{vmatrix}$ 继续分解: $$ \\begin{vmatrix} a,0,0\\ d,e,f\\ g,h,i \\end{vmatrix}= \\begin{vmatrix} a,0,0\\ d,0,0\\ g,h,i \\end{vmatrix}+ \\begin{vmatrix} a,0,0\\ 0,e,0\\ g,h,i \\end{vmatrix}+ \\begin{vmatrix} a,0,0\\ 0,0,f\\ g,h,i \\end{vmatrix} $$ 没错，每次分解都会产生n个分解结果，下面还是只分解第一项 $\\begin{vmatrix}a,0,0\\d,0,0\\g,h,i\\end{vmatrix}$ ，第二三项省略： $$ \\begin{vmatrix} a,0,0\\ d,0,0\\ g,h,i \\end{vmatrix}= \\begin{vmatrix} a,0,0\\ d,0,0\\ g,0,0 \\end{vmatrix}+ \\begin{vmatrix} a,0,0\\ d,0,0\\ 0,h,0 \\end{vmatrix}+ \\begin{vmatrix} a,0,0\\ d,0,0\\ 0,0,i \\end{vmatrix} $$ 分解出来是这个形状的，我们只做了下图中绿色的部分\n我们只做了27个行列式中的3个，故只做了1/9。但是这个分解的结果显示最后的行列式都为0，因为总有一列全是0，但是这种情况并不是所有人都是0，这里我不再继续分解别的行列式了，但是我给出最后不是0的行列式（下图中红色为非零行列式）\n所以我们可以得到，一定是每个行选出一个元素，每个列也只能选出一个元素组成的行列式才是非零的，那么这就是一个组合的问题，也就是说，第一行我们可以选择任意一列，故有n种方式，第二行只能有n-1列的选择余地，以此类推到最后一行没有自由度，只能选最后的一个，所以，有 $n!$ 个项，那么怎么求呢？\n$$ det(A)=\\ \\begin{vmatrix} a_{11}\u0026amp;\u0026amp;\\ \u0026amp;a_{22}\u0026amp;\\ \u0026amp;\u0026amp;a_{33} \\end{vmatrix}+ \\begin{vmatrix} a_{11}\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;a_{23}\\ \u0026amp;a_{32}\u0026amp; \\end{vmatrix}+ \\begin{vmatrix} \u0026amp;a_{12}\u0026amp;\\ a_{21}\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;a_{33} \\end{vmatrix}\\ +\\begin{vmatrix} \u0026amp;a_{12}\u0026amp;\\ \u0026amp;\u0026amp;a_{23}\\ a_{31}\u0026amp;\u0026amp; \\end{vmatrix}+ \\begin{vmatrix} \u0026amp;\u0026amp;a_{13}\\ \u0026amp;a_{22}\u0026amp;\\ a_{31}\u0026amp;\u0026amp; \\end{vmatrix}+ \\begin{vmatrix} \u0026amp;\u0026amp;a_{13}\\ a_{21}\u0026amp;\u0026amp;\\ \u0026amp;a_{32}\u0026amp; \\end{vmatrix}\\ =a_{11}a_{22}a_{33}\\begin{vmatrix} 1\u0026amp;\u0026amp;\\ \u0026amp;1\u0026amp;\\ \u0026amp;\u0026amp;1 \\end{vmatrix}+ a_{11}a_{23}a_{32}\\begin{vmatrix} 1\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;1\\ \u0026amp;1\u0026amp; \\end{vmatrix}+ a_{12}a_{21}a_{33}\\begin{vmatrix} \u0026amp;1\u0026amp;\\ 1\u0026amp;\u0026amp;\\ \u0026amp;\u0026amp;1 \\end{vmatrix}\\ +a_{12}a_{23}a_{31}\\begin{vmatrix} \u0026amp;1\u0026amp;\\ \u0026amp;\u0026amp;1\\ 1\u0026amp;\u0026amp; \\end{vmatrix}+ a_{13}a_{22}a_{31}\\begin{vmatrix} \u0026amp;\u0026amp;1\\ \u0026amp;1\u0026amp;\\ 1\u0026amp;\u0026amp; \\end{vmatrix}+ a_{13}a_{21}a_{32}\\begin{vmatrix} \u0026amp;\u0026amp;1\\ 1\u0026amp;\u0026amp;\\ \u0026amp;1\u0026amp; \\end{vmatrix} $$\n累死我了。。。这么多公式，最后总结就是选出来不是一行一列的元素相乘，再乘上对应的permutation的行列式值 $\\pm1$ ，最后大家加在一起，对于二维矩阵就是主对角线，减去另一条对角线。 $$ det(A)=\\sum det(P)a_{1\\alpha}a_{2\\alpha}\\dots a_{n\\alpha} $$ 够big的了，这个方法以后尽量不提了，出了三乘三一下的矩阵，谁用这个方法估计是脑子进水了。。再见\n代数余子式方程 说了不提上面的公式，但是还是要提一下，我们观察上面的第二幅图，其实每个非零分支都能看成是一个小的矩阵，这个小的矩阵去掉了上一步分解过程中被选中的元素所在的那一行，和那一列（因为后面所选出来的元素不能与之前的元素在同一行或者同一列，这样被首先选中的那个元素的所在行和列的所有元素没有存在的意义）剩下的行列组成了一个小一号的矩阵，也就是下一个分支（不会去包含选中元素的那行或者那列的所在分支），这个分支的求解也是按照原始规则，选择一个元素，并去掉这一行这一列所有元素得到一个更小一号的矩阵。 描述有点模糊，下面是算法过程\n 选择任意一个元素 去掉该元素所在行或者列的其他元素后形成一个小一号的矩阵 重复到1直到2中的小一号矩阵只有一个元素  这个迭代的过程就是big formula的一种算法实现，但是我们重新定义一下，小一号的那个矩阵的行列式我们称为 cofactor 所以公式就变成了 $$ det(A)=a_{i1}C_{i1}+a_{i2}C_{i2}+\\dots +a_{in}C_{i1n} $$ 这是按照行元素来划分的行列式，也可以按照列，或者按照任意方式，但是前提是这些a不可以在同一行或者同一列。 这些cofactor的符号怎么求？看big formula里面的说明吧，就是permutation行列式决定的。\n总结 这节计算课可以总结为pivot formula利用rule5 和 rule 7 就能推导出determinant的值和pivot乘积相等，从而可以通过消元elimination得到determinant，然后就是big formula的计算方法了，通过优化big formula 的过程就得到了cofactor的计算方法，同时得到了个cofactor的定义，明天继续。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-5-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 行列式的几种求法，以及相关的衍生问题\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Determinants，\u0026lsquo;Pivot Formula\u0026rsquo;，\u0026lsquo;Big Formula\u0026rsquo;，\u0026lsquo;Cofactors Formula\u0026rsquo;，Cofactors，Permutations\u003c/p\u003e","title":"【线性代数】5-2:置换和余因子(Permutations and Cofactors)"},{"content":"Abstract: 本文介绍矩阵的行列式相关性质 Keywords: Determinants，Properties of the Determinants\n行列式性质 行列式 懵逼场景1 啊。。行列式。。大学学线性代数第一课就是这个，各位老铁啊，你们知道一脸懵逼是什么样子么？我当时就是，这是个啥玩意？怎么就跑出来了，然后我清楚的记得，老师告诉我们线性代数就是解方程，但是各位啊这特么行列式跟方程有毛关系啊。\n懵逼场景2 前两天看MIT的多变量微积分，极坐标代换的时候,例如： $$ \\int \\int_R xdA=\\int \\int_R xdxdy\\ x=cos(\\theta)r\\ y=sin(\\theta)r\\ \\int \\int _R cos(\\theta)r \\cdot rdrd\\theta $$ 这是没错的过程，但是如果我们正常的带入一下好像不太对： $$ dx=-sin(\\theta)rd\\theta+cos(\\theta)dr\\ dy= cos(\\theta)rd\\theta+sin(\\theta)dr $$ 带入到原式： $$ \\int \\int_R cos(\\theta)r d(cos(\\theta)r)d(sin(\\theta)r)\\ =\\int \\int_R cos(\\theta)r \\cdot (-sin(\\theta)rd\\theta+cos(\\theta)dr) \\cdot (cos(\\theta)rd\\theta+sin(\\theta)dr)\\ =\\int \\int cos(\\theta)r(-sin(\\theta)cos(\\theta)r^2d^2\\theta+cos^2(\\theta)rdrd\\theta-sin^2(\\theta)rd\\theta dr+sin(\\theta)cos(\\theta)d^2r) $$ 什么情况，这个带入看起来没啥问题啊,但是这个结果肯定不对，二重积分可以简单看做是面积的求和，所以这个小面积有问题（就是被积分的部分有问题，具体的下面继续），微积分教材在这里引入了雅克比矩阵，因为啥他们没说就说要这么算，所以我们到线性代数上寻找一下结果\n行列式是个数字(Determinants is a Number) 行列式最后得到了一个数字，行列式应该称为一种运算，像加减法一样，结果是个数字，对于矩阵A， $det(A)$ 这个数字包含了很多关于矩阵A的很多信息，比如矩阵是否可逆，并且有一些有趣的几何物理这是计算行列的是用途，行列式具有很多性质，就像加法有交换和结合的性质一样，行列式的性质就是今天我们主要要写的内容，至于解决懵逼场景，学完这章，大家都应该豁然了。 求行列式的方法主要有三种:\n pivot formula :通过pivots这个是电脑的计算方法，通过消元(elimination)得到pivots，通过pivots的乘积得到determinant，如果矩阵rank小于行或列，矩阵是奇异的，这时候没有逆，determinant是0（因为pivots数量等于rank，小于行或者列，那么剩下的pivots位置是0所以乘起来就是0） big formula :哈哈哈，就是我大学课本上写的，就是通过一个比较复杂的公式给出计算方法 cofactor formula:代数余子式的方法  到目前为止，任何关于行列式长什么样我们都不知道，也不知道他能干啥，但是好像有很多不错的性质，所以Pro. Stang说，孩子们，直接看定义没什么意思，我们来通过性质来学习下吧。\n这种方式让我想到了陶哲轩大神的analysis（中文译：《实分析》）目前只看了第一章，通过皮阿诺公理来推到出自然数的加法乘法。。。可以充分的感受到这些大师们对我们小白的爱，他们洗的书有点“来吧，孩子，我来告诉你怎么来学会这些知识”，读起来比较友好。\n行列式的性质(Properties of the Determinant) 上面废话那么多，下面开始学术一点了，一下的性质中前三个是basic properties（rule 1 2 3），可以用前三个推导出后面的所有。 行列式的写法， $det(A)$ 或者 $|A|$ $$ det(\\begin{bmatrix}a\u0026amp;b\\c\u0026amp;d\\end{bmatrix})=\\begin{vmatrix}a\u0026amp;b\\c\u0026amp;d\\end{vmatrix} $$ 讲了大半天，终于知道行列式长什么样了，下面看性质了\n1.单位矩阵的行列式 $det(I)=1$ $$\\begin{vmatrix} 1\u0026amp;\u0026amp;\\ \u0026amp;\\ddots\u0026amp;\\ \u0026amp;\u0026amp;1 \\\\end{vmatrix}=1$$ 2.交换行的时候，行列式结果变换正负号 $$\\begin{vmatrix} a\u0026amp;b\\ c\u0026amp;d \\\\end{vmatrix}= -\\begin{vmatrix} c\u0026amp;d\\ a\u0026amp;b \\end{vmatrix} $$ eg. 结合rule1 和rule2 能够得到一个关于permutation matrix P的性质，因为P是单位矩阵I经过行变换来的，所以 $det(P)=1$ 如果做了偶数次行变换， $det(P)=-1$ 如果做了奇数次行变换 3.行列式对于矩阵中的每个行是线性的，注意这里说的是对矩阵中每个行是线性的，并不是对整个矩阵是线性的，比如最明显的: $ det(I+I)\\neq det(I)+det(I) $ , 这里的线性也表现在行乘法和行加法 $$ \\begin{vmatrix} ta\u0026amp;tb\\ c\u0026amp;d \\end{vmatrix}= t\\begin{vmatrix} a\u0026amp;b\\ c\u0026amp;d \\end{vmatrix}\\ \\begin{vmatrix} a+a\u0026rsquo;\u0026amp;b+b\u0026rsquo;\\ c\u0026amp;d \\end{vmatrix}= \\begin{vmatrix} a\u0026amp;b\\ c\u0026amp;d \\end{vmatrix}+ \\begin{vmatrix} a\u0026rsquo;\u0026amp;b\u0026rsquo;\\ c\u0026amp;d \\end{vmatrix} $$ 上面的性质跟面积和体积非常相似，比如你把一条边扩大t倍，体积也被扩大了t倍，并且单位矩阵的行列式等于1，这些都和体积（多于3维的我们也暂时叫体积吧） 上面三条已经能完全决定行列式的值了，到这我们就可以通过性质推到出 $n \\times n$ 的矩阵的行列式的值了，但是那就没啥意思了，继续推到其他性质可能会更有帮助（原文说直接推到行列式的结果有点难，我认为大致思路就是通过乘法和加法分解成 $det(I)$ 的线性组合） 4.行列式的两行相等，行列式的值是0 $$ \\begin{vmatrix}a\u0026amp;b\\a\u0026amp;b\\end{vmatrix}=0 $$ 证明方法很多，书上用到rule2，交换两行，变换符号后发现 $$ \\begin{vmatrix}a\u0026amp;b\\a\u0026amp;b\\end{vmatrix}=-\\begin{vmatrix}a\u0026amp;b\\a\u0026amp;b\\end{vmatrix} $$ 所以: $\\begin{vmatrix}a\u0026amp;b\\a\u0026amp;b\\end{vmatrix}=0$ 这里有个延伸，在Boolean Algebra中上述论证不成立，因为-1=1，啥意思？c++中的bool变量除了0其他任何值都是1，所以-1=1，这时候基础规则变成了1，2，4，这三条规则可以构建boolean algebra中行列式。 5.某一行减去另一行的倍数，行列式不变 $$ \\begin{vmatrix}a\u0026amp;b\\c-\\ell a\u0026amp;d-\\ell b\\end{vmatrix} =\\begin{vmatrix}a\u0026amp;b\\c\u0026amp;d\\end{vmatrix} $$ 证明过程: $$ \\begin{vmatrix}a\u0026amp;b\\c-\\ell a\u0026amp;d-\\ell b\\end{vmatrix} =\\begin{vmatrix}a\u0026amp;b\\c \u0026amp;d \\end{vmatrix}+ \\begin{vmatrix}a\u0026amp;b\\newline -\\ell a\u0026amp;-\\ell b\\end{vmatrix} =\\begin{vmatrix}a\u0026amp;b\\c \u0026amp;d \\end{vmatrix} -\\ell\\begin{vmatrix}a\u0026amp;b\\a\u0026amp;b\\end{vmatrix} =\\begin{vmatrix}a\u0026amp;b\\c\u0026amp;d\\end{vmatrix} $$ 使用到了rule3 ，rule4。 6.当行列式中有一行都是0的时候，行列式为0 $$ \\begin{vmatrix}0\u0026amp;0\\c\u0026amp;d\\end{vmatrix}=0\\ \\begin{vmatrix}a\u0026amp;b\\0\u0026amp;0\\end{vmatrix}=0 $$ 证明使用rule3 和rule4 可以轻松得到（把非0行加到0行，得到rule4的条件） 7.三角矩阵A， $det(A)=a_{11}a_{22}\\dots a_{nn}$ $$ \\begin{vmatrix}a\u0026amp;b\\0\u0026amp;d\\end{vmatrix}=ad\\ \\begin{vmatrix}a\u0026amp;0\\c\u0026amp;d\\end{vmatrix}=ad $$ 证明也不难，用rule5 能够得到一个对角矩阵（消掉b），再用rule3和rule1,就得到了结果 8.奇异矩阵的行列式为0，可逆矩阵的行列式不等于0\n证明前面应该提到了，从消元的角度来说，当一个矩阵的行列式是0证明其中有pivot不存在（在矩阵中用0填充）这样的话行列式存在全是0的行，所以行列式为0. 这里利用rule7加上消元就能得到行列式的一种算法： $$ \\begin{vmatrix}a\u0026amp;b\\c\u0026amp;d\\end{vmatrix}= \\begin{vmatrix}a\u0026amp;b\\c-a \\cdot \\frac{c}{a}\u0026amp;d-b \\cdot \\frac{c}{a}\\end{vmatrix}= \\begin{vmatrix}a\u0026amp;b\\0\u0026amp;d-b \\cdot \\frac{c}{a}\\end{vmatrix}=a(d-b \\cdot \\frac{c}{a})=ad-bc $$ 第一步使用rule5 ,然后是rule7 值得注意的是消元过程可能会有permutation的过程那么 $PA=LU$ 可以得到： $$ det(P)det(A)=det(L)det(U)\\ det(P)=\\pm 1\\ det(L)=1\\ det(A)=\\pm det(U) $$ 9.det(AB)=det(A)det(B)\n这个看起来好像挺正常，但证明确有些困难，证明2x2的矩阵都要算上一会儿，但是这时候Pro. Stang给出了个snappy的方式，证明：当 $det(B)\\neq 0$ 的情况下， $D(A)=\\frac{det(AB)}{det(B)}$ 满足rule1 rule2 rule3，那么D(A)就是det(A） (这个地方的逻辑我有点混乱，为啥通过他满足1，2，3就知道他一定是det(A)呢: 解答一下，想了一下想明白了，我们是怎么定义的determinant的？没有定义？没错，说到现在我们根本没说过行列式的定义，给出了表达形式，和8个性质，其中前3（4）个性质是basic，后面为衍生的，也就是说定义行列式从目前来讲是通过前三个性质来定义的，所以满足性质者就是（条件中矩阵的）行列式)\nproperty 1： 如果 $A=I$ 那么 $\\frac{det(B)}{det(B)}=1$ 看起来rule1 没什么问题 property 2： 如果A中的任意两行进行交换，那么根据矩阵乘法，AB的中两行也进行了交换，那么 $\\frac{det(AB)}{det(B)}$ 要变号，rule2 满足 property 3： 先看乘法，A其中一行乘以一个系数 $\\ell$ ，根据乘法法则AB的结果也在同一行乘以了 $\\ell$ ，那么乘法满足；继续看加法如果在A中的一行加上一个行向量那么： $$ \\begin{bmatrix} a+a\u0026rsquo;\u0026amp;b+b\u0026rsquo;\\ c\u0026amp;d \\end{bmatrix} \\begin{bmatrix} e\u0026amp;f\\ g\u0026amp;h \\end{bmatrix}= (\\begin{bmatrix} a\u0026amp;b\\ c\u0026amp;d \\end{bmatrix}+ \\begin{bmatrix} a\u0026rsquo;\u0026amp;b\u0026rsquo;\\ 0\u0026amp;0 \\end{bmatrix}) \\begin{bmatrix} e\u0026amp;f\\ g\u0026amp;h \\end{bmatrix}= \\begin{bmatrix} a\u0026amp;b\\ c\u0026amp;d \\end{bmatrix} \\begin{bmatrix} e\u0026amp;f\\ g\u0026amp;h \\end{bmatrix}+ \\begin{bmatrix} a\u0026rsquo;\u0026amp;b\u0026rsquo;\\ 0\u0026amp;0 \\end{bmatrix} \\begin{bmatrix} e\u0026amp;f\\ g\u0026amp;h \\end{bmatrix} $$ 这样来看 $\\begin{bmatrix}a\u0026amp;b\\c\u0026amp;d\\end{bmatrix}\\begin{bmatrix}e\u0026amp;f\\g\u0026amp;h\\end{bmatrix}$ 就是AB, $\\begin{bmatrix}a\u0026rsquo;\u0026amp;b\u0026rsquo;\\0\u0026amp;0\\end{bmatrix}\\begin{bmatrix}e\u0026amp;f\\g\u0026amp;h\\end{bmatrix}$ 只有第一行有数字，通过简单的计算得出，A中加上一行，AB中也加上了一行，满足property3（如果这个地方看着有点乱，那个笔算一下） 也就是上面给出证明对于矩阵A的一种计算D满足rule1，rule2，rule3，那么我们确定D是关于A的行列式操作，证毕 所以 $det(AB)=det(A)det(B)$ .并且衍生出了当 $B=A^{-1}$ 是 $AA^{-1}=I$ 那么 $det(A)det(A^{-1})=1$ 10.$det(A^T)=det(A)$ 矩阵的determinant对于矩阵的转置保持不变性， 证明过程如下： 对A进行分解：$PA=LU$ 两侧同时转置 $A^TP^T=U^TL^T$ 根据rule9： 原始分解形式 $det(P)det(A)=det(L)det(U)$ 转置后 $det(P^T)det(A^T)=det(L^T)det(U^T)$\n首先 $det(P)=det(P^T)=\\pm 1$ 其次 $det(L)=det(L^T)=1$ 对角线上全是1 第三 $det(U)=det(U^T)$ 对角矩阵，determinant就是对角线乘积，转置前后不变rule7； 所以 $det(A)=det(A^T)$ 证毕\nrule10，相当于把前面的rules扩大了一倍，因为所有的对行性质都能扩展到对列的了，比如一列都是0 的行列式结果是0\n总结 今天臭不要脸的把自己的博客发给了爱可可老师，结果爱老师还真就帮忙转发了，希望今天能看到留言，三个月了，一条留言都没有，太尴尬了。还有网友留言问有没有微积分和数学分析，概率论。哈哈。这都是我要写的，这样来说还是有人愿意学知识的，明天继续。。。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-5-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文介绍矩阵的行列式相关性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Determinants，Properties of the Determinants\u003c/p\u003e","title":"【线性代数】5-1:行列式性质(The Properties of Determinants)"},{"content":"Abstract: 通过将正交的向量组合成矩阵探索其中的一些有趣的性质和用途 Keywords: Orthogonal Matrix Q，Gram-Schmidt Algorithm，QR\n正交基和Gram算法 刚才去了趟医院，回来的路上看到了Alpha zero的项目的一些说明，从科研或者工程角度，我们必须对这个结果进行高度的肯定，肯定什么？首先是训练时间，足够短，对于原始的神经网络一训练就是一个月半年的情况已经得到了极大的优化，第二就是训练效果，能够提高那么多也说明无论是算法结构上来说，或者训练方法上一定有很大的改进（我没看论文，只看了一些报道，这里面强化学习得到了更多的关注）；这些对于业内人士都是好消息，第一我们可以用更少的计算资源来解决更大的问题，并且更加准确。 依靠神经网络来实现人工智能，可行与否现在其实还不好说，因为我们现在只是开发出了神经网络的一些比较厉害的结果，但是对智能是否能够起到准确的模仿和实现还有待开发，连发明者们都在怀疑。观众们却被媒体忽悠的热血沸腾，到行业来做应用和研发是不明智的，一天一旦这个东西被论证为不可以的话，那岂不是又要改行，就像当年那些心怀理想加入传呼机维修，塞班应用开发的人一样，他们现在都在干什么？🐶\n正交矩阵(Orthogonal Matrix)Q 如果我们有几个相互正交的向量，并且长度为1，把他们按列组合在一起，形成一个矩阵Q。这个矩阵就是一个正交矩阵（Orthogonal Matrix）。 这个矩阵拥有的第一个性质就是 $$ Q^TQ=I $$\n其转置和自己的乘积是单位矩阵，为什么？我们把Q展开看，如果你瞬间就明白为什么了，就跳过下面这一小段：\n$$ Q= \\begin{bmatrix} \\vdots\u0026amp;\\vdots\u0026amp;\\vdots\\ q_1\u0026amp;q_2\u0026amp;q_3\\ \\vdots\u0026amp;\\vdots\u0026amp;\\vdots \\end{bmatrix}\\ Q^TQ= \\begin{bmatrix} \\dots\u0026amp; q_1^T\u0026amp; \\dots \\ \\dots \u0026amp; q_2^T\u0026amp; \\dots \\ \\dots \u0026amp; q_3^T\u0026amp; \\dots \\ \\end{bmatrix} \\begin{bmatrix} \\vdots\u0026amp;\\vdots\u0026amp;\\vdots\\ q_1\u0026amp;q_2\u0026amp;q_3\\ \\vdots\u0026amp;\\vdots\u0026amp;\\vdots\\ \\end{bmatrix}= \\begin{bmatrix} q_1^Tq_1 \u0026amp; q_1^Tq_2 \u0026amp; q_1^Tq_3 \\ q_2^Tq_1 \u0026amp; q_2^Tq_2 \u0026amp; q_2^Tq_3 \\ q_3^Tq_1 \u0026amp; q_3^Tq_2 \u0026amp; q_3^Tq_3 \\end{bmatrix}= \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\n证毕，如果不太明白读一下前提，$q_1,q_2,q_3$ 相互正交，其dot product是0，并且其长度都是1，所以自己和自己的dot product是1。\n这个性质是比较重要和基础的，如果进行特异性，这个矩阵是方阵的时候，那么Q的逆就是他的转置，而且我们知道一个矩阵的逆既是左逆又是右逆，那么 $$ Q^TQ=I\\ QQ^T=I\\ $$ 举几个Q的例子，有些我们在前面都见过，\n 最简单的，$I=\\begin{bmatrix}1\u0026amp;0\u0026amp;0\\0\u0026amp;1\u0026amp;0\\0\u0026amp;0\u0026amp;1\\end{bmatrix}$ 这个家伙就是个不折不扣的Q，当然他也是I 同样的由1组成的矩阵， permutation也都是Q方阵，因为我们当时在学的时候就说，P可以把一个向量变形， $P^T$ 能帮我们把他变回来（这里的P不是上一节的projection matrix是permutation matrix）。 Rotation: $R=\\begin{bmatrix}cos\\theta \u0026amp; -sin\\theta\\ sin\\theta \u0026amp; cos \\theta\\end{bmatrix}$ 旋转矩阵，也是一个Q Reflection， 当 $\\vec{u}$ 是单位向量的话 $Q^T=I-2\\vec{u}\\vec{u}^T$ 是正交矩阵，并且 $Q^T=Q^{-1}=Q$  这是几个简单的例子，Q还有一个有点就是他不会改变被乘向量的长度， $|Qx|=|x|$这是个很好的性质，如果需要迭代多次，Q能保持稳定，我之前在哪本书里见过这个说法，在某个算法里，当时觉得好奇怪，也很神奇，现在大概知道就会觉得，这个太美妙了，具体为什么不会改变，下一小节会给出完整的证明。\n使用正交基映射(Projections Using Orthogonal Bases) 映射是上一篇（4-2）我们主要研究的内容，把结论拿出来给大家重复展示下，哒哒哒哒： $$ \\vec{p}=B\\hat{\\vec{x}}=\\frac{BB^T}{B^TB}\\vec{a} $$ 这个结果已经解释过了，但是动机要再复述一遍，我们为了把 $\\vec{a}$ 映射到B span的subspace里面，也就是B的 column space，我们说到过，一个子空间是通过basis来描述的，这个B里面的每一个主列（pivot column）就是一个基，这时我们有了一个想法，就是如果我们让这些basis互相正交且长度为1，那么我们将得到一个叫Q的B（orthogonal matrix），没错，这就把前面projection的内容成功的搞到了这一篇。 $$ \\vec{p}=Q\\hat{\\vec{x}}=\\frac{QQ^T}{Q^TQ}\\vec{a}\\ Q^TQ=I\\ so:, \\vec{p}=QQ^T\\vec{a}=Q(Q^T\\vec{a}) $$ 这个看着不玄乎，我们换个角度，拆开仔细看\n$$ \\begin{bmatrix} |\u0026amp;\u0026amp;|\\ q_1\u0026amp;\\dots\u0026amp;q_n\\ |\u0026amp;\u0026amp;| \\end{bmatrix} \\begin{bmatrix} q^T_1b\\ \\vdots\\ q^T_nb \\end{bmatrix}= q_1(q^T_1b)+\\dots+q_1(q^T_nb) $$ 这个看的就有点邪乎了 $q^Tb$是个标量，也就是说，如果把一个子空间用一组正交基表示，对子空间外的向量进行projection会得到一个以这组基为基的线性组合，这个重要的性质是傅里叶等数学中great transforms的基础，卧槽，怕不怕，线性代数居然整到了傅里叶，意外不意外，惊喜不惊喜。\nGram-schmidt 这个算法是为了给subspace换正交基的，“如果，你有了一组非正交的基，你是否也渴望有一组像隔壁老王一样的正交基了呢？告别计算烦恼，再也不用低三下四看人脸色，马上拿起电话联系Gram，不要998，只要198\u0026hellip;.” 编不下去了Gram主要是帮我做了一个算法，在已知一组非正交基的时候来找一组正交基，据Prof Strang课上讲，他也不清楚schmidt在这个算法里面做出了什么工作，但有他的名字，那么我们就当做他很重要吧， 这个算法的主要原理用到的还是projection，我们之前在projection中提到了力的分解，速度的分解，当时我就是随口一说，想起了自己的初中物理老师而已，但是这没想到在这里居然有用，假设，我们在一个三维空间里面有一个速度，我想把他分解到xy轴，这个大家都应该会，根据project to line 的方法都能得到分量，那么如果我们用原速度减去xy的分量得到的是什么？没错z轴的分量，不知道为什么？再提示你一下，你把xy分量的两个向量加起来，得到的是xy平面（子空间）的分量，对应的 $\\vec{e}$ 就是z轴分量（因为Z轴是xy子空间的 orthogonal complement）： 如果还是不太明白，没关系，往下看吧： Gram-Schmidt算法分为以下几步，我们先假设一组向量，尾巴在一起是原点，那么这些向量都指向不同的方向，并且他们线性独立，那么我们要调整这些向量，最原始的想法就是首先，我们固定一条向量，然后调整第二个跟第一个向量垂直，然后再迭代这个过程。直到所有的向量都就位，那么这样的正交基有多少组？没错，无数组。 官方描述：\n Step 1. choosing $A=\\vec{a}$\n  Step 2. First Gram-Schmidt step $B=\\vec{b}-\\frac{A^T\\vec{b}}{A^TA}A$\n  Step 3. Next Gram-Schmidt step $C=\\vec{c}-\\frac{A^T\\vec{c}}{A^TA}A-\\frac{B^T\\vec{c}}{B^TB}B$\n  Step n. iteration\n 来分析下，我们之前给出的简单思路应该算是算法最初的小种子，另一种描述是，算法的核心就是把当前要处理的向量减去之前已经做好正交的那组向量（这个向量处理之前的那些向量）的分量，也就是说要求未处理的向量不能在已处理向量空间里。这就是核心思想，别问我怎么知道的，书上就这么写的，和我们的小思路也基本一致，啦啦啦，也就这样了。\n$A=QR$ 矩阵分解从刚开始学到矩阵就开始讲各种各样的分解，LU，LDU这些都是以解方程组为背景的，应该属于我们的第一第二个阶段，现在我们都第三个阶段了，该说说QR分解了，那就说来话长了，那事是我刚来到深圳，前途事业一片渺茫（现在也渺茫），刚刚开始自学计算机视觉的这类知识，没有人指导，也不知道咋学，听说PCA很厉害，也不管有没有什么先验知识要学习，上来就用c语言写，查各种博客，就接触到了SVD（奇异值分解，我们后面会讲）用到了QR分解，具体的我们后面会详细讲述，但是现在回忆就是，没有人带你入门会很痛苦，你自己短时间没办法接入一个全新的或者是之前完全不了解的领域，但是一旦你自己学会了怎么入门，那么你入门其他领域应该也不会太难，弯路走了不少，但收获也会更多一些，所以我说自己是野路子，也经常告诉一些身边的小伙伴怎么快速入门，但是一旦你走了快速入门的路，你就相当于放弃了寻找入门方法的能力。 回过来继续说QR分解，上一小节介绍了Gram算法，其中有个很值得寻味的方法，就是我们只根据前面的结果和当前的输入来确定如何让这些向量正交，这就有一个很好的效果，例如\n 我们处理第0个向量（初始那一步）这个向量的得出只和自己有关，跟后面的向量们毛关系没有。 处理第一个向量，只和第0个向量和自己有关（如果不理解可以回看上面的公式） 第二个向量，只和第0，1个向量以及自己有关 。。。。 得到完整的正交向量组  看一下数学表达: $$ A= \\begin{bmatrix}\u0026amp;\u0026amp;\\a\u0026amp;b\u0026amp;c\\\u0026amp;\u0026amp;\\end{bmatrix}= \\begin{bmatrix}\u0026amp;\u0026amp;\\q_1\u0026amp;q_2\u0026amp;q_3\\\u0026amp;\u0026amp;\\end{bmatrix} \\begin{bmatrix} q_1^Ta\u0026amp;q_1^Tb\u0026amp;q_1^Tc\\ \u0026amp;q_2^Tb\u0026amp;q_2^Tc\\ \u0026amp;\u0026amp;q_3^Tc \\end{bmatrix} $$\n说实话这个过程有点不好懂，先不说为什么会形成上三角矩阵，就说为啥要分解就不太理解，首先QR分解肯定有在实际算法中的需要，需要利用Q的某些特殊性质，而一开始并不知道分解成Q和一个矩阵的形式时，这个矩阵会是三角矩阵，但是根据我们上面的描述，以及关于上面我们关于Q的描述:\n $$ \\begin{bmatrix} |\u0026amp;\u0026amp;|\\ q_1\u0026amp;\\dots\u0026amp;q_n\\ |\u0026amp;\u0026amp;| \\end{bmatrix} \\begin{bmatrix} q^T_1b\\ \\vdots\\ q^T_nb \\end{bmatrix}= q_1(q^T_1b)+\\dots+q_1(q^T_nb) $$\n  这个看的就有点邪乎了 $q^Tb$ 是个标量，也就是说，如果把一个子空间用一组正交基表示，对子空间外的向量进行projection会得到一个以这组基为基的线性组合，这个重要的性质是傅里叶等数学中great transforms的基础\n 我们把上面的QR矩阵还原一下，就能得和这段话呼应的一种形式了 $$ a=q_1q_1^Ta\\ b=q_1q_1^Ta+q_2q_2^Tb\\ c=q_1q_1^Tc+q_2q_2^Tc+q_3q_3^Tc $$ 必要的说明就是,如果结合力的分解那段，或者那张图会看的非常清楚，就是把当前向量映射到一组正交基中，在处理 $\\vec{a}$ 的时候正交矩阵中只有一个正交基 $q_1$,处理 $\\vec{b}$ 的时候正交矩阵中只有两个个正交基 $q_1$ $q_2$ 以此类推，这也是Gram算法实现的另一种神奇的效果会产生一个上三角矩阵R R也有一些神奇的性质，比如对角线上的元素始终都是正数，并且表示对应被分解向量的模长： $$ A= \\begin{bmatrix} 1\u0026amp;2\u0026amp;3\\ -1\u0026amp;0\u0026amp;-3\\ 0\u0026amp;-2\u0026amp;3 \\end{bmatrix} \\begin{bmatrix} 1/\\sqrt{2}\u0026amp;1/\\sqrt{6}\u0026amp;1/\\sqrt{3}\\newline -1/\\sqrt{2}\u0026amp;1/\\sqrt{6}\u0026amp;1/\\sqrt{3}\\newline 0\u0026amp;-2/\\sqrt{6}\u0026amp;1/\\sqrt{3} \\end{bmatrix} \\begin{bmatrix} \\sqrt{2}\u0026amp;\\sqrt{2}\u0026amp;\\sqrt{18}\\ 0\u0026amp;\\sqrt{6}\u0026amp;-\\sqrt{6}\\ 0\u0026amp;0\u0026amp;\\sqrt{3} \\end{bmatrix}=QR $$ R 的对角线元素等于A中每一个列向量的长度，神奇么?意外么？不意外，因为Q的一个性质是他不改变被乘向量的模长，前面证明过，\n在实际分解中并不是使用QR分解，而是有更快的计算方法，这个我们后面肯定还要讲，因为要研究计算方法和矩阵分析，这就留个引子吧，Gram算法的具体步骤就贴个书上的吧：\nConclusion 小结就是这一章的内容都好多，想写成短小精悍的都不行，因为写的少了根本解释不清楚，自己在梳理过程中也体会到了很多读书的时候没有察觉的内容，后面我们开始继续新的一章，如果有什么问题欢迎大家留言（到目前，还没有留言。。。）\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-4-4.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 通过将正交的向量组合成矩阵探索其中的一些有趣的性质和用途\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Orthogonal Matrix Q，Gram-Schmidt Algorithm，QR\u003c/p\u003e","title":"【线性代数】4-4:正交基和Gram算法(Orthogonal Bases and Gram-Schmidt)"},{"content":"Abstract: 从线性代数的角度理解计算最小二乘法，以及解释最小化误差的思想。介绍部分应用，包括曲线拟合等 Keywords: - Least Squares Approximations,Minimizing the Error,Fitting a Straight Linear\n说明 本文应该详细介绍从线性代数角度解释最小二乘法，但是，经过我仔细分析，这篇在整个线性代数体系里面算是应用，也就是说，即使不学这篇，也不影响整个知识体系的建立，但是在机器学习和优化中这个方法却是基础知识，所以我决定将本篇整合到后面的机器学习，或者优化的文章中，到时候可以结合多变量微积分最值求法以及其他理论一起从多个角度讨论最小二乘法，感谢各位理解。 连接：(还没写)[http://www.face2ai.com]\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-4-3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 从线性代数的角度理解计算最小二乘法，以及解释最小化误差的思想。介绍部分应用，包括曲线拟合等\n\u003cstrong\u003eKeywords:\u003c/strong\u003e - Least Squares Approximations,Minimizing the Error,Fitting a Straight Linear\u003c/p\u003e","title":"【线性代数】4-3:最小二乘近似(Least Squares Approximations)"},{"content":"Abstract: 本篇主要介绍的就是向量的映射，以映射到直线为引导，重点在于映射到子空间。 Keywords: Projections，Projection Onto a Subspace\n投影 映射，投影，感觉怎么翻译都不太对，总能想到函数，不过好像在这部分，投影矩阵和函数的功能非常类似。在典型的三维正交基向量空间内，一个向量的投影到一个平面上一般是下面这种形式： 向量b投影到xy平面，和b投影到z轴的一种几何上的反应，当然超过三维，就没办法画出来的，但是原理都一样，通过垂直（正交），将不在子空间的向量转换到子空间内最接近原始向量 $\\vec{b}$ 的投影向量 $\\hat{\\vec{b}}$来近似原始向量，这种方法在最小二乘法中得到了完美的应用，以及后面将要做的一些分解，上一篇提到的split（分解到子空间的split），都可以利用projection的原理。 继续解读上图，向量被分级到了正交的两个子空间，xy平面，和z轴，这两个子空间互为orthogonal complements，并且满足下面两种关系： $$ \\vec{p_1}+\\vec{p_2}=\\vec{b}\\ P_1+P_2=I $$ 第一个式子就是个典型的split，比如物理里面力的分解，速度分解，都是把向量分解到你想要的方向，然后我们把向量分解到orthogonal complements的子空间中，就得到了我们想要的projection 第二个式子是projection matrix之间的关系，这里可以轻易的看出来，映射到xy平面$P_1=\\begin{bmatrix}1\u0026amp;0\u0026amp;0\\0\u0026amp;1\u0026amp;0\\0\u0026amp;0\u0026amp;0\\end{bmatrix}$，同理到z轴的就是$P_2=\\begin{bmatrix}0\u0026amp;0\u0026amp;0\\0\u0026amp;0\u0026amp;0\\0\u0026amp;0\u0026amp;1\\end{bmatrix}$，可以看出$P_1+P_2=I$ 。\n映射到直线 点乘映射 把一个向量 $\\vec{b}$ 映射到一条直线$a$，等价于问题类似于把一个向量projection到另一个向量上，这个和我们之前学习的dot product有点像，如果 $$ when,|\\vec{i}|=1\\ \\vec{b} \\cdot \\vec{i}=|\\vec{b}||\\vec{i}|cos(\\theta)=|\\vec{b}|cos(\\theta) $$ 其中夹角就是下图中 $\\theta$\n可以看到映射到向量a上等价于和a方向上的单位向量dot product,假设 $\\vec{p}$ 是 $\\vec{b}$ 的投影结果，那么 $\\vec{i}=\\frac{\\vec{a}}{|\\vec{a}|}$ $$ |\\vec{p}|=|\\vec{b}|cos(\\theta)=\\vec{b} \\cdot \\vec{i}=\\vec{b} \\cdot \\frac{\\vec{a}}{|\\vec{a}|}\\ \\vec{p}=|\\vec{p}|\\vec{i}=|\\vec{p}|\\frac{\\vec{a}}{|\\vec{a}|}\\ so:\\ \\vec{p}=\\frac{\\vec{b}\\cdot\\vec{a}}{|\\vec{a}||\\vec{a}|}\\vec{a} $$ 把向量中的dot product都换成转置相乘的模式就得到了 $$ \\vec{p}=\\frac{\\vec{b}^T\\vec{a}}{\\vec{a}^T \\vec{a}}\\vec{a} $$ 没错，跟书上的推到方式不太一样，但是，从另一个角度得出了正确结论，其实这段不算困难，只是符号和向量长度的问题，但是由于在草稿纸上没写清楚，刚才有不得不重新推到了一边跟书上不一样是因为系数转置的原因，除法最后得出的是个数字，所以把它转置不影响结果： $$ \\vec{p}=(\\frac{\\vec{b}^T\\vec{a}}{\\vec{a}^T \\vec{a}})^T\\vec{a}=\\frac{\\vec{a}^T\\vec{b}}{\\vec{a}^T \\vec{a}}\\vec{a} $$ 这样就得出了和书上一样的结论，书上是通过正交得出的结论，在介绍书上的办法之前要说一个我刚发现的很基础的东西，就是向量连续乘法和矩阵（非向量矩阵）连续乘法不一样，矩阵连续乘法可以结合，向量不可以，比如我们用矩阵规模来解释一下 $$ ABC:(2 \\times 3)(3 \\times 4)(4 \\times 5)=(2 \\times 5)\\ (AB)C=A(BC)\\ $$ 但是对于向量： $$ \\vec{a}^T \\vec{b}\\vec{c}:(1 \\times n)(n \\times 1)(x \\times y)=(x \\times y) $$ 不可以结合，因为尺寸不满足需求，上面的变形中间就有这个问题，所以向量乘法结合律最好别用，用的话也自己搞好大家的尺寸，别最后对不上。\n和投影正交 不错这个才是正统书上的方法，推到相对上面还要简单一些主要用到了一个向量的差与投影结果$\\vec{p}$垂直这一关系，具体贴上原书两张图： 还是将 $\\vec{b}$ 投影到 $\\vec{a}$ ，假设得到的投影是 $\\hat{x}\\vec{a}$ ;可以得到一个差向量 $\\vec{e}$ （这个向量其实就是后面最小二乘法的误差，error的缩写）有些时候我们必须把向量近似到一个空间，以方便进一步计算，这时候希望得到误差最小的近似，误差就是这个 $\\vec{e}$ 的模长，并且这个 $\\vec{e}$ 和 $\\vec{a}$ 是垂直关系，也就是正交，那么就有了 $\\vec{a}^T(\\vec{b}-\\hat{x}\\vec{a})$=0,最后能求出投影长度 $\\hat{x}$, $$ \\vec{p} =\\hat{x}\\vec{a}=\\vec{a}\\hat{x} =\\vec{a}\\frac{\\vec{a}^T\\vec{b}}{\\vec{a}^T \\vec{a}} $$ 下图就是详细的集合描述，大家不要过度依赖图片，因为有时候想象力更重要，因为一旦超过四维，图就没有了，只能靠想象和逻辑推导了。\n根据上面公式可以得出投影矩阵 $$ \\vec{p} =P\\vec{b} =\\frac{\\vec{a}\\vec{a}^T}{\\vec{a}^T \\vec{a}}\\vec{b}\\ So:\\ P=\\frac{\\vec{a}\\vec{a}^T}{\\vec{a}^T \\vec{a}} $$\n这个投影矩阵是投影到向量a的，同理，其实可以投影到向量e所在的直线，因为 $\\vec{e}=\\vec{b}-\\vec{p}$ 所以可以得出下面等式： $$ \\vec{e}=\\vec{b}-P\\vec{b}=(I-P)\\vec{b} $$ 那么这样的话， $(I-P)$ 也是一个投影矩阵，投影到向量e所在直线的投影矩阵。\n小结 如果 $\\vec{a}$ 和 $\\vec{b}$ 在同一条直线上，那么可以得出 $P\\vec{b}=\\vec{b}$; 如果 $\\vec{a}$ 和 $\\vec{b}$ 垂直，那么$P\\vec{b}=0$; 这里的投影矩阵相当于一个函数映射，把一个矩阵垂直的映射到另一个向量，这个投影矩阵就是我们非常关心，也具有很多有趣性质的矩阵 P\n映射到子空间 上面说的映射到直线，用一个向量就能准确的描述出直线的方向，但是对于子空间，最好的描述方式就是basis，把basis作为列组合到一起，就是矩阵B，子空间就是B的列空间。这样更方便下面的计算，投影到子空间 $C(B)$ ,这次试用书中映射到直线的类比方法，假设 $\\vec{a}$ 向 $C(B)$ 投影得到向量 $\\vec{p}=B\\hat{x}$ ，这样我们就可以得到 $\\vec{e}$ 了,并且 $\\vec{e}$ 正交于子空间 $C(B)$ ，那么 $$ \\vec{e}= \\vec{a}-B\\hat{x}\\ (col(B)_i)^T\\vec{e}=0\\ B^T\\vec{e}=0 $$ B的每一列都是一个basis，向量e和每一个basis都正交，也就是B的任一一列，故 $B^T\\vec{e}=0$. 那么就可以按照上面直线的方法继续向下计算了 $$ B^T(a-B\\hat{x})=0\\ \\vec{p}=B\\hat{x}=\\frac{BB^T}{B^TB}\\vec{a}\\ P=\\frac{BB^T}{B^TB} $$ 思维过程和直线相似，就不在详细描述这里指的注意的是。\n subspace是B的列空间 向量e和列向量正交，根据上一篇的结论 $\\vec{e}=(\\vec{a}-B\\hat{x})\\in N(B^T)$ ,那么 $B^T(a-B\\hat{x})=0$ 这里面是涉及到了一个求逆过程 $B^TB$ 的逆，存在且仅存在于当B的所有列线性独立  解释：$B^TB$ 的逆，存在且仅存在于当B的所有列线性独立 证明，检查$B^TB$ 的nullspace $$ B^TB\\vec{x}=0\\ B^T(B\\vec{x})=0\\ for: , B^T\\neq 0\\ so: , B\\vec{x}=0 $$ 也就是说 $B^TB$ 的Nullspace和 $B$ 的 Nullspace是相同的，如果想可逆，那么nullspace必须只有0，所以B必须是线性独立的，证毕。 $B^TB$ 得到的是个小的方矩阵（对称，可逆），如果是 $BB^T$ 得到的是个大方阵，怎么搞的都不会可逆（但是依然对称），他的rank就B的rank。 另外P还有些小性质：\n $P^2=P$ $P^n=P$ distance from $\\vec{a}$ to subspace is $\\vec{e}$  总结 本来以为这篇会短点，结果还是这么长，可以得出很多有意思的结论也算有所值，今天到此为止，我们来热烈庆祝十九大顺利召开。。蛤蛤蛤。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-4-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本篇主要介绍的就是向量的映射，以映射到直线为引导，重点在于映射到子空间。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Projections，Projection Onto a Subspace\u003c/p\u003e","title":"【线性代数】4-2:投影(Porjections)"},{"content":"Abstract: 本篇介绍正交性，向量正交，矩阵正交，子空间正交 Keywords: Orthogonality,Four Subspace,Orthogonal Complements,Fundamental Theorem of Linear Algebra ,Combining Bases from Subspaces,Split\n四个正交子空间 正交 这个地方大师Gilbert写了关于$Ax$的三个境界：\n This is only a number It is combination of column vectors It shows Subspaces  这个跟王国维的人生三大境界有的一拼，这里必须要展示下我的文学功底了（其实是上高中抄别人作文学会的）\u0026ndash;\u0026ldquo;古今之成大事业、大学问者，必经过三种之境界：\u0026ldquo;昨夜西风凋碧树。独上高楼，望尽天涯路。\u0026ldquo;此第一境也。\u0026ldquo;衣带渐宽终不悔，为伊消得人憔悴。\u0026ldquo;此第二境也。\u0026ldquo;众里寻他千百度，蓦然回首，那人却在灯火阑珊处。\u0026ldquo;此第三境也。此等语皆非大词人不能道。然遽以此意解释诸词，恐为晏欧诸公所不许也。\u0026rdquo; \u0026quot; 差不多就这意思，对事物的追求是逐渐加深的，当我们走到了深处，木然回首，一看，线性代数也就那么回事。 不扯没用的，继续说正交（orthogonality） 正交的三个层次是\n 向量正交 矩阵正交 子空间正交  两个向量正交是说他们的dot product为0 $$ v^tw=0 ,, and ,, ||v||^2+||w||^2=||v+w||^2 $$ 前一个式子表明了位置关系，后面的距离表明了长度关系，当$v$和$w$是二维向量的话，这个也证明了平面勾股定理的正确性，当然，如果把勾股定理扩展到高维，也是成立的。 解释下垂直和正交的关系，垂直说的是相交直线间的角度关系，如果两个向量不想交，但是他们也可以有正交关系。 这里我们先略过矩阵正交，直接看子空间，因为矩阵正交在后面有个比较实用的分解 下面将要正式推出本文最最最重要的一个知识点，就是四个子空间的正交关系，子空间正交，就是说子空间A内的任意向量和子空间B内的所有向量全部互相正交。\n definition: $$ v^tw=0 ,,for,,all,,v,,in,,V,,and,,all,,w,,in,,W $$\n 这个条件看似挺严格，但是这个例子会让你豁然开朗：$Ax=0$ 这个是矩阵A的nullspace的表达，x属于nullspace，如果我们把A写的详细点($\\dots a_i\\dots$，表示矩阵中的一行),并且加入一个行向量 $r^t$（作用下面再说）: $$ r^tAx=r^t(Ax)= r^t\\begin{bmatrix} \\dots a_1\\dots\\ \\dots a_2\\dots \\ \\vdots\\ \\dots a_n\\dots \\end{bmatrix} \\begin{bmatrix} x_1\\ x_2\\ \\vdots\\ x_n \\end{bmatrix}= r^t\\begin{bmatrix} 0\\ 0\\ \\vdots\\ 0 \\end{bmatrix} $$ 所以 $$ r^tAx=0 $$ 那么这说明了什么呢？$r^tA$是什么呢？同学们，这可是A的行空间啊，x是nullspace，他俩乘在一起是0啊，说明对于任意这两个空间的内的向量的dot product都是0啊，亲人们，正交啊。 上面的完整推到过程只用到了$Ax=0$这个事实，也就是规定x属于Nullspace的前提。\n同样的道理可以推导出，列空间和左Nullspace也是正交的，那么我们就有另一个部分Fundamental Theorem了：\n   Fundamental Theorem Part II（不完整版）     The row space is perpendicular to the nullspace   The column space is perpendicular to the nullspace of $A^T$    Fundamental Theorem I 说的是四个subspaces之间的维度的相互关系，第二部分说的是四个subspaces之间的正交关系，看来线性代数的核心是这四个subspace没错了。\n正交互补(Orthogonal Complements) This is very important ,The Fundamental subspace are more than just orthogonal in pairs.Their dimansions are also right.说实话，right这个词我想了半天也不知道对应中文那个词，三维空间中的两条线正交，但是他们并不可能是一个属于$3\\times 3$矩阵的nullspace和rowspace因为他们都是dimension 1的加起来并不是3？你要问我为什么，往下看喽\n Definition:Orthogonal Complement of a subspace V contains every vector that is perpendicular to V.This orthogonal subsapce is denot $V^{\\perp}$(pronounced \u0026ldquo;V perp\u0026rdquo;)\n 通过上面的定义以及上面nullspace和rowspace正交的推到可以得出，row space的orthogonal Complement肯定是nullspace中所有vector，也就是说不存在一个向量，正交与rowspace却不属于nullspace。 证明非常日能够以，基本就是改改上面的那段推导。 反过来依然成立，如果以vector正交于nullspace，那么它一定属于rowspace，证明如下：\n如果一个vector 正交于nullspace，而不属于rowspace，那么把vector添加到矩阵A最下一行形成A\u0026rsquo;，那么$A\u0026rsquo;x=0$依然成立，也就是说nullspace不会改变，但是rowspace却增加了（rank增加了1），那么$r+(n-r)=n$将不再成立，所以vector一定属于rowspace\n各位，大招来了，本章，本书的重点：\n没错，就是之前上一张重点的图的一个信息补全，这里面值得注意的包括subspace之间的正交关系，以及dimension之间的互补关系，但要注意一下，其实在使用消元确定$Ax=b$解的时候可以得到nullsapce和columnspace之间的dimension互补关系，但是这里面把rowspace和nullspace写到一起主要还是正交的关系，而且向量长度相同n，加上rowspace和columnspace的dimension永远是一致的（rank），所以也就是这么放了（以上是我自己的理解）\n   Fundamental Theorem Part II（完整版）     $N(A)$ is the orthogonal complement of the row space $C(A^T)$ (in $R^n$)   $N(A^T)$ is the orthogonal complement of the row space $C(A)$ (in $R^m$)   解读：Fundamental Theorem Part I给出维度关系，Fundamental Theorem Part II给出垂直关系，complement时表示一个向量$x,in,R^n$总能分解到rowspace和nullspace两部分，而且根据两个subspace之间的dimension关系，可以确定，rowspace和nullspace加起来是完整的$R^n$空间（这个后面有讨论）   当$A(x_r+x_n)$时，奇迹出现了，还记得我们写Ax=b的无数个解的时候的完整解么?   $$ x=x_{particular}+x_n$$   和上面这种形式是对应的   矩阵A和向量相乘，可以有很多种解读，但是说到最根本的地方就是，$Ax$就是为了让x goes to column space，并没有其他什么更高级的功能。      通过上面那张图可以看出，任何一个n维向量可以被分解到rowspace和nullspace，然后通过A goes to column space和 0   这里最重要的是任何一个属于$R^n$的向量都能被分解到row space 和 null space    下面是我的幡然悔悟的一个非常重要的问题，为什么不是每个矩阵都有逆： 来仔细观察箭头，$x_r$-\u0026gt;$x_b$,是单射，没错，如果是单射就有逆运算，对于列空间的一个元素都有且只有一个行空间中的向量与之对应，但是由于不存在$A^{-1}$ 使得 $x_n=A^{-1}0$成立，也就是说不存在$A^{-1}$ 使得 $x=A^{-1}b$这也是为什么不是所有的矩阵都有逆，但是我们能确定一个 $\\hat{A}^{-1}$ 使得 $x_r=\\hat{A}^{-1}b$成立，这个戴帽子的A叫做pseudoinvers ，伪逆 这也告诉我并不是因为矩阵行列式为0他才没有逆，以前老师说，来算行列式，值是0矩阵没有逆，说的的确是真理，但是他一直也没说为啥。\n组合子空间基 这段补充说明下矩阵基bases的一些扩展，主要还是用到上面那张图，就是离这里最近的那张带映射的图，rowspace和nullspace是可以span整个$R^n$的，比如在rowspace中找到r个线性独立的bases加上nullspace里面n-r个线性独立的bases，他们就能span出整个空间，为啥？因为这两个空间里的bases肯定线性独立，人家都正交了，哪来线性关系。所以，任何一个n维向量都能备份家到rowspace和nullspace，如果nullspace只有0向量，恭喜，$x=x_r$，A可逆，Ax=b有唯一解。在$R^n$ 中任何n个线性独立的vectors必然能span出$R^n$，反过来说也对，同理写成矩阵形式，一个由n个线性独立的向量组成的$n \\times n$矩阵必然可逆。\n注意:任何一个n维向量都能被split成一个rowspace中的向量和一个nullspace中的向量\n总结 这篇内容实在是多到不行，而且都是精华，慢慢吸收，后面我如果发现问题可能还会补充，欢迎大家讨论\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-4-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本篇介绍正交性，向量正交，矩阵正交，子空间正交\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Orthogonality,Four Subspace,Orthogonal Complements,Fundamental Theorem of Linear Algebra ,Combining Bases from Subspaces,Split\u003c/p\u003e","title":"【线性代数】4-1:四个正交子空间(Orthogonality of the Four Subspace)"},{"content":"Abstract: In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity\n提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度\nKeywords: 人脸识别,FaceNet,GoogleNet\nFaceNet论文阅读 从老的wordpress转移过来，之前一年一直在做人脸识别算法研究，现在回头看看too young too simple，但是这些经历也算对自己的有很多帮助，认清自己的水平，知道自己什么方面比较差，这样也算是有所帮助\n Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches.\n 深度卷积网络直接优化embedding，而不是以往深度学习的连接层\n To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method\n 在训练过程中，我们使用粗略对齐的匹配和非匹配人脸区块的triplets，这些triplets是通过一个出色的在线triplet挖掘方法得到的\n 128-bytes per face\n 每个人脸使用128维向量表征\nIntroduction  The network is trained such that the squared L2 distances in the embedding space directly correspond to face similarity: faces of the same person have small distances and faces of distinct people have large distances.\n 网络输出embedding ，在embedding空间直接描述人脸的相似程度，同一个人有较近的距离，不同的人有较远的距离\n Previous face recognition approaches based on deep networks use a classification layer [15, 17] trained over a set of known face identities and then take an intermediate bottleneck layer as a representation used to generalize recognition beyond the set of identities used in training\n 之前人脸识别给予深度网络的使用一个分类器层，独立地使用一组已知的人脸来训练，然后使用一个中间瓶颈层作为一个人脸的表征，来用于识别不同于训练样本类别的人脸\n The downsides of this approach are its indirectness and its inefficiency: one has to hope that the bottleneck representation generalizes well to new faces; and by using a bottleneck layer the representation size per face is usually very large (1000s of dimensions)\n 以前识别算法的劣势在于不直接和低效：其必须希望瓶颈层的表征对于新的人脸有好的范化能力；使用瓶颈层的表征的维数非常大，1000或更多。\n Some recent work [15] has reduced this dimensionality using PCA, but this is a linear transformation that can be easily learnt in one layer of the network\n 一些最近的工作为了减少维数使用pca降维，但是其是一个线性变换，可以在任意一层网络层轻松学到。\n FaceNet directly trains its output to be a compact 128-D embedding using a triplet-based loss function based on LMNN [19].\n FaceNet直接训练他的输出使其达到紧凑的128维embedding，使用基于LMNN 【19】的triplet 损失函数\n Our triplets consist of two matching face thumbnails and a non-matching face thumbnail and the loss aims to separate the positive pair from the negative by a distance margin\n triplet包含两张匹配的人脸，和一张非匹配的人脸，目的是用一对正例，和一对负例，并保证ap和an之间差一个margin（空隙，空间）\n The thumbnails are tight crops of the face area, no 2D or 3D alignment, other than scale and translation is performed\n 样本内容为人脸紧凑的切割，不需要任何2D，3D的对齐，尺度变换，和其他变换\n Choosing which triplets to use turns out to be very important for achieving good performance and,\n 选择triplets很重要\n inspired by curriculum learning [1], we present a novel online negative exemplar mining strategy which ensures consistently increasing difficulty of triplets as the network trains.\n 受到【1】的启发，我们提出了一个新奇的在线负样本挖掘策略，来确保在网络训练过程中持续增加triplets的难度\nTo improve clustering accuracy, we also explore hard-positive mining techniques which encourage spherical clusters for the embeddings of a single person.\n   为了提高聚类正确性，使用hard正样本挖掘技术，来确保单个人脸的EMbedding约束在球形内\n 可以处理以前觉得很难的样本，如下：\nRelatedWork  Similarly to other recent works which employ deep networks [15, 17], our approach is a purely data driven method which learns its representation directly from the pixels of the face. Rather than using engineered features, we use a large dataset of labelled faces to attain the appropriate invariances to pose, illumination, and other variational conditions.\n 类似于15，17，我们的应用是纯粹的数据驱动的方法，使用人脸图片的像素作为输入，而不是使用人工构建的特征，使用大量的标记后的人脸数据使网络对于pose，illumination和其他conditions稳定。\n本文讨论了两种网络架构：\n 1:The first architecture is based on the Zeiler\u0026amp;Fergus [22] model which consists of multiple inter-leaved layers of convolutions, non-linear activations, local response normalizations, and max pooling layers\n  2:The second architecture is based on the Inception model of Szegedy et al. which was recently used as the winning approach for ImageNet 2014 [16]\n  We have found that these models can reduce the number of parameters by up to 20 times and have the potential to reduce the number of FLOPS required for comparable performance\n 以上两种网络结构能减少20倍以上的参数，并且能潜在的减少FLOPS的数量（与同类的网络）\n近期人脸识别工作一大堆，挑几个代表性的说：\n The works of [15, 17, 23] all employ a complex system of multiple stages, that combines the output of a deep con-volutional network with PCA for dimensionality reduction and an SVM for classification：\n 深度学习+PCA+SVM\n Zhenyao et al. [23] employ a deep network to “warp” faces into a canonical frontal view and then learn CNN that classifies each face as belonging to a known identity. For face verification, PCA on the network output in conjunction with an ensemble of SVMs is used.\n 使用深度网络把脸弄成正面视角的，然后训练网络，verification的时候PCA网络输出，然后用svm分类\n Taigman et al. [17] propose a multi-stage approach that aligns faces to a general 3D shape model. Amulti-class network is trained to perform the face recognition task on over four thousand identities.\n 多阶段应用来align人脸，得到3d的形状，然后训练多类网络来进行识别\n Sun et al. [14, 15] propose a compact and therefore relatively cheap to compute network. They use an ensemble of 25 of these network, each operating on a different face patch.\n 小型紧凑的网络模型，易于计算，共使用25个这样的网络，每个网络对应于人脸的不同区块。\n The verification loss is similar to the triplet loss we employ [12, 19], in that it minimizes the L2-distance between faces of the same identity and enforces a margin between the distance of faces of different identities.\n Verification loss 类似于triplet loss 我们引用自12，19的方法，最小化L2距离，在同一个人之间和不同人之间的margin。\nMethod two different core architectures:\n The Zeiler\u0026amp;Fergus [22] style networks   the recent Inception [16] type networks\n Given the model details, and treating it as a black box (see Figure 2), the most important part of our approach lies in the end-to-end learning of the whole system.\n网络模型当做黑盒如下图，我们主要关心更重要的。端对端的系统学习\nTo this end we employ the triplet loss that directly reflects what we want to achieve in face verification, recognition and clustering.\n在一端，我们使用triplet loss 直接反应我们想要在人脸分辨，识别，聚类中达成的目标\nNamely, we strive for an embedding f(x), from an image x into a feature space Rd,\n我们使用embedding ， f（x）直接将图像从x域映射到特征空间R（d维），\nThe triplet loss, however, tries to enforce a margin between each pair of faces from one person to all other faces. This allows the faces for one identity to live on a manifold, while still enforcing the distance and thus discriminability to other identities\nTriplet loss 试图去注意在正样本对和负样本对之间的margin，这使得同一类的人脸存在于多种情况，同时也关注其间的距离，因此对于其他类可分辨。\nTriplet Loss Here we want to ensure that an image 我们要确保对于一张图：\n1.of a specific person is closer to all other images 同一人的不同图片距离$x_i^p(positive)$\n2.of the same person than it is to any image 母样本，或者称为锚点$x_i^a(anchor)$\n3.of any other person\n对于其他样本（负样本）$x_i^n(negative)$\n我们希望确保同一人的ap相近，ab距离远，如下图 因此，我们希望：\n$$ ||x_i^a-x_i^p||_2^2+\\alpha \u0026lt; ||x_i^a-x_i^n||_2^2 ,\\forall(x_i^a,x_i^p,x_i^n)\\in\\tau $$ alpha表示margin在正例对和负例对之间的距离，T是一组可能的triplets在训练数据集中的。基数为N。\n最小化的Loss：\n$$\\sum_i^N[||f(x_i^a)-f(x_i^p)||^2_2-||f(x_i^a)-f(x_i^n)||^2_2+\\alpha]_{+}$$\n Generating all possible triplets would result in many triplets that are easily satisfied (i.e. fulfill the constraint in Eq. (1)). These triplets would not contribute to the training and result in slower convergence, as they would still be passed through the network. It is crucial to select hard triplets, that are active and can therefore contribute to improving the model.\n 计算所有可能的triplet容易导致其中很多对于网络是无用的（网络可以轻易使其满足公式1），这些triplet对于训练和结果收敛速度没有贡献，选择难以分类的是重要的，这些是有吸引力的，而且可以帮助提高模型\nTriplet Selection In order to ensure fast convergence it is crucial to select triplets that violate the triplet constraint in Eq. (1)\n为了快速收敛，选择违反公式1的triplet具有决定性作用。\n选择hard-positive和hard-nagetive：\n$$ argmax_{x_i^p}||f(x_i^a)-f(x_i^p)||^2_2 $$\n$$ argmax_{x_i^p}||f(x_i^a)-f(x_i^n)||^2_2 $$\nIt is infeasible to compute the argmin and argmax across the whole training set.\n在完整数据集上计算argmin和argmax是不可能的 Additionally, it might lead to poor training, as mislabelled and poorly imaged faces would dominate the hard positives and negatives. There are two obvious choices that avoid this issue:\n尤其是，其可能导致不好的结果，因为有一些训练数据未准确标记，模糊的图片可能会决定hard positives或者hard negative，有两种明确的选择可以避免这两个问题：\n Generate triplets offline every n steps, using the most recent network checkpoint and computing the argmin and argmax on a subset of the data.\n 每N步后线下寻找triplet，使用最近生成的网络，计算argmin和argmax在数据的一个子集上\n Generate triplets online. This can be done by selecting the hard positive/negative exemplars from within a mini-batch.\n 在线获取triplet，通过从一个小的batch中选取hard positive或者hard negative样本。\n Here, we focus on the online generation and use large mini-batches in the order of a few thousand exemplars and only compute the argmin and argmax within a mini-batch.\n 这里我们专注于在线获取，使用大的mini-batches包含几千个样本，而且只计算argmin和argmax在这个mini-batche上\n To have a meaningful representation of the anchor-positive distances, it needs to be ensured that a minimal number of exemplars of any one identity is present in each mini-batch. In our experiments we sample the training data such that around 40 faces are selected per identity per mini-batch. Additionally, randomly sampled negative faces are added to each mini-batch.\n 为了获得一个有意义的AP距离的表达，其需要确保在每一个mini-batch任一类必须有少量的样本被选出，我们的经验是我们在训练样本中采样大约40个人脸图像每个人每个mini-batch。此外，随机采样负样本人脸，加入到每一个mini-batch\n Instead of picking the hardest positive, we use all anchor-positive pairs in a mini-batch while still selecting the hard negatives.\n 我们选取mini-batch中所有的ap对而不是选择最难的ap对。但是我们依然选择最难的an对\n We don’t have a side-by-side comparison of hard anchor-positive pairs versus all anchor-positive pairs within a mini-batch, but we found in practice that the all anchor- positive method was more stable and converged slightly faster at the beginning of training.\n 我们没有对mini-batch中的所有ap对进行side-by-side的hard程度比较，但是我们的经验是所有的ap对都使用能够增强收敛稳定性和速度，在训练初始阶段。\n We also explored the offline generation of triplets in conjunction with the online generation and it may allow the use of smaller batch sizes, but the experiments were inconclusive.\n offline和online结合，这种方法允许使用更小的batch大小，但是结果是没啥结果。。\n Selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically it can result in a collapsed model (i.e. f(x) = 0). In order to mitigate this, it helps to select xni such that\n 选择最难的AN对可以在训练早起引导向局部最小值，特别的，他能勾引起模型塌陷，为了避免这种情况，我们选取n的时候要满足以下条件：\n$$||f(x_i^a)-f(x_i^p)||^2_2\u0026lt;||f(x_i^a)-f(x_i^n)||^2_2$$\n We call these negative exemplars semi-hard, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchor-positive distance. Those negatives lie inside the margin α.\n 我们称这些negative为semi-hard，因为他们距离anchor比positive远，但是还难以区分，因为距离还是比较接近ap距离，这些negative在margin alpha内！\n As mentioned before, correct triplet selection is crucial for fast convergence.\n 选好triplet能加速收敛\n On the one hand we would like to use small mini-batches as these tend to improve convergence during Stochastic Gradient Descent (SGD) [20].\n 一方面我们使用小的mini-batch加速收敛，使用sgd\n On the other hand, implementation details make batches of tens to hundreds of exemplars more efficient.\n 另一方面，执行细节使得样本的几十到几千的batch更有效\n The main constraint with regards to the batch size, however, is the way we select hard relevant triplets from within the mini-batches.\n Batch size 的主要的约束是我们从mini-batches选择hard triplet的方式\n In most experiments we use a batch size of around 1,800 exemplars\n Batch size大约1800个样例\nDeep Convolutional Networks  In all our experiments we train the CNN using Stochastic Gradient Descent (SGD) with standard backprop [8, 11] and AdaGrad [5]. In most experiments we start with a learning rate of 0.05 which we lower to finalize the model. The models are initialized from random, similar to [16], and trained on a CPU cluster for 1,000 to 2,000 hours. The decrease in the loss (and increase in accuracy) slows down drastically after 500h of training, but additional training can still significantly improve performance. The margin α is set to 0.2.\n 使用BP算法和SGD优化来训练CNN，AdaGrad用来调整步长，初始化步长为0.05，模型随机初始化，类似于16，使用cpu集群训练了1000-2000小时，500小时后训练效果变化变慢，但还是继续训练还是有效果的，alpha选择0.2\n The first category, shown in Table 1, adds 1×1×d convolutional layers, as suggested in [9], between the standard convolutional layers of the Zeiler\u0026amp;Fergus [22] architecture and results in a model 22 layers deep. It has a total of 140 million parameters and requires around 1.6 billion FLOPS per image. The second category we use is based on GoogLeNet style Inception models [16]. These models have 20× fewer parameters (around 6.6M-7.5M) and up to 5×fewer FLOPS(between 500M-1.6B).\n ","permalink":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-facenet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity\u003c/p\u003e\n\u003cp\u003e提出一个系统，此系统能将人脸图片直接映射到欧几里得空间的一个向量，这些向量之间的距离能直接度量人脸之间的相似度\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKeywords:\u003c/strong\u003e 人脸识别,FaceNet,GoogleNet\u003c/p\u003e","title":"FaceNet论文阅读"},{"content":"Abstract: 四个向量空间的dimensions的一些性质 Keywords: Dimensions,Four Subspaces\n四个子空间的维度 这几天在一边完成线性代数的博客一边研究微积分，还有一部分时间在看概率论，这写课程现在感觉看起来很容易，也有可能是公开课讲的比较简单，没有大学本科老师讲的那么深入，所以做起来感觉还没什么阻力，希望花这么多时间补习的结果能帮助后面对机器学习算法和人工智能知识学习有所帮助。 痛苦一直在持续，因为我们没有看到光明之前，放弃努力就等于放弃光明，等待和虚度不会得到任何你想要的东西，继续学习，继续努力。\n四个子空间(Four Subspaces) 本篇应该不长，因为关于矩阵的四个子空间的难点在后面所有章节，我们这张主要讲线性独立，基，维度这些基本概念，同时联系到rank，融合前面elimination的基本知识。 上面这句话如果不看前面的文章基本被干晕了，但是如果前面每篇都完全读懂了，这句话就变得非常好理解。 线性代数核心问题就是这四个子空间： 矩阵的四个子空间分别是：column space ，row space ，nullspace，left nullspace， 左nullspace就是矩阵转置的nullspace： 对于行空间和列空间，他们的dimensions就是rank，上一篇已经有所说明。 nullspace就是free columns的数量n-r，left nullspace就是free rows的数量m-r\n$R$ R是经过消元的矩阵，可以很容易的看出矩阵的rank，假设矩阵A是m by n的规模：\n $N(R)$ 的dimensions是n-r $C(R)$ 的dimensions是r $C(R^T)$ 的dimensions是m-r  $A$ 对于没elimination的矩阵A与R有相同的row space，但是column space不同，但是elimination改变rows，但是row space不变。column space不同，但是column space的dimension相同都是rank。对于A矩阵的子空间的dimension总结如下\n  A has the same row space as R.Same dimension r and same basis The column space of A has dimension r.For every matrix this is essential. A has the same nullspace as R.Same dimension n-r and same basis The left nullspace of A(the nullspace of$A^T$) has dimension m-r   Fundamental Theorem of Linear Algebra ,Part 1 The column space and row space both have dimension r The nullspaces have dimensions n-r and m-r\n凡是叫fundamental开头的定理都是非常非常重要。\nconclusion 总结这篇就是简单的总结下A和R的dimensions之间的关系，没有新的知识更像是个总结： 下章开始研究总结正交，各位加油。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-3-6.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 四个向量空间的dimensions的一些性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Dimensions,Four Subspaces\u003c/p\u003e","title":"【线性代数】3-6:四个子空间的维度(Dimensions of the Four Subspaces)"},{"content":"Abstract: 本文是本章最重要的知识点，也是整个线性代数中非常核心的内容，包括independence ，basis和dimension等多个概念 Keywords: Independence，Basis，Dimension，Span\n独立性，基和维度 在没有系统学习线性代数之前，对很多里面的名词有所畏惧，现在思考发现，很多听不懂的名词都是因为不明白背后的原理和知识才会产生畏惧，也有可能这个名词背后真的蕴藏的一个非常深奥的系统知识，但是如果我们慢慢的从头开始抽丝剥茧的把每一个知识点都掌握了，最后听到这个名词就会觉得这是个很平常的词汇而已，但是没有学习之前就会一头雾水，还有一个感觉就是，如果这些基础知识不掌握，论文种可能是个很简单的过程，作者略过了，如果基础不牢就会迷惑，或者自己瞎猜，其实迷惑不可怕，起码自己知道这里有问题，但是瞎猜就有问题了，而且还猜的理直气壮，觉得自己猜的都对，这种人是永远不会进步的。 今天我们就逐个解释线性代数中比较常出现的几个非常重要的概念。\n线性独立 Linear Independence可以拆开看，Linear就是我们的基础关系，线性，满足线性组合的基本要求1-1:Linear Combinations有详细说明，就是满足add 和scalar的组合；Independence表示独立，谁和谁也不相关，其实不相关的这个概念在概率论中让我记忆深刻的，而且一直也不懂到底是啥意思（现在也不懂），不相关就是没办法关联起来。 现在抛弃上面的所有思路，从矩阵角度来看，矩阵角度也就是向量角度，因为Linear Independence是针对向量矩阵是向量合起来写的一种方式：\n Definition: The columns of A are linearly independent when the only solution to $Ax=0$ is $x=0$ No other combination $Ax$ of the columns gives the zero vector\n 定义是说，当向量汇聚成矩阵后，矩阵的nullspace只有0向量的时候，这些向量线性独立，nullspace只有0，说明elimination后的rank=column number。这样nullspace就只有0了。 另一个定义：\n Definition: The sequence of vectors $v_1,\\dots,v_n$ is linearly Independence if the only combination that gives the zero vector is $0v_1+0v_2+ \\dots +0v_n$\n $x_1v_1+x_2v_2+\\dots+x_nv_n=0$ only happens when all x\u0026rsquo;s are zero\n只有当x全是0的时候，组合向量v才能得到0，其他x不能完成这个任务，就说这些v线性独立。 注意，只有向量有线性独立的说法，一个矩阵不能线性独立，当然entry是矩阵的向量也可以线性独立，那就有点复杂了，不过也是一样的道理，满足条件就可以。 如果向量sequence中包含0向量，那么这个他们不会Linear Independence。 上面提到了rank和矩阵大小的关系对是否线性相关有影响，当$r=n\\leq m$时，线性独立，但是当$r\\leq m \u0026lt; n$时，必然线性相关。 在另一本书上《Linear algebra done right》上说当一个向量sequence里其中一个可以被其他线性组合出来，那么他们线性相关，否则线性无关，这个和上面的nullspace的说法含以上是一致的，但是感觉更形象。\n向量张成子空间（Row Space） 本来想写span但是总记得已经写过了，回去一查果然有说明，span的概念比较好理解，就是若干个向量通过线性组合得到的一个向量空间（满足向量空间的所有要求），具体的说明可以复习下：Span.列向量是矩阵中所有的列span成的空间。 举个🌰 ： $$ v_1=\\begin{bmatrix}1\\0\\end{bmatrix}\\ v_2=\\begin{bmatrix}0\\1\\end{bmatrix} $$\n这两个向量可以线性组合出二维实数空间的所有向量，也就是说$v_1$和$v_2$ span $\\Re^2$ 前面我们介绍过列空间，矩阵列span出来的空间，对应的，矩阵每行span出来的空间叫做row space，矩阵A的row space与$A^T$的column space相同。 $$ A=\\begin{bmatrix} 1\u0026amp;4\\ 2\u0026amp;7\\ 3\u0026amp;5 \\end{bmatrix}\\ $$ 这个矩阵的列空间： $$ C(A)= x_1 \\begin{bmatrix} 1\\ 2\\ 3 \\end{bmatrix}+ x_2 \\begin{bmatrix} 4\\ 7\\ 5 \\end{bmatrix} $$ 行空间： $$ C(A^T)= x_1 \\begin{bmatrix}1\\4\\end{bmatrix}+x_2 \\begin{bmatrix}2\\7\\end{bmatrix}+x_3 \\begin{bmatrix}3\\5\\end{bmatrix} $$ 同样的矩阵，同样的数字，组合出来的空间却完全不同，列向量在 $\\Re^m$中，行向量在 $\\Re^n$\n基(Basis) 好的理解了span的概念我们对basis的概念就理解了一般，如果我们有五个向量可以span出来一个空间S，那么我们的一种想法是不是能不能用这五个向量中的其中几个span出等价的空间，答案是有可能的，其根本原因是线性组合的计算性质，如果五个向量中的一个可以用其他四个组合出来，就可以省去这个被组合出来的向量，这句话熟悉么？没错上面提到的线性独立和线性相关的概念。\n Definition: A basis for a vector space is a sequence of vectors with two properties: The basis vectors are linearty independent and they span the space\n 如果你有一组向量，他们span了一个space，如果你想得到这个space的基，那么就找出这组向量中线性独立的部分，把dependent的向量去掉。 Linear combination的性质是所有线性代数知识的合法性来源。 根据线性组合的性质可以继续发掘出很多关于basis的性质。\n There is one and only one way to write v as a combination of the basis vectors\n 对于已经确定的一组基向量，这个空间中的所有向量只有一种方式通过基向量线性组合而的\n The columns of the n by n identity Matrix give the \u0026ldquo;Standard basis\u0026rdquo; for $\\Re^n$\n 这个是说标准基的来源，单位矩阵的每一列就是standard basis。\n The vectors $v_1,\\dots v_n$ are basis for $\\Re^n$ exactly when they are the columns of an n by n invertible matrix. Thus $\\Re^n$ has infinitely many different bases\n 一组基组合在一起形成方阵，那么这个矩阵的rank=n=m，那么这个矩阵可逆，并且span出$\\Re^n$。这个结论反推可以得到一个space可以有infinity个基\n The pivot columns of A are a basis for its column space.The pivot rows of A are a basis for its row space . So are the pivot rows of its echelon form R.\n 消元后的主行和主列有一个特点，就是他不能被其他行和列线性组合出来，也就是在这个行和列中挑选出来的线性独立的行和列就是主行和主列，那么一个矩阵的行空间或列空间的基就是主行和主列。\n那么如果我们有5个向量，每个向量属于 $\\Re^7$，那么我们怎么确定这些向量span的空间的基呢？ 两种方法，把这些向量作为行组合成矩阵，或者把这些矩阵作为列组合成矩阵，然后消元，对应的主行和主列就是基。\n If $v_1,\\dots,v_m$ and $w_1,\\dots,w_n$ are both basis for the same vector space ,then m=n.\n 对于一个确定的vector space其不同的基，具有相同的向量数量。\n维度  If $v_1,\\dots,v_m$ and $w_1,\\dots,w_n$ are both basis for the same vector space ,then m=n.\n 这个性质可以直接到出dimension的定义，m或者n这个实数就是dimension。\n Definition: The dimension of a space is the number of vectors in every basis.\n 一个空间的dimension，是其一组基向量内向量的个数。 对于一个矩阵，他的column space的基是主列，主列的个数和pivot的个数相同，pivot的个数就是rank，哈哈，没错一个矩阵的列空间的dimension是rank，没错吧，所有知识点都是相互勾连在一起的，而这些相互关系的最终基础就是linear combination。\n矩阵空间和函数空间(Matrix Space and Function Space0 前面说过矩阵和函数可以是向量的组成元素，那么矩阵向量的基和函数向量的基与上面实数向量的定义和性质完全一致：\nConclusion 这几篇写的都很长，感觉写的力不从心，一边参考教材一边写，没办法把书扔掉直接码字，还是要继续努力，后面还要做些练习，非常感谢MIT做出的无私奉献，把所有课程相关的内容开放下载，大家继续加油。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-3-5.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文是本章最重要的知识点，也是整个线性代数中非常核心的内容，包括independence ，basis和dimension等多个概念\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Independence，Basis，Dimension，Span\u003c/p\u003e","title":"【线性代数】3-5:独立性，基和维度(Independence,Basis and Dimension)"},{"content":"Abstract: Ax=b的完整解，以及一个解，infinity个解，没有解的所有条件和说明 Keywords: Ax=b,Special Solution,Full Column Rank,Full Row Rank,Complete Solution\n方程组的完整解 $Ax=b$ 之前我们已经研究了 $Ax=0$的相关内容，值得说一下的是，列空间和nullspace是有些区别的，列空间指的是b所在的空间，而nullspace是x所在的空间，这个要区别一下，这些所有空间都是针对矩阵的。\n特解(Particular Solution) 搞不懂Particular Solution和Sceptical Solution有啥区别的可以仔细看看了，之前我也没发现其有什么根本不同，Particular Solution是把所有的free variables设置为0，来一个完整的例子 $$ \\begin{bmatrix} 1\u0026amp;3\u0026amp;0\u0026amp;2\\ 0\u0026amp;0\u0026amp;1\u0026amp;4\\ 1\u0026amp;3\u0026amp;1\u0026amp;6 \\end{bmatrix} \\begin{bmatrix} x_1\\x_2\\x_3\\x_4 \\end{bmatrix} \\begin{bmatrix} 1\\6\\7 \\end{bmatrix} $$ Augmented Matrix: $$ \\begin{bmatrix} 1\u0026amp;3\u0026amp;0\u0026amp;2\u0026amp;1\\ 0\u0026amp;0\u0026amp;1\u0026amp;4\u0026amp;6\\ 1\u0026amp;3\u0026amp;1\u0026amp;6\u0026amp;7 \\end{bmatrix} \\begin{bmatrix} A\u0026amp;b \\end{bmatrix} $$ 经过消元，对原始矩阵方程消元和对Augment Matrix消元得到结果相同，这里就只写augment matrix： $$ \\begin{bmatrix} 1\u0026amp;3\u0026amp;0\u0026amp;2\u0026amp;1\\ 0\u0026amp;0\u0026amp;1\u0026amp;4\u0026amp;6\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0 \\end{bmatrix} \\begin{bmatrix} R\u0026amp;d \\end{bmatrix} $$ 通过定位pivot可以确定free columns是第二列和第四列，如果我们把$x_2,x_4$设置为零，那么就得到了x为 $$ \\begin{bmatrix} x_1\\x_2\\x_3\\x_4 \\end{bmatrix} \\begin{bmatrix} 1\\0\\6\\0 \\end{bmatrix} =x_{particular} $$ 可以通过回代验证$Rx=d$或者$Ax=b$这里的free variables都是0，那么就说这个解是particular的，而且可以发现pivot对应的x位置与d有关系，按顺序就是d对应的元素，这不是巧合，观察矩阵可以得到相应的结论。\n这是我们到目前位置讨论的方程组的两种解，一个当b=0的时候，一个当b不等于0的时候，如果我们把这两个方程组左右相加，那么就得到 $$ Ax_p=b\\ Ax_n=0\\ Ax_p+Ax_n=b+0\\ A(x_p+x_n)=b $$ 那么完整的解(式子中的数字来自上面的例子，nullspace我没有写出来，大家可以自行验证)： 为什么完整解是上看的式子呢，可以看下一节的详细介绍。\n完整解(The Complete Solution) 前面我们确定了完整的解就是$x_p+x_n$，那么我们到底有多少个解呢？\n   序号 m\u0026amp;r n\u0026amp;r Matrix Shape Ax=b Solution     1 r=m r=n Square and Invertible Ax=b 1   2 r=m r\u0026lt;n Short and Wide Ax=b $\\infty$   3 r\u0026lt;m r=n Tall and Thin Ax=b 0 or 1   4 r\u0026lt;m r\u0026lt;n Not full rank Ax=b 0 or $\\infty$    这四种情况，简单讲解下：\n 第一种：是方阵，我们最标准的方程组，rank与m,n相等，那么就一个解。这种情况下，消元后的R是单位矩阵： $$ R=\\begin{bmatrix}I\\end{bmatrix} $$\n 第二种：如果方程组的个数小于未知数的个数，而rank与行相同，rank=m\u0026lt;n,消元后的R是如下矩阵： $$ R=\\begin{bmatrix}I\u0026amp;F\\end{bmatrix} $$\n 第三种：如果方程组的个数大于未知数的个数，而rank与列数相同，rank=n\u0026lt;m,消元后的R是如下矩阵： $$ R=\\begin{bmatrix}I\\0\\end{bmatrix} $$ 如果b与R下面全是0的行对应的行不是0，那么就没有解\n 第四种：如果有效的方程组的个数小于未知数的个数，与情况二相似，但是有效方程数还小于远方程数，rank\u0026lt;m,rank\u0026lt;n,消元后的R是如下矩阵： $$ R=\\begin{bmatrix}I\u0026amp;F\\0\u0026amp;0\\end{bmatrix} $$ 如果b与R下面全是0的行对应的行不是0，那么就没有解\n自此，Ax=b 的所有解以及对应的情况都已经搞定了，***但是为啥complete solution是$x_p+x_n$呢？***这就是个悬案了，后面一定给出答案，现在就暂时记住就好了（回归课堂教育，先记住，就没有然后了）。\nConclusion 这篇也比较凌乱，不像写算法那种博客很流畅，解释，代码，总结这种套路在这不太实用，因为知识错综复杂，本文上面的四种解的情况才是重点，包括什么时候没有解，什么时候无数个解，如果按照表和矩阵R的形式来看，就会豁然开朗，后面继续。。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-3-4.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e Ax=b的完整解，以及一个解，infinity个解，没有解的所有条件和说明\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Ax=b,Special Solution,Full Column Rank,Full Row Rank,Complete Solution\u003c/p\u003e","title":"【线性代数】3-4:方程组的完整解( $Ax=b$ )"},{"content":"Abstract: 本文将介绍线性代数中最最最重要的概念之一，秩（Rank） Keywords: Rank,Row Reduced form,Pivot Columns,Free Columns,Special Solutions\n秩(Rank) 来段废话吧，之前写过一个小短文关于学习的，我把能力分成知识和经验两种方面来说，知识要考学习，而经验要依靠不断的练习和思考，以及通过知识作为基础来升级创造经验。不过看看周围那些掌握了更多社会资源的人，他们经验相当丰富，各种各样的经验，但是知识却有所欠缺，这样就使得某些产品和服务发生了根本上的变化，为了掩盖其中有一些不良变化，这些人又会利用自己丰富的经验来掩盖或者转移问题，这样下去不知道会产生些什么。\n秩(Rank) Rank，the ordinary members of an organization (such as the enlisted soldiers of an army)，谋组织原有的成员，中文翻译“秩”，把八个字翻译成一个字，意思字形上还没什么关系，所以我们以后不会用这个字，博客中仅使用Rank。 前面我们所有的知识都依靠方程组来推进的，Rank也一样，毕竟线性代数就是研究方程组的，方程组$Ax=b$在消元过程中，有些行最后的结果是$0x=0$这个现象从字面上理解就是上面的方程和这行的方程存在倍数关系，比如本行的方程是$2x+2y=2$那么上面可能有一行$x+y=1$或者其他等价方程，还有可能是上面几个方程的线性组合刚好和本行方程一致，这样的意思就表明，这行方程对于全局求解没有提供任何有用的信息，比如描述一个人，A说你是个男的，B说你不是个女的，B这个信息就没用了（也可以说A的信息没用），那这样虽然有两条信息，但是原有的信息（原有的成员）就是1，B的信息可以通过A的演化出来，所以Rank就是矩阵所表示的方程组中那些原有的成员的数量。\n Defination: The rank of A is the number of pivots.This number is r\n 解释，如果矩阵中存在pivot那么证明，第i行的pivot下面的行没办法表示出第i行，上面的也没办法表示第i行\n秩一 Rank) 只有一个pivot的矩阵，是一种特殊的矩阵 $$ A=\\begin{bmatrix} 1\u0026amp;3\u0026amp;10\\ 2\u0026amp;6\u0026amp;20\\ 3\u0026amp;9\u0026amp;30 \\end{bmatrix} \\to R=\\begin{bmatrix} 1\u0026amp;3\u0026amp;10\\ 0\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;0 \\end{bmatrix} \\begin{bmatrix} 1\\2\\3 \\end{bmatrix} \\begin{bmatrix} 1\u0026amp;3\u0026amp;10 \\end{bmatrix} =uv^T $$ 对于上面的例子可以看出，Rank one矩阵的分解形式可以理解为两个向量的outer product，那么矩阵A的Nullspace等于R的Nullspace： $$ Ax=0 \\to uv^Tx =0 \\to u(v^Tx)=0 \\to v^Tx=0 $$ 可以观察出，$Ax=0$表明x在A的Nullspace中, $v^Tx=0$表明 $x$和 $av^T$ 相互垂直 也就是行空间内的任意向量($av^T$)和Nullspace内的任意向量垂直。\nRank r还是行空间和列空间的维数，同时跟Nullspace的维度有关！\n自由列和主列 3-2:Nullspace中对Free Columns和Pivot Columns有相关介绍，下面再介绍一些相关的性质，Free Columns是不包含Pivot的列，Pivot Columns则是包含Pivot的列， 对于任何一个Free Column都是其前面pivot列的线性组合。 举个例子，我们不看具体数字就看形状 $$ \\begin{bmatrix} 1\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\\ 0\u0026amp;1\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;*\u0026amp;x\\ 0\u0026amp;0\u0026amp;0\u0026amp;1\u0026amp;x\u0026amp;x\u0026amp;x\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;1\u0026amp;x\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;1\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\\ \\end{bmatrix} $$ 我们直接把矩阵写成R的形式（A是原始矩阵，U是上三角矩阵，R是reduced row echelon matrix，pivot上下都是0，通过方程线性组合而得到） 这个R中第三列明显是个free column，星号表示任意数字，都能够用第一列和第二列的线性组合得到，第五列也可以通过一二四列组合出来。\n Defination: The pivot columns are not combinations of earlier columns.The free columns are combinations of earlier columns.The combinations are the Special Solutions\n 特解(Special Solutions) Special Solutions上一篇也提到过了，我不知道怎么翻译（particular solution和special solution分不清谁是谁）。通俗得说就是给free variables赋予特殊值，然后求出pivot对应的未知数的，一般我们都是分别设置0和1，一个为1其他是0，然后迭代。\n $Ax=0$ has r pivots and n-r free variables: n columns minus r pivot columns.The nullspace matrix M contains the n-r special solutions .Then $AN=0$\n 可以这么理解，n-r个free variables每个都被设置成1一次，共n-r个special solutions.\n原方程组中独立的方程数就是rank的大小，独立的概念在后面介绍，到时候回来看会非常惊艳。\n最后一个知识利用到了前面讲到的分块乘法，回忆我们的方程组，如果未知数按照$x_1,x_2,x_3,\\dots,x_n$的方法排列，我们可以得到系数矩阵A，但是我们稍微调换一下未知数顺序比如$x_2,x_1,x_3,\\dots,x_n$他的系数矩阵B就是A中第一列和第二列交换的结果。所以我们可以根据pivot所在的列进行冲洗排列，比如上面的那个星号矩阵就可以得到： $$ \\begin{bmatrix} 1\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\\ 0\u0026amp;1\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\u0026amp;x\\ 0\u0026amp;0\u0026amp;1\u0026amp;x\u0026amp;x\u0026amp;0\u0026amp;x\\ 0\u0026amp;0\u0026amp;0\u0026amp;1\u0026amp;x\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\\ \\end{bmatrix} \\to \\begin{bmatrix} 1\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;x\u0026amp;x\\ 0\u0026amp;1\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;x\u0026amp;x\\ 0\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;x\\ 0\u0026amp;0\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;1\u0026amp;0\u0026amp;0\\ 0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\u0026amp;0\\ \\end{bmatrix} =\\begin{bmatrix} I\u0026amp;F\\ 0\u0026amp;0 \\end{bmatrix} $$ 那么这个新的R我们称为$R_{new}$，那么他的Nullspace： $$ R_{new}= \\begin{bmatrix} I\u0026amp;F\\ 0\u0026amp;0 \\end{bmatrix} N=\\begin{bmatrix} -F\\I \\end{bmatrix} R_{new}N=0 $$ $-F$ 包含r个pivot variables，并且$R_{new}$中的I和N中I规模是不一样的。\nConclusion 总结一下，这篇从原文来看有点模糊，就是前一篇的知识也有后一篇的知识也有而且融合在一些，想捋顺出来一点点说有点费劲，写了一篇用了一下午，下面看看还能不能再写两篇。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-3-3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文将介绍线性代数中最最最重要的概念之一，秩（Rank）\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Rank,Row Reduced form,Pivot Columns,Free Columns,Special Solutions\u003c/p\u003e","title":"【线性代数】3-3:秩(Rank)"},{"content":"Abstract: 零空间的相关知识点，使用到前面的消元过程 Keywords: Nullspace，Pivot Columns，Free Columns，Special Solutions，Ux=0，Rx=0\n零空间 $Ax=0$ 之前讲$Ax=b$的时候提到过，正着看反着看的例子，其实这个办法是MIT18.01Caculus里面讲的一种技巧，不同的方向含义不同，今天更直接了当，把b改成o，好啦，来吧，怎么能让A的列组合出来0？不用说0肯定可以，那么只有0么？并不是。\n The nullspace of A consists of all solutions to Ax=0.These vectors x are in $\\Re^n$ the nullspace containing all solutions of Ax=0 is donate by $N(A)$\n 其实这个nullspace还是挺别致的，起码他包含0，而之前Ax=b就不一定包含0。所以可以看出，nullspace是个subspace，原因是如果x，y向量Nullspace里面的两个向量，那么$A(x+y)=0$，并且$A(cx)=0$成立，所以nullspace是个子空间 $Ax=b$并不一定是。\nSpecial Solutions 一般情况下，我们理解都是A0=0，这是最常规的，Ax=0的其他解一看就不是什么正经解，的确是这样的，看看下面这个例子： 没错看吧： $A=[1,2,3]$,$N(A)$是个啥？$1x+2y+3z=0$是个通过原点的平面，一个方程，三个未知数，那么有两个变量是随意的（这两个变量的值是随意的，但这两个变量并不是随意的），只要另一个委屈自己，使整体满足就可以，那么另外两个变量随意选择，这个随意选择就有技术含量了，你选两个0，那么x只能是0，这两个随意的变量叫做“free variables”,其个数等于m或n中大的那个减去主元的个数，而且上面说的随便两个变量也不是那么随便的，需要对应一定的列，下面的例子会仔细说明。\n The nullspace consists of all combinations of the special solutions\n 也就是如果有两个free variable 那么nullspace就是个平面，根据上面我们的研究发现确实这样的，x+y+z=0是个过原点的平面。这两个Special solutions的linear combination就是N(A).\n这个例子是找C矩阵的nullspace，上面忘了说了，如果你想研究nullspace首先要进行一些列的elimination操作，也就是把原始矩阵转化成U(上三角矩阵)，矩阵U中有主元的叫做主列，也就是说这部分系数是完整的，比如有两个主列，那么这两个主列对应的x中的元素就是不自由要委屈自己的，因为他有足够多的限制，而那些没有主元的列就是free列了，这些列在x中对应的元素就可以放飞自己了，但是一般情况下我们会分别让他们是0和1，这样free部分可以以标准基的形式出现，张成整个free空间，然后让主列的x元素去委屈自己，这个地方这么理解可能有点困难，仔细看下面这段话： 首先我们必须承认Nullspace是矩阵的子空间，子空间就是不完整的A的列所张成的列空间，维度必然低于（等于）A的列空间的维度，一个五维空间中的向量（向量属于 $\\Re^5$ ）三维子空间必然有两维不是完整的（不自由的），但是另外三维是完整的（自由的）三维空间那么这三维的基就是我们选的标准基，如果你执意要选不标准的也可以，只要能张成三维空间就行。其实这就是特解的完整理解，上面的话都是我说的，不严谨可以提出。 举个例子: $$ x+y=0 $$ 那么他的A $$ A=\\begin{bmatrix}1\u0026amp;1\\end{bmatrix} $$ 第一列是主列，第二列是free的，我们可以选择1，那么x=-1。（-1，1）的任何倍数都是0空间内的向量，我们就得到了一条直线。注意A的列空间是个平面，而有一个free variable的nullspace是个直线，少一维，free variable控制这vector在这条直线上的位置。 回来继续看上面C的例子 Special solutions span the Nullspace！！！！！\n$Ux=0$ \u0026amp; $Rx=0$ 求解nullspace的过程\n 1：Forward elimination to U or its reduced form R 2：Back substitution in Ux=0 or Rx=0 product x\n 解释下，reduced form 就是通过向上消元，把pivot上方的元素消掉变成0，同时缩放，把pivot全部变成0.$N(A)=N(U)=N(A)$ U或者R中没有pivot的列是free列，对应x中的一个free variable，如果当m\u0026lt;n的时候，至少包含1个free variable。\nConclusion 这篇主要介绍Nullspace，线性代数最重要的四个spaces已经搞定两个，还剩下两个就比较容易通过前两个推导了（什么？我居然没说是哪四个？好吧，列空间，Nullspace，行空间，左Nullspace）。这篇文字叙述有点多，因为没办法形象的\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-3-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 零空间的相关知识点，使用到前面的消元过程\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Nullspace，Pivot Columns，Free Columns，Special Solutions，Ux=0，Rx=0\u003c/p\u003e","title":"【线性代数】3-2:零空间(Nullspace)"},{"content":"Abstract: 本章介绍线性代数的核心内容，关于Vectors Space和subspace的一些观点，本文作为第一篇，主要说明基础知识 Keywords: 向量空间，子空间，列空间，张成\n开篇废话 好几天没有更新博客了，前天本来想写，发现环境出现点问题，因为我本身对网站知识并不是很了解，hexo g的时候有个warning，我就开始修，结果很简单，直接修的全是error，好在都有备份，昨天就重新搞了一下，总体来说hexo对于使用来说还是比较简单的，所以一上午也就差不多搞定了，今天开始写线性代数最重要的一部分，也有点困难，所以我决定写的慢一点，啰嗦一点，必要的时候把前面的东西重新拿过来，数学很难，如果之前的知识掌握不住，后面的基本就是云里雾里，但是哪个学科不是这样呢？所以，融会贯通，回头来看，就会豁然开朗了\n向量空间（Spaces of Vectors） 向量空间，我刚又看了一下Pro. Strang老爷子的书，很遗憾，没找到定义，可能是受封建教育搞得不来个黑体字定义，就不知道啥是啥了，通俗的解释下吧。 我们都认识向量 $$ v=\\begin{bmatrix}1\\2\\end{bmatrix} $$ 他的一般形式呢，就是一个向量含有两个实数元素，这里写的1和2也可是250和520，只要是实数，这个向量都是在一个圈子里,这个圈子呢就是我们的小标题，向量空间，一个向量空间包含无数组向量，但是这些向量必须满足一定的规则，并且向量空间作为一个类，有自己的固有属性，我假定你对面向对象的编程有些了解，所以说出了类和对象之类的名词，不专业，但是能形象一点，方便大家理解。 $$ v=\\begin{bmatrix}x_1\\x_2\\end{bmatrix}\\ x_1,x_2 \\in \\Re $$ 可以看出，这个向量是个二维的向量，那么我们把这个向量空间写成： $$ \\Re^2 $$ 怎么样，眼熟么？惊喜不惊喜，意外不意外？ 我刚才说了，这是个特殊的例子，但是也说明了一定的规则，首先元素要满足一定范围，可以是实数，可以是复数，也可以不是数字，比如可以是函数，或者其他任何东西。 脑洞时间：那么我有一个问题，如果每个元素都是不一样的东西，这个能算向量空间么？比如 $$ u=v=\\begin{bmatrix}book\\food\\number\\end{bmatrix}\\ $$ 在python里这些可以组成list，我们慢慢看看，这个\u0026quot;向量空间\u0026quot;\u0026ldquo;是否满足所有条件！\n$R^n$ $$ v=\\begin{bmatrix}x_1\\x_2\\ \\vdots\\x_n\\end{bmatrix}\\ x_1,x_2, \\dots ,x_n \\in \\Re $$ 所有元素属于实数，并且有n个元素，那么我们可以得到我们的向量空间 $$ \\Re^n $$ 这个可以根据上一节的结论进行推演，把2换成了n，其中n是确定的常数\n$C^n$ $$ v=\\begin{bmatrix}c_1\\c_2\\ \\vdots\\c_n\\end{bmatrix}\\ c_1,c_2, \\dots ,c_n \\in C $$ 所有元素属于复数，并且有n个元素，那么我们可以得到我们的向量空间 $$ C^n $$ 这个可以根据上一节的结论进行推演，把实数换成了复数，其中n是确定的常数\nM,F and Z 同理，元素可以属于任何其他域，比如可以使矩阵，函数，或者是0 M:The vector space of all real 2x2 matrices F:The vector spcae of all real functions f(x) Z:The vector space that consists only of a zero vector 这里面有个比较特殊的是F好吧，我承认都挺特殊，但是F是个无限维函数空间， 一个与F类似，但是维度有限的例子是多项式空间$P$或者$P_n$ $$ a_0+a_1x+a_2x^2+\\dots+a_nx^n $$ Z向量空间值得说一下，我们平时可以把它写成 $$ \\Re^0 $$ 他的维度是0，维度说了半天发现好像前面没说过什么是维度，那后面补上。Z是最小的向量空间，所有空间必须包含0向量，但是每个空间的零向量都不相同，根据自己的特点来产生0向量的形状。\n8 Conditions（八项基本原则） 来来来，现在该说正事了，刚才举了几个例子，都是从书上抄的，那么到底什么样的空间才能是空间呢？或者说我们脑洞问题怎么才能解答？首先向量空间可以不是数字组合出来，但是必须遵守八项基本原则(8 conditions)\n 八项要求： 1、要同党中央保持高度一致，不阳奉阴违、自行其是； 2、要遵守民主集中制，不独断专行、软弱放任； 3、要依法行使权力，不滥用职权、玩忽职守； 4、要廉洁奉公，不接受任何影响公正执行公务的利益； 5、要管好配偶、子女和身边工作人员，不允许他们利用本人的影响谋取私利； 6、要公道正派用人，不任人唯亲、营私舞弊； 7、要艰苦奋斗，不奢侈浪费、贪图享受； 8、要务实为民，不弄虚作假、与民争利。 这个是他们主流的八项原则，我们的是非主流的，哈哈\n 1: $x+y=y+x$ 加法交换律\n2: $x+(y+z)=(x+y)+z$ 加法结合律\n3: 存在一个0向量满足：$x+0=x$ 加法0向量，加上任何向量都等于这个向量本身\n4: 对于所有x存在$-x$满足$x+(-x)=0$ 加法反向量（这个名字是我想的），就是能够找到一个向量，使得其与原始向量相加为0\n5: 1乘以$x$要等于$x$ 乘法1向量，乘以谁就得谁\n6:$(c_1c_2)x=c_1(c_2x)$ 乘法结合律\n7:$c(x+y)=cx+cy$ 乘法分配率，注意这个c是数，不是向量\n8:$ (c_1+c_2)x=c_1x+c_2x$ 乘法分配率，上面的分配的是scalar，这个分配的是向量，$c_1,c_2$是数字\n那么判断是不是向量空间的任务就变的有依据了，我的脑洞显然不是，比如book里面就没有anti-book可以把书变没。 FMZP这几个家伙就都满足，不信你就自己试试，我试过，没问题！\nSubspace(子空间) 子空间肯定对应有父空间什么的，实际上没有，因为sub前缀根本就没有继承的意思，而是说是个小一些的，包含少一些的，所以子空间对应于他的上一级空间，少了一些东西，准确的说是少了很多东西，这些东西就是向量，比如一个平面，那么平面上的一条线，他是平面的一部分，但是只是一小部分，就是这种关系，书上的描述是a vector space inside $\\Re^n$ ,those are Subspace of $\\Re^n$。\n A subspace of a vector space is a set of vectors (include 0) that satisfies two requirement: if v and w are vectors in the subspace and c is any scalar ,then (i) $v+w$is in the subspace (ii) $ cv$is in the subspace\n 以上(i)(ii)是判断子空间的重要标准，这个也是我们之前提到的，线性代数最最最最最基础的最关键的两种计算。 Linear combination中详细的描述了这两种计算。子空间主要强调closed，在加法和乘法上封闭，你不能两个空间内的向量，加乘出了空间，飞翔到外太空，这样就不能称为子空间了，概括一下子空间就是包含在某个空间内，自身对于加法和scalar封闭的这么一个空间。\nSome Facts： 1：当(ii)中c为0的时候0向量在空间内，也就是说子空间必须包括0向量 2：如果向量v,w在子空间内，那么其线性组合$cv+dw$一定也在子空间内 3：一个三维空间，过原点的直线是子空间 4：0向量是一个空间的子空间\n子空间是个相当有用知识，在《线性代数应该这样学》中就是从空间这个切入点来建立整个线性代数体系的。书上主要通过二维和三维的例子来介绍子空间的，比较容易理解。\nColumn space of A（矩阵A的列空间） A是个矩阵，有m行，n列，每行有n个元素，每列有m个元素，刚开始学数字图像处理的时候，总是分不清行和列，高和宽是啥。哈哈哈，还记得我们的矩阵乘法么？我们首先研究的是$Ax=b$里面的\u0026quot;Column Picture\u0026quot;就是列空间的一种体现，也就是把A中的每一列当做空间内的一个向量，通过(ii)的方式组合出的新向量 $\\textbf{b}$一定也是在A的所有列所在的空间内，这就是列空间。\n The column space consists of all linear combinations of the columns.The combination are all possible vectors $Ax$.They fill the column space C(A)\n 所以Ax=b如果关注x就是解方程组，如果关注b就是列向量空间，当然，这两个的主要决定因素还是A，如果b在A的列向量空间内，也就是某个$x_1$线性组合A的列，得到$b_1$，那么这个$Ax=b_1$的解存在（唯一不唯一不一定），而且$x_1$是一个解；这又引出了另一个大课题，$Ax=b$到底特么的有多少个解？！\n the system $Ax=b$ is solvable if and only if b is in the column space of A\n 举一个🌰：: 例子就是上面那段话的正规描述。\nSpan(张成) 张成，tensor翻译成\u0026quot;张弛\u0026rdquo;，搞得数学名词都姓张，但是张成这个词还是比较形象的，想象一把伞，散的骨架有很多细的支撑骨架，一打开，张成一把伞，其实这个很形象；如果你有棍子和布，如果有一根棍子，你只能撑出一个直线（晾床单模型），如果两个棍子不摆在一条直线，你可以撑起一个面，如果你有三根棍子（两两不在一条直线），就可以撑出一个四面体，一个三维图形，棍子就张成了对应的空间。这个有点牵强附会，但是可以简单的类比一下，向量也是，两个不在同一条直线上的向量，就能通过线性组合，组合出他们张成平面上的所有向量，对于更高维同理。对于矩阵A的n列，他们可以张成n维空间，或者小于n维的空间，当m(行数)\u0026gt;n(列数)那么就是 $\\Re^m$的n维（或者小于n维）子空间。 描述的有点啰嗦，如果能听懂最好，听不懂就可以去看看书了。\n S = Set of vectors in V($v_1,v_2,v_3,\\dots$,v_n) $$ SS=c_1v_1+c_2v_2+\\dots+c_nv_n $$\n 其中 $\\textbf{c}$ 可以使任何实数，我们就说SS是S张成的，并且SS是S所在空间的一个子空间。 张成对应基的概念在下一课。\n Definition: A set of vectors spans a space if their linear combinations fill the Space\n Conclusion 这篇有点长啊，写了三天，当然是断断续续的，如果阅读不连贯也是正常的，这个是线性代数的核心内容，希望对大家有帮助。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-3-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本章介绍线性代数的核心内容，关于Vectors Space和subspace的一些观点，本文作为第一篇，主要说明基础知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 向量空间，子空间，列空间，张成\u003c/p\u003e","title":"【线性代数】3-1:向量空间(Space of Vectors)"},{"content":"Abstract: 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下 Keywords: CNN Visualizing\n卷积神经网络的可视化 前言 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下，写成博客，代码后续给出，不足之处还请大家指出。 大型卷积神经网络在图片分类上很成功，然而我们不知道他为什么能表现的如此不错，或者如何提高。\nAbstract：  In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of inter-mediate feature layers and the operation of the classifier 我们研究一个优秀的可视化技术，能够给出函数内部特征层以及分类层的信息 Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. 可视化使我们找到比Kri在ImageNet分类更好的网络架构。 We also perform an ablation study to discover the performance contribution from different model layers. 我们通过切块研究发现不同层对分类的作用。 We show our ImageNet model generalizes well to other datasets: when\u0026gt;the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. 我们展示了我们的 ImageNet 模型在其他数据集上获得优秀的表现：当我们重新训练SoftMax分类器。其结果信服的打败了当前SOTA结果，在Caltech-101和Caltech-256数据集上\n 评论：作者要解决的是可视化深度学习模型，来给出内部的结构，工作原理，以及内在结构的相关性等。并且在这个基础上反向选择优化不同的深度架构（模型）来得到更好的模型，并给出了监督学习的Pre-training方法，在不同测试数据集上表现不俗。\nIntroduction 卷积神经网络很牛，在各种分类比赛上获得state-of-the-art的结果。\n 卷积神经网络在各大测试集上获得好结果的原因： Several factors are responsible for this renewed interest in convnet models:\n  (i) the availability of much larger training sets, with 大量的训练数据\n  (ii) powerful GPU implementations, making the training of very large models practical GPU的高效计算\n  (iii) better model regularization strategies, such as Dropout (Hinton et al., 2012). 更优秀的网络结构（例如Dropout）\n  Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error. 如果不知道内部原因，我们的新模型只能停留在实验，观察的基础上\n  In this paper we introduce a visualization technique that reveals the\u0026gt;input stimuli that excite individual feature maps at any layer in the\u0026gt;model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. 本文提出了一种可视化技术，其能够揭示输入是如何激活那些独立的特征映射在模型中的任一层。这项技术也允许我们来观察特征在训练过程中的进化过程来判断模型潜在的问题。\n  The visualization technique we propose uses a multi-layered\u0026gt;Deconvolutional Network (deconvnet), as proposed by (Zeiler et al., 2011), to project the feature activations back to the input pixel space. 我们提出了使用多层逆卷积网络，（Zeiler et al 2014年）提出的，将特征反向映射会到输入层观察结果\n  We also perform a sensitivity analysis of the classifier output by occluding portions of the input image, revealing which parts of the scene are important for classification 通过遮挡输入图片的部分，对分类器进行分析，来揭示哪些部分对分类结果产生相对重的影响\n  Using these tools, we start with the architecture of (Krizhevsky et al., 2012) and explore different architectures, discovering ones that\u0026gt;outperform their results on ImageNet. 使用这些工具，我们开始使用此架构探索不同的架构，认识在ImageNet上表现出色的结构\n  We then explore the generalization ability of the model to other datasets, just retraining the softmax classifier on top. 我们随后探索架构对于其他数据集的范化能力，在只重新训练softmax分类器的基础上。\n  As such, this is a form of supervised pre-training, which contrasts with the unsupervised pre-training methods popularized by (Hinton et\u0026gt;al., 2006) and others (Bengio et al., 2007; Vincent et al., 2008) 监督学习的Pre-training来对比无监督的Pre-training方法（Hinton et al., 2006 Bengio et al., 2007; Vincent et al., 2008）\n 评论：主要就是说，以前都是不知道为啥深度学习会工作，不知道如何优化，只是考实验观察，现在我们能牛x的知道为啥能工作了，虽然没有数学证明，但我们知道怎么调了，知道工作原理，知道怎么Pre-training。。。\nRelated Work  Our approach, by contrast, provides a non-parametric view of invariance, showing which patterns from the training set activate the feature map. 我们的工作提出了一种无参数的不变性观点，来展示训练数据的哪些部分激活了特征映射\n 评论：没有评论\nApproach  We use standard fully supervised convnet models throughout the paper, as defined by (LeCun et al., 1989) and (Krizhevsky et al., 2012). 我们在整篇文章使用标准完全监督卷积网络模型，在 (LeCun et al., 1989) and (Krizhevsky et al., 2012)定义的。 (i) convolution of the previous layer output (or, in the case of the 1st layer, the input image) with a set of learned filters;\n  (ii) passing the responses through a rectified linear function (relu(x) = max(x, 0));\n  (iii) max pooling over local neighborhoodsand\n  (iv) a local contrast operation that normalizes the responses across feature maps.\n  The top few layers of the network are conventional fully-connected\n  networks and the final layer is a softmax classifie\n Visualization with a Deconvnet  We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps. 我们提出了一个高级的方法来映射激活反向到输入像素空间，来展示在特征空间哪一部分输入引起了这个给定的激活 In (Zeiler et al., 2011), deconvnets were proposed as a way of\u0026gt;performing unsupervised learning. Here, they are not used in any learning capacity, just as a probe of an already trained convent. 在(Zeiler et al., 2011)，Deconvnets 被提出作为一种表现非监督学习的方法。这里他们不再用于任何其学习能力，只是用于研究已经训练好的Convnet To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer. 为了测试一个给定的神经元激活，我们设置所有其他的同层神经元激活值为零，传导特征映射作为输入来激活deconvnet层 Then we successively (i) unpool, (ii) rectify and (iii) filter to reconstruct the activity in the layer beneath that gave rise to the chosen activation. This is then repeated until input pixel space is\u0026gt;reached. 在激活选定特征的神经元后Unpool，rectify，filter来重建本层的激活。\n Unpooling：  In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the\u0026gt;deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. See Fig. 1(bottom) for an illustration of the procedure 最大池化不可逆，我们通过记录位置来进行近似，记录被称为一组switch值，在deconvnet中，逆池化使用这些switch值来定位重建上一层，保留激活分布。在Fig1中说明\n Rectification：  The convnet uses relu non-linearities, which rectify the feature maps thus ensuring the feature maps are always positive. To obtain valid feature reconstructions at each layer (which also should be positive),we pass the reconstructed signal through a relu non-linearity. Convnet使用ReLu非线性函数，保证激活值非负；为保证每层特征可重建，我们让所有重建信号经过ReLu层。\n Filtering：  The convnet uses learned filters to convolve the feature maps from the previous layer. To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not\u0026gt;the output of the layer beneath. Convnet使用学习到的Filters从前一层来获取特征映射。相反，deconvnet使用同一filter的转置，但是操作的对象是整流结果(Rectification)，而不是之前的层。\n 总结：  Since the model is trained discriminatively, they implicitly show which parts of the input image are discriminative 由于模型是训练成有区别的，因此他们理所应当的展示输入图片的那些部分是有区别的。 Note that these projections are not samples from the model, since there is no generative process involved 注意这些映射不是从模型中采样，因为没有范化处理涉及。\n 评论：此段描述了具体如何反向将特征映射到像素空间。\nTraining Details  The architecture, shown in Fig. 3, is similar to that used by (Krizhevsky et al., 2012) for ImageNet classification 结构在Fig 3中（本文第一张图）。。。\n 训练方法：\n The model was trained on the ImageNet 2012 training set (1.3 million images, spread over 1000 different classes). Each RGB image was preprocessed by resizing the smallest dimension to 256, cropping the center 256x256 region, subtracting the per-pixel mean (across all images) and then using 10 different sub-crops of size 224x224 (corners+ center with(out) horizontal flips). Stochastic gradient descent with a mini-batch size of 128 was used to update the parameters, starting with a learning rate of 10−2, in conjunction with a momentum term of 0.9. We anneal the learning rate throughout training manually when the validation error plateaus. Dropout (Hinton et al., 2012) is used in the fully connected layers (6 and 7) with a rate of 0.5. 此处翻译略过，描述了卷积神经网络的训练方法。 Visualization of the first layer filters during training reveals that a few of them dominate, as shown in Fig. 6(a). To combat this, we\u0026gt;renormalize each filter in the convolutional layers whose RMS value exceeds a fixed radius of 10−1 to this fixed radius 第一层 Filter的可视化在训练过程揭示，其中一部分起支配作用，如Fig 6 a 所示，为了对抗这种情况，我们重新归一化RMS值超过fixed-radius的0.1倍的每一个在卷基层的Filter\n 评论：详细的训练过程\nConvnet Visualization 关于特征：\n Feature Visualization: Fig. 2 shows feature visualizations from our model once training is complete. However, instead of showing the single strongest activation for a given feature map, we show the top 9 activations. 特征可视化：图2显示的特征可视化是当模型训练完成时就确定的，然而不显示对于给定特征映射的单一强刺激而显示top9\n  Alongside these visualizations we show the corresponding image patches. These have greater variation than visualizations as the latter solely focus on the discriminant structure within each patch. 沿着这个可视化我们可以观察到相当的当前图像区域。这有相当大的可视化成都相对于单独把注意力放到每一个path。\n  The projections from each layer show the hierarchical nature of the features in the network. 不同层的映射表现出网络中不同自然层级的特征\n  Feature Evolution during Training: Fig. 4 visualizes the progression during training of the strongest activation (across all training examples) within a given feature map projected back to pixel space. 特征在训练过程中的进化：图4，训练较强反应的神经元（在所有训练样本中）在给定特征映射逆向投影到像素空间的过程中的可视化。\n  Sudden jumps in appearance result from a change in the image from which the strongest activation originates. 表面上突然的跳跃来自图像最强的激活区域（此区域能够激发网络中的部分神经元产生大的特征变化） The lower layers of the model can be seen to converge within a few\u0026gt;epochs. However, the upper layers only develop after a considerable number of epochs (40-50), demonstrating the need to let the models train until fully converged. 模型的较低层可以在一定周期内观察到。然而，高层的网络只在相当大的周期后才能被建立起来，表明模型需要继续训练到完全收敛 Feature Invariance: Fig. 5 shows 5 sample images being translated,rotated and scaled by varying degrees while looking at the changes in the feature vectors from the top and bottom layers of the model,relative to the untransformed feature. 特征独立性：图5，显示五个样本图经过变换，旋转，缩放多种随机模型，然后从底层到高层观察特征向量与未变换的特征向量进行对比\n 小的变换对于模型的第一层有显著影响，但是对于顶层特征影响不大，对于变换和缩放大致呈线性\n The network output is stable to translations and scalings. 网络输出对于变换和尺度缩放稳定。 In general, the output is not invariant to rotation, except for\u0026gt;object with rotational symmetry (e.g. entertainment center). 然而，输出对于旋转变换不稳定，除非是对齐式的旋转。\n 评论：训练过程中的特征是怎么来的。\nArchitecture Selection  While visualization of a trained model gives insight into its operation, it can also assist with selecting good architectures in the first place. 当可视化一个训练好的模型给出了其内部的操作，也能帮助我们选取更好的架构。 The first layer filters are a mix of extremely high and low frequency information, with little coverage of the mid frequencies. 第一层filter是混合了极其高频和极其低频的信息，只有少量的中频信息。 Additionally, the 2nd layer visualization shows aliasing artifacts caused by the large stride 4 used in the 1st layer convolutions. 第二层可视化展示了混淆的手工结果由在第一层中较大的步长（4）引起的 To remedy these problems, we 解决办法： (i) reduced the 1st layer filter size from 11x11 to 7x7 (ii) made the stride of the convolution 2, rather than 4.\n  This new architecture retains much more information in\u0026gt;the 1st and 2nd layer fea- tures, as shown in Fig. 6(c) \u0026amp; (e). More importantly, it also improves the classification performance as shown in Section 5.1.\n 评论：本段讲如何选取架构，说明步长在其中的影响\nOcclusion Sensitivity  With image classification approaches, a natural question is if themodel is truly identifying the location of the object in the image, or just using the surrounding context. 对于图像分类的应用，一个自然的问题是模型是否只利用图片中的物体，还是使用周围的上下文信息 Fig. 7 attempts to answer this question by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier. 图7试图回答这个问题，通过系统的遮挡输入图片不同的位置，使用一个灰色方框，然后监视分类器的输出\n  When the occluder covers the image region that appears in the visualization, we see a strong drop in activity in the feature map. 当遮挡覆盖到可视化中出现的区域，我们发现在特征映射层有一个强烈的drop Fig. 4 and Fig. 2. 图4和图2\n 评论：遮挡不同区域的影响，不同区域敏感度不同。\nCorrespondence Analysis 一致性分析\n Deep models differ from many existing recognition approaches in that there is no explicit mechanism for establishing correspondence between\u0026gt;specific object parts in different images (e.g. faces have a particular spatial configuration of the eyes and nose) 深度学习模型与现存其他识别机制不同在于：其不存在对于在不同图片之间某些物体的特殊部分之间的准确的区别关系（例如：脸部存在一个鼻子和脸的特别空间关系） 不同的特征向量计算公式：\n  We then measure the consistency of this difference vector delta between all related image pairs (i, j):\n  我们然后计算所有图片对之间的不同。 where H is Hamming distance. H为汉明距离 A lower value indicates greater consistency in the change resulting from the masking operation, hence tighter correspondence between the same object parts in different images (i.e. blocking the left eye) 在遮挡操作的变换结果中，一个较低的值表示部件之间较大的相关性fig8：\n Table 1 评论：不同特征的独立性验证，如果你有鼻子眼睛嘴的脸部特征，遮住鼻子对最后的特征向量影响不大，说明他们之间的相关性比较强，类似于一张图如果有鼻子，基本也有眼睛，所以你遮住眼睛也会得到差不多的特征向量。 总结：简单的学习了一下这篇文章，后面第五部分讲的是经验，关于如何训练高质量的网络，会在下一篇推出，欢迎收看。\n总结 又是一片翻译文，哈哈哈哈，有需要的可以读读，营养还是有的，只是我现在不太喜欢这种文章\n","permalink":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-visualizing-and-understanding-cnn.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 研究卷积神经网络，把阅读到的一些文献经典的部分翻译一下\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CNN Visualizing\u003c/p\u003e","title":"Visualizing and Understanding CNN"},{"content":"Abstract: 学习LeNet的论文总结Stochastic Gradient 与 Batch Gradient 的比较 以及收敛速度 Keywords: Stochastic Gradient,Batch Gradient，Faster Convergence\nLeNet论文解读 Stochastic Gradient vs Batch Gradient 基于梯度的算法可以使用两种中的一种来更新参数\n 第一种叫做Batch Gradient 另一种叫Stochastic Gradient  Batch Gradient(BG) 这是最传统的一种梯度算法，梯度是通过对所有训练样本进行计算得出，然后更新所有参数。\nStochastic Gradient(SG) 是一种局部的，包含噪声的梯度估计法，这种方法只计算一个或者一组较小数量的训练样本来估算梯度，之后更新所有参数。\n样本一般算计选取，或者先把样本随机排队，然后逐个（组）使用。\n随机梯度下降在中国著名讲师Ng的公开课中也是最先提到的两种方法之一，另一种就是Batch Gradient，在随机梯度下降中，参数更新更快，单次更新方向虽然不是朝向全局最小方向，但大量迭代后，能够快速收敛到最优解。\n尤其是大数据量的时候，SG具有显著优势\nBG vs SG 本文表示，截止到论文发表时，原理未知，但是可以通过简单的实验来测试得出一个直观上的认识，下面简单叙述一下实验： 受限在一个完整的训练集里又完全一毛一样的两个子集组成，例如全集是A，子集 a={1，2，3，4} ，那么全集A就是{1，2，3，4，1，2，3，4}。\n方案一：通过全集来估计梯度，很明显，大量pattern（pattern被翻译成模式，我觉得不太直观，还不如就叫pattern，可以理解为每个样本特有的一些性质）是重复的。\n方案二：使用SG，每次随机选A的一半来更新梯度，与方案一在同样的运行时间，方案二迭代了接近两次。在数据量方面，有效信息，方案二并不比一少很多。\n这种方式可以范化到所有包含重复Pattern的训练样本的情况。\n其他方法 另外还有很多优化方法：\n二阶算法： 1.例如Gauss-Newton（GN）方法或者Levenberg-Marquardt（LM）方法 2. Quasi-Newton 方法包含Broyden-Fletcher-Goldfarb-Shanno（BFGS） 方法，Limited-storage BFGS（ls-BFGS）算法 3. Conjugate Gradients（CG）算法的变种\n很遗憾，以上方法在大训练样本的情况下都不稳定。\n GN和LM是 $O(N^3)$ 的时间复杂度，N是参数数量 QN是 $O(N^2)$ 的时间复杂度 ls-BFGS和CG是 $O(N)$ 的复杂度 以上算法的收敛速度取决于选择下降的方向是否准确，而要想准确就要使用全体数据（BG）  对于大量样本，以上方法的加速结果远不如SG的效果，一些研究者想使用小batch的cg算法，目前没啥效果。\n作者的经验是使用随机方法来优化参数，在error surface上寻找最小值：\nerror surface就是：\nConditions for Faster Convergence \u0026ldquo;Gradient Based Learning Applied to Document Recognition \u0026ldquo; LeCun, Y Bottou, L Bengio, Y Haffner, P 学习LeNet，从附录开始，附录介绍了一些非常有用的基础知识，简单总结一下\n压缩函数 Squashing function，压缩函数，定义域的范围远远大于值域，这样的函数就是压缩函数，神经网络最常用的就双曲正切函数：\n$$f(a)=A tanh(Sa)$$\n$$tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n本文指出：\n收敛速度 对称函数能产生更快的收敛，当权重weights非常小的时候，收敛将变得及其的缓慢收敛变慢的原因：在权重空间，学习动力（learning dynamic）是一个固定点，或者，进入鞍点，各个方向都是下降方向。 上图为最简单的单元，包含输入weights和激活函数（压缩函数），其反向传导（如不有疑问查询BP神经网络相关知识）公式为：\n$\\delta_i^{(l)}=(\\sum_{i=1}^{s_{l+1}}W^{(l)}_{ji}\\delta^{l+1}_j)f\u0026rsquo;(z_i^{(l)})$$\n观察公式可以得出，如果导数过小或者weight过小都会使更新距离变短，这样会使收敛速度变慢。\n把激活函数设置成A=1.7159，S=2/3 ，$f(1)=1$，$f(-1)=-1$这么做背后的原因是在正常的处理情况下，压缩的结果会在1附近，这样神经网络的表现将会非常简单。并且f的二次导数在1和-1达到最大值，这使得优化朝向学习最后的阶段（权重收敛的全局最优解）。\n观察下图： tanh函数图像： tanh函数一阶导数： tanh函数二阶导数 二阶导数绝对值在+-1附近最大，二阶导数最大意味着一阶导数最大，这样就意味着梯度（一阶导数）在这附近有最大的变化，这样使得损失函数顺着下降的方向走去。。\n参数初始化 特别选择的参数只是能够方便一些，并不影响最终结果\n参数初始化：无论是人手工设计还是随机设置，都不太影响结果；\n训练开始之前，对所有权重进行均匀分布随机初始化（在 $\\frac{2.4}{F_i}$ 和 $\\frac{-2.4}{F_i}$ 之间,F为所有连接的总数fan-in，扇入）\n图中每一个w所在的输入都被称为一个fan-in（像扇子一样的输入），图中有五个扇入。\n如果一些连接有相同的权重，那么问题就变得复杂，但本文的网络（卷积层）对应同一Fan-in的所有权值都是share的，通过fan-in来划分的原因是我们希望加权和的初始化标准差在一个范围内，并且这个加权和落在激活函数的正常工作区。如果初始化权值太小，梯度会很小，优化速度将会变慢，如果权值过大，激活函数会饱和，梯度也会很小。当输入相互独立的时候加权和的标准差的比例类似于输入数量的标准差，当输入高度相关的时候，加权和的标准差接近于输入数量的线性关系。我们选择第二种假设，因为输入往往高度相关\n卷积神经网络第一大特点：权值共享，也就是说同一fan-in，在整个过程中值是一样的，将fan-in初始化到一个范围内，可以控制其结果落在激活函数的一定区域内：\n因为落在外面区域将会使得激活函数过大，一阶导数过小。\n按照原文所说，如果输入数据相关性强，输出大概呈现线性关系，如果输入不相关，输出呈现平方根关系。因为输入是图像的一个区域，高度相关，所以其结果大致呈线性。\n本系列还有后续，慢慢期待\n总结 还是中期读论文的博客风格，有点说教的感觉，大家可以在看论文看不太懂的时候来这里找找看有没有你需要的知识。\n","permalink":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-lenet.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 学习LeNet的论文总结Stochastic Gradient 与 Batch Gradient 的比较 以及收敛速度\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Stochastic Gradient,Batch Gradient，Faster Convergence\u003c/p\u003e","title":"LeNet论文解读"},{"content":"Abstract: 使用dropout处理过拟合，原论文的理解总结 Keywords: Dropout，Overfit\nDroupout 的论文 本文从原来的博客迁移过来，主要是学习dropout的论文，一些翻译和自己的理解\nAbstract 深度神经网络因为拥有大量的参数，所有具有非常强大的学习功能，而大量参数带来的问题就是过拟合（overfit），过拟合严重影响模型的范化性能，而大型深度神经网络计算速度又较慢，很难在实际应用中通过联合多个模型来给出结果。\nDropout就是来解决这种问题的方法，其中心思想是在训练的过程中随机的裁剪神经网络中的部分单元来获得完整模型的子模型，对子模型进行训练。这使得所有单元对于训练数据的过度适配。子模型的数量对于原模型是指数级的（子集与全集的关系）。\n在测试过程中，对所有子网络预测结果的联合结果也有比较简单的近似方法，通过减小整个网络的连接权重来近似平均。\n值得注意的是，这种方法可以有效减轻过拟合，相比于其他正则化方法，dropout表现更好，在监督学习如视觉识别，语义识别，文档分类等给出了state-of-the-art的表现。\nIntroduction 深度神经网络包含多个非线性隐含层，这使得它能够学习非常复杂的模型，在有限的训练集上一些噪声的采样会造成某些复杂的输入输出关系，所以他们只存在于训练样本中，而在实际的测试阶段不存在这类样本，即使其拥有与实际样本相同的概率分布。这就造成了过拟合，为此我们提出了很多方法来减轻其影响：\n1. 当出现过拟合迹象时马上停止训练\n2. 权重惩罚，包含L1，L2正则化等\n3. 软权重共享（soft weight sharing）\n如果有无限的计算资源，最好的正则化的方法是对于一个固定大小的模型，对于所有可能的参数集给出的预测进行加权平均，每个模型的权重由其对训练样本后验概率给出。这种方法对于简单的小模型比较有用，但是我们更想要使用更小的计算量来完成贝叶斯黄金准则（Bayesian gold standard）。为完成上述过程，我们对共享参数的模型（一个完整的大模型）的指数级个数的子模型（大模型dropout的子模型）的预测进行等权重几何平均。\n模型联合几乎总是可以提升机器学习方法的表现，但对于大型模型，这个将会非常难达成。当每个模型之间完全独立时，联合多个模型时能有测试表现有较好的提升，而对于获得完全独立的模型，要么有不同的架构（architecture）要么使用不同的训练数据。训练不同架构的模型看起来有些吃力，因为每个模型都拥有很多的超参数，优化一个模型将耗费大量的时间，并且一个大的模型通常需要大量的数据，所以训练多个大型模型使用多个不同的大训练集，这非常困难。即使有人能够满足上面所有的条件，无限的计算量，超大的数据集，但是在测试的时候需要快速响应，这也是个计算瓶颈。\ndropout就是来解决上述两个关键点的：\n1. 大量独立的模型\n2. 不是那么大量的训练集\n其基本描述是阻止过拟合，提供一种方法有效的联合指数级的不同的神经网络的近似形式。dropout的意思是丢掉网络中的一些单元（可见的或隐藏的神经元），通过暂时性的移除一些单元，包括其输入输出链接全部移除，移除的单元是随机挑选的，如图：\n对于一个简单的例子，每个单元按照一个固定概率P决定取舍，所有单元之间相互独立，P可以设置为一个集合（每个单元拥有自己的P）也可以简单的把所有单元都设置成0.5。设置成0.5就接近于找到模型所有子集的集合。对于输入单元，p一般选择大于0.5小于1之间的数（使输入尽量完整些）。\n对于一个子模型，是dropout后剩下的所有单元的子网络，如图1b，一个有n个单元的网络拥有 $ 2^n $个可能的子集。所有这些网络共享参数，所以参数还是 $O(n^2)$ 或者更少。然后按照正常方法训练子网络。所以训练一个dropout的神经网络可以被看做训练一个$2^n$个子网络的集合，所有子网络共享权值，有些子网络很少，甚至没有被训练过。\n在测试阶段，明确的平均地平均指数级子网络的输出结果是不可能的，然而在实际中有一种非常简单的近似方法，这种思想被用于神经网络的测试阶段，而不需要把网络dropout后再测试。网络中的权重被缩小（与训练时的子网络比较），如果子网络选取方法是按照概率p来dropout，测试时所有权重乘以p来， 如图2：\n来确保任何隐含层测试时的输出与激活输出（训练时dropout的情况下的激活输出）一致，通过scale-down（缩小）2^n个共享权值的网络组合成一个网络在test过程中使用。我们发现使用这种训练时dropout，测试时加权合并近似的网络，在大量分类任务中 与其他正则化方方法相比，dropout的泛化误差相当的小。\ndropout的思想不止用于feed-forward神经网络，其可以被更广泛的用于以图（graph）为基础的模型上，例如玻尔兹曼机，在这篇文章中，我们将介绍dropout版受限玻尔兹曼机，并与标准的受限玻尔兹曼机（RBM）相比较，作者经验，dropout RBM比较好。\n本文结构：\nSec2：此方法的灵感来源\nSec3：类似工作\nSec4：正式描述dropout模型\nSec5：训练dropout模型的算法\nSec6：给出作者们实验的数据\nSec7：分析dropout在不同性质的神经网络的上表现，并描述为何dropout能工作\nSec8：dropout RBMs\nSec9：marginalizing dropout的思想\nAppendix A：实际训练网络的知道，包括实际情况下详细的分析，包括在训练dropout时如何选择超参数\nMotivation dropout的动机是进化论里面的有性生殖法则，有性生殖是通过获得双亲各一半的基因加上一小部分突变来构成一个子代。无性生殖是直接复制上一代完整基因序列完成。表面看起来无性生殖应该是更好的方法优化个体适应性，因为一套已经工作很好的基因序列可以完整的直接传递给下一代。另一方面，有性生殖似乎打破了这套调整有限（不太变化）的基因集合，特别是这个集合非常庞大，表面上看，这使得这套已经进化的非常复杂的基因对于有机体的适应性降低。然而，有性生殖是生物进化的最优方式。\n一个可能的解释有性生殖优势的可能是，长时间以来，自然选择准则可能不是个体适应性能够应对的，而需要基因组合后的综合能力，通过基因随机组合使个体变得强大，由于基因不能表现出所有祖先的，但是能够学习到一些有用的特点，或者和一些其他的小数其他基因配合产生个体强有力的适应性（基因突变）。 根据这理论，有性生殖不只是保留并传递有用的基因，同时也减少个体基因对个体过度的封闭性（这将不利于个体获得新的特性）。类似的，神经网络的每一层隐藏单元使用dropout训练必须随机选取网络中的单元。这使得隐含层单元更robust，并驱使子网络学习有用的特征，依靠单元本身而不是其他层的单元来修正自身错误。而且，同一层隐含单元各自学习不同的特征。可能有人认为通过复制每个隐含层单元来使得网络比dropout训练出来的更加robust，但实际上这个烂主意。 一个对于dropout相近但又稍微不同的动机，来自如何搞破坏，如果十个阴谋每个阴谋需要五个人来执行，和五十个人完成一个大阴谋相比，前者的破坏性更强，因为对于五十个人要紧密合作产生的破坏性，五个人更容易完成一些小阴谋，所以当大环境变了，五十个人的训练可能与实际环境不一样，阴谋没办法产生效果，五个人的阴谋成功概率会大一些，所以产生的破坏性更大一些。\nRelated Work Dropout可以当做通过在隐含层加噪声来正则化神经网络。给神经元加噪声这种方法被用于DAEs，在那里，噪声加到自动解码的输入单元，神经网络被训练来重建没有噪声的输入。本文的工作延展了这个思路，通过展现Dropout能够有效地利用在隐含层，也可以解释为模型的平均。我们也发现，加入噪声不知能用于非监督特征学习，也可以用来延展监督学习问题。实际上，我们的方法可以被用于其他以神经元（unit）结构，例如：玻尔兹曼机。Dropout 裁剪掉20%的输入单元，50%的隐含层单元经常是最优的。\n由于dropout可以被看做随机正则化技巧，自然的被看做近似边缘化噪声的同类问题。本文中，我们可以用简单的例子说明，Dropout可以边缘化来得到确定性的正则方法。最近也有人用dropout类似的方法做实验，但仅限于dropout输入，而未对隐含层进行。\n在dropout过程中，我们随机优化包含噪声分布的损失函数，这能够被看成是最小化一个损失函数期望。\n原理 对于一个L层隐含层的神经网络 $l\\in {1,\u0026hellip;..,L}$，$z^{(l)}$ ,第l层的输入，$y^{(l)}$ 定义为第l层的输出 $y^{(0)}=x$ 是输入，$W^{(l)}$ 和 $b^{(l)}$为weight和biases\nfeed-forward：\n$$z_i^{(l+1)}=w_i^{(l+1)}+b_i^{(l+1)}\\ y_i^{(l+1)}=f(z_i^{(l+1)})$$ f是激活函数： $$f(x)=1/(1+e^{(-x)})$$\n加入dropout后:（敲黑板，同志们，注意啦，下面是整个文章的核心啦） $$ r_j^{(l)}\\sim Bernoulli(p)\\ \\tilde{y}^{(l)}=r^{(l)}*y^{(l)} \\ z_i^{(l+1)}=w_i^{(l+1)}\\tilde{y}^l+b_i^{(l+1)}\\ y_i^{(l+1)}=f(z_i^{(l+1)}) $$\n上图为正常神经网络和dropout后的网络结构比较 对于layer l $r^{(l)}$ 是一个独立的伯努利随机分布的向量，其中元素为1的可能性为p。这个向量的目的是dropout，随机的选取一些输出，也就是说随机的去掉一些连接，这样的结果就是从 $y^{(l)}$ 来得到dropout后的裁剪过的 $\\tilde{y}^{(l)}$ ，这个裁剪过的输出作为下一个单元的输入，这种操作应用于所有layer，这样的整体结果，网络被随机裁剪成一个子网络。 在训练阶段，loss函数的导数反向传导只经过子网络，相当于其他被dropout的节点将不被更新。 在测试阶段，所有weight被缩放 $W_{test}^{(l)}=pW^{(l)}$ ，此后不再使用dropout 如图： 总结 现在来看，这篇文章主要是翻译工作，没有什么自己的理解和感受，因为阅读原文你会发现，你能想到的，文章都有，你想不到的，文章也有一部分，可能这就是大牛的论文吧\n","permalink":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-%E9%98%85%E8%AF%BB-dropout-prevent-nn-from-overfitting.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 使用dropout处理过拟合，原论文的理解总结\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Dropout，Overfit\u003c/p\u003e","title":"Droupout 的论文解读"},{"content":"Abstract: 矩阵的转置和行变换（permutation），包含一些运算的转置，以及对称概念的提出和相关性质 Keywords: Transposes,Permutation,Symmetric,Inner Products,R\u0026rsquo;R\n本课视频课程已上线：转置和换行\n开篇废话 这些基本运算的篇，好难写，公式和基本逻辑太多，说少了说不明白，说多了又啰嗦。本来计划的是写短小精悍的，基本每篇就写一个知识点，现在看看是不行了，这些东西都太连贯了，没办法拆开，争取后面到了高级算法的时候就可以每篇写很短，写精髓了，这一些就是一两千字，对我有点挑战啊，哈哈哈。如果各位有看不懂的，请回顾以前的文章，因为我是按照基本逻辑来的，就是一个知识点衍生另一，不会凭空就搞出来什么知识点，那样又变成大学上课了，big Picture一定要有，就是我们第一篇线性代数的，big Picture！\n转置(Transposes) 转置(Transposes) 转置是矩阵特有的计算，他的根本就是矩阵是一块数字，其中有顺序和位置关系，今天说的转置和置换，都是针对位置的，也就是元素的数值并不改变，要改变的是元素的位置关系，permutation我们后面再说，transpose的计算规则的就是，对于某元素，其位置行和列相互交换 $$ (A^T){ij}=A{ji} $$ 一个下三角矩阵的transpose是上三角矩阵。 但是下三角矩阵的逆还是下三角矩阵。\n映射(Properties) sum: $$(A+B)^T=A^T+B^T$$ Products: $$(AB)^{T}=B^{T}A^{T}$$ Inverse: $$ (A^{-1})^T=(A^T)^{-1} $$ 这里唯一有点疑惑的可能就是Product需要调换位置了，想要理解为啥要换位置，可以回归最简单的 $Ax=b$ 等号有个很好的作用就是，如果对两侧同时做某一操作的时候，等号不会改变， $$ (Ax)^T=b^T $$ 我们来分析下，如果按照我们之前的解释，Ax是x作为系数关于A的线性组合，从矩阵乘法的角度就Column Model，其结果是b，一个列向量，用A各列线性组合出来的结果。如果b转置了，变成了一个行向量，那么我们对应的也应该改成Row Model来调整等号前面的结构了，所以 $$ b^T=x^TA^T $$ x转置变成了行，A的列也变成了行，那么行模型可以很好的让他们的结果与b的行形式相等 没明白的同学，拿笔算算，就会发现确实是这样的，而且如果不调换位置，矩阵不满足乘法要求，尺寸对不上。 那么根据上面的特征，组合多个x得到矩阵B $$ B=\\begin{bmatrix}x_1\u0026amp;x_2\\end{bmatrix} AB=\\begin{bmatrix}Ax_1\u0026amp;Ax_2\\end{bmatrix} B^TA^T=\\begin{bmatrix}x_1^TAT\\x_2^TA^T\\end{bmatrix} $$ 与上面情况吻合，证明在矩阵与矩阵相乘过程也符合情况。 在更多的矩阵相乘的时候，与逆的情况类似 比如上一节的男猪脚： $$ A=LDU\\ A^T=U^TD^TL^T\\ D^T=D\\ $$\n对角矩阵的转置还是他自己\n逆的转置就是转置的逆 $$ A^{-1}A=I\\ A^T(A^{-1})^T=I^T=I\\ $$ so $$ (A^T)^{-1}=(A^{-1})^T $$\n再说 Inner Product 之前不完整的介绍过内积，就是点乘，但是一般情况两个向量内积，写成转置形式\n$$ \\langle a,b\\rangle=a^Tb $$\n后面的转置才比较正规，内积普遍用于实际生产，比如机械，金融等各类公式，这样我们就非常接近应用数学的核心了。\n$$ (Ax)^Ty=x^T(A^Ty) $$\n$(Ax)$ 与 $y$ 的内积等于 $x$ 与 $A^Ty$ 的内积.\n对称(symmetric) 对称的定义是： $$ A^T=A $$ Means $$ a_{ji}=a_{ij} $$ 对称矩阵的逆也是对称矩阵，本篇讨论的主要是矩阵内部位置关系相关的计算。 对于对称矩阵A: $$ (A^{-1})^T=(A^T)^{-1}=A^{-1} $$\n$R^TR$ 对于一个转置和其本身的乘积，也是一个对称矩阵，证明很简单： $$ (R^TR)^T=R^TR $$ so $R^TR$是一个对称矩阵。 对称矩阵的消元过程也比较简单，对于对称矩阵 $A=LDU$ 其中$L^T=U$也就说上三角矩阵和下三角矩阵是转置关系，$(LDL^T)^T=LDL^T$ 也证明A是对称的，对称矩阵消元需要的计算量应该是非对称矩阵的一半大概是 $n^3/6$\nPermutation Permutation就是之前消元里面的行变换的矩阵化表示，其定义是：\n A Permutation matrix P has the rows of the identity I in any order 置换矩阵有很多特殊的性质，比如说 $p^{-1}=p^T$ 对于nxn的Permutation矩阵组,一共有 $n!$ 个矩阵\n $PA=LU$ 上节的LU分解有一个前提假设就是，不存在主元是0的情况，也就是说不需要行变换就可以达到消元的效果，但是现在我们去掉这个假设，使问题更为一般化，PA就是对A的某些行进行变换，后进行分解 对于行变换，有两种方式: 1:先对A进行行变换，然后分解，那么就是 $PA=LU$ 2:一边分解一边进行行变换，也就是在中间过程 $A=L_1P_1U_1$ 更多情况下，我们选择第一种方式，因为第一种方式更为简单方便。\nConclusion 总结一下就是转置和对称的相互关系，以及其一些特性，矩阵中元素的位置变换成为了本文重点。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-2-7.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 矩阵的转置和行变换（permutation），包含一些运算的转置，以及对称概念的提出和相关性质\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Transposes,Permutation,Symmetric,Inner Products,R\u0026rsquo;R\u003c/p\u003e","title":"【线性代数】2-7:转置与变换(Transposes and Permutation)"},{"content":"Abstract: 如何将矩阵分解成三角矩阵 Keywords: A=LU,A=LDU,Factorization\n本课视频课程已上线：A=LU分解\n三角矩阵 今晚苹果要新版本iPhone了，不知不觉iPhone已经十年了，然而我只用过iPhone4和6，技术的不断创新，给人们带来了方便，也改变了产业结构和生活方式，这应该与自然的变迁类似，无法阻挡的历史潮流，人类一切的进步都源自于对未知事物的探索，希望各位继续努力，为人类的进步，为人类与自然的和谐相处努力。\n因数分解(Factorization) 因式分解，开始学的时候肯定是分解多项式，将一串长的式子分解成几个因式相乘的形式，矩阵也可以，把一个矩阵分解成几个矩阵相乘的形式，但是问题来了，从表述上看，多项式分解的结果是整体变得简单了，但是矩阵分解好像越分越多啊，是多了，但是多出来这些矩阵都很有特点，他们的形状固定，大部分元素是0. 回想一下消元的过程\n A to U $$ E_{21}A= \\begin{bmatrix}1\u0026amp;0\\newline -3\u0026amp;1\\end{bmatrix} \\begin{bmatrix}2\u0026amp;1\\newline 6\u0026amp;8\\end{bmatrix}= \\begin{bmatrix}2\u0026amp;1\\newline 0\u0026amp;5\\end{bmatrix}=U $$ U to A\n $$ E_{21}^{-1}U= \\begin{bmatrix}1\u0026amp;0\\newline 3\u0026amp;1\\end{bmatrix}= \\begin{bmatrix}2\u0026amp;1\\newline 0\u0026amp;5\\end{bmatrix}= \\begin{bmatrix}2\u0026amp;1\\newline 6\u0026amp;8\\end{bmatrix}=A U $$ 从U到A的过程就是我们今天的男一号，$A=LU$\n消元的解释说明  1:$E^{-1}$ 都是lower triangular 下三角矩阵，对角线元素全部为1\n  2:$E^{-1}$ 就是$L$，把U变回A的系数矩阵\n  3:每个消元系数$l_{ij}$ 只会把对应的（i，j）位置的元素干掉，不会影响其他位置，尤其是已经完成消元的位置\n $A=LU$ 这是不包含行交换的消元过程的矩阵形式，上三角矩阵U对角线上全是pivot，对角下为0，L的对角线全是1，对角线下面是消元的乘子。 有两条规律，总结如下\n 1:A的某行开头是0，那么对应的L的位置也是0 2:A的某列开头是0，那么对应的U的位置也是0\n $A=LU$ 成立的关键还是要回顾消元过程，当某元素（pivot）所在的列一下都是0了以后，做下一个pivot消元的时候，对已完成的，为0的列没有影响，但是后面的非零列是有影响的，大部分非零列已经不是原始的列了，这种做法能保证解空间不变，但是矩阵的列空间已经发生改变了，具体后面讲到列空间的时候会有说明。\n$A=LDU$ $A=LU$ 数学家们喜欢0，喜欢1，喜欢对称，$A=LU$ 显然不那么对称，U对角线上是主元，L对角线上是1，这太不美观了 然后再分解一下， Divide U by a diagonal natrix D that contains the pivots 啥意思？就是这个意思: $$ \\begin{bmatrix} d_1\u0026amp;,\u0026amp;,\u0026amp;,\\newline ,\u0026amp;d_2\u0026amp;,\u0026amp;,\\newline ,\u0026amp;,\u0026amp;\\ddots\u0026amp;,\\newline ,\u0026amp;,\u0026amp;,\u0026amp;d_n\\newline \\end{bmatrix} \\begin{bmatrix} 1\u0026amp;u_{12}/d_1\u0026amp;u_{13}/d_1\u0026amp;.\\newline ,\u0026amp;1\u0026amp;u_{23}/d_2\u0026amp;.\\newline ,\u0026amp;,\u0026amp;\\ddots\u0026amp;\\vdots\\newline ,\u0026amp;,\u0026amp;,\u0026amp;1\\newline \\end{bmatrix} $$ 如果除法不好理解那就当做$D^{-1}U$然后根据row model 或者 col model一看，发现上面式子没问题。 所以系数矩阵A就能分解成 $A=LDU$\nWhy 你一定跟我当时一样心中一万匹羊驼在奔腾，觉得折腾这玩意有啥用啊，折腾过来折腾过去，没啥用啊，这么弄的目的是啥嘛，但是当天晚上回家看数值分析的书，刚好也讲这个过程，原来这么做的目的是为了减轻计算，举个例子$Ax=b$这种计算过程在工程应用里非常常见，而且多半时间是A不变，不同的b来解不同的x，那么按照高斯消元法，每次要从头消元，因为b改变了增广矩阵，但是很多计算是冗余的，所以使用三角矩阵的好处就是可以大大减少冗余计算\n 第一步：就是把矩阵分解成 $LU$ 或者 $LDU$ 形式（factor） 第二步：通过回代，把x求出来（solve）\n $$ Ax=b\\ LUx=b\\ c=L^{-1}b\u0026hellip;\u0026hellip;(1)\\ Ux=c\\ x=U^{-1}c\u0026hellip;\u0026hellip;(2) $$ 过程(1)(2)并不需要求逆，而是通过回代的过程进行，根据计算时间复杂度（也就是计算量，计算次数），factor的时间复杂度是$O(\\frac{1}{3}n^3)$，solve的时间复杂度大概是$O(n^2)$，如果你对时间复杂度不了解，可以去看《算法导论》的最开始那一章，这个理论还是非常有用的，尤其是对研究算法的童鞋。通过回代而不是消元，能够降低不少多余的计算。\n带状矩阵(band matrix) 带状矩阵，只有对角线附近元素不为0，其他位置都是0，这种矩阵factor的时间复杂度是$O(nw^2)$，solve的时间复杂度大概是$O(2nw)$ w是非零元素的个数\n总结 本文知识点比较连贯，就不总结了，总结的是一点，如果看一本书看不太懂，可以看看别的书，或者别人的解释，兼听则明，然后就有可能豁然开朗。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-2-6.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 如何将矩阵分解成三角矩阵\n\u003cstrong\u003eKeywords:\u003c/strong\u003e A=LU,A=LDU,Factorization\u003c/p\u003e","title":"【线性代数】2-6:三角矩阵( $A=LU$ and $A=LDU$ )"},{"content":"Abstract: 矩阵的“逆”，以及相关计算 Keywords: Inverse，Singular，Gauss-Jordan\n本课视频课程已上线：\n 逆矩阵 1 逆矩阵 2  矩阵的逆 逆 $A^{-1}$ 逆，就是乘法的逆，也就是你和你的逆乘起来等于单位的你，如果你是矩阵，那就是单位矩阵，如果你是实数，那逆就是倒数,当然如果是是0，你就没有逆了，如果有了，那就逆天了😆 逆的表示很简单 $$ I=AA^{-1} $$ 上面就是我那段解释的数学语言，$A^{-1}$ 是 $A$ 的逆，由于矩阵乘法有顺序问题，当A是方阵的时候： $$ I=A^{-1}A $$\n一个矩阵可逆，那么他的左逆和右逆一致，就是他的逆。\nNotes 可以理解为矩阵逆的性质或者特点，原文标记为Note\n Note1: The Inverse exist if and only if elimination produces n pivots(row exchanges are allowed)\n 并不是所有矩阵都有逆！所以，如何判断矩阵是否可逆（invertible）就是求矩阵逆的第一步，要是不可逆，那就不能求逆了。矩阵逆的存在当且仅当消元后产生n个主元（允许行交换）。\n Note2: The matrix A cannot have two different inverse.\n 解释下，利用了乘法结合律(parentheses)，利用左乘和右乘来证明矩阵的左逆和右逆相等 简单的证明： Suppose $BA=I$ , $AC=I$ Then $B=C$ : $$ B(AC)=(BA)C $$ Gives $$ BI=IC $$ or $$ B=C $$\n Note3: if A is invertible, the one and only solution to $Ax=b$ is $x=A^{-1}b$\n 这个Note主要是说明$Ax=b$有解的情况，也就是系数矩阵可逆，或者说主元数量为N 证明： $$ Ax=b $$ Then: $$ x=A^{-1}Ax=A^{-1}b\\\\ x=A^{-1}b $$\n Note4: This is important,Suppose there is nonzero vector $\\textbf{x}$ such that $Ax=0$ then Acannot have an inverse . No matrix can bring $\\textbf{0}$ back to $\\textbf{x}$\n 解释一下，这一条是重要的一个note，这是后面向量空间的一个重要结论（后面再说，这个不着急，这是线性代数的最核心内容），如果 $Ax=0$ ,其中x不是0，那么A没有逆，证明如下： $$ Ax=0\\ A^{-1}Ax=A^{-1}0\\ x=A^{-1}0 $$ 上面式子中$x\\neq0$所以等号不可能成立，那么$A^{-1}$不存在\n Note5: A 2x2 matrix is invertible if and only if $ad-bc$ is not zero: $$ \\begin{bmatrix} a\u0026amp;b\\newline c\u0026amp;d\\end{bmatrix}^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix}d\u0026amp;-b\\newline{-c}\u0026amp;a\\end{bmatrix} $$ 本note的解释就是，后面讲到行列式的时候就会有详细的证明了\n  Note6: A diagonal matrix has an inverse provided no diagonal entries are zero:\n $$ A=\\begin{bmatrix}d_1\u0026amp;,\u0026amp;, \\,\u0026amp; \\ddots \u0026amp;,\\,\u0026amp;,\u0026amp; d_n\\end{bmatrix} $$ then $$ A^{-1}=\\begin{bmatrix}1/d_1\u0026amp;,\u0026amp;,\\,\u0026amp; \\ddots \u0026amp;,\\,\u0026amp;,\u0026amp; 1/d_n\\end{bmatrix} $$ 这是一个关于对角矩阵的故事，对角矩阵的对角元素全部非零，其他元素为0，其逆是其所有元素的倒数\n$(AB)^{-1}$ and $(AB\\dots Z)^{-1}$ 两个矩阵相乘的逆，当然你可以把结果一步一步算出来，得到算数结果，然后求逆，但这里的逆视为一种操作，不关系结果是多少，只关注本操作的一系列特性，因为这些特性能是计算变得更简单，按照最基本计算规律可能无法计算，这也是“计算方法”或者“数值分析”课程所最核心的内容，按照某些算法和计算性质，能够优化计算速度，提高计算结果精确度，当然这些都是通过计算机计算的，所数值分析是cs的课程 两个矩阵相乘的逆 $$ (AB)^{-1}=B^{-1}A^{-1} $$ 两个矩阵要交换位置 $$ (ABC)^{-1}=C^{-1}B^{-1}A^{-1} $$ $$ (AB\\dots Z)^{-1}=Z^{-1} \\dots B^{-1}A^{-1} $$ 三个或者更多个的时候，要完全倒过来 证明方法灰常简单： $$ (AB)^{-1}(AB)=B^{-1}(A^{-1}A)B=B^{-1}(I)B=B^{-1}B=I $$ 逆运算与乘法在一起结合就是这样的。\n高斯乔丹消元(GAUSS-JORDAN Elimination) 高斯乔丹消元求矩阵的逆，适合小型矩阵的笔算。 其主要基础是矩阵乘法的分块性质 $$ A^{-1}\\begin{bmatrix}A\u0026amp;\u0026amp;I\\end{bmatrix}\\ =\\begin{bmatrix}A^{-1}A\u0026amp;\u0026amp;A^{-1}I\\end{bmatrix}\\ =\\begin{bmatrix}I\u0026amp;\u0026amp;A^{-1}\\end{bmatrix} $$ 其中最关键的一点是如何让 $A$ 变成 $I$,这里就是高斯消元的主要问题点，首先生成一个曾广矩阵，然后消元小区下三角矩阵，以及上三角矩阵，最后只有一个部分对角矩阵，然后用对角矩阵乘以其倒数，右侧的I就变成了$A^{-1}$\n其过程大概如下 $$ \\begin{bmatrix}A\u0026amp;\u0026amp;I\\end{bmatrix}\\ A=LR\\ $$\n$$ R=L^{-1}A\\dots (1)\\ L^{-1}\\begin{bmatrix}A\u0026amp;\u0026amp;I\\end{bmatrix}=\\begin{bmatrix}R\u0026amp;\u0026amp;L^{-1}I\\end{bmatrix} $$ (1)的主要过程就是通过消元，使得增广矩阵中的A矩阵变成一个上三角矩阵R，对角线一下都是零 $$ R=UD \\dots(2)\\ D=U^{-1}R\\ U^{-1}\\begin{bmatrix}R\u0026amp;\u0026amp;L^{-1}I\\end{bmatrix}=\\begin{bmatrix}D\u0026amp;\u0026amp;U^{-1}L^{-1}I\\end{bmatrix} $$ (2)回代过程，D只有对角线上有元素，其他全部是零 $$ I=D^{-1}D\\dots(3)\\ \\begin{bmatrix}D\u0026amp;\u0026amp;L^{-1}IU^{-1}\\end{bmatrix}D^{-1}=\\begin{bmatrix}I\u0026amp;\u0026amp;D^{-1}U^{-1}L^{-1}I\\end{bmatrix} $$ (3)对角线归一化，使得A对角线上的元素变成1 同样的一些列操作作用在右半部分$I$上，就能得到一个$D^{-1}U^{-1}L^{-1}$的矩阵 因为 $$A=LUD$$ 所以 $$A^{-1}=D^{-1}U^{-1}L^{-1}$$ 可知，其结果是正确的。\n逆矩阵的性质(Properties)  1：一个矩阵如果是对称的，并且有逆，那么逆也是对称的。 2：三角矩阵的逆如果存在可能是一个稠密矩阵\n 性质2很重要，有些稀疏矩阵的逆可能是稠密矩阵，这在数值分析中导致某些算法失效，或者效率急剧下降。\n奇异和可逆(Singular vs Invertible0 奇异矩阵，和可逆矩阵是一对反义词，奇异这个翻译不知道是谁根据啥想出来的，但是很有迷惑性，不知道啥是奇异矩阵，什么又是非奇异的，奇异矩阵的具体问题我们会在后面学到，但是目前奇异矩阵可以当做没有逆的矩阵。 三角矩阵可逆的唯一条件是对角元素全部非0\nConclusion 本文主要介绍下矩阵逆的一些性质，证明Gauss-Jordan的过程是我自己写的，可能有问题，如果有不严谨的地方，希望大家给予指点，谢谢\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-2-5.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 矩阵的“逆”，以及相关计算\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Inverse，Singular，Gauss-Jordan\u003c/p\u003e","title":"【线性代数】2-5:矩阵的逆(Inverse)"},{"content":"Abstract: 不同的输入对CNN的影响 Keywords: CNN，输入数据\nCNN训练数据讨论 问题回顾 对于神经网络，同一样本不同的表现形式，不同的Normalization的方式，作为输入会带来不同的结果。\n具体数据 分析 所谓同一样本的不同形式，比如彩色图片的不同色彩空间描述，RGB，YUV(YCbCr)，LIS等色彩空间有着不同的表现方式，其本质（数学模式）虽然不同，但视觉效果都表征同一样本，或者将样本变形成灰度图像，样本类别没有发生变化，只是描述发生了转换，更深层的说法就是变量的域发生了改变。 以上描述了同一样本不同域的变化，相反，在同一域下，数值的scale也会引起训练结果的变化，原因笔者目前还不清楚，期待研究明白后能给出通俗，简单的解释。\n实验1 文章对行人进行性别分类，网络结构采用最古老的LeNet作为模板，对训练样本进行不同的处理产生了不同的结果，比较如下：\n色彩空间变换，RGB，GRAY，YCbCr YCbCr转换公式： Normalization方式：\n实验结果： 总结 不同变量域最后的结果差别较大 不同的Normalization的结果也不相同，Normalization的结果优于不normalization的结果。 另外，Normalization也会影响收敛速度\n  Ng, Choon-Boon, Yong-Haur Tay, and Bok-Min Goi. \u0026ldquo;Comparing image representations for training a convolutional neural network to classify gender.\u0026rdquo; In 2013 1st International Conference on Artificial Intelligence, Modelling and Simulation, pp. 29-33. IEEE, 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://go.face2ai.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/deep-learning-cnn%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E8%AE%A8%E8%AE%BA.zh/","summary":"Abstract: 不同的输入对CNN的影响 Keywords: CNN，输入数据\nCNN训练数据讨论 问题回顾 对于神经网络，同一样本不同的表现形式，不同的Normalization的方式，作为输入会带来不同的结果。\n具体数据 分析 所谓同一样本的不同形式，比如彩色图片的不同色彩空间描述，RGB，YUV(YCbCr)，LIS等色彩空间有着不同的表现方式，其本质（数学模式）虽然不同，但视觉效果都表征同一样本，或者将样本变形成灰度图像，样本类别没有发生变化，只是描述发生了转换，更深层的说法就是变量的域发生了改变。 以上描述了同一样本不同域的变化，相反，在同一域下，数值的scale也会引起训练结果的变化，原因笔者目前还不清楚，期待研究明白后能给出通俗，简单的解释。\n实验1 文章对行人进行性别分类，网络结构采用最古老的LeNet作为模板，对训练样本进行不同的处理产生了不同的结果，比较如下：\n色彩空间变换，RGB，GRAY，YCbCr YCbCr转换公式： Normalization方式：\n实验结果： 总结 不同变量域最后的结果差别较大 不同的Normalization的结果也不相同，Normalization的结果优于不normalization的结果。 另外，Normalization也会影响收敛速度\n  Ng, Choon-Boon, Yong-Haur Tay, and Bok-Min Goi. \u0026ldquo;Comparing image representations for training a convolutional neural network to classify gender.\u0026rdquo; In 2013 1st International Conference on Artificial Intelligence, Modelling and Simulation, pp. 29-33. IEEE, 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","title":"CNN训练数据讨论"},{"content":"Abstract: 机器学习数据预处理的小总结 Keywords: 数据，预处理\n训练数据选择血泪史 这一幅图就能完整说明整个训练过程，说明 从左到右，从上到下的顺序，表明整个训练\n  图一中黑圈绿点是目标pattern或者feature的真实（ground truth）分布（应该是更多维我只能画二维的） 图二红色点是训练时选出的训练样本，可见其覆盖不广泛 图三是训练过程，蓝色实线代表分类器（线性的，也可以是非线性的） 图四到图八是完整的训练过程 图九为最后训练结果，包括recall部分，missing的部分，以及false的部分   整个过程最直观的外在表现就是随着训练不断深入，模型的在测试集上的Recall逐渐变差 如果继续复杂化训练模型，会产生严重的过拟合\n解决方法：\n  样本复杂化，尽量覆盖整个分布 更多样本  ","permalink":"https://go.face2ai.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/machinelearning-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%80%89%E6%8B%A9%E8%A1%80%E6%B3%AA%E5%8F%B2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 机器学习数据预处理的小总结\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 数据，预处理\u003c/p\u003e","title":"训练数据选择血泪史"},{"content":"Abstract: tony 推荐读物 Keywords: 书籍推荐\n推荐书单 读书时学习最好的途径之一，不完全推荐下相关领域不错的书籍，博主不才，也只读了一部分，罗列供参考\n基础 《深入理解计算机系统》（原书第2版） [Computer Systems]\n本人启蒙读物，难度系数5.0，适合专业从事运维，计算机底层人事阅读，但也推荐所有程序员阅读，只有了解硬件，才能写出飞快的程序，问题就是，这本书非常难，而且中文翻译一般\n《鸟哥的Linux私房菜 》\n入门必备，了解linux，由于国内windows来的比较早，80%的电脑用户停留在xp的印象中，不了解操作系统还可以这个样子，而windows又不开放源代码和内核设计，所以，更好的了解计算机操作系统，从linux开始，初级用户推荐ubuntu，如果厉害了，其他应该也不是问题\n《现代操作系统》\n没完全看过，但据说很厉害，看了一点内存管理机制，估计一点都记不得了\n《x86汇编语言》\n为了开明白《深入理解计算机系统》买的，国内作者写的，质量不错，告诉你计算机是怎么启动运行的\nPython python研究处于初级阶段，c++也是，但是c研究的比较多，感觉编程语言就是招式，心法是算法，数据结构以及操作系统。 之前公司的CTO峰哥告诉我一句话印象十分深刻“站在操作系统和编译器的角度看，所有编程语言都一样”\n《Python核心编程》\n好几个版本，都买来压箱底，有时间就看看，没时间过段时间再看看，python在某些领域非常方便\n《Effective Python：编写高质量Python代码的59个有效方法》\n优化python的方法，优化要在写作时进行，千万别说先写再优化，later就是never，思想一旦形成，在设计时就会产生一个相对快速稳定的编码方式\n数据结构和算法 看似不常用，实际最有用的东西\n《大话数据结构》\n比较好的国内作品，画面形象有趣，读起来比较快，读这类书，一定要边读边写！一定要自己实现以下\n《数据结构与算法分析：C语言描述（原书第2版）》\n自己边看边code一下，看不出学会了什么，但气质提升非常大\n《算法导论（原书第3版）》\n经典，之所以成为经典，是经历了时间考研的，别跟我说你是多资深的专业人员，没研究过经典，你永远不专业，书籍有难度，读起来会很慢，但是不着急，慢慢来\n《计算机程序设计艺术》\n经典中的经典，买了英文原版，还没读完，不吹不黑，贼厉害的书！为了装逼也要摆在书架上\n机器学习与深度学习 深度学习的书没什么好看的，看机器学习的就好了，数学书很重要，用到什么补什么，会非常痛苦，不痛苦就不会卓越，自己权衡吧 只推荐机器学习相关的\n《机器学习》周志华 著\n国内较好的作品，可作为总纲，然后逐一细化，边看边code！\n《机器学习》 段菲译\n同上\n《统计学习方法》\n需要深入研究，数学，code\n《模式分类》\n与《摄入理解计算机系统》一样，贼难，数学贼多，贝叶斯贼烦，研究的越明白越牛x\n《Tensorflow 实战Google深度学习框架》\n照着写网络，会写了以后就可以扔了，或者垫桌角。。。\n数学类（2017年9月7日补充） 对于想深入学习研究机器学习人工智能的同学，数学书必须要看，还要学的好，如果你想找个公司混口饭吃，上面那些已经能够让你在你的圈子“大放异彩”了。 目前我总结了一下数学学习的过程，针对机器学习的战友们，不一定要从头学一遍，但是对应的补充学习还是需要的. 知识结构如下 https://github.com/Tony-Tan/MachineLearningMath\n相关书籍推荐\n（集合论：留空）\n《微积分与解析几何》\n微积分我搞了一本MIT的教材，有中国影印的，不贵（跟国外的价格相比），国外的数学书有个优点就是说人话，你能读下来，然后可以慢慢理解，中国的有很多大牛老师写的书，说实话，我断句都有问题\n《Introduction to Linear Algebra》 Gilbert Strang\n也是MIT的教材，中国没有卖的，准备出国搞一本，目前看的是某宝搞到的纸质版，我希望支持原版，但是确实没有渠道，就像《机器学习与模式识别》那本原著，我当时想都没想就买了一本原版，800多rmb心疼的够呛，支持好书，我觉得是有必要的\n《线性代数及其应用》 David C Lay\n还没看。。。\n《矩阵分析》 Roger A Hom\n还没看。。。\n《实分析》 陶哲轩\n大神关爱弱智儿童的书籍，没看完，但是翻了一下，感觉也都是说的人话，没什么晦涩的语句，至于整体情况，看完再来评论\n（复变函数：留空）\n（微分方程：留空）\n《初等概率》钟开莱\n还没看。。。 《概率论教程》 钟开莱\n还没看。。。\n《概率论与数理统计》 陈希孺 也没看。。。\n《概率及其应用》 威廉费勒\n也没看\n还没看到\n未完，待续。。。 原文地址：https://www.face2ai.com/other-Tony推荐读物转载请标明出处\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-tony%E6%8E%A8%E8%8D%90%E8%AF%BB%E7%89%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e tony 推荐读物\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 书籍推荐\u003c/p\u003e","title":"Tony 推荐读物"},{"content":"Abstract: 关于学习的小感想 Keywords:\n学习心得 开篇废话 好久没有写博客了，最近几个月发生了不少事情，目前来看好事不多，多半是（应该是全部）不好的事，做的事情有问题。。balabala一大堆烂事，但是这个不是不写博客的理由，主要原因还是因为比较懒，第二个愿意觉得自己以前写的一些文字都处于初级无知阶段，虽然现在来看基本还都是正确的，但明显不够深层次，说道博客不得不说CSDN那是开始写博客的地方，写了一百多篇，有营养的不多，但很多都偏向理论，没有什么具体应用的问题作为铺垫，所以访问量一直so so，之前我认为博客的访问量就是对博客质量的直接描述，现在看来并不如此，访问量是对读者受众群数量的衡量，同受众群的博客访问量才有意义，举个简单的例子，爱因斯坦的著作如果已博客的形式发布，那么他的访问量绝对不如牛顿三大定律的访问，原因是牛顿的定律要高考，中国几千万高考考生，懂爱因斯坦的全世界也没几个。这个例子经不住推敲，只是一个简单的类比，也别跟我抬杠，这个杠抬起来也没什么营养。\n关于学习 上面扯了一堆没用的，是想说学习，说实话，按照我对周围同龄人（毕业三年内的人）了解来看，大家并不喜欢学习，问问原因，直截了当：“学够了”。学够了并不是说学的知识够了，而是学习学腻了，我们经历的教育制度强迫大家做了太多自己不喜欢的事，然后给这些事加一个名词叫做学习，而且还给学习加了个属性，就是学习好才能改变命运，但是衡量学习好不好要依靠考试，而考试如果按照我的理解，考试是一门技术，类似于技术考核，用一个考察工人的方法来检测一些人有没有做学者的潜质，就类似于让哈士奇来捉老鼠，捉的好的哈士奇雪橇肯定拉的666。当然我们不排除有些大牛工人和学者都是厉害的，肯定有，而且不少，但是这个不少是数量的不少，不是比例的不少，为啥这么说，如果按照百分之一的人同时具有工人和学者的属性，乘以我们国家人口的15亿，快比西方不少国家的总人口数还多。所以，我们国家目前来说发展还是非常迅速的，这个不谈了，扯的太远。 说实话，写这么多文字并不是想劝谁弃恶从善，毕竟每个人对生活有不同的理解，而且更重要的是，我认为，人是无法被另外一个人改变的，如果想改变，必须要在环境极其刺激他的情况下让其自己思考，那这个人基本也改头换面了，如果你以一个导师的身份通过自己的经验来劝导某个人应该怎么做，我觉得这个难度不小 不要想着去改变别人，你可以告诉他你的经验是什么，但别人采纳与否与你无关\n学习不是在某个特定的时间环境下才能进行的，我说的时间和环境主要指的是大时间，比如青年时期或者在学校的时间，学习应该是持续一辈子的，上学的时候听说能力比知识重要，而且相当一段时间非常相信这句话，但现在一想，naive！能力是什么？\n能力是知识和经验结合的结果\n如果把能力理解为社交或者是处理人际关系的读者，您可以走人了，我这里的所有内容根本不适合您的品味和阶层。 我想说的能力是处理与自身工作内容相关的问题的时候完美解决的概率，或者评分，比如今天你的程序出了问题，同学A有90%的概率用一上午解决，而你只有10%的概率在一上午解决，那么我可以认为A比你的工作能力强9倍。 而调程序需要对程序功能算法的理解和语法知识的掌握，以及根据相关错误结合经验来定位bug位置的组合能力来完成工作的。\n经验需要时间，知识需要学习\n做了一张low到无法言语的图，比较简陋后面可能会更新 未完待续","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-tony%E8%AF%B4%E5%AD%A6%E4%B9%A0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 关于学习的小感想\n\u003cstrong\u003eKeywords:\u003c/strong\u003e\u003c/p\u003e","title":"Tony 说学习"},{"content":"Abstract: 矩阵基本计算，包括加减乘法，主要是乘法的几种不同的理解\nKeywords: Addition，Subtraction，Multiplication，Inner Product，Outer Product\n本课视频课程已上线：4 矩阵计算法则\n矩阵操作 矩阵加法、减法 矩阵加减法，规则很简单，矩阵要求尺寸一样，row一样，column也得一样，这样按照对位相加减就行了。 $$ \\begin{bmatrix}a_{11}\u0026amp;\u0026amp;a_{12}\u0026amp;\u0026amp;a_{13}\\ a_{21}\u0026amp;\u0026amp;a_{22}\u0026amp;\u0026amp;a_{23}\\ a_{31}\u0026amp;\u0026amp;a_{32}\u0026amp;\u0026amp;a_{33}\\end{bmatrix} \\pm \\begin{bmatrix}b_{11}\u0026amp;\u0026amp;b_{12}\u0026amp;\u0026amp;b_{13}\\ b_{21}\u0026amp;\u0026amp;b_{22}\u0026amp;\u0026amp;b_{23}\\ b_{31}\u0026amp;\u0026amp;b_{32}\u0026amp;\u0026amp;b_{33}\\end{bmatrix}\\= \\begin{bmatrix}a_{11}\\pm a_{11}\u0026amp;\u0026amp;a_{12}\\pm b_{12}\u0026amp;\u0026amp;a_{13}\\pm b_{13}\\ a_{21}\\pm b_{21}\u0026amp;\u0026amp;a_{22}\\pm b_{22}\u0026amp;\u0026amp;a_{23}\\pm b_{23}\\ a_{31}\\pm b_{31}\u0026amp;\u0026amp;a_{32}\\pm b_{32}\u0026amp;\u0026amp;a_{33}\\pm b_{33}\\end{bmatrix} $$ 这个没啥好说的，别减错地方就行。一对一进行\n乘法 乘法才是矩阵计算的关键，计算意义，计算量，等很多是些非常有意义的研究课题，我记得本科学习线性代数的时候，老师先来将行列式，接着就是矩阵的计算法则，然后接着就是rank类的东西了，确实书本是这么讲的，但是现在想想，好像没啥逻辑，所以就没去上课了（给自己随便找个借口逃课）。\n矩阵规模 相乘的两个矩阵尺寸上有些要求，例如对于： $AB$ If A has n column,B must have n rows 如果A为$m\\times n$，B为 $n\\times p$那么他们相乘的结果： $$ (m\\times n)(n \\times p)=(m\\times p) $$\n行乘列(Row Dot Product Column) Dot product之前已经讲过了，就是如何通过两个向量，pia的一下变成一个数字，矩阵可以看做是向量组成的，所以给出第一个规则，假设乘法为$AB=C$ $$ C_{ij}=(row;i;of;A) \\cdot (column;j;of;B) $$ 这个规则是我之前上课学的，也是最基本的矩阵乘法公式，当然也可以写成求和的形式，但是我觉得通过dot product来看这个反而更直观一些，就不再把求和那个写出来了，补充句，在矩阵相乘的编程处理中，经过调整内外层循环，可以最大化利用高速缓冲，能够提高乘法速度。这个让我想起来之前项目，同事非要自己写乘法，然后就按照公式计算顺序，写了一个一毛一样的东西出来，虽然只做3x3的一个小矩阵乘法，速度什么的基本没什么影响，但是我觉得这当你不能保证你写的功能的稳定性和速度的时候，使用稳定的第三方库是个不错的选择。\n矩阵内乘、外乘(Inner or Outer) dot product也叫inner product，当时我就在想有没有outer product，Pro. Strang在课堂上没有介绍outer但是在书上写了下，inner的矩阵形式是这样的： $$ \\begin{bmatrix}a_1\u0026amp;a_2\u0026amp;\\dots\u0026amp;a_n\\end{bmatrix} \\begin{bmatrix}b_1\\b_2\\newline \\vdots \\b_n\\end{bmatrix}=\\sum_{i=0}^{n}a_i*b_i $$ outer的矩阵形式，就是。。 $$ \\begin{bmatrix}a_1\\a_2\\newline \\vdots \\a_n\\end{bmatrix} \\begin{bmatrix}b_1\u0026amp;b_2\u0026amp;\\dots\u0026amp;b_m\\end{bmatrix} =\\begin{bmatrix} a_1b_1\u0026amp;\u0026amp;a_1b_2\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;a_1b_m\\ a_2b_1\u0026amp;\u0026amp;a_2b_2\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;a_2b_m\\ \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;\\vdots\\ a_nb_1\u0026amp;\u0026amp;a_nb_2\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;a_nb_m\\ \\end{bmatrix} $$ 其实outer的过程通过一行一列产生一个矩阵，所以当多列和多行（行和列必须相等数量）的矩阵相乘的时候就会产生多个矩阵，再对矩阵进行相加，这个过程的编程实现时，缓存利用率很高，就是本段开头说道的矩阵相乘的速度问题，当然这是不是最快的解决办法我也不知道，知识从哪本书上看到了这种说法，印象比较深刻\n列模型(Column Model) 列模型，如果把矩阵看做是很多列的组合，那么可以回归到最早的$Ax=b$的过程 $A$是被乘矩阵, $\\textbf{x}$ 扩展成多列的矩阵$X$ $$ X=\\begin{bmatrix} \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;\\vdots\\ x_1\u0026amp;\u0026amp;x_2\u0026amp;\u0026amp;\\dots\u0026amp;\u0026amp;x_n\\ \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;\\vdots\\ \\end{bmatrix} $$ 把x代入 $$ AX= A \\begin{bmatrix} \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;\\vdots\\ x_1\u0026amp;\u0026amp;x_2\u0026amp;\u0026amp;\\dots\u0026amp;\u0026amp;x_n\\ \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;\\vdots\\ \\end{bmatrix}\\ \\begin{bmatrix} \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;\\vdots\\ A x_1\u0026amp;\u0026amp;A x_2\u0026amp;\u0026amp;\\dots\u0026amp;\u0026amp;A x_n\\ \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\dots \u0026amp;\u0026amp;\\vdots\\ \\end{bmatrix} $$ 写数学类的博客就是累，还是贴代码那种技术博客好写。 把X分解成多个列，每列与A的乘积作为结果对应的列，这就是列视角，或者叫做列模型\n行模型(Row Model) 有行就有列，有列就有行： $$ A=\\begin{bmatrix} \\dots\u0026amp;\u0026amp; a_1 \u0026amp;\u0026amp;\\dots\\ \\dots\u0026amp;\u0026amp; a_2 \u0026amp;\u0026amp;\\dots\\ \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\vdots\\ \\dots\u0026amp;\u0026amp; a_m \u0026amp;\u0026amp;\\dots\\ \\end{bmatrix}\\ AX= \\begin{bmatrix} \\dots\u0026amp;\u0026amp; a_1 \u0026amp;\u0026amp;\\dots\\ \\dots\u0026amp;\u0026amp; a_2 \u0026amp;\u0026amp;\\dots\\ \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\vdots\\ \\dots\u0026amp;\u0026amp; a_m \u0026amp;\u0026amp;\\dots\\ \\end{bmatrix}X \\begin{bmatrix} \\dots\u0026amp;\u0026amp; a_1X \u0026amp;\u0026amp;\\dots\\ \\dots\u0026amp;\u0026amp; a_2X \u0026amp;\u0026amp;\\dots\\ \\vdots\u0026amp;\u0026amp;\\vdots\u0026amp;\u0026amp;\\vdots\\ \\dots\u0026amp;\u0026amp; a_mX \u0026amp;\u0026amp;\\dots\\ \\end{bmatrix} $$ 行过程和列过程基本呈现一种对称关系，这也是线性代数有趣的一点，经常是左右开工，得到相同的结果。\n块(Block) 没错，矩阵是可以切块的，最极端的方式就是每个矩阵按照一个块一个元素的切法，那就和原始矩阵一样了，这种切块粒度太小，如果把一个矩阵当做一块，那粒度又太大，举个一般的例子🌰 继续使用上一节消元的矩阵 $$ E=\\begin{bmatrix} 1\u0026amp;\u0026amp;0\u0026amp;\u0026amp;0\\ -3\u0026amp;\u0026amp;1\u0026amp;\u0026amp;0\\ 0\u0026amp;\u0026amp;0\u0026amp;\u0026amp;1\\ \\end{bmatrix}\\ A=\\begin{bmatrix} 1\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\ 3\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\ 4\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\ \\end{bmatrix}\\ EA= \\left[\\begin{array}{c|cc} 1\u0026amp;0\u0026amp;0\\\\hline -3\u0026amp;1\u0026amp;0\\ 0\u0026amp;0\u0026amp;1\\ \\end{array}\\right] \\left[\\begin{array}{c|cc} 1\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\\\hline 3\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\ 4\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\ \\end{array}\\right]\\ \\left[\\begin{array}{c|cc} 1\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\\\hline 0\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\ 0\u0026amp;\u0026amp;x\u0026amp;\u0026amp;x\\ \\end{array}\\right] $$ 分块进行 $$ EA= \\left[\\begin{array}{c|c} I\u0026amp;0\\\\hline -CA^{-1}\u0026amp;I\\ \\end{array}\\right] \\left[\\begin{array}{c|c} A\u0026amp;B\\\\hline C\u0026amp;D\\ \\end{array}\\right]\\ \\left[\\begin{array}{c|c} IA+0C\u0026amp;IB+0D\\\\hline -CA^{-1}A+IC\u0026amp;-CA^{-1}B+D\\ \\end{array}\\right] $$\n吃完饭码了这些公式，然后我又饿了，这是个消元的过程，E是消元矩阵，我们把3x3矩阵分块成了2x2的块矩阵，分块的规则是，对应要相乘的矩阵规模必须匹配正确（原文match）。然后把块当做元素继续按照乘法法则进行相乘.\n注意：分成的块矩阵相乘与矩阵相乘要求一致，顺序不能互换。$AB\\neq BA$\nSchur Complement 矩阵分块消元，右下角的那个矩阵，也就是 $$ -CA^{-1}B+D $$ 被叫做Schur Complement。\n矩阵法则 法律说明，你必须遵守法律，矩阵的法律和现实的法律有点不一样，现实的法律是“法不禁止即可为”，但是矩阵的法律如果没说明可以，也没规定不行，那这个law你就要小心使用，最好在论证后进行使用！ $$ A+B=B+A\\ c(A+B)=cA+cB\\ A+(B+C)=(A+B)+C\\ AB \\neq BA\\ C(A+B)=CA+CB\\ (A+B)C=AC+BC\\ A(BC)=(AB)C\\ A^p=AA\\dots A(p;factors)\\ A^p\\cdot A^q=A^{p+q}\\ (A^p)^q=A^{pq}\\ $$\n以上是不完全的矩阵法则\n$$ A(B+C)=AB+AC $$ 证明： 逐列进行，比如b，c是B和C的对应两个列 $$A(b+c)=Ab+Ac$$ 这是所有的关键\u0026mdash;线性（linearity）\n总结 这是矩阵基本操作的一个总结，依法办事，做法律允许的计算才能得到正确的结果，当然也取决于机器的问题，比如后面要写的数值分析，里面就有不少算法精度收到机器的限制，导致合法操作得不到准确结果，各位加油，我们后面继续！\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-2-4.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 矩阵基本计算，包括加减乘法，主要是乘法的几种不同的理解\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKeywords:\u003c/strong\u003e Addition，Subtraction，Multiplication，Inner Product，Outer Product\u003c/p\u003e","title":"【线性代数】2-4:矩阵操作(Matrix Operations)"},{"content":"Abstract: 用大学的方法消元，也就是整个消元过程矩阵化，引出矩阵乘法 Keywords: Elimination Matrix，Matrix Multiplication，Augmented Matrix\n本课视频课程已上线：使用矩阵消元\n消元与矩阵的关系 发现这个教材真的很有章法，从big picture 的角度来看，所有知识环环相扣，每一环单独看还都不难，组合起来发现整个知识体系就完整了，我至今不懂为啥我们伟大社会主义中国的学者们写书都是从行列式开始干，自学看书根本看不懂，当年线代上机考试之前三个小时还在玩魔兽世界🐶🐶🐶，这个故事告诉我们，出来混迟早要还的😂😂😂😂\n用矩阵消元(Elimination Using Matrix) 本节的主要目的就是告诉你，小子，矩阵是用来解方程的，你会算行列式只能考上研究生，哈哈哈（博主吃不到狐狸说葡萄酸） 来个方程： $$ \\begin{bmatrix}x_1\\x_2\\x_3\\end{bmatrix} $$ 是一个n维空间(space)，在这个空间里，我们找一个特定或者若干个特定的vector使得$ Ax=b $成立 根据row picture，和column pictur : $$ A\\textbf{x}=col(1)x_1+rcol(2)x_2+\\dots+col(n)x_n\\ $$ 所以，Ax的第j行的元素结果是 $$ (A\\textbf{x})j=col(j,1)x_1+rcol(j,2)x_2+\\dots+col(j,n)x_n\\ $$ 观察上面的式子不难发现，Ax的第j个元素就是A的第j行和x的乘积 that is $$ \\sum{j=i}^{n}a_{ji}x_i $$ 各位注意啦，这步放在这里主要是为了后面算矩阵乘以矩阵的，虽然看起来有点啰嗦，但是这样下来整个思路是完整的，不会出现漏洞\n矩阵形式(Matrix form) 还是刚才说的，这节就是把消元的所有动作集成在一个矩阵里，我们的目的是 $$ A\\textbf{x}=\\textbf{b}\\ EA\\textbf{x}=E\\textbf{b}\\ U\\textbf{x}=E\\textbf{b}\\ $$ U是上三角矩阵 目标明确以后，对于上面的图片中的矩阵，我们开始消元，第一步， $(第二个方程)-2\\times (第一个方程)$ 得到的右侧b：\n$$ b_{new}=\\begin{bmatrix}2\\4\\10\\end{bmatrix}\\ b_{new}=Eb $$\n没错我们关系E是个啥，先给出E，求解E的过程看后面。\n$$ E=\\begin{bmatrix}1\u0026amp; 0 \u0026amp; 0\\ -2 \u0026amp; 1\u0026amp; 0\\ 0\u0026amp;0\u0026amp;1\\end{bmatrix} $$\n算下Eb的结果，row picture或col picture给出的答案都是 $b_{new}$ 不错，起码这个E是我们要找的变换矩阵。观察一下E，发现E是从$I$变形来的，把其中一个改成要乘以的那个系数的负数，比如我们要减去2倍的第一行，就把某个位置改成-2。但是具体改哪个位置是个关键：改的位置就是被减去的那一行的减去那一行的行的那一列，怎么样，迷糊没，迷糊就对了 E的第2行第1列是-2：我们要消去的是原矩阵A的第二行，的第一列元素\n$$ Eb=b_{new}\\ \\begin{bmatrix} 1\u0026amp;0\u0026amp;0\\ {-2}\u0026amp;1\u0026amp;0\\ 0\u0026amp;0\u0026amp;1 \\end{bmatrix} \\begin{bmatrix} b_1\\ b_2\\ b_3 \\end{bmatrix} =\\begin{bmatrix} b_1\\ b_2-2b_1\\ b_3 \\end{bmatrix} $$ 这个过程全靠自己领悟，如果这两种语言你都没明白，那就过几天再看一遍吧。\n矩阵乘法(Matrix Multiplication) 我在考虑要不要新开篇，一想还是算了吧，新写一篇还要写开篇废话，麻烦死。 我们学会了矩阵乘以向量，那么矩阵乘以矩阵就是要解决上面那个EA的问题， 先说几个矩阵乘法的性质： $$ A(BC)=(AB)C\\ often;AB\\not=BA\\ AB=A\\begin{bmatrix}b_1\u0026amp;b_2\u0026amp;b_3\\end{bmatrix}=\\begin{bmatrix}Ab_1\u0026amp;Ab_2\u0026amp;Ab_3\\end{bmatrix} $$ 这上面这个是乘法法则比较重要的一条，下一篇有详细介绍，包括乘法的具体算法，本篇只是大体观察一下乘法性质\n交换行(Row Exchange) Permutation Matrix也是从$I$演变出来的一种矩阵，其主要作用是交换矩阵的两行 eg：交换第二和第三行 $$\\begin{bmatrix}1\u0026amp;0\u0026amp;0\\0\u0026amp;0\u0026amp;1\\0\u0026amp;1\u0026amp;0\\end{bmatrix}\\begin{bmatrix}1\u0026amp;2\u0026amp;3\\2\u0026amp;3\u0026amp;4\\3\u0026amp;4\u0026amp;5\\end{bmatrix}=\\begin{bmatrix}1\u0026amp;2\u0026amp;3\\3\u0026amp;4\u0026amp;5\\2\u0026amp;3\u0026amp;4\\end{bmatrix}$$ 没错，就这么神奇，不信自己慢慢算，交换可以用矩阵表示，之前的行之间的减法也可以用矩阵乘法，那么说Elimination基本可以用矩阵连续想成表示，比如我想消元之后再换行 $$ PEA=PEb $$ 这样就能得到一个可以back的upper triangular matrix了，或者更多的P和E。。。\n增广矩阵(Augmented Matrix) 把b也放到A里面，放最后一列，那么矩阵就不是方阵了： $$\\begin{bmatrix}2\u0026amp;4\u0026amp;-2\u0026amp;2\\4\u0026amp;9\u0026amp;-3\u0026amp;8 \\{-2}\u0026amp;-3\u0026amp;7\u0026amp;10\\end{bmatrix}$$\nconclusion 所以消元可以通过在A矩阵前面乘以一些列的变换，换行矩阵，得到最后的上三角矩阵 $$ U=P_nE_m\\cdots P_2E_2P_1E_1A $$ 乘法的外表一些基本性质就是这些，我以为multiplication rule也在这一篇，原来不在，吓死我了，明天继续写，明天是个硬骨头，各位加油啊！\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-2-3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 用大学的方法消元，也就是整个消元过程矩阵化，引出矩阵乘法\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Elimination Matrix，Matrix Multiplication，Augmented Matrix\u003c/p\u003e","title":"【线性代数】2-3:消元与矩阵的关系(Elimination and Matrix)"},{"content":"Abstract: 从小学解方程的消元开始，推导出线性代数的知识，包括先关矩阵计算 Keywords: Eliminate 消元，Pivot 主元，Row Exchange 行交换，Upper Triangular 上三角矩阵\n本课视频课程已上线：消元思想\n消元 小学老师教我们解方程，受限就是把两个未知数变换成一个未知数，或者说用另一个未知数来表示当前未知数。\n$$ x+y=1\\ x-y=0\\ $$ 我们会把第一个方程变形，然后和第一个方程做减法或者加法计算 $$ -x-y=-1\u0026hellip;\u0026hellip;..(1_{temp}) $$ 左右同时和(2)相加 $$ -2y=-1\u0026hellip;\u0026hellip;\u0026hellip;(2) $$ 当这步完成时，第一步方程没变，我们减去的变形版本是个中间版本，所以方程组： $$ x+y=1\\ 0x+y=\\frac{1}{2} $$ 消元的顺序很多，这个只是我的习惯，不过和线性代数书上刚好差不多，经过消元，我们得到了Upper Triangular Matrix $A=\\begin{bmatrix}1\u0026amp;1\\0\u0026amp;1 \\end{bmatrix}$\n上三角矩阵(Upper Triangular Matrix) 只有对角线，及对角线上方数字不为0，其他部分都是0的矩阵 $$ \\left(\\begin{array}{cccc} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ \u0026amp;a_{22} \u0026amp; \\cdots \u0026amp;a_{2n} \\ \u0026amp; \u0026amp; \\ddots \u0026amp; \\vdots \\ \u0026amp; \u0026amp; \u0026amp;a_{nn} \\end{array}\\right) $$ 没错，a不能全为0 消元的最后终结果就是上一篇中的 $A\\textbf{x}=\\textbf{b}$ 变成了 $$ U\\textbf{x}=\\textbf{c} $$\n带回(Back Substitution) 从最后一行开始，把未知数一个个回带解出来\n主值(Pivot) 消元过程中，我们要看列的未知数，和未知数的系数，策略就是首先干掉第一行一下的所有的第一个未知数，通过对第一行进行缩放，来得到中间（temp）表达式，使得第一行下面的所有行的第一个未知数全部消失，然后对第二行做类似的操作，消掉第二行以下的所有第二个未知数。最后是系数矩阵白城上三角矩阵。 那么问题来了，有的时候，你消元到第二行的时候，第二个未知数在刚才的消元过程中牺牲了，顺带也给干掉了，很不幸，我们这时候为了得到标准的上三角矩阵，肯定要用其他行（第一行不行）的方程和第二行的方程进行置换，来保证第二个未知数存在。\n在消元过程中，系数矩阵每行中第一个不为0的数字叫主元（pivot） 消元后，主元在主对角线上。 eg： 上面upper triangular matrix的对角线元素就是主元。\n注意：主元不等于零，如果一行全是0，那么没有主元。 消元过程中当前行乘以某个系数得到temp过程中，系数 $$ l_{ij}=\\frac{entry,to,eliminate,in,row,i}{pivot,in,row,j} $$ Pivot in Row j 就是当前的主元\n如果主元个数小于未知数个数，又会引出一个新的概念，singular后面再说\n消元失败(Eliminate Faile) 消元失败，我们记得解方程会有两种特殊情况，一种是无数个解，一种是没有解。 比如你有三个方程，两个未知数，就很有可能没有解 如果你有一个方程，两个未知数，肯定有英菲尼迪个解 但是这个描述是小学的，根据back substitution的过程，我们确定，方程解的个数和pivot直接相关： 总结 消元那步看不懂，就想象着把矩阵变成三角的，然后按照这个思路自己写个方程试试，过程中感受下主元的威力，主元是个非常重要的概念，用处特别多。。。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-2-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 从小学解方程的消元开始，推导出线性代数的知识，包括先关矩阵计算\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Eliminate 消元，Pivot 主元，Row Exchange 行交换，Upper Triangular 上三角矩阵\u003c/p\u003e","title":"【线性代数】2-2:消元(Eliminate)"},{"content":"Abstract: 通过不同的角度解方程组$Ax=b$ Keywords: row picture，column Picture，system of equations\n本课视频课程已上线：向量和线性方程组\n解方程组 解方程组 $$ x-2y=1 \\ 3x+2y=11 $$ 同志们，来解方程组，这是小学四五年级的数学题，也是线性代数的核心问题，解方程组，没错2x2的方程组没啥好说的，咔咔咔，就算粗来了，但是200x200的规模就有点大了，所以线性代数知识就有用了。\n行视角(Row Picture) 不知道Picture这个词本身就是这种含义，还是Pro Strang喜欢这么说，Open Course和书上都是各种各样的Picture。 什么是Row Picture？ 看到方程组中的两个等式么，每一行就是一个Picture，或者叫做Graph，在二维坐标系下，表现出来的是一条直线，同样第二个方程也是一条直线，直线上所有的点都满足方程，所以两条直线相交处就是方程组的解。 列视角(Column Picture) 这个是重点了，因为这个能引出后面一些列的知识，如果我们竖着看，把方程的系数排列整齐，把每个未知数的所有系数按照列向量排列： $$ x\\begin{bmatrix} 1\\1 \\end{bmatrix}+y\\begin{bmatrix}-2\\2 \\end{bmatrix}=\\begin{bmatrix}1\\11 \\end{bmatrix}=\\textbf{b} $$ 怎么样，意外不意外，惊喜不惊喜，和前面讲到的Linear Combination是不是一毛一样，经过scale的两个向量相加得到另一个向量，接着我们就开始寻求scalars了。\n***也就是我们通过寻找特定的scalars，来组合出我们规定的 $\\textbf{b}$ ***\n在图像上，column picture 就变成了两个（或若干个）向量组合得到目标向量了，如图：\n系数矩阵(Coefficient Matrix) 下面矩阵正式出场，我们把上面那两个系数向量挨着拼接起来，就能得到一个系数矩阵(Coefficient Matrix) $$ A=\\begin{bmatrix}1\u0026amp;-2\\3\u0026amp;2\\end{bmatrix} $$ 然后写成方程形式就是： $$ A\\textbf{x}=\\textbf{b} $$ 其中 $\\textbf{x}=\\begin{bmatrix}x\\y\\end{bmatrix}$,x 和 y就是上面的未知数。\n矩阵乘向量(Matrix · Vector) 没错，上面的表示就是一个系数矩阵x向量，更通用一些，我们不在局限于上面的2x2的A，而是放飞自我的A，x也是放飞自我的x，那么\n行Row $$ A\\textbf{x}=\\begin{bmatrix} row(1)\\cdot \\textbf{x}\\ row(2)\\cdot \\textbf{x}\\ \\dots\\ row(n)\\cdot \\textbf{x}\\ \\end{bmatrix} $$ 系数矩阵每一行和未知数向量点乘，得到的就是方程组的原始形式。\n列Column $$ A\\textbf{x}=col(1)x_1+rcol(2)x_2+\\dots+col(n)x_n\\ $$ 系数矩阵的每一列的线性组合\n单位矩阵(Identity Matrix) 神奇矩阵$I$，$IA=A$看到没，这就是他牛的地方，和谁乘在一起都是谁 $$ \\begin{equation} I=\\begin{bmatrix} 1\\ \u0026amp;1 \u0026amp; \u0026amp; \\ \u0026amp;\u0026amp;\\ddots\\ \u0026amp;\u0026amp;\u0026amp; 1\\ \u0026amp;\u0026amp;\u0026amp;\u0026amp; 1 \\end{bmatrix} \\end{equation} $$ 空白处全是0 左乘右乘都不变！\n更多未知数 当n超过3的是后row picture就不能picture了，三维以上的就画不粗来了，当然column Picture也画不出来，但是多维向量更容易想象，并不是说column picture比row好，但是从线性代数角度，col的意义更丰富.\n总结 这是线性最基础，最核心的思想之一，虽然简单，但却是所有知识的源头。\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-2-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 通过不同的角度解方程组$Ax=b$\n\u003cstrong\u003eKeywords:\u003c/strong\u003e row picture，column Picture，system of equations\u003c/p\u003e","title":"【线性代数】2-1:解方程组(Ax=b)"},{"content":"Abstract: 介绍点乘，length Keywords: dot product，length\n视频合并了1.0 1.1 1.2\n  点乘和长度 点乘 点乘，也就是说向量乘法不止一种，我们今天来介绍的是比较常用的点乘，出了乘法，其实里面还有加法： 定义 dot product or inner product of $\\textbf{v}$ and $\\textbf{w}$ That is $$ \\textbf{v}=\\begin{bmatrix} v_1\\v_2 \\end{bmatrix}\\ \\textbf{w}=\\begin{bmatrix} w_1\\w_2 \\end{bmatrix}\\ $$\n$$ \\textbf{v}\\cdot \\textbf{w} = v_1 \\times w_1+v_2 \\times w_2 $$\nv和w和互换位置，也就是交换律存在。对于多维向量 点乘的结果是：\n$$ \\textbf{v}\\cdot\\textbf{w}=\\sum_{i=1}^{n}v_i*w_i $$\n几何理解 其实代数是完全不需要借助几何图形，也能够非常严谨完备证明所有之间的内在关系，但是将代数与几何相互结合的情况下，可以比较轻松的解决一些几何问题，通过简单的数字计算\n长度 长度，向量是有长度有方向的，方向，我们按照坐标起点和终点来确定的，长度也是，按照二维平面，两点之间距离（更准确的说是笛卡尔坐标系下，利用勾股定理来计算的距离） 比如 $(0,0)$ 到 $(v_1,v_2)$ 的距离 $|\\textbf{v}|=\\sqrt{v_1^2+v_2^2}$ so $|\\textbf{v}|^2=v_1^2+v_2^2$ 眼熟不？厉害不？意外不？ 没错就是$\\textbf{v}\\cdot\\textbf{v}$\n自己和自己的点乘结果就是向量长度的平方。\n单位长度向量(Unit Vector) 长度为1的向量，获得方法，非零向量，所有分量除以自己的长度\nAngles 90° 点乘结果为0的时候，两个向量夹角为直角，证明：\n其他角度 后面我们使用到矩阵以后，这个夹角基本没用，但是单位长度的向量相乘，其结果是他们夹角的cos值\nConclusion 这一章讲了线性代数的核心，也就是我们知识树的根基算是讲完了，然后顺着根不断的遍历，这样先后顺序能使知识贯通，顺便吐个槽，我tm就不明白了，为啥上学的时候老师上来就干行列式，干了三个星期，直接干迷糊了，所以，我劝大家，看线性代数的书，如果前三章就有行列式了，这书就不用看了！\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-1-2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 介绍点乘，length\n\u003cstrong\u003eKeywords:\u003c/strong\u003e dot product，length\u003c/p\u003e","title":"【线性代数】1-2:点乘和长度(Dot Products and Length)"},{"content":"Abstract: 线性组合详细说明 Keywords: Linear Combinations\n视频合并了1.0 1.1 1.2\n  # 线性组合 ## 列向量 上文我们简单的看了一眼核心，核心也是最简单的东西，在我国，初中高中的小盆友们就应该已经知道向量加减法了，但是美国的小朋友们可能到高中大学才接触，所以书中给出了详细的加减乘除算法，我们必须明确一点，一般说道的向量和写出来的都是列向量，就是竖着的 like this one： $$ \\begin{bmatrix} 4\\\\5 \\end{bmatrix} $$ 向量加法和乘法计算 这里简单写一下加法和乘法计算\nVECTOR ADDITION: $$ \\textbf{v}=\\begin{bmatrix} v_1\\v_2 \\end{bmatrix}\\ \\textbf{w}=\\begin{bmatrix} w_1\\w_2 \\end{bmatrix}\\ $$ add to: $$ \\textbf{v}+\\textbf{w}=\\begin{bmatrix} v_1+w_1\\v_2+w_2 \\end{bmatrix}\\ $$\nVECTOR MULTIPLICATION: $$ 2\\textbf{v}=\\begin{bmatrix} 2v_1\\2v_2 \\end{bmatrix}\\ -\\textbf{v}=\\begin{bmatrix} -v_1\\newline -v_2 \\end{bmatrix}\\ $$ （写公式真累！！） 注意零向量和数字常量0的不同\n线性组合 $$ c\\textbf{v}+d\\textbf{w}\\ $$ 就是线性代数的基础\n定义： the sum of $c\\textbf{v}$ and $d\\textbf{w}$ is a linear combination of $\\textbf{v}$ and $\\textbf{w}$\n向量的表示 这个大家都会，画箭头嘛，从0点，画向坐标位置，标个箭头就okay了 Important Questions Suppose $\\textbf{u}$ $\\textbf{v}$ $\\textbf{w}$ are three-dimensional non-zero:\n$c\\textbf{u}$ fill a Linear\n$c\\textbf{u}+d\\textbf{v}$ fill a plane\n$c\\textbf{u}+d\\textbf{v}+e\\textbf{w}$ fill a space(3d)\n前提是这三个向量不在同一直线或同一个二维平面上，上面的三个important才成立！\n总结 此篇详细说明了线性组合的一些基本问题\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-1-1.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 线性组合详细说明\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Linear Combinations\u003c/p\u003e","title":"【线性代数】1-1:线性组合(Linear Combinations)"},{"content":"Abstract: 本文主要介绍向量和线性组合的相关知识 Keywords: 向量，线性组合\n视频合并了1.0 1.1 1.2\n  向量 线性代数的核心：向量(The Heart of Linear Algebra) 很少有这么开门见山的教材，第一章第一句话就是告诉你，“嘿小子！凭借老夫多年经验线性代数的核心，就是两种计算，而且这两种计算都是对 向量 进行的！” （插播一句，知乎上有人说看这个公开课对考研用处不大，所以要根据这个考试的人，请你走开了，这里不适合你）。\n加法 $$ \\textbf{v}+\\textbf{w} $$ 上式就是两个向量相加，注意 $ \\textbf{v} $这种画风的是向量，$ c $这种的是常量，向量和常量都是啥这个我就不介绍了。\n乘法 $$ c\\textbf{v} $$ 这种乘法，不是向量乘以向量，这个是一个常数乘以向量，英文也叫scalar multiplication。\n线性组合(Linear Combinations) 有了上面两种根基，我们就发展出了一个超级无敌牛的组合： $$ c\\textbf{v}+d\\textbf{w}\\ $$ Suppose: $$ v=\\begin{bmatrix} 1\\1 \\end{bmatrix}\\ w=\\begin{bmatrix} 2\\3 \\end{bmatrix} $$ so 当c=2，d=1的时候，我们可以根据乘法和向量加法原则，先乘法后加法，得到 $$ \\begin{bmatrix} 4\\5 \\end{bmatrix} $$ 这是当c，d确定的时候，我们可以用两个向量来组合出另一个向量\nThe Whole Plane 当v和w不在一条直线上的时候（我默认你知道向量在坐标系里的表示），通过调整c和d，我们可以得到二维平面上的任一一个向量，这个是线性代数最核心的概念，对于一维空间，线性组合的图形表现是一条直线，二维是一个plane，三维或更多维度下就是空间。\n总结 给出线性代数的核心，线性组合（通过我们知识点图谱也能看出这一点，Linear Combination在树的根部）\n","permalink":"https://go.face2ai.com/math/math-linear-algebra-chapter-1-0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 本文主要介绍向量和线性组合的相关知识\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 向量，线性组合\u003c/p\u003e","title":"【线性代数】1-0:向量(Vector)"},{"content":"Abstract：本文推荐一些小插件，和小技巧，目的是快速的完成自己的博客，博客的主要目的是传递经验和思想，这就和观赏性网站有了区别，不需要华丽，简洁，清楚的风格更适合技术人员。 Keywords：LaTeX，Markdown，Hexo，Atom\n用atom markdown hexo github 写博客 开篇废话 为啥我们要写博客，因为我们是文化人，我为啥写博客，因为我当年在吃不上饭的时候通过看书和写博客翻身做主了。。所以我认为想要挣钱也好实现人生目标也好，读书，思考和写作是最好的方式，当然这对我来说是不错的一个组合，当然也有高人通过冥想，做梦，请教高手，自已摸索，拿来主义等多种方式获得知识，这些都是好的，只要能学到东西，能够创造出健壮稳定的功能，都是值得肯定的。但是，大家别忘了魁北克那座倒了的桥，当你写的每一行代码，算的每一组数据关乎人命的时候，不要愧对工程师这个称号！\n本文推荐一些小插件，和小技巧，目的是快速的完成自己的博客，博客的主要目的是传递经验和思想，这就和观赏性网站有了区别，不需要华丽，简洁，清楚的风格更适合技术人员。\n本人主要做一些机器学习算法研究，所以写博客主要是汉字，英文，数学公式，代码，以及不少的图片来解释数据。\n本文的目的是介绍一些功能和插件，具体如何写markdown，LaTeX等内容不在本文介绍之内，可以去google出很多详细资料！\n用到了atom，以及hexo一些功能来支持\nAtom写作环境 atom是github搞得一个神一个级别的编辑器，编辑器，说实话，vim我用不来，有人说牛人都用vim，我承认，我不是牛人，但是atom好像搞成了一个乐高玩具一样的模式，可以安装众多的插件，或者你可以自己写一些插件。 本文用到的插件包括但不限于：\nmarkdown-preview-plus 实时预览： 像这样 以及LaTeX数学公式预览 像这样 从word类软件启蒙的我们对所见即所得的模式产生依赖，看到结果也好，可以及时纠正一些排版问题\nmarkdown-scroll-sync 同步滚动，就是编辑器和预览能保持一致，这个很方便，就是你写到哪，预览就能生成到哪。这个6666，csdn也有这个功能，非常赞。\nlanguage-markdown 代码增强，让你的代码好看点。。没啥用。。但是看着舒服\nmarkdown-image-paste 贴图工具，这个是非常好用的功能，如果你输入的图片非常多，markdown可能是你的噩梦，但是这个工具可以减少一些复杂的操作，但具体的要看你的熟练程度，本人写博客一般都不会写特别长，但是喜欢用图片，一幅图胜过一百句话。\nmarkdown-table-editor 表格，markdown的表格也是一大奇葩，这个可以减轻点尴尬\n   姓名 性别 年龄     tony M 18   tan M 16   生成上面的图代码是这样的      | 姓名 | 性别 | 年龄 | |---- |:---:|:---:| |tony|M|18| |tan|M|16| 注意空格。。反正挺尴尬 讲究看吧，咱们是做研究技术的，又不是做美术的。。哈哈 以上是Atom本地的一些功能插件，主要参考http://www.cnblogs.com/libin-1/p/6638165.html的博客\nMarkdown \u0026amp; LaTeX markdown的语法我是通过在csdn写博客的时候掌握的，当时前三天很痛苦，但是学会了你会发现，word类的软件真的很烦，至于如何学习，这个自行Google。再见，不送。。\nLaTeX是写数学公式的法宝，可以预览的情况下，简直是神器，hexo环境下配置具体可以参考 hexo latex  具体的也是。。自己Google去吧 图片工具，这个是后补充的，安装\nnpm install https://github.com/CodeFalling/hexo-asset-image --save 很重要，不然图片没办法显示\nGit 和 Github git是啥，程序员都知道，你不知道？那就快去学习吧，哪怕只会clone也是好的，毕竟可以给老板交差，如果你有很多星星。那我要感谢你为人类软件事业做出的杰出贡献。 github是全球最大的同性交友平台，差不多吧，然后可以把hexo弄上去，意外吧，惊喜吧，不用你买服务器，自己搞个域名就可以，wordpress就行qq空间，一直都挺恶心的，但是之前不知道有hexo，现在果断弃暗投明，具体hexo和github怎么组合网上也一大堆，我用的主题来自yilia，感谢他花时间帮助我们这群没有美感的技术们搞了一个还算不错的样子。\nother 其他一些小的修修补补，比如添加目录什么的，都可以通过baidu得到答案，百度搜索，给你的不只是莆田系，还有不少入门知识！\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-%E7%94%A8atom-markdown-hexo-github%E5%86%99%E5%8D%9A%E5%AE%A2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract\u003c/strong\u003e：本文推荐一些小插件，和小技巧，目的是快速的完成自己的博客，博客的主要目的是传递经验和思想，这就和观赏性网站有了区别，不需要华丽，简洁，清楚的风格更适合技术人员。\n\u003cstrong\u003eKeywords\u003c/strong\u003e：LaTeX，Markdown，Hexo，Atom\u003c/p\u003e","title":"用atom+markdown+hexo+github写博客"},{"content":"Abstract: 通过学习MIT 18.06课程，总结出的线性代数的知识点相互依赖关系，后续博客将会按照相应的依赖关系进行介绍。 Keywords: Linear Algebra,Big Picture\n  线性代数总览 网易公开课有MIT 18.06的课程翻译，MIT OCW提供相关练习，如有需要都可以进行下载。 Gilbert Strang教授的讲授能够让大多数人入门，掌握这门课的大部分内容。 本课程教材使用的也是professor Strang的书籍，很遗憾，中国目前好像没有销售。 18.06的最大一个优点就是让你知道，你还不是特别笨，线性代数也没有大学老师讲的那么难。\n关系图 知识图：\n此图由Graphviz生成，相关项目见： Github：https://github.com/Tony-Tan/MachineLearningMath\n总结 线性代数是机器学习的基础数学之一，之前看到知乎上有一段话总结： 基本和我的一些思想不谋而合，很中肯，但是社会浮躁，能安心学习基础的人少之又少。 必须承认的是Professor Strang的讲授是非常有帮助的，如果有幸能见到其本人，我一定会给他鞠个躬，问一句老师好的。 后面线性代数部分的博客基本按照关系图一层一层的啃下去，不会特别长，但是知识点尽量做到环环相扣，欢迎关注。 完整目录整理如下：\n 1.0 向量 1.1 线性组合 1.2 点乘和长度 2.1 Ax=b 2.2 消元 2.3 消元和矩阵 2.4 矩阵计算 2.5 逆 2.6 LU和LDU分解 2.7 映射与排列 3.1 向量空间 3.2 Null 空间 3.3 秩 3.4 Ax=B 3.5 线性独立，基和维度 3.6 四个空间的维度 4.1 四个子空间的正交 4.2 投影 4.3 最小二乘（略） 4.4 正交基和Gram-Schmidt算法 5.1 行列式的性质 5.2 排列和代数余子式 5.3 Cramer\u0026rsquo;s 法则,逆和体积 6.1 特征值介绍 6.2 矩阵对角化 6.3 微分方程应用（略） 6.4 对称矩阵 6.5 正定矩阵 6.6 相似矩阵 6.7 奇异值分解 7.1 线性变换思想 7.2 线性变换的矩阵 7.3 对角化和伪逆 ","permalink":"https://go.face2ai.com/math/math-linear_algebra_big_picture.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 通过学习MIT 18.06课程，总结出的线性代数的知识点相互依赖关系，后续博客将会按照相应的依赖关系进行介绍。\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Linear Algebra,Big Picture\u003c/p\u003e","title":"【线性代数】线性代数总览"},{"content":"本站包含作者原创的关于人工智能的理论，算法等博客，目前包括：强化学习，深度学习，机器学习，线性代数，概率论，数理统计，Python，爬虫等在目前人工智能领域需要用到的基础知识，欢迎大家订阅关注。\n本站目录 首先插入一下我的整体研究思路，也是人工智能的技能树，我们要顺着这个树找到自己的研究方向： 人工智能 - 数学理论： 集合论  1 样本集 2 集合操作  数学分析  微积分 数学分析 实分析 复分析 泛函分析 测度论  线性代数  《Introduction to Linear Algebra》  线性代数BigPicture 1.0 向量 1.1 线性组合 1.2 点乘和长度 2.1 Ax=b 2.2 消元 2.3 消元和矩阵 2.4 矩阵计算 2.5 逆 2.6 LU和LDU分解 2.7 映射与排列 3.1 向量空间 3.2 Null 空间 3.3 秩 3.4 Ax=B 3.5 线性独立，基和维度 3.6 四个空间的维度 4.1 四个子空间的正交 4.2 投影 4.3 最小二乘（略） 4.4 正交基和Gram-Schmidt算法 5.1 行列式的性质 5.2 排列和代数余子式 5.3 Cramer\u0026rsquo;s 法则,逆和体积 6.1 特征值介绍 6.2 矩阵对角化 6.3 微分方程应用（略） 6.4 对称矩阵 6.5 正定矩阵 6.6 相似矩阵 6.7 奇异值分解 7.1 线性变换思想 7.2 线性变换的矩阵 7.3 对角化和伪逆    微分方程 拓扑学 概率论  概率基础  概率论BigPicture 1.0 概率介绍、试验、事件、公理化的概率 1.1 样本空间、柯氏公理、概率的性质 1.2 古典概率、乘法原理、排列 1.3 组合、二项式定理、多项式定理 1.4 有限事件并的概率、概率欺骗了你 2.1 条件概率、全概率公式 2.2 事件独立、条件独立 2.3 Bayes’ Theorem 3.1 随机变量和离散分布 3.2 连续分布 3.3 Cumulative Distribution Function 3.4 双变量分布 3.5 边缘分布不和独立随机变量 3.6 条件分布 (Part I) 3.6 条件分布 (Part II) 3.7 多变量分布（Part I） 3.7 多变量分布（Part II） 3.8 随机变量的函数 3.9 多随机变量的函数 4.1 随机变量的期望 (Part I) 4.1 随机变量的期望 (Part II) 4.2 期望的性质 4.3 方差 4.4 距 4.5 均值和中值 4.6 协方差和相关性 4.7 条件期望 5.1 分布介绍 5.2 伯努利和二项分布 5.3 超几何分布 5.4 泊松分布 5.5 负二项分布 5.6 正态分布(Part I) 5.6 正态分布(Part II) 5.6 正态分布(Part III) 5.7 Gamma分布(Part I) 5.7 Gamma分布(Part II) 5.8 Beta分布 5.9 多项式分布 5.10 二维正态分布 6.1 大样本介绍 6.2 大数定理 6.3 中心极限定理 6.4 连续性修正    数理统计  数理统计学基础  0.0 Big Picture 1.1 导言   《数理统计学简史》  介绍 早期概率论——从萌芽到《推测术》 1.1 卡丹诺的著作 1.2 分赌本问题 1.3 帕斯卡和费马的通信 1.4 惠更斯的“机遇与规律” 1.5 《推测术》前三部分内容提要 1.6 关于概率的几点看法 1.7 伯努利大数定律 2.0 狄莫弗的二项概率逼近 2.1 狄莫弗研究的动因  2.2 狄莫弗的初步结果 2.3 初步结果的改进，与斯特林的联系    随机过程 信息论  信息论、推理与学习算法  介绍    凸优化 数值分析  数值分析基础  课程介绍(Big Picture) 0.1 多项式求值 0.2 二进制 0.3 浮点数 0.4 有效数字    算法 强化学习  Reinforcement Learning: An Introduction  1.0 强化学习介绍 1.1.0 强化学习 1.1.1 强化学习、监督学习和非监督学习 1.1.2 “探索”(Exploration)还是“ 利用”(Exploitation)都要“面向目标”(Goal-Direct) 1.1.3 强化学习基本框架 1.1.4 强化学习和人工智能 1.2 强化学习举例 1.3 强化学习基础元素 1.4.0 “进化方法”（Evolutionary Method）和 “决策梯度方法” (Policy Gradient Methods) 概论 1.4.1 强化学习与优化方法 1.5 强化学习的一个扩展举例 1.6 本章总结、强化学习历史简述 2.0 多臂赌博机 2.1 k臂赌博机问题 2.2 行为评价方法(Action-value Methods)    机器学习算法  训练数据选择血泪史 人脸检测分析之 Haar-like，Adaboost，级联(Cascade)  统计学习算法 深度学习算法  FaceNet论文阅读 可视化CNN LeNet Dropout CNN训练数据选择  图像处理  刚萨雷斯  1.1 灰度级 1.2 灰度变换，gama变换，对数，反对数变换 2.1 一维DFT 2.2 二维DFT 2.3 FFT算法理解与c语言的实现 2.4 二维FFT,IFFT,c语言实现 2.5 图像傅里叶变换（快速傅里叶变换FFT） 3.0 二值图像 3.1 二值图像-形态学处理 数学形态学 3.2 二值图像-形态学处理 腐蚀和膨胀 3.3 二值图像-形态学处理 开操作和闭操作 3.4 二值图像-形态学处理4 其他操作 4.0 灰度图像 4.1 灰度图像-形态学处理 4.2 灰度图像-频域滤波 傅里叶变换之卷积 4.3 灰度图像-频域滤波 傅里叶变换之连续周期信号傅里叶级数 4.4 灰度图像-频域滤波 傅里叶变换之离散周期信号傅里叶级数 4.5 灰度图像-频域滤波 傅里叶变换之连续信号傅里叶变换（FT） 4.6 灰度图像-频域滤波 傅里叶变换之采样定理 4.7 灰度图像-频域滤波 傅里叶变换之离散时间傅里叶变换(DTFT) 4.8 灰度图像-频域滤波 傅里叶变换之离散傅里叶变换(DFT) 4.9 灰度图像-频域滤波 傅里叶变换之二维离散傅里叶变换 4.10 灰度图像-频域滤波 概论 4.11 灰度图像-频域滤波 滤波器 4.12 灰度图像-频域滤波 同态滤波 5.0 灰度图像-空域滤波 基础：卷积和相关 5.1 灰度图像-图像增强 综合介绍 5.2 灰度图像-图像增强 平滑之均值滤波、高斯滤波 5.3 灰度图像-图像增强 双边滤波 Bilateral Filtering 5.4 灰度图像-图像增强 中值滤波 5.5 灰度图像-图像增强 锐化基础 5.6 灰度图像\u0026ndash;图像增强 拉普拉斯算子 5.7 灰度图像-图像增强 非锐化掩蔽 （Unsharpening Mask） 5.8 灰度图像-图像增强 Robert算子、Sobel算子 5.9 灰度图像\u0026ndash;图像增强 灰度变换 5.10 灰度图像\u0026ndash;图像增强 直方图均衡化（Histogram Equalization) 5.11 灰度图像-图像增强 直方图匹配（规定化）Histogram Specification 6.0 灰度图像-图像分割 综合介绍 6.1 灰度图像-图像分割 边缘模型 6.2 灰度图像-图像分割 边缘检测算子 综述 6.3 灰度图像-图像分割 Robert算子 6.4 灰度图像-图像分割 Sobel算子 6.5 灰度图像-图像分割 Prewitt算子 6.6 灰度图像-图像分割 Scharr算子 6.7 灰度图像-图像分割 Sobel算子，Prewitt算子和Scharr算子平滑能力比较 6.8 灰度图像-图像分割 Canny边缘检测 6.9 灰度图像-图像分割 Marr-Hildreth算子（LoG算子） 6.10 灰度图像-图像分割 霍夫变换(Hough Transform)\u0026ndash;直线 7.0 灰度图像-图像分割 阈值处理综述 7.1 灰度图像-图像分割 阈值处理之平均阈值 7.2 灰度图像-图像分割 阈值处理之P-Tile阈值 7.3 灰度图像\u0026ndash;图像分割 阈值处理之迭代均值阈值 7.4 灰度图像-图像分割 阈值处理之谷底阈值、峰顶平均 7.5 灰度图像-图像分割 阈值处理之OTSU阈值 7.6 灰度图像\u0026ndash;图像分割 阈值处理之补充说明 7.7 灰度图像-图像分割 阈值处理之局部阈值 7.8 灰度图像-图像分割 区域分割之区域生长 7.9 灰度图像-图像分割 区域分割之区域分离 7.10 灰度图像-图像分割 区域分割之分水岭算法 8.0 彩色模型，CIE XYZ，CIE RGB 8.1 彩色图像-色彩空间 综述 8.2 彩色图像-色彩空间 RGB系列 8.3 彩色图像-色彩空间 CMY(K)空间 8.4 彩色图像-色彩空间 YIQ 、YUV 、YCbCr 、YC1C2 和I1I2I3 8.5 彩色图像-色彩空间 CIELAB、CIELUV 8.6 彩色图像-色彩空间 HSI(HSL)、HSV(HSB) 8.7 彩色图像-色彩空间 总结 9.1 彩色图像-伪彩处理 灰度图转伪彩色图像 9.2 彩色图像-彩色变换 补色处理 10.1 彩色图像-图像增强 直方图增强 10.2 彩色图像-图像增强 图像平滑 10.3 彩色图像-图像增强 图像锐化 10.4 彩色图像-图像分割 彩色空间分割    遗传算法 人工智能 - 神经生物学 人工智能 - 编程语言 Julia  Julia基础  Julia编程语言介绍    C/C++  《30天自制操作系统》  保护模式 IPL，BIOS 汇编和C GDT IDT 中断处理 中断 鼠标 内存管理 内存管理和叠加处理 定时器 计时器 MAC环境搭建 Mac环境搭建 MAC下的工具 FIFO 高分辨率 多任务I 多任务II 一个半成品    Python 爬虫  1.0 Introduction 2.0 分析目标网站 3.0 三只虫 3.1 HTTP协议（一） 3.2 HTTP协议（二） 3.3 数据抓取  GPU编程（CUDA）  CUDA基础  0.0 腾讯云CUDA环境搭建 1.0 并行计算与计算机架构 1.1 异构计算与CUDA 2.0 CUDA编程模型概述(一) 2.1 CUDA编程模型概述(二) 2.2 给核函数计时 2.3 组织并行线程 2.4 设备信息查询 3.1 CUDA执行模型概述 3.2 理解线程束执行的本质（Part I） 3.2 理解线程束执行的本质（Part II） 3.3 并行性表现 3.4 避免分支分化 3.5 循环展开 3.6 动态并行 4.0 全局内存 4.1 内存模型概述 4.2 内存管理 4.3 内存访问模式 4.4 核函数可达到的带宽 4.5 使用统一内存的向量加法 5.0 共享内存和常量内存 5.1 CUDA共享内存概述 5.2 共享内存的数据布局 5.3 减少全局内存访问 5.4 合并的全局内存访问 5.5 常量内存 5.6 线程束洗牌指令 6.0 流和并发 6.1 流和事件概述 6.2 并发内核执行 6.3 重叠内核执行和数据传输 6.4 重叠GPU和CPU的执行 6.5 流回调    框架  OpenCV  OpenCV矩阵计算分析   TensorFlow .etc  设计实现框架  PineNut  其他笔记  推荐读物 学习 Hexo博客搭建 Hexo畅言评论PC和移动端同步 Hexo下next主题valine强化版本的改造 【斯坦福大学】人工智能课程安排 【麻省理工学院】人工智能课程安排  原文地址：https://www.face2ai.com/Other-Website-Contents转载请标明出处\n","permalink":"https://go.face2ai.com/%E5%85%B6%E4%BB%96/other-website-contents.zh/","summary":"\u003cp\u003e本站包含作者原创的关于人工智能的理论，算法等博客，目前包括：强化学习，深度学习，机器学习，线性代数，概率论，数理统计，Python，爬虫等在目前人工智能领域需要用到的基础知识，欢迎大家订阅关注。\u003c/p\u003e","title":"本站目录"},{"content":"30天自制操作系统1 Mac下工具的使用 现在来介绍官网上下的工具怎么用首先是官网地址，书上有个注释上有:hrb.osask.jp 翻译成中文大概是这个样子滴。\n上面有两个文件可以下载，一个是工具，一个是工具的源代码，很好的学习资料\n下面把工具复制出来\n看到很多可执行文件。。感觉好舒服。。\n然后把我们随便一个project复制到z_tools的同级目录下\nproject的内容可以修改，因为批处理可以下岗了：\n然后可能是难度最大的部分出现了:修改makefile\n如果没用过makefile可以先找点资料看看。很简单的语法，很强大的功能： 修改完成后是：\n TOOLPATH = ../z_tools/ INCPATH = ../z_tools/haribote/ ################################################### MAKE = make NASK = $(TOOLPATH)nask CC1 = $(TOOLPATH)gocc1 -I$(INCPATH) -Os -Wall -quiet GAS2NASK = $(TOOLPATH)gas2nask -a OBJ2BIM = $(TOOLPATH)obj2bim BIM2HRB = $(TOOLPATH)bim2hrb RULEFILE = $(TOOLPATH)haribote/haribote.rul EDIMG = $(TOOLPATH)edimg IMGTOL = $(TOOLPATH)imgtol.com COPY = cp DEL = rm ################################################### ## ÉfÉtÉHÉãÉgìÆçÏ default : $(MAKE) img ## ÉtÉ@ÉCÉãê∂ê¨ãKë• ipl10.bin : ipl10.nas Makefile $(NASK) ipl10.nas ipl10.bin ipl10.lst asmhead.bin : asmhead.nas Makefile $(NASK) asmhead.nas asmhead.bin asmhead.lst bootpack.gas : bootpack.c Makefile $(CC1) -o bootpack.gas bootpack.c bootpack.nas : bootpack.gas Makefile $(GAS2NASK) bootpack.gas bootpack.nas bootpack.obj : bootpack.nas Makefile $(NASK) bootpack.nas bootpack.obj bootpack.lst bootpack.bim : bootpack.obj Makefile $(OBJ2BIM) @$(RULEFILE) out:bootpack.bim stack:3136k map:bootpack.map \\ bootpack.obj ## 3MB+64KB=3136KB bootpack.hrb : bootpack.bim Makefile $(BIM2HRB) bootpack.bim bootpack.hrb 0 ################################################## haribote.sys : asmhead.bin bootpack.hrb Makefile cat asmhead.bin bootpack.hrb\u0026gt;haribote.sys ################################################### haribote.img : ipl10.bin haribote.sys Makefile $(EDIMG) imgin:../z_tools/fdimg0at.tek \\ wbinimg src:ipl10.bin len:512 from:0 to:0 \\ copy from:haribote.sys to:@: \\ imgout:haribote.img ## ÉRÉ}ÉìÉh img : $(MAKE) haribote.img ################################################# run : $(MAKE) img qemu -fda haribote.img ################################################### clean : -$(DEL) *.bin -$(DEL) *.lst -$(DEL) *.gas -$(DEL) *.obj -$(DEL) bootpack.nas -$(DEL) bootpack.map -$(DEL) bootpack.bim -$(DEL) bootpack.hrb -$(DEL) haribote.sys src_only : $(MAKE) clean -$(DEL) haribote.img 乱码是原来的日语。。华丽的忽视掉，一排#之间的就是修改过的地方， 然后就是make run了。。 结果如下：\nMac环境搭建 (2013-12-13 14:20:00) 弄了三天了，终于弄好了，先说结果，就是作者在网站上放了os x的工具（hrb.osask.jp，也有linux下的工具，可以自己去下载），也就是说我白忙活了三天。。。\n再说一下这几天都干啥了，主要是想把c语言和nasm汇编连在一起。这个很多人都做过，但在网上现有的资料很少有在os X上做的的，也或者做了大家都没人说。。。。先贴代码：\nextern void swap(int *,int *); void main(){ int a=1; int b=2; swap(\u0026amp;a,\u0026amp;b); while(a==2) ; } 这是c代码，调用swap交换两个值，为了不调用标准库，我没写显示函数，而是用了一个死循环代替，如果程序停住了，说明运行成功，再贴下汇编，这是我第一次写汇编哦。。啦啦啦啦啦\nGLOBAL _swap [section .text] _swap: mov EDX,[ESP+4] mov EAX,[ESP+8] mov EBX,[EDX] mov ECX,[EAX] mov [EDX],ECX mov [EAX],EBX ret 代码很简单，但是和书上格式有些不同，作者说的他用的是nask是他自己改版的nasm所以有些关键字用不了。。。\n然后是编译成obj文件，这个很纠结，一开始不会用gcc编译出32位obj后来发现要加：\n\\-m32 就可以了。\n编译过程如下图：\n整个编译连接过程，最后光标停止，说明函数执行成功，如果nasm中写了什么中断或者什么其他系统不允许的可能会有总线错误（bus error）或者段错误（详情可以去看《c专家编程》，有相关说明）。 值得注意的是nasm -f 的参数：\nvalid output formats for -f are (\\`\\*\u0026#39; denotes default): \\* bin flat-form binary files (e.g. DOS .COM, .SYS) aout Linux a.out object files aoutb NetBSD/FreeBSD a.out object files coff COFF (i386) object files (e.g. DJGPP for DOS) elf ELF32 (i386) object files (e.g. Linux) as86 Linux as86 (bin86 version 0.3) object files obj MS-DOS 16-bit/32-bit OMF object files win32 Microsoft Win32 (i386) object files rdf Relocatable Dynamic Object File Format v2.0 ieee IEEE-695 (LADsoft variant) object file format macho NeXTstep/OpenStep/Rhapsody/Darwin/MacOS X object files 这个参数纠结了好久，最后还是看帮助搞定的，因为linux下都是elf，但是os x用elf参数最后ld会报错，说找不到xxx函数定义。。 ld的相关问题：\n1：ld: symbol(s) not found for inferred architecture i386 2：ld: symbol(s) not found for inferred architecture x86\\_64 3：ld: warning: ignoring file xxxx.o, file was built for unsupported file format 1，2和3的问题原因都是-f参数选的不对，或者gcc编译出来的是64位obj，nasm只能编译出来32或者16位目标代码。\n如果和系统可运行程序不对应，ld不会给你链接的\n最后是objcopy，这个是GNU 的binutils的工具包的一部分。作用是操作二进制文件，可以任意改格式，具体参考说明，吧之前链接好的用objcopy生成纯二进制文件后，和作者的比较发现，不一样，运行时qemu卡死，得到结论就是这两天又白忙活了。还好算是找到了工具，也有源代码，值得好好学习。\nMac环境下的工具介绍 (2013-12-09 20:20:00) 这几天一直在搞这个破环境，尝试各种做法，网上各种垃圾信息，浪费了很多时间，说的基本都是废话，不过还是找到了一些，赶紧写下来，不然这个过几天又忘了\n首先是环境，我用的是Max os Maverick 64，就是10.9，硬件（有点低0.0，对于小菜的我已经完全够用啦）：\n10.9刚发布，新出来的东西总问题一大堆，然后解决办法没几个。但是还是能用了，编译器NASM（xcode中command line tools带的那个）xcode真的很大，但是不可否认，安装比vs2010快多了。基本指令是dd指令，和cat指令，这两个和linux下差不多. 首先建立一个.img的镜像文件：\ndd if=xxxxx of=xxxx.img count=20000 注意，if后面的参数是个文件夹，用空的，of参数是输出的img文件 count 是大小，自己十几次就知道了, 然后编写ipl。 注意，后缀用.s而不用nas哦，因为.s的文件vi编辑器会自动语法高亮 然后要说一下ipl中的一句话\nRESB0x7fde-$ 这句话汇编通不过：提示这一行有非法操作符。。我也不知道咋回事。。。。。 改成：\n就可以顺利的汇编成bin文件了接着，用CAT指令把bin塞到img中\ncat ill.bin \u0026gt; xxxx.img 然后用qemu执行以下\nqemu -fda xxxx.img 结果如图： 用到的工具软件我会上传，安装比较简单，都是dmg的，双击就好。后天继续把c语言弄进来\n保护模式 (2013-11-27 16:55:00) 软盘？不需要！ 昨天一天看了5天内容，把觉得有些可能不好理解的写下来，内容不分先后，感觉作者写的通俗易懂，而且代码以及工具在xp下运行流畅，根本不需要软盘，直接在工具提供的虚拟机上跑就可以，下面来描述下昨天的学习心得和问题。\n首先是内存，对内存的管理是至关重要的，所以我们应该先了解一下内存的具体分布:\n这个图是我找的，如果有打错的地方或者有什么变动，请留言，不胜感激。 这就是内存的分配，对于小菜理解有些困难。接下来可能就是从加电到系统启动的过程的理解了. 实模式下（为保护模式做准备），启动并加载过程如下图：\n至于为什么到0x07c00，原因是“两头约定”（就是设计BIOS的大伯们和设计操作系统的大叔们商量好，把启动程序放那，一后大家开发都方便）。 里面用到的汇编命令，可以去看一些汇编的书，王爽老师的那本书不错，还有《深入理解操作系统》里面的知识和这里用到的也很合拍，都是好书，可以看看。 没有写完，下一篇继续写前五天的。\nIPL，BIOS (2013-11-27 18:20:00) IPL 其实还是前五天的，现在继续说，第三天，p49页下面部分说“0x8000到0x81ff是启动区”而0x7c00到0x7dff也是启动区，这个地方困扰了我好久，不过今天好像看到类似的说法了:在linux中启动区启动后会将启动区复制到0x90000的地方，原因也没说，知识含糊的说为后面的栈操作做准备，其实无论怎么放，这只是表明，IPL功能已经工作了，成功的实现了转移，我们可以自己来控制电脑了。 在IPL的实现过程中，我发现BIOS中断的威力相当大，就像平时我们用库函数一样，各种寄存器就是这些函数的参数，可以实现硬件的各种操作，准备明天起早找点BIOS中断的资料贴一下，如果能熟练运用会有很牛的感觉，已经接近硬件了，很兴奋。。。汇编是一把利器。\n汇编与C语言 (2013-11-27 19:07:00) 汇编语言函数用c语言调用 其实我们可以把这些在完成操作系统编写时写的函数称为库函数（注意：不是标准库函数），但是c在调用库函数（普通函数），压栈的顺序是从右向左的，这个是肯定，所以，参数出栈的时候先出来的是后面的参数，明白了这个问题，就很好理解io_out()函数参数的出栈和参数的使用了，还有函数如果有返回值，保存在eax（32位返回值），64位返回值保存在 eax 和edx中，edx保存高32位，eax保存低32位。\n知道了这些，参数和返回值就已经搞定了，剩下的就是定义和具体代码的实现了，什么利用中断啊，MOV给你MOV给我啊的什么就可以自由发挥了。\n看起来已经很酷了。 函数定义，搜了一下，发现定义都不同，但都是写global声明下函数名（名字前面加下划线），然后以函数名为标号写下函数体，返回用ret（但有些函数好像不能用RET返回。。具体以后再说）。\n还有就是函数指针，由于博主以前研究了好长时间指针，虽然不能说特别明白，但是看这个书上的指针还是没什么障碍，像什么2【p】这种访问方式，以前也都见过，如果有人这里有疑问推荐几本书：《c专家编程》《c陷阱与缺陷》《c和指针》都有详细描述，自己写个程序试试就行了，这里不再赘述。\nGDT和IDT (2013-11-27 20:12:00) GDT和IDT 全局描述表（GDT ，Global Descriptor Table）：首先看到全局说明这货很重要，应该是掌握了一些，其次这个表是个数组，所以，这是一个重要的数据结构。\nGDT数组中装的是段描述符【段地址，段的最大长度，访问权限】。因为这个描述符太大了，没有这么大的寄存器（64位），所以就把这个描述符放在内存里，成为了重要的GDT，intel的大叔们设计了一个寄存器GDTR（LGDT为装入此寄存器指令）用来存放GDT的入口地址，因为GDT可以设计到内存的任何位置，而GDTR可以帮助快速定位，这样段寄存器的13位就能索引到GDT了。GDT是保护模式必备的数据结构。能够很好的划分内存，控制程序的内存方位（不然会出大问题）。\n中断描述符表（IDT Interrupt Descriptor Table）：之前已经见识过中断的强大了，这个描述符表是管理中断的，很明显是高手中的高手，其实他是一个8字节的描述符数组，也是一种重要的数据结构。IDT只要保存256个描述符就够了，因为最多也不会有超过256种中断，甚至IDT可以少于256，只要够用就行。同样，IDT可以放在内存的各个角落，只要你愿意，但是一定要是线性的，不然就找不到了（这是个数组，当然线性了），IDT也有专门的寄存器存放入口地址，叫IDTR，这个寄存器中含有IDT表32位的基地址（高32位）和16位的长度（限长）值（低16位）。IDT表基地址应该对齐在8字节边界上以提高处理器的访问效率。LIDT和SIDT指令分别用于加载和保存IDTR寄存器的内容。LIDT指令用于把内存中的限长值和基地址操作数加载到IDTR寄存器中。该指令仅能由当前特权级CPL是0的代码执行，通常被用于创建IDT时的操作系统初始化代码中。如果有越界访问IDT的现象会触发一个保护性异常。\n中断向量表：实模式下的前1K个字节为中断向量表，每项有四字节（2字节的段地址，2字节的偏移地址，来定位实模式下的1M空间），用来指向中断处理程序的位置，但是在保护模式下，4字节不够用，需要8字节，所以改名IDT，而且位置也不局限于开始的0x00000处，使用全新的方式完成中断处理。\nIDT中的每一项叫“门”（门描述符），很明显是传送门。。哈哈。。\n中断处理 (2013-11-28 16:20:00) 中断处理 我感觉中断处理应该是系统底层设计的关键，因为这是在控制硬件，和给c提供接口，如果接口搞不好，一路兵败如山倒\n分割编译其实就是c语言初等知识，就是别写的太乱，每个文件放不同的东西，头文件加以说明和声明，以及一些宏定义或者什么的，可以去查查相关c语言的资料。 今天让我纠结了一下午的东西在p113，也就是调整栈内容，用以组合出GDTR的48位数据那个地方，很纠结，现在来说明一下：\n首先是c语言函数调用时参数传递问题，先从右边开始压栈，直到所有参数完成，但是在CALL的时候还要对CS：EIP进行压栈，这是个啥东西我也不清楚，所以我们可以看到取参数都从【ESP+4】开始取的，取参数的位置是【ESP+4】,【ESP+5】\u0026hellip;.取多少位就一直加加加。还有就是超过8位以上的数据在内存中存放的规律，如果是8位以下的一个内存单元就可以了，但要是16位32位呢，这就涉及到了大端机（Big-endian）和小端机（Little-endian）的区别了，首先我们以0x12345678为例，32的数据存储：\n就是高地址存高位还是低位的区别，不是很好理解，慢慢熟悉就好了，具体机器是大端还是小端写个c程序就行了，用一个联合来测试一下，具体自己发挥，《c语言深度解剖》有相关描述，好多书都提到过。 下面是113页困扰我很久的问题的具体解析：\n这就是那个GDT初始化函数调用时栈的内容，首先很纠结的就是栈底在高地址，栈是向下生长的，所以，看着很别扭，还有就是esp（栈顶指针）不是指向第一个参数的，剩下的问题就很好说了，如果要从内存0x02abf8读取两个字节，那么跟着0x2abf8后面的f9会被连带访问而不是f7。。。。。 接下来说PIC，这是个硬件，如果有点电路知识就应该没啥障碍了，是一个存储设备，cpu可以从他取数据，具体怎么去要看这个片子功能的设计和相关信号的传递方法，没有什么需要解释的，设计完全是为了便于开发。 中断程序的制作应该是核心问题，以及缓冲区的使用，明天再说。感谢收看，\n中断 (2013-11-29 13:23:00) 中断处理程序 今天看的还是比较顺利，很快一天的就看完了，总结一下今天的主要知识。\n首先是中断处理程序，中断处理程序尽量高效，短小精悍，所以像显示啊什么的操作尽量不再中断处理程序中出现，因为一旦进入中断处理程序，其他中断将会被屏蔽，因为如果不屏蔽就会出现乱套的现象，一个中断还没完另一个已经来了，你说你干不干，干哪个，所以，尽量减少中断处理时间，这样就可以减小中断同时发生而后发生的没办法处理的现象，所以，缓冲区出现了用武之地。\n之前一直不知道缓冲区是干嘛的，今天终于见到其真正的用途了，就是保存中断信息，然后慢慢处理，之后讲到FIFO缓冲区（讲的不严谨点就是队列），队列的大小随意定义，为了减少数据移动，使用了循环队列，高效，但结构里面有一个len用来判断对是否满，可以用头指针和尾指针的位置关系来判断，但没有多大效率上和空间上的区别，所以应该都可以，这里的队列采用数组而不是链表，数组在空间上连续，所以读的速度更快。效率才是硬道理！\n发现了对中断的处理和处理时对下一个中断的到来以及处理安排上是很重要的，需要我们严加设计的，应该也是系统设计的重要部分，还有书中出现的各种编号（像不同端口的地址什么的），不知道具体去哪找，不同的cpu不知道有没有什么区别，这是不是就Intel那个好几千页的手册所要说明的。\n还有就是鼠标和键盘的电路是一个芯片组上的。\n这几天基本都是用c语言设计，感觉只用汇编写了那些最基本的函数，如果系统想要更强大是不是应该多用汇编写点更多的函数以供调用。\n鼠标 (2013-11-30 15:44:00) 啊哈，鼠标 今天看的有点纠结，因为竟然看困了，很少有这种情况，主要是有好多东西作者并没有做深入介绍，只是说这样是对的，至于为啥这样以后再说，这让我感觉很不爽。具体来说说。\n首先，先解决了鼠标运动的问题，我感觉这个不是什么大问题，只是读取数据那需要点技巧，就是验证数据的有效性，以前用单片机发送串口数据时也用到过类似的做法，作者提出的是检查第一个字的低八位以及高两位，如果高两位是0，而低八位是8，那么数据没问题，但并不一定绝对没问题，比如发送的数据为 0x08 0x00 0x11下一组为0x08 0x08 0x01时也就是队列里面是0x08 0x00 0x11 0x08 0x08 0x01时如果红色字体丢失任意一组数据，其结果和下一组结果都是错误的，但发生的概率很小，我们也没必要要求硬件每发送一个数据都要加上校验位，那样冗余太大，也浪费资源，而且估计鼠标也不会总坏。\n个人来讲，我不想将桌面作为系统开发的前期所应该做的（等我自己写的时候不会这么弄），一个黑漆漆的命令行而具有更强大的功能，我觉得更好，而且将桌面和系统剥离开，也就是linux那样，感觉更加自由。哈哈，这都是自己的想法，没有任何科学根据。接下来就到了让我纠结的地方了，就是作者解释了之前没解释的汇编代码：\n ; haribote-os boot asm ; TAB=4 BOTPAK\tEQU\t0x00280000\t; c语言编写部分的入口位置 DSKCAC\tEQU\t0x00100000\t; 启动区将要被复制到的地方 DSKCAC0\tEQU\t0x00008000\t; dskcaco启动区存储地址，其实里面没啥，知识为了预留，这也是之前纠结的地方 ; BOOT_INFO関係 CYLS\tEQU\t0x0ff0\t; LEDS\tEQU\t0x0ff1 VMODE\tEQU\t0x0ff2\t; SCRNX\tEQU\t0x0ff4\t; SCRNY\tEQU\t0x0ff6\t; VRAM\tEQU\t0x0ff8\t; ORG\t0xc200\t; ; 显示设置，中断号10H具体中断用法参照google MOV\tAL,0x13\t; MOV\tAH,0x00 INT\t0x10 MOV\tBYTE [VMODE],8\t; MOV\tWORD [SCRNX],320 MOV\tWORD [SCRNY],200 MOV\tDWORD [VRAM],0x000a0000 ; 设置键盘中断，中断号16H，自行百度 MOV\tAH,0x02 INT\t0x16 ; keyboard BIOS MOV\t[LEDS],AL ; ;\tPIC必须在CLI之前设置好 ;\t有些机型不能连续使用OUT指令，中间要停一下 ;\tPIC初始化 ; CLI指令后CPU屏蔽所有中断 MOV\tAL,0xff OUT\t0x21,AL NOP\t; OUT\t0xa1,AL CLI\t; ; 开启A20设置，这里的具体原理见下文。。 CALL\twaitkbdout MOV\tAL,0xd1 OUT\t0x64,AL CALL\twaitkbdout MOV\tAL,0xdf\t; enable A20 OUT\t0x60,AL CALL\twaitkbdout ; 这就是个简单设置并等待反馈的过程，书上说可以同时读取鼠标和键盘。。。我没看出来 [INSTRSET \u0026#34;i486p\u0026#34;]\t; 指令集，不知道酷睿2应该咋表示0.0 LGDT\t[GDTR0]\t; 设定GDT MOV\tEAX,CR0 AND\tEAX,0x7fffffff\t; OR\tEAX,0x00000001\t; 将EAX设置成二进制 1xxx xxxx xxxx xxxx xxxx xxxx xxxx xxx1的形式 MOV\tCR0,EAX ；CR0很高端的寄存器，只有操作系统可以使用 JMP\tpipelineflush pipelineflush: MOV\tAX,1*8\t; 1*8不知道是啥意思。。。。。 MOV\tDS,AX MOV\tES,AX MOV\tFS,AX MOV\tGS,AX MOV\tSS,AX ; bootpack MOV\tESI,bootpack\t; 复制的原始地址 MOV\tEDI,BOTPAK\t; 复制的目标地址 MOV\tECX,512*1024/4 ；DWORD是单位 CALL\tmemcpy MOV\tESI,0x7c00\t; 同上 MOV\tEDI,DSKCAC\t; 同上 MOV\tECX,512/4 CALL\tmemcpy ; 残り全部 MOV\tESI,DSKCAC0+512\t; MOV\tEDI,DSKCAC+512\t; MOV\tECX,0 MOV\tCL,BYTE [CYLS] IMUL\tECX,512*18*2/4\t; SUB\tECX,512/4\t; 真心不知道这步干啥用 CALL\tmemcpy ; bootpack开启 MOV\tEBX,BOTPAK MOV\tECX,[EBX+16] ADD\tECX,3\t; ECX += 3; SHR\tECX,2\t; ECX /= 4; JZ\tskip\t; 如果哦ECX\u0026gt;\u0026gt;2==0，则跳转 MOV\tESI,[EBX+20]\t; 还是复制。。 ADD\tESI,EBX MOV\tEDI,[EBX+12]\t; CALL\tmemcpy skip: MOV\tESP,[EBX+12]\t; 不懂在干啥。。。 JMP\tDWORD 2*8:0x0000001b waitkbdout: IN\tAL,0x64 AND\tAL,0x02 JNZ\twaitkbdout\t; 读入0x64的响应和0x02做与运算，非零则跳转 RET memcpy: MOV\tEAX,[ESI] ADD\tESI,4 MOV\t[EDI],EAX ADD\tEDI,4 SUB\tECX,1 JNZ\tmemcpy\t; 复制过程比较简单 RET ALIGNB\t16 GDT0: RESB\t8\t; DW\t0xffff,0x0000,0x9200,0x00cf\t; DW\t0xffff,0x0000,0x9a28,0x0047\t; 不知道在干嘛。。写了这么多奇怪的地址 DW\t0 GDTR0: DW\t8*3-1 ；依旧不知道8*3哈意思 DD\tGDT0 ALIGNB\t16 bootpack: 好吧，我的理解就只能到上面这个地步，不知道再继续往下看还能多领悟点不了。\n为什么是向键盘发送指令来控制A20GATE呢？百度到的原因是为了控制是否使用超过1M以上的内存，并和16位兼容，IBM的老爷爷们使用键盘控制器剩下的一根信号线来控制A20，A20不是控制是否进入保护模式。\n原始16位处理器访问最高地址为0xFFFF：0xFFFF=0xFFFF0+0xFFFF=0x10EFFEF,很明显多于1M对于多出来的0xEFFEF的访问就需要另外的地址线。但是系统的做法是当程序员访问多于1M的地址时，CPU将地址按1M取模。。这样就不会超过1M了，这种技术被称为wrap-around\n到了80286出现了一些问题，当程序员试图访问1M到0x10EFFEF时系统并没有循环回去而是直接访问，这导致了和之前产品不兼容，于是为了兼容，设计了第21根信号线就是上面提到的键盘控制器多余的这根，称为A20来控制是否访问1M以后的地址，当设置为打开时可以访问到多于1M的地址，关闭时则只能循环回0按8086的方式访问。\n以上都是实模式下的，在保护模式下，CPU访问的内存增加，如果这个20位的控制线不被打开，那我们的第20号位地址将被视为无效，内存将会被切割成小碎片，系统将只能访问以基数兆的内存。。所以必须要打开A20才能完整访问。至于A20和键盘的关系。就是没啥关系，只不过用一个控制器。 一下内容来自互联网（没有验证准确性）：\n 多数PC都使用键盘控制器（8042芯片）来处理A20Gate。 从理论上讲，打开A20Gate的方法是通过设置8042芯片输出端口（64h）的2nd-bit，但事实上，当你向8042芯片输出端口进行写操作的时候，在键盘缓冲区中，或许还有别的数据尚未处理，因此你必须首先处理这些数据。 流程如下：  禁止中断；  等待，直到8042 Inputbuffer为空为止； 发送禁止键盘操作命令到8042Input buffer； 等待，直到8042 Inputbuffer为空为止； 发送读取8042 OutputPort命令； 等待，直到8042 Outputbuffer有数据为止； 读取8042 Outputbuffer，并保存得到的字节； 等待，直到8042 Inputbuffer为空为止； 发送Write 8042Output Port命令到8042 Input buffer；  等待，直到8042 Inputbuffer为空为止；  将从8042 OutputPort得到的字节的第2位置1（OR 2），然后写入8042 Input buffer；  等待，直到8042 Inputbuffer为空为止；  发送允许键盘操作命令到8042Input buffer；  打开中断。   内存管理 (2013-12-01 13:47:00) 内存管理 对于一个系统来说，资源是最重要的，管理资源应该说就像计划你口袋里面的钱怎么花一样（不太准确。。但是重要性是相似的）。\n首先是检查内存大小，BIOS应该是提供内存大小检查功能的，但是不同的BIOS查找内存大小的方式不同，所以我们应该以一种更为通用的方法进行，以获得更好的系统兼容性，而且现在的CPU都配有各种缓存，一级，二级，三级。。。先要禁止高速缓存，才能确保访问的变量全部在内存中，所以，先禁用缓存（设置CR0的某个标志位为指定值，本书还检查了一下机器是否有高速缓存，这个现在基本不用检查了，我还真就没见过386长什么样）。禁用掉高速缓存我们就要对内存挨个访问了，看看到底有没有这个位置，对0xaa55aa55的反转和比较再反转，我感觉没有什么具体意义，你可以改成任意的数字，如果访问的内存地址不存在，则返回记录到的最大的位置值，后面的程序是优化次功能程序，如果只为了检查内存的大小，完全可不用一个一个来，而是十个十个来或者一万个一万个来，只检查最后的几位就行，这个的速度会按照相应的倍数提升而准确度也会随之下降。如果要做内存健康检查就要一个一个来了。书中还有一个涉及到编译器的优化问题，这个会在后续的博客中详细介绍。\n接下来，检查完内存就要对内存分配进行管理了，这个应该是个很困难的问题，书中的方法有两种。\n第一种：首先设置内存分配的最小单位，书中为4KB，把所有能用的字节都统计出来划分成4KB一段的好多个内存块，然后用一个字符数组来记录那些很忙，那些很闲，然后分配给需要的程序，这样的代价是每4K就需要8BIT的空间来标记这块的可用性（8/（4×1024）=1/512）所以代价是恒定的，不论多大内存N GB，都需要（N/512GB的内存来记录），这样的好处也是第二种方法的缺点就是不用做拼接，碎片问题容易解决。据说内存碎片是个很可怕的问题，尤其是对于长时间运行的服务器。\n第二种：用一个数据结构记录所有内存的使用情况，这个数据结构内包含一个可用空间大小的记录，还有就是N个用于记录内存起始地址和可用大小的子结构数组了。\nstruct FREEINFO { unsigned int addr, size;//起始地址，和大小  }; struct MEMMAN { int frees, maxfrees, lostsize, losts;\t//这是内存的一些记录，包括可用空间，最大可用空间，释放失败大小，释放失败的次数  struct FREEINFO free[MEMMAN_FREES]; //具体的信息。。  }; 第二种方法速度快，占用空间小，但是分配时产生的细小的内存空间没有被使用，和回收内存时的合并操作都相当麻烦。。所以有利有弊，当内存碎片过多，而系统回收算法不够强壮时有可能是灾难性的。\n至于设计系统时用什么样的内存管理方法，目前还没想好，总之，这是个很关键的问题。会找一些资料，在后续的博文中陆续推出，欢迎收看\n内存管理和叠加处理 (2013-12-02 13:42:00) 内存管理和叠加处理 今天的代码比较多，对于写过程序的人来说应该还是比较容易看懂的。 首先说的内存管理，昨天说道把很多内存分成0x1000字节大小的块，然后分配给程序使用，但是不一定所有的程序用到的都是0x1000的整数倍字节，所以就要设计一个取整的方法，就有了书中提到的聪明做法（size+0xfff）\u0026amp;0xfffff000;这个做法或者称为算法的正确性证明很简单，带两个数试一下就知道玄机了，就是把一个12位非零二进制数向前进位，而零不进位，最后把后3位归零。我们还可以把大于x的任何数进位，小于等于x的舍去（size+0x1000-x-1）\u0026amp;0xfffff000；哈哈，应该是对的，我也没有证明。不过x=0是和原式一样的。举一反三山寨之本。\n接着就到了我很疑惑的地方了。叠加处理，但是！！！注意！！！作者开始给结构体显示的分配内存了，就是图层的控制结构体，我不明白，之前写的那些结构体也没分配内存，为啥突然从这开始分配了，而作者根本没提，难道是因为昨天才写的内存管理，今天就开始用了？那之前的程序怎么办呢。我一会儿还是好好研究一下。\n感觉要设计系统，就应该对这个系统的宏观结构应该完全清楚，骨架清晰，内容就可以随意发挥了。现在是骨架还没搞清楚，所以博猪没有一开始就写自己山寨版，而是想把书看完，然后找点操作系统相关的书再看看然后再做个试验品出来玩玩。。哈哈。。不知道思路正确不正确。。\n至于这个叠加处理，其基本目的是控制多个图层，设计相应的数据结构。而刷新那里作者反复的实验，但其本质就是减少冗余计算，就是说有些计算能剩就不做，就像一个简单的图像卷积（好吧，博猪以前做过一段时间图像处理，听不懂的可以完全忽略。。）如果按照算法做，时间复杂度是 $O(xysizesize)$（x，y是图像的大小 $xy$ 个像素，size是卷积核宽度），但经过优化可以做到只要 $O(x*y)$ ，而方法就是要用到前一步的计算结果，减少冗余的计算量（好吧，这是博猪第一次面试的面试题，很幸运。公司没要我0.0）。好，叠加的优化原理是哪个地方改变了，就刷新改变的区域的像素。至于实现细节，书中的代码有详细介绍，关键是确定叠加区域，然后重绘叠加区域。。。 明天继续努力。\n定时器 (2013-12-03 14:06:00) 闪烁和定时器 今天的主要内容是屏幕刷时的闪烁问题，和定时器的小部分内容。\n首先说明定时器应该是系统重要部分，所以明天会好好记录一下计时器的心得。\n关于屏幕刷新时闪烁，这个之前也遇到过（之前做单片机数据绘图时，因为单位时间数据量很大，一直刷新画面就会导致波形图闪烁，而解决这个问题的方法是使用双缓存技术），而这次不同的是如果系统桌面都闪烁，就没人会使用我们的系统了。。所以问题很严重，需要马上解决，作者的解决方法是改变刷新的范围和层数，减少无谓的刷新。其实作者从开始所提出的桌面显示的完整体系就是一种最简单的设计（作者想带着我们一步一步走向高端，但如果看完全书后再去写的话就不需要走弯路了），我们设计时完全可以自己发挥，设计出更高效更华丽的桌面环境，作者最后提到的在内存中开辟map区域其实就是一种高效的方法，而这个方法可以在一开始显示的时候就设计进去，所以，桌面显示并不应该成为我们学习的重点（在系统体系完成后再详细设计规划）。\n相反，计时器才是我们应该好好研究的，这是系统的根本之一。\n计时器 (2013-12-05 19:00:00) 计时器 昨天一直在搭建环境，想在mac下实现书上制作镜像和编译连接相关文件的功能，无奈，弄了一天还是不太好用，看到另外一个微博上的童鞋用Linux完成了相关操作，想试一下，但os x上没有objcopy等命令，而且gcc好像也是改版的，而且现在被我折腾的连命令行下使用command line tools 都有问题，所以，我还是回来把笔记补上。一会儿再去弄，争取放假之前能把环境搞定，这样寒假就可以专心研制自己的系统了，而不用把时间花到细枝末节上。\n昨天的内容讲的是计时器，我一开始没觉得定时器有啥功能，无非是弄个钟表出来，后来我看到了超时（timeout）恍然觉得这个东西很主要，作者并没有直接说去定时器在以后的实现中有什么作用，但是一直对中断处理程序进行优化可以看出，这个东西以后要被不停地使用，而且作者提出定时器后，马上有多弄了几个，而且最多能实现500个定时器，第十二天的主要工作就是优化处理速度，因为中断处理时所有的中断信号都被屏蔽了，所以必须快速的处理，恢复中断响应。\n下面的图是作者最开始的设计：\n其中条状的是定时器的执行时间，横坐标是时间，纵坐标是各定时器，不论有多少个定时器，都要循环500次（也就是最多个的情况），所以这对要求效率的工作并不是个好主意，于是开始优化。\n首先可以将定时器排队，就像我们有两场球赛在不同的频道，一个十分钟后在c1频道，一个二十分钟后在c2频道，我们只要盯着那个十分钟后开始的c1频道就可以了，你可以每两分钟过来看一下，如果c1开始了，我们才会去关注c2是否开始（寝室有一哥们能同时看三场球。。。。）也就是说c1不发生，理论上来说c2绝不会发生，所以就优化出了下图：\n上图横轴是时间，没有纵轴，每个小格子就是不同的定时器之间的间隔，每个定时器都是从头开始到对应的线，也就是把所有定时器从小到大排列起来，然后弄到一根轴上，这样就可以只监视下一个要超时的定时器就可以了。\n而上图还是要监视500个，于是下一步的优化就是记录当前计时器的个数，减少循环，之前还有一步就是尽量减少循环中的计算，包括比较（if语句）尽量减少少数情况的判断，提高速度。 涉及到的主要问题就是速度优化，下面列举一些常用到的比较简单的循环体的优化方式：\n1：将在循环里面多次计算，但是结果不会改变的计算，移到循环外面去。\n2：减少函数调用\n3：减少内存访问\n4：减少对少数特殊情况的测试\n目前就这些，以后用到定时器会继续说。。\n FIFO (2013-12-16 14:57:00) 首先是FIFO缓冲区的一个通用化，这样就相当于在一个队列里面加入了很多中断处理收到的数据，CPU可以根据自己的节奏一个一个来处理，包括鼠标，键盘，定时器，以后还有可能是其他的设备，这样做的好处就是先后顺序很明确，谁先来就先处理谁，如果有多个缓冲区就无法确定哪个应该先处理，哪个应该暂时等待，而且这样设计更有利于编写程序，一举多得。 接着就是比较重要的性能测试，说实话，这个是非常重要而且想让我们的程序飞起来就必须反复去做的，作者也在这方面写了很多，但是方法很简单\n1：写程序，运行，记录时间，观察代码 2：优化代码 3：继续跑程序，记录时间，返回第二步\n什么时候感觉已经很快了，这就算结束了，但是最最最重要的是优化代码，这个一句话两句话根本说不明白，我也是个水菜鸟，所以这里就不瞎说了，大家自己看书学习吧，哈哈，反正觉得效率才是硬道理，效率和稳定，我觉得是核心种的核心，无论什么原因这两个都不能变。 具体方法，作者提到了一个链表，这个就是数据结构的应用，希望以后设计操作系统的时候能用到更多的其他数据结构来优化程序性能\n高分辨率 (2013-12-16 15:21:00) 高分辨率 高分辨率，这个是个必须要说的话题，因为好像没有啥电脑显示器用320*200的分辨率了，所以提高分辨率势在必行。。哈哈 一个重要的操作就是查询VBE 中断编号 0x10 参数ES：DI指向的地址将会被VBE的内容覆盖 参数AX存储一个返回值，如果是0x004f就是有VBE不是这个值，就意味着VBE不可用 画面模式信息：\n WORD 【ES:DI + 0x00】 :模式属性 WORD 【ES:DI + 0x12】 :x分辨率 WORD 【ES:DI + 0x14】 :y分辨率 BYTE 【ES:DI + 0x19】 :颜色数。。。必须为8 BYTE 【ES:DI + 0x1b】 :颜色的指定方法。。。必须为4（4为调色板模式） DWORD 【ES:DI + 0x28】 :VRAM的地址 然后就是现实键盘输入，移动对话框，不在赘述\n多任务 I (2013-12-17 15:20:00) 今天的内容是多任务，所谓多任务就是“同时”执行多个任务，作者写的是在单核cpu上轮流执行多个任务，在短时间进行各种切换，但是实际上还是串联的，虽然表面上看的是同时执行，不知道现在的多核处理器是如何完成多任务的，我一会儿google一下，写出来，应该有很些不同。 先说今天的内容，对于任务切换，处理器是有代价的，也就是说，切来切去是要消耗时钟周期的，例如如果切换一次需要消耗额外的1s但是你每2s切换一次，也就是执行一个任务一共用了3s（因为任务完成后总要切换，所以就可以把切换时间算入总的执行时间）但真正的执行只是2s这样效率只有66.67%如果1s切换一次，就是有50%了。所以几秒切一次很重要，如果10000s切换一次，效率近似与99.99%但是估计使用者会等疯，卡死了。\n上面的1s切换时间是我扯淡的，没有那么慢，书上说千分之一秒，估计现在的处理器会更快。\n重要概念：TSS\nTSS：（Task Status Segment）任务状态段，用于记录任务的执行情况，和已经执行的进度，就像打游戏的读档，就可以直接玩上次存档的地方了，而不是从头接着打，而这个档就是TSS负责记录各种状态（装备等级，任务进度什么乱七八糟的）任务切换时查询TSS然后继续运行。\nCPU特殊机制：执行带有段地址的命令式就会去GDT种查询一下，看是否是另外的程序，如果是，就进行任务切换。\nGDT是个很重要的管理者，应该去画个图把整个体系操作系统体系画出来，这样以后设计的时候就能参考蓝图一步步设计了。看完这本书就去画。\n提高运行速度，其实作者把修改刷新显示频率作为第一步优化，我觉得这个完全在定时器的时候就应该修改系统原始设计，因为屏幕的刷新率一般就那么几种，刷的过快没效果而且消耗资源，过慢当然也不行，既然写到这，那就算优化吧，边做边修改，也是一种好方法，没谁能一开始就制定天衣无缝的计划。。。\n第二点提高速度很精彩，就是把任务切换定时器从缓冲区中分离，也就是键盘，鼠标，普通定时器等中断在FIFO的缓冲区，而任务切换的定时器不在其中，不论其中是否有未处理的中断时间一到，马上切换，但是，问题是，如果FIFO中又任务A的中断处理，但是此时任务A恰好切换到B。。这岂不是坑爹了。。。不太明白，看看查查资料有没有详细的介绍。\n还有就是return不能随便用，在开发系统的时候，还有函数调用时对EFLAG寄存器的影响，都是应该注意的。\n多任务 II (2013-12-19 14:51:00) 今天是多任务第二讲，比昨天的层次更高，更加接近实际使用的设计。\n首先是任务的自动化，所谓任务就是正在执行的程序，程序成千上万，可以自己随意安装，所以，管理就不能靠人工修改代码，而要自动管理，自动分配空间，自动注册相关设置，所以就要建立一个结构体来管理众多任务，记录当前任务的信息。\n任务的休眠，有些任务需要等待数据，或者等待中断程序，等待的时候CPU等就会跟着闲着，这可不好，我们必须让CPU尽可能的处于运行状态，所以就有了任务的休眠，让一个等待的任务不再处于运行状态，而让那些高计算量的任务过来运行，而当任务休眠时，数据到来，马上唤醒任务，让他继续执行。就是这样的。\n接下来就涉及到任务的优先级了，谁应该有更多的系统资源，比如你听音乐和写程序，一输入代码音乐就卡这可不好，分成1，2，3，4，5\u0026hellip;\u0026hellip;12各种等级，我们可以音乐12级，文本编辑5级，这样优先执行音乐，音乐不会卡，文本就算卡也没办法。。自找的。。跟我们音乐大哥抢资源是抢不过滴。\n其实这并不是科学的划分，如果超过12个任务，必然有同级任务存在，那到时候怎么办呢，音乐和电影抢资源，谁应该赢呢，即使这个问题解决了，优先级运行时间划分也存在问题，如果安装1级1s（假设），2级2秒。。。。10级十秒这种划分也不是很科学的，因为2级和1级只差一级，但2级运行时间是1级的一倍，而3级和2级之间也只差一级，但却只有1.5倍所以这也是个问题。。这个姑且放一下，以后再考虑。\n作者解决音乐和电影的方法是把等级再分层，类似与金字塔结构，每层有不同的任务，任务永远只在顶层切换，而不会轮到下层，知道上层都运行结束，也就是顶层消灭了，才会轮到现在的顶层，之前的第二层。而如果高层来人了，当前层当前运行任务结束后，无条件切换到上层。\n都是些理论，不知道现在实际种工作的操作系统是怎么弄得，查到资料了再写出来\n一个半成品 (2014-09-14 22:56:00) 之前看《30天自制操作系统》，参考而成，和书中系统并不完全一致，是在原有基础上按照自己的习惯而成，由于水平和工作原因，未完成内存管理和文件系统，有兴趣者可以通过以下网址https://github.com/TonyTan1991/Jupiter，欢迎留言，欢迎参与项目并改进。 因想寻求图像处理的工作，接下来将开始写作图像处理类博客，内容包括算法原理，实现，OpenCV实现等，欢迎关注\n  川合秀実. 30 日でできる! OS 自作入門. マイナビ出版, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30_days_system/","summary":"30天自制操作系统1 Mac下工具的使用 现在来介绍官网上下的工具怎么用首先是官网地址，书上有个注释上有:hrb.osask.jp 翻译成中文大概是这个样子滴。\n上面有两个文件可以下载，一个是工具，一个是工具的源代码，很好的学习资料\n下面把工具复制出来\n看到很多可执行文件。。感觉好舒服。。\n然后把我们随便一个project复制到z_tools的同级目录下\nproject的内容可以修改，因为批处理可以下岗了：\n然后可能是难度最大的部分出现了:修改makefile\n如果没用过makefile可以先找点资料看看。很简单的语法，很强大的功能： 修改完成后是：\n TOOLPATH = ../z_tools/ INCPATH = ../z_tools/haribote/ ################################################### MAKE = make NASK = $(TOOLPATH)nask CC1 = $(TOOLPATH)gocc1 -I$(INCPATH) -Os -Wall -quiet GAS2NASK = $(TOOLPATH)gas2nask -a OBJ2BIM = $(TOOLPATH)obj2bim BIM2HRB = $(TOOLPATH)bim2hrb RULEFILE = $(TOOLPATH)haribote/haribote.rul EDIMG = $(TOOLPATH)edimg IMGTOL = $(TOOLPATH)imgtol.com COPY = cp DEL = rm ################################################### ## ÉfÉtÉHÉãÉgìÆçÏ default : $(MAKE) img ## ÉtÉ@ÉCÉãê∂ê¨ãKë• ipl10.bin : ipl10.nas Makefile $(NASK) ipl10.","title":"30天自制操作系统"},{"content":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years. So far, I\u0026rsquo;ve learned and reviewed some papers that are necessary for reinforcement learning research, and some mathematics, like calculus, linear algebra, probability, and so on.\nHowever the bigger one in the picture is me, Tony, and the smaller one is my dog, Potato(He was too young to take a shower when we took the picture, and now he is no more a dirty puppy😀)\nWhy the blogs These blogs here are used to simply explain what I have learned, according to the Feyman Technique. And blogging what I\u0026rsquo;ve just learned is the most important part of learning. The whole process is:\n Choosing a concept or theory I would like to know(collecting necessary materials )  Outlining what prior knowledge of this concept or theory Taking note of this prior knowledge   Try to explain the new theory to readers of the website without any new words and concepts in the theory (draft) Go back to the source and fill in the gap in understanding Simplify the explaining(rewrite and post)  What in blogs These blogs contain:\n Mathematics Neuroscience Algorithms  Deep Learning Reinforcement Learning    And some of these posts might also be represented by videos on my YouTube channel.\n","permalink":"https://go.face2ai.com/about/","summary":"About Me Hi, I\u0026rsquo;m Anthony Tan(谭升). I am now living in Shenzhen, China. I\u0026rsquo;m a full-time computer vision algorithm engineer and a part-time individual reinforcement learning researcher. I have had a great interest in artificial intelligence since I watched the movie \u0026ldquo;Iron man\u0026rdquo; when I was a middle school student. And to get deeper into these subjects, I\u0026rsquo;d like to apply for a Ph.D. project on reinforcement learning in the following years.","title":""},{"content":"","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/crawler/crawler/","summary":"","title":""},{"content":"Abstract: 《30天自制操作系统》闪烁和定时器 Keywords: 《30天自制操作系统》，Macbook，定时器\n闪烁和定时器 今天的主要内容是屏幕刷时的闪烁问题，和定时器的小部分内容。 首先说明定时器应该是系统重要部分，所以明天会好好记录一下计时器的心得。 关于屏幕刷新时闪烁，这个之前也遇到过（之前做单片机数据绘图时，因为单位时间数据量很大，一直刷新画面就会导致波形图闪烁，而解决这个问题的方法是使用双缓存技术），而这次不同的是如果系统桌面都闪烁，就没人会使用我们的系统了。。所以问题很严重，需要马上解决，作者的解决方法是改变刷新的范围和层数，减少无谓的刷新。其实作者从开始所提出的桌面显示的完整体系就是一种最简单的设计（作者想带着我们一步一步走向高端，但如果看完全书后再去写的话就不需要走弯路了），我们设计时完全可以自己发挥，设计出更高效更华丽的桌面环境，作者最后提到的在内存中开辟map区域其实就是一种高效的方法，而这个方法可以在一开始显示的时候就设计进去，所以，桌面显示并不应该成为我们学习的重点（在系统体系完成后再详细设计规划）。 相反，计时器才是我们应该好好研究的，这是系统的根本之一。 今天内容比较少，明天继续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC11%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》闪烁和定时器\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，定时器\u003c/p\u003e","title":"【30天自制操作系】 定时器"},{"content":"Abstract: 《30天自制操作系统》FIFO Keywords: 《30天自制操作系统》，Macbook，FIFO\nFIFO 首先是FIFO缓冲区的一个通用化，这样就相当于在一个队列里面加入了很多中断处理收到的数据，CPU可以根据自己的节奏一个一个来处理，包括鼠标，键盘，定时器，以后还有可能是其他的设备，这样做的好处就是先后顺序很明确，谁先来就先处理谁，如果有多个缓冲区就无法确定哪个应该先处理，哪个应该暂时等待，而且这样设计更有利于编写程序，一举多得。 接着就是比较重要的性能测试，说实话，这个是非常重要而且想让我们的程序飞起来就必须反复去做的，作者也在这方面写了很多，但是方法很简单\n1：写程序，运行，记录时间，观察代码 2：优化代码 3：继续跑程序，记录时间，返回第二步\n什么时候感觉已经很快了，这就算结束了，但是最最最重要的是优化代码，这个一句话两句话根本说不明白，我也是个水菜鸟，所以这里就不瞎说了，大家自己看书学习吧，哈哈，反正觉得效率才是硬道理，效率和稳定，我觉得是核心种的核心，无论什么原因这两个都不能变。 具体方法，作者提到了一个链表，这个就是数据结构的应用，希望以后设计操作系统的时候能用到更多的其他数据结构来优化程序性能\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC13%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》FIFO\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，FIFO\u003c/p\u003e","title":"【30天自制操作系统】 FIFO"},{"content":"Abstract: 《30天自制操作系统》GDT和IDT介绍 Keywords: 《30天自制操作系统》，Macbook，GDT，IDT\nGDT和IDT 全局描述表（GDT ，Global Descriptor Table）：首先看到全局说明这货很重要，应该是掌握了一些，其次这个表是个数组，所以，这是一个重要的数据结构。 GDT数组中装的是段描述符【段地址，段的最大长度，访问权限】。因为这个描述符太大了，没有这么大的寄存器（64位），所以就把这个描述符放在内存里，成为了重要的GDT，intel的大叔们设计了一个寄存器GDTR（LGDT为装入此寄存器指令）用来存放GDT的入口地址，因为GDT可以设计到内存的任何位置，而GDTR可以帮助快速定位，这样段寄存器的13位就能索引到GDT了。GDT是保护模式必备的数据结构。能够很好的划分内存，控制程序的内存方位（不然会出大问题）。 中断描述符表（IDT Interrupt Descriptor Table）：之前已经见识过中断的强大了，这个描述符表是管理中断的，很明显是高手中的高手，其实他是一个8字节的描述符数组，也是一种重要的数据结构。IDT只要保存256个描述符就够了，因为最多也不会有超过256种中断，甚至IDT可以少于256，只要够用就行。同样，IDT可以放在内存的各个角落，只要你愿意，但是一定要是线性的，不然就找不到了（这是个数组，当然线性了），IDT也有专门的寄存器存放入口地址，叫IDTR，这个寄存器中含有IDT表32位的基地址（高32位）和16位的长度（限长）值（低16位）。IDT表基地址应该对齐在8字节边界上以提高处理器的访问效率。LIDT和SIDT指令分别用于加载和保存IDTR寄存器的内容。LIDT指令用于把内存中的限长值和基地址操作数加载到IDTR寄存器中。该指令仅能由当前特权级CPL是0的代码执行，通常被用于创建IDT时的操作系统初始化代码中。如果有越界访问IDT的现象会触发一个保护性异常。\n中断向量表：实模式下的前1K个字节为中断向量表，每项有四字节（2字节的段地址，2字节的偏移地址，来定位实模式下的1M空间），用来指向中断处理程序的位置，但是在保护模式下，4字节不够用，需要8字节，所以改名IDT，而且位置也不局限于开始的0x00000处，使用全新的方式完成中断处理。\nIDT中的每一项叫“门”（门描述符），很明显是传送门。。哈哈。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC5%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》GDT和IDT介绍\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，GDT，IDT\u003c/p\u003e","title":"【30天自制操作系统】 GDT和IDT"},{"content":"Abstract:《30天自制操作系统》IPL机制，BIOS介绍 Keywords: IPL，BIOS\nIPL 其实还是前五天的，现在继续说，第三天，p49页下面部分说“0x8000到0x81ff是启动区”而0x7c00到0x7dff也是启动区，这个地方困扰了我好久，不过今天好像看到类似的说法了:在linux中启动区启动后会将启动区复制到0x90000的地方，原因也没说，知识含糊的说为后面的栈操作做准备，其实无论怎么放，这只是表明，IPL功能已经工作了，成功的实现了转移，我们可以自己来控制电脑了。 在IPL的实现过程中，我发现BIOS中断的威力相当大，就像平时我们用库函数一样，各种寄存器就是这些函数的参数，可以实现硬件的各种操作，准备明天起早找点BIOS中断的资料贴一下，如果能熟练运用会有很牛的感觉，已经接近硬件了，很兴奋。。。汇编是一把利器。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC3%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e《30天自制操作系统》IPL机制，BIOS介绍\n\u003cstrong\u003eKeywords:\u003c/strong\u003e IPL，BIOS\u003c/p\u003e","title":"【30天自制操作系统】 IPL，BIOS"},{"content":"Abstract: Mac 下 《30天自制操作系统》环境工具介绍 Keywords: 《30天自制操作系统》，Macbook，环境，工具\n这几天又有点不务正业了，书也没看，一直在搞这个破环境，尝试各种做法，网上各种垃圾信息，浪费了很多时间，说的基本都是废话，不过还是找到了一些，赶紧写下来，不然这个过几天又忘了 首先是环境，我用的是Max os Maverick 64，就是10.9，硬件（有点低0.0，对于小菜的我已经完全够用啦）： 10.9刚发布，新出来的东西总问题一大堆，然后解决办法没几个。但是还是能用了，编译器NASM（xcode中command line tools带的那个）xcode真的很大，但是不可否认，安装比vs2010快多了。基本指令是dd指令，和cat指令，这两个和linux下差不多. 首先建立一个.img的镜像文件：\ndd if=xxxxx of=xxxx.img count=20000 注意，if后面的参数是个文件夹，用空的，of参数是输出的img文件 count 是大小，自己十几次就知道了 然后编写ipl 注意，后缀用.s而不用nas哦，因为.s的文件vi编辑器会自动语法高亮 然后要说一下ipl中的一句话\nRESB0x7fde-$ 这句话汇编通不过：提示这一行有非法操作符。。我也不知道咋回事。。。。。 改成： 就可以顺利的汇编成bin文件了 接着，用CAT指令把bin塞到img中\ncat ill.bin \u0026gt; xxxx.img 然后用qemu执行以下\nqemu -fda xxxx.img 结果如图： 用到的工具软件我会上传，安装比较简单，都是dmg的，双击就好。。。后天继续把c语言弄进来。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%95%AA%E5%A4%96%E7%AF%87%E4%B9%8Bmac%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e Mac 下 《30天自制操作系统》环境工具介绍\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，环境，工具\u003c/p\u003e","title":"【30天自制操作系统】 Mac环境下的工具介绍"},{"content":"Abstract: Mac 下 《30天自制操作系统》环境 Keywords: 《30天自制操作系统》，Macbook，环境\n弄了三天了，终于弄好了，先说结果，就是作者在网站上放了os x的工具（hrb.osask.jp，也有linux下的工具，可以自己去下载），也就是说我白忙活了三天。。。 再说一下这几天都干啥了，主要是想把c语言和nasm汇编连在一起。这个很多人都做过，但在网上现有的资料很少有在os X上做的的，也或者做了大家都没人说。。。。先贴代码：\nextern void swap(int *,int *); void main(){ int a=1; int b=2; swap(\u0026amp;a,\u0026amp;b); while(a==2) ; } 这是c代码，调用swap交换两个值，为了不调用标准库，我没写显示函数，而是用了一个死循环代替，如果程序停住了，说明运行成功，再贴下汇编，这是我第一次写汇编哦。。啦啦啦啦啦\nGLOBAL _swap [section .text] _swap: mov EDX,[ESP+4] mov EAX,[ESP+8] mov EBX,[EDX] mov ECX,[EAX] mov [EDX],ECX mov [EAX],EBX ret 代码很简单，但是和书上格式有些不同，作者说的他用的是nask是他自己改版的nasm所以有些关键字用不了。。。\n然后是编译成obj文件，这个很纠结，一开始不会用gcc编译出32位obj后来发现要加：\n\\-m32 就可以了。\n编译过程如下图：\n整个编译连接过程，最后光标停止，说明函数执行成功，如果nasm中写了什么中断或者什么其他系统不允许的可能会有总线错误（bus error）或者段错误（详情可以去看《c专家编程》，有相关说明）。 值得注意的是nasm -f 的参数：\nvalid output formats for -f are (\\`\\*\u0026#39; denotes default): \\* bin flat-form binary files (e.g. DOS .COM, .SYS) aout Linux a.out object files aoutb NetBSD/FreeBSD a.out object files coff COFF (i386) object files (e.g. DJGPP for DOS) elf ELF32 (i386) object files (e.g. Linux) as86 Linux as86 (bin86 version 0.3) object files obj MS-DOS 16-bit/32-bit OMF object files win32 Microsoft Win32 (i386) object files rdf Relocatable Dynamic Object File Format v2.0 ieee IEEE-695 (LADsoft variant) object file format macho NeXTstep/OpenStep/Rhapsody/Darwin/MacOS X object files 这个参数纠结了好久，最后还是看帮助搞定的，因为linux下都是elf，但是os x用elf参数最后ld会报错，说找不到xxx函数定义。。 ld的相关问题：\n1：ld: symbol(s) not found for inferred architecture i386 2：ld: symbol(s) not found for inferred architecture x86\\_64 3：ld: warning: ignoring file xxxx.o, file was built for unsupported file format 1，2和3的问题原因都是-f参数选的不对，或者gcc编译出来的是64位obj，nasm只能编译出来32或者16位目标代码。\n如果和系统可运行程序不对应，ld不会给你链接的哦。。 最后是objcopy，这个是GNU 的binutils的工具包的一部分。作用是操作二进制文件，可以任意改格式，具体参考说明，吧之前链接好的用objcopy 生成纯二进制文件后，和作者的比较发现，不一样，运行时qemu卡死，得到结论就是这两天又白忙活了。。。还好算是找到了工具，也有源代码，值得好好学习。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--mac%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e Mac 下 《30天自制操作系统》环境\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，环境\u003c/p\u003e","title":"【30天自制操作系统】 Mac环境搭建"},{"content":"Abstract: 一个半成品 Keywords: 《30天自制操作系统》，Macbook，半成品\n之前看《30天自制操作系统》，参考而成，和书中系统并不完全一致，是在原有基础上按照自己的习惯而成，由于水平和工作原因，未完成内存管理和文件系统，有兴趣者可以通过以下网址(https://github.com/TonyTan1991/Jupiter)[https://github.com/TonyTan1991/Jupiter]，欢迎留言，欢迎参与项目并改进。 因想寻求图像处理的工作，接下来将开始写作图像处理类博客，内容包括算法原理，实现，OpenCV实现等，欢迎关注\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC%E5%A5%BD%E5%A4%9A%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 一个半成品\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，半成品\u003c/p\u003e","title":"【30天自制操作系统】 一个半成品"},{"content":"Abstract: 《30天自制操作系统》中断处理程序 Keywords: 《30天自制操作系统》，Macbook，中断处理程序\n中断处理程序 今天看的还是比较顺利，很快一天的就看完了，总结一下今天的主要知识。 首先是中断处理程序，中断处理程序尽量高效，短小精悍，所以像显示啊什么的操作尽量不再中断处理程序中出现，因为一旦进入中断处理程序，其他中断将会被屏蔽，因为如果不屏蔽就会出现乱套的现象，一个中断还没完另一个已经来了，你说你干不干，干哪个，所以，尽量减少中断处理时间，这样就可以减小中断同时发生而后发生的没办法处理的现象，所以，缓冲区出现了用武之地。 之前一直不知道缓冲区是干嘛的，今天终于见到其真正的用途了，就是保存中断信息，然后慢慢处理，之后讲到FIFO缓冲区（讲的不严谨点就是队列），队列的大小随意定义，为了减少数据移动，使用了循环队列，高效，但结构里面有一个len用来判断对是否满，可以用头指针和尾指针的位置关系来判断，但没有多大效率上和空间上的区别，所以应该都可以，这里的队列采用数组而不是链表，数组在空间上连续，所以读的速度更快。效率才是硬道理！ 发现了对中断的处理和处理时对下一个中断的到来以及处理安排上是很重要的，需要我们严加设计的，应该也是系统设计的重要部分，还有书中出现的各种编号（像不同端口的地址什么的），不知道具体去哪找，不同的cpu不知道有没有什么区别，这是不是就Intel那个好几千页的手册所要说明的。。 还有就是鼠标和键盘的电路是一个芯片组上的。。。。。 这几天基本都是用c语言设计，感觉只用汇编写了那些最基本的函数，如果系统想要更强大是不是应该多用汇编写点更多的函数以供调用。\n明天继续。。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC7%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》中断处理程序\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，中断处理程序\u003c/p\u003e","title":"【30天自制操作系统】 中断"},{"content":"Abstract: 《30天自制操作系统》中断处理 Keywords: 《30天自制操作系统》，Macbook，中断\n中断处理 我感觉中断处理应该是系统底层设计的关键，因为这是在控制硬件，和给c提供接口，如果接口搞不好，一路兵败如山倒。。。 分割编译其实就是c语言初等知识，就是别写的太乱，每个文件放不同的东西，头文件加以说明和声明，以及一些宏定义或者什么的，可以去查查相关c语言的资料。 今天让我纠结了一下午的东西在p113，也就是调整栈内容，用以组合出GDTR的48位数据那个地方，很纠结，现在来说明一下： 首先是c语言函数调用时参数传递问题，先从右边开始压栈，直到所有参数完成，但是在CALL的时候还要对CS：EIP进行压栈，这是个啥东西我也不清楚，所以我们可以看到取参数都从【ESP+4】开始取的，取参数的位置是【ESP+4】,【ESP+5】\u0026hellip;.取多少位就一直加加加。还有就是超过8位以上的数据在内存中存放的规律，如果是8位以下的一个内存单元就可以了，但要是16位32位呢，这就涉及到了大端机（Big-endian）和小端机（Little-endian）的区别了，首先我们以0x12345678为例，32的数据存储：\n就是高地址存高位还是低位的区别，不是很好理解，慢慢熟悉就好了，具体机器是大端还是小端写个c程序就行了，用一个联合来测试一下，具体自己发挥，《c语言深度解剖》有相关描述，好多书都提到过。 下面是113页困扰我很久的问题的具体解析： 这就是那个GDT初始化函数调用时栈的内容，首先很纠结的就是栈底在高地址，栈是向下生长的，所以，看着很别扭，还有就是esp（栈顶指针）不是指向第一个参数的，剩下的问题就很好说了，如果要从内存0x02abf8读取两个字节，那么跟着0x2abf8后面的f9会被连带访问而不是f7。。。。。 接下来说PIC，这是个硬件，如果有点电路知识就应该没啥障碍了，是一个存储设备，cpu可以从他取数据，具体怎么去要看这个片子功能的设计和相关信号的传递方法，没有什么需要解释的，设计完全是为了便于开发。 中断程序的制作应该是核心问题，以及缓冲区的使用，明天再说。感谢收看，\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC6%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》中断处理\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，中断\u003c/p\u003e","title":"【30天自制操作系统】 中断处理"},{"content":"Abstract: 软盘，启动，保护模式的初次使用 Keywords: 《30天自制操作系统》，Macbook，软盘，启动，保护模式\n软盘？不需要！ 昨天一天看了5天内容，把觉得有些可能不好理解的写下来，内容不分先后，感觉作者写的通俗易懂，而且代码以及工具在xp下运行流畅，根本不需要软盘，直接在工具提供的虚拟机上跑就可以，下面来描述下昨天的学习心得和问题0.0。\n首先是内存，对内存的管理是至关重要的，所以我们应该先了解一下内存的具体分布： 这个图是我找的，如果有打错的地方或者有什么变动，请留言，不胜感激。 这就是内存的分配，对于小菜理解有些困难。 接下来可能就是从加电到系统启动的过程的理解了： 实模式下（为保护模式做准备），启动并加载过程如下图：至于为什么到0x07c00，原因是“两头约定”（就是设计BIOS的大伯们和设计操作系统的大叔们商量好，把启动程序放那，一后大家开发都方便）。 里面用到的汇编命令，可以去看一些汇编的书，王爽老师的那本书不错，还有《深入理解操作系统》里面的知识和这里用到的也很合拍，都是好书，可以看看。 没有写完，下一篇继续写前五天的。 相关工具和这两张图片可以在这里下载：(http://download.csdn.net/detail/tonyshengtan/6620387)[http://download.csdn.net/detail/tonyshengtan/6620387]\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC2%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 软盘，启动，保护模式的初次使用\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，软盘，启动，保护模式\u003c/p\u003e","title":"【30天自制操作系统】 保护模式"},{"content":"Abstract: 30天自制操作系统》内存管理 Keywords: 《30天自制操作系统》，Macbook，内存管理\n内存管理 对于一个系统来说，资源是最重要的，管理资源应该说就像计划你口袋里面的钱怎么花一样（不太准确。。但是重要性是相似的）。 首先是检查内存大小，BIOS应该是提供内存大小检查功能的，但是不同的BIOS查找内存大小的方式不同，所以我们应该以一种更为通用的方法进行，以获得更好的系统兼容性，而且现在的CPU都配有各种缓存，一级，二级，三级。。。先要禁止高速缓存，才能确保访问的变量全部在内存中，所以，先禁用缓存（设置CR0的某个标志位为指定值，本书还检查了一下机器是否有高速缓存，这个现在基本不用检查了，我还真就没见过386长什么样）。禁用掉高速缓存我们就要对内存挨个访问了，看看到底有没有这个位置，对0xaa55aa55的反转和比较再反转，我感觉没有什么具体意义，你可以改成任意的数字，如果访问的内存地址不存在，则返回记录到的最大的位置值，后面的程序是优化次功能程序，如果只为了检查内存的大小，完全可不用一个一个来，而是十个十个来或者一万个一万个来，只检查最后的几位就行，这个的速度会按照相应的倍数提升而准确度也会随之下降。如果要做内存健康检查就要一个一个来了。。。书中还有一个涉及到编译器的优化问题，这个会在后续的博客中详细介绍。。 接下来，检查完内存就要对内存分配进行管理了，这个应该是个很困难的问题，书中的方法有两种。 第一种：首先设置内存分配的最小单位，书中为4KB，把所有能用的字节都统计出来划分成4KB一段的好多个内存块，然后用一个字符数组来记录那些很忙，那些很闲，然后分配给需要的程序，这样的代价是每4K就需要8BIT的空间来标记这块的可用性（8/（4×1024）=1/512）所以代价是恒定的，不论多大内存N GB，都需要（N/512GB的内存来记录），这样的好处也是第二种方法的缺点就是不用做拼接，碎片问题容易解决。据说内存碎片是个很可怕的问题，尤其是对于长时间运行的服务器。。。 第二种：用一个数据结构记录所有内存的使用情况，这个数据结构内包含一个可用空间大小的记录，还有就是N个用于记录内存起始地址和可用大小的子结构数组了。。\nstruct FREEINFO { unsigned int addr, size;//起始地址，和大小  }; struct MEMMAN { int frees, maxfrees, lostsize, losts;\t//这是内存的一些记录，包括可用空间，最大可用空间，释放失败大小，释放失败的次数  struct FREEINFO free[MEMMAN_FREES]; //具体的信息。。  }; 第二种方法速度快，占用空间小，但是分配时产生的细小的内存空间没有被使用，和回收内存时的合并操作都相当麻烦。。所以有利有弊，当内存碎片过多，而系统回收算法不够强壮时有可能是灾难性的。。 至于设计系统时用什么样的内存管理方法，目前还没想好，总之，这是个很关键的问题。。会找一些资料，在后续的博文中陆续推出，欢迎收看\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC9%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 30天自制操作系统》内存管理\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，内存管理\u003c/p\u003e","title":"【30天自制操作系统】 内存管理"},{"content":"Abstract: 《30天自制操作系统》内存管理和叠加处理 Keywords: 《30天自制操作系统》，Macbook，内存管理,叠加处理\n内存管理和叠加处理 今天的代码比较多，对于写过程序的人来说应该还是比较容易看懂的。 首先说的内存管理，昨天说道把很多内存分成0x1000字节大小的块，然后分配给程序使用，但是不一定所有的程序用到的都是0x1000的整数倍字节，所以就要设计一个取整的方法，就有了书中提到的聪明做法（size+0xfff）\u0026amp;0xfffff000;这个做法或者称为算法的正确性证明很简单，带两个数试一下就知道玄机了，就是把一个12位非零二进制数向前进位，而零不进位，最后把后3位归零。我们还可以把大于x的任何数进位，小于等于x的舍去（size+0x1000-x-1）\u0026amp;0xfffff000；哈哈，应该是对的，我也没有证明。。不过x=0是和原式一样的。。举一反三山寨之本。。。 接着就到了我很疑惑的地方了。叠加处理，但是！！！注意！！！作者开始给结构体显示的分配内存了，就是图层的控制结构体，我不明白，之前写的那些结构体也没分配内存，为啥突然从这开始分配了，而作者根本没提，难道是因为昨天才写的内存管理，今天就开始用了？那之前的程序怎么办呢。我一会儿还是好好研究一下。 感觉要设计系统，就应该对这个系统的宏观结构应该完全清楚，骨架清晰，内容就可以随意发挥了。现在是骨架还没搞清楚，所以博猪没有一开始就写自己山寨版，而是想把书看完，然后找点操作系统相关的书再看看然后再做个试验品出来玩玩。。哈哈。。不知道思路正确不正确。。 至于这个叠加处理，其基本目的是控制多个图层，设计相应的数据结构。而刷新那里作者反复的实验，但其本质就是减少冗余计算，就是说有些计算能剩就不做，就像一个简单的图像卷积（好吧，博猪以前做过一段时间图像处理，听不懂的可以完全忽略。。）如果按照算法做，时间复杂度是O（x*y*size*size）（x，y是图像的大小x*y个像素，size是卷积核宽度），但经过优化可以做到只要O（x*y），而方法就是要用到前一步的计算结果，减少冗余的计算量（好吧，这是博猪第一次面试的面试题，很幸运。公司没要我0.0）。好，叠加的优化原理是哪个地方改变了，就刷新改变的区域的像素。至于实现细节，书中的代码有详细介绍，关键是确定叠加区域，然后重绘叠加区域。。。 明天继续努力。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC10%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》内存管理和叠加处理\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，内存管理,叠加处理\u003c/p\u003e","title":"【30天自制操作系统】 内存管理和叠加处理"},{"content":"Abstract: 《30天自制操作系统》计时器 Keywords: 《30天自制操作系统》，Macbook，计时器\n计时器 昨天一直在搭建环境，想在mac下实现书上制作镜像和编译连接相关文件的功能，无奈，弄了一天还是不太好用，看到另外一个微博上的童鞋用Linux完成了相关操作，想试一下，但os x上没有objcopy等命令，而且gcc好像也是改版的，而且现在被我折腾的连命令行下使用command line tools 都有问题，所以，我还是回来把笔记补上。一会儿再去弄，争取放假之前能把环境搞定，这样寒假就可以专心研制自己的系统了，而不用把时间花到细枝末节上。 昨天的内容讲的是计时器，我一开始没觉得定时器有啥功能，无非是弄个钟表出来，后来我看到了超时（timeout）恍然觉得这个东西很主要，作者并没有直接说去定时器在以后的实现中有什么作用，但是一直对中断处理程序进行优化可以看出，这个东西以后要被不停地使用，而且作者提出定时器后，马上有多弄了几个，而且最多能实现500个定时器，第十二天的主要工作就是优化处理速度，因为中断处理时所有的中断信号都被屏蔽了，所以必须快速的处理，恢复中断响应。 下面的图是作者最开始的设计： 其中条状的是定时器的执行时间，横坐标是时间，纵坐标是各定时器，不论有多少个定时器，都要循环500次（也就是最多个的情况），所以这对要求效率的工作并不是个好主意，于是开始优化。 首先可以将定时器排队，就像我们有两场球赛在不同的频道，一个十分钟后在c1频道，一个二十分钟后在c2频道，我们只要盯着那个十分钟后开始的c1频道就可以了，你可以每两分钟过来看一下，如果c1开始了，我们才会去关注c2是否开始（寝室有一哥们能同时看三场球。。。。）也就是说c1不发生，理论上来说c2绝不会发生，所以就优化出了下图： 上图横轴是时间，没有纵轴，每个小格子就是不同的定时器之间的间隔，每个定时器都是从头开始到对应的线，也就是把所有定时器从小到大排列起来，然后弄到一根轴上，这样就可以只监视下一个要超时的定时器就可以了。 而上图还是要监视500个，于是下一步的优化就是记录当前计时器的个数，减少循环，之前还有一步就是尽量减少循环中的计算，包括比较（if语句）尽量减少少数情况的判断，提高速度。 涉及到的主要问题就是速度优化，下面列举一些常用到的比较简单的循环体的优化方式：\n1：将在循环里面多次计算，但是结果不会改变的计算，移到循环外面去。\n2：减少函数调用\n3：减少内存访问\n4：减少对少数特殊情况的测试\n目前就这些，以后用到定时器会继续说。。\n ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC12%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》计时器\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，计时器\u003c/p\u003e","title":"【30天自制操作系统】 计时器"},{"content":"Abstract: 《30天自制操作系统》高分辨率 Keywords: 《30天自制操作系统》，Macbook，高分辨率\n高分辨率 高分辨率，这个是个必须要说的话题，因为好像没有啥电脑显示器用320*200的分辨率了，所以提高分辨率势在必行。。哈哈 一个重要的操作就是查询VBE 中断编号 0x10 参数ES：DI指向的地址将会被VBE的内容覆盖 参数AX存储一个返回值，如果是0x004f就是有VBE不是这个值，就意味着VBE不可用 画面模式信息：\n WORD 【ES:DI + 0x00】 :模式属性 WORD 【ES:DI + 0x12】 :x分辨率 WORD 【ES:DI + 0x14】 :y分辨率 BYTE 【ES:DI + 0x19】 :颜色数。。。必须为8 BYTE 【ES:DI + 0x1b】 :颜色的指定方法。。。必须为4（4为调色板模式） DWORD 【ES:DI + 0x28】 :VRAM的地址 ``` 然后就是现实键盘输入，移动对话框，不在赘述 ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC14%E5%A4%A9.zh/","summary":"Abstract: 《30天自制操作系统》高分辨率 Keywords: 《30天自制操作系统》，Macbook，高分辨率\n高分辨率 高分辨率，这个是个必须要说的话题，因为好像没有啥电脑显示器用320*200的分辨率了，所以提高分辨率势在必行。。哈哈 一个重要的操作就是查询VBE 中断编号 0x10 参数ES：DI指向的地址将会被VBE的内容覆盖 参数AX存储一个返回值，如果是0x004f就是有VBE不是这个值，就意味着VBE不可用 画面模式信息：\n WORD 【ES:DI + 0x00】 :模式属性 WORD 【ES:DI + 0x12】 :x分辨率 WORD 【ES:DI + 0x14】 :y分辨率 BYTE 【ES:DI + 0x19】 :颜色数。。。必须为8 BYTE 【ES:DI + 0x1b】 :颜色的指定方法。。。必须为4（4为调色板模式） DWORD 【ES:DI + 0x28】 :VRAM的地址 ``` 然后就是现实键盘输入，移动对话框，不在赘述 ","title":"【30天自制操作系统】 高分辨率"},{"content":"Abstract: 《30天自制操作系统》鼠标显示 Keywords: 《30天自制操作系统》，Macbook，鼠标\n啊哈，鼠标 今天看的有点纠结，因为竟然看困了，很少有这种情况，主要是有好多东西作者并没有做深入介绍，只是说这样是对的，至于为啥这样以后再说，这让我感觉很不爽。具体来说说。 首先，先解决了鼠标运动的问题，我感觉这个不是什么大问题，只是读取数据那需要点技巧，就是验证数据的有效性，以前用单片机发送串口数据时也用到过类似的做法，作者提出的是检查第一个字的低八位以及高两位，如果高两位是0，而低八位是8，那么数据没问题，但并不一定绝对没问题，比如发送的数据为 0x08 0x00 0x11下一组为0x08 0x08 0x01时也就是队列里面是0x08 0x00 0x11 0x08 0x08 0x01时如果红色字体丢失任意一组数据，其结果和下一组结果都是错误的，但发生的概率很小，我们也没必要要求硬件每发送一个数据都要加上校验位，那样冗余太大，也浪费资源，而且估计鼠标也不会总坏。。。 个人来讲，我不想将桌面作为系统开发的前期所应该做的（等我自己写的时候不会这么弄），一个黑漆漆的命令行而具有更强大的功能，我觉得更好，而且将桌面和系统剥离开，也就是linux那样，感觉更加自由。哈哈，这都是自己的想法，没有任何科学根据。接下来就到了让我纠结的地方了，就是作者解释了之前没解释的汇编代码：\n ; haribote-os boot asm ; TAB=4 BOTPAK\tEQU\t0x00280000\t; c语言编写部分的入口位置 DSKCAC\tEQU\t0x00100000\t; 启动区将要被复制到的地方 DSKCAC0\tEQU\t0x00008000\t; dskcaco启动区存储地址，其实里面没啥，知识为了预留，这也是之前纠结的地方 ; BOOT_INFO関係 CYLS\tEQU\t0x0ff0\t; LEDS\tEQU\t0x0ff1 VMODE\tEQU\t0x0ff2\t; SCRNX\tEQU\t0x0ff4\t; SCRNY\tEQU\t0x0ff6\t; VRAM\tEQU\t0x0ff8\t; ORG\t0xc200\t; ; 显示设置，中断号10H具体中断用法参照google MOV\tAL,0x13\t; MOV\tAH,0x00 INT\t0x10 MOV\tBYTE [VMODE],8\t; MOV\tWORD [SCRNX],320 MOV\tWORD [SCRNY],200 MOV\tDWORD [VRAM],0x000a0000 ; 设置键盘中断，中断号16H，自行百度 MOV\tAH,0x02 INT\t0x16 ; keyboard BIOS MOV\t[LEDS],AL ; ;\tPIC必须在CLI之前设置好 ;\t有些机型不能连续使用OUT指令，中间要停一下 ;\tPIC初始化 ; CLI指令后CPU屏蔽所有中断 MOV\tAL,0xff OUT\t0x21,AL NOP\t; OUT\t0xa1,AL CLI\t; ; 开启A20设置，这里的具体原理见下文。。 CALL\twaitkbdout MOV\tAL,0xd1 OUT\t0x64,AL CALL\twaitkbdout MOV\tAL,0xdf\t; enable A20 OUT\t0x60,AL CALL\twaitkbdout ; 这就是个简单设置并等待反馈的过程，书上说可以同时读取鼠标和键盘。。。我没看出来 [INSTRSET \u0026#34;i486p\u0026#34;]\t; 指令集，不知道酷睿2应该咋表示0.0 LGDT\t[GDTR0]\t; 设定GDT MOV\tEAX,CR0 AND\tEAX,0x7fffffff\t; OR\tEAX,0x00000001\t; 将EAX设置成二进制 1xxx xxxx xxxx xxxx xxxx xxxx xxxx xxx1的形式 MOV\tCR0,EAX ；CR0很高端的寄存器，只有操作系统可以使用 JMP\tpipelineflush pipelineflush: MOV\tAX,1*8\t; 1*8不知道是啥意思。。。。。 MOV\tDS,AX MOV\tES,AX MOV\tFS,AX MOV\tGS,AX MOV\tSS,AX ; bootpack MOV\tESI,bootpack\t; 复制的原始地址 MOV\tEDI,BOTPAK\t; 复制的目标地址 MOV\tECX,512*1024/4 ；DWORD是单位 CALL\tmemcpy MOV\tESI,0x7c00\t; 同上 MOV\tEDI,DSKCAC\t; 同上 MOV\tECX,512/4 CALL\tmemcpy ; 残り全部 MOV\tESI,DSKCAC0+512\t; MOV\tEDI,DSKCAC+512\t; MOV\tECX,0 MOV\tCL,BYTE [CYLS] IMUL\tECX,512*18*2/4\t; SUB\tECX,512/4\t; 真心不知道这步干啥用 CALL\tmemcpy ; bootpack开启 MOV\tEBX,BOTPAK MOV\tECX,[EBX+16] ADD\tECX,3\t; ECX += 3; SHR\tECX,2\t; ECX /= 4; JZ\tskip\t; 如果哦ECX\u0026gt;\u0026gt;2==0，则跳转 MOV\tESI,[EBX+20]\t; 还是复制。。 ADD\tESI,EBX MOV\tEDI,[EBX+12]\t; CALL\tmemcpy skip: MOV\tESP,[EBX+12]\t; 不懂在干啥。。。 JMP\tDWORD 2*8:0x0000001b waitkbdout: IN\tAL,0x64 AND\tAL,0x02 JNZ\twaitkbdout\t; 读入0x64的响应和0x02做与运算，非零则跳转 RET memcpy: MOV\tEAX,[ESI] ADD\tESI,4 MOV\t[EDI],EAX ADD\tEDI,4 SUB\tECX,1 JNZ\tmemcpy\t; 复制过程比较简单 RET ALIGNB\t16 GDT0: RESB\t8\t; DW\t0xffff,0x0000,0x9200,0x00cf\t; DW\t0xffff,0x0000,0x9a28,0x0047\t; 不知道在干嘛。。写了这么多奇怪的地址 DW\t0 GDTR0: DW\t8*3-1 ；依旧不知道8*3哈意思 DD\tGDT0 ALIGNB\t16 bootpack: 好吧，我的理解就只能到上面这个地步，不知道再继续往下看还能多领悟点不了。。\n为什么是向键盘发送指令来控制A20GATE呢？百度到的原因是为了控制是否使用超过1M以上的内存，并和16位兼容，IBM的老爷爷们使用键盘控制器剩下的一根信号线来控制A20，A20不是控制是否进入保护模式。\n原始16位处理器访问最高地址为0xFFFF：0xFFFF=0xFFFF0+0xFFFF=0x10EFFEF,很明显多于1M对于多出来的0xEFFEF的访问就需要另外的地址线。但是系统的做法是当程序员访问多于1M的地址时，CPU将地址按1M取模。。这样就不会超过1M了，这种技术被称为wrap-around 到了80286出现了一些问题，当程序员试图访问1M到0x10EFFEF时系统并没有循环回去而是直接访问，这导致了和之前产品不兼容，于是为了兼容，设计了第21根信号线就是上面提到的键盘控制器多余的这根，称为A20来控制是否访问1M以后的地址，当设置为打开时可以访问到多于1M的地址，关闭时则只能循环回0按8086的方式访问。 以上都是实模式下的，在保护模式下，CPU访问的内存增加，如果这个20位的控制线不被打开，那我们的第20号位地址将被视为无效，内存将会被切割成小碎片，系统将只能访问以基数兆的内存。。所以必须要打开A20才能完整访问。至于A20和键盘的关系。。。就是没啥关系，只不过用一个控制器。。。 一下内容来自互联网（没有验证准确性）：\n 多数PC都使用键盘控制器（8042芯片）来处理A20Gate。 从理论上讲，打开A20Gate的方法是通过设置8042芯片输出端口（64h）的2nd-bit，但事实上，当你向8042芯片输出端口进行写操作的时候，在键盘缓冲区中，或许还有别的数据尚未处理，因此你必须首先处理这些数据。 流程如下：   禁止中断；  等待，直到8042 Inputbuffer为空为止； 发送禁止键盘操作命令到8042Input buffer； 等待，直到8042 Inputbuffer为空为止； 发送读取8042 OutputPort命令； 等待，直到8042 Outputbuffer有数据为止； 读取8042 Outputbuffer，并保存得到的字节； 等待，直到8042 Inputbuffer为空为止； 发送Write 8042Output Port命令到8042 Input buffer；  等待，直到8042 Inputbuffer为空为止；  将从8042 OutputPort得到的字节的第2位置1（OR 2），然后写入8042 Input buffer；  等待，直到8042 Inputbuffer为空为止；  发送允许键盘操作命令到8042Input buffer；  打开中断。  今天就这样。。明天继续看。。。谢谢收看。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC8%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》鼠标显示\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，鼠标\u003c/p\u003e","title":"【30天自制操作系统】 鼠标"},{"content":"Abstract: Mac 下 《30天自制操作系统》工具使用 Keywords: 《30天自制操作系统》，Macbook，环境，工具\n现在来介绍官网上下的工具怎么用首先是官网地址，书上有个注释上有:hrb.osask.jp 翻译成中文大概是这个样子滴。 上面有两个文件可以下载，一个是工具，一个是工具的源代码，很好的学习资料 下面把工具复制出来 看到很多可执行文件。。感觉好舒服。。 然后把我们随便一个project复制到z_tools的同级目录下 project的内容可以修改，因为批处理可以下岗了： 然后可能是难度最大的部分出现了。。。修改makefile 如果没用过makefile可以先找点资料看看。很简单的语法，很强大的功能： 修改完成后是：\n TOOLPATH = ../z_tools/ INCPATH = ../z_tools/haribote/ ################################################### MAKE = make NASK = $(TOOLPATH)nask CC1 = $(TOOLPATH)gocc1 -I$(INCPATH) -Os -Wall -quiet GAS2NASK = $(TOOLPATH)gas2nask -a OBJ2BIM = $(TOOLPATH)obj2bim BIM2HRB = $(TOOLPATH)bim2hrb RULEFILE = $(TOOLPATH)haribote/haribote.rul EDIMG = $(TOOLPATH)edimg IMGTOL = $(TOOLPATH)imgtol.com COPY = cp DEL = rm ################################################### ## ÉfÉtÉHÉãÉgìÆçÏ default : $(MAKE) img ## ÉtÉ@ÉCÉãê∂ê¨ãKë• ipl10.bin : ipl10.nas Makefile $(NASK) ipl10.nas ipl10.bin ipl10.lst asmhead.bin : asmhead.nas Makefile $(NASK) asmhead.nas asmhead.bin asmhead.lst bootpack.gas : bootpack.c Makefile $(CC1) -o bootpack.gas bootpack.c bootpack.nas : bootpack.gas Makefile $(GAS2NASK) bootpack.gas bootpack.nas bootpack.obj : bootpack.nas Makefile $(NASK) bootpack.nas bootpack.obj bootpack.lst bootpack.bim : bootpack.obj Makefile $(OBJ2BIM) @$(RULEFILE) out:bootpack.bim stack:3136k map:bootpack.map \\ bootpack.obj ## 3MB+64KB=3136KB bootpack.hrb : bootpack.bim Makefile $(BIM2HRB) bootpack.bim bootpack.hrb 0 ################################################## haribote.sys : asmhead.bin bootpack.hrb Makefile cat asmhead.bin bootpack.hrb\u0026gt;haribote.sys ################################################### haribote.img : ipl10.bin haribote.sys Makefile $(EDIMG) imgin:../z_tools/fdimg0at.tek \\ wbinimg src:ipl10.bin len:512 from:0 to:0 \\ copy from:haribote.sys to:@: \\ imgout:haribote.img ## ÉRÉ}ÉìÉh img : $(MAKE) haribote.img ################################################# run : $(MAKE) img qemu -fda haribote.img ################################################### clean : -$(DEL) *.bin -$(DEL) *.lst -$(DEL) *.gas -$(DEL) *.obj -$(DEL) bootpack.nas -$(DEL) bootpack.map -$(DEL) bootpack.bim -$(DEL) bootpack.hrb -$(DEL) haribote.sys src_only : $(MAKE) clean -$(DEL) haribote.img 乱码是原来的日语。。华丽的忽视掉，一排#之间的就是修改过的地方， 然后就是make run了。。 结果如下： 工具准备齐全了，明天开始研究理论，博客继续更新。。。 我什么时候才能成为非主流专家呢。。哇哈哈\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--mac%E4%B8%8B%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e Mac 下 《30天自制操作系统》工具使用\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，环境，工具\u003c/p\u003e","title":"【30天自制操作系统】Mac下工具的使用"},{"content":"Abstract: 《30天自制操作系统》多任务 Keywords: 《30天自制操作系统》，Macbook，多任务\n今天的内容是多任务，所谓多任务就是“同时”执行多个任务，作者写的是在单核cpu上轮流执行多个任务，在短时间进行各种切换，但是实际上还是串联的，虽然表面上看的是同时执行，不知道现在的多核处理器是如何完成多任务的，我一会儿google一下，写出来，应该有很些不同。 先说今天的内容，对于任务切换，处理器是有代价的，也就是说，切来切去是要消耗时钟周期的，例如如果切换一次需要消耗额外的1s但是你每2s切换一次，也就是执行一个任务一共用了3s（因为任务完成后总要切换，所以就可以把切换时间算入总的执行时间）但真正的执行只是2s这样效率只有66.67%如果1s切换一次，就是有50%了。。所以几秒切一次很重要，如果10000s切换一次，效率近似与99.99%但是估计使用者会等疯，卡死了。。。。 上面的1s切换时间是我扯淡的，没有那么慢，书上说千分之一秒，估计现在的处理器会更快。 重要概念：TSS TSS：（Task Status Segment）任务状态段，用于记录任务的执行情况，和已经执行的进度，就像打游戏的读档，就可以直接玩上次存档的地方了，而不是从头接着打，而这个档就是TSS负责记录各种状态（装备等级，任务进度什么乱七八糟的）任务切换时查询TSS然后继续运行。 CPU特殊机制：执行带有段地址的命令式就会去GDT种查询一下，看是否是另外的程序，如果是，就进行任务切换。 GDT是个很重要的管理者，应该去画个图把整个体系操作系统体系画出来，这样以后设计的时候就能参考蓝图一步步设计了。。。。。。看完这本书就去画。。。。。。 提高运行速度，其实作者把修改刷新显示频率作为第一步优化，我觉得这个完全在定时器的时候就应该修改系统原始设计，因为屏幕的刷新率一般就那么几种，刷的过快没效果而且消耗资源，过慢当然也不行，既然写到这，那就算优化吧，边做边修改，也是一种好方法，没谁能一开始就制定天衣无缝的计划。。。 第二点提高速度很精彩，就是把任务切换定时器从缓冲区中分离，也就是键盘，鼠标，普通定时器等中断在FIFO的缓冲区，而任务切换的定时器不在其中，不论其中是否有未处理的中断时间一到，马上切换，但是，问题是，如果FIFO中又任务A的中断处理，但是此时任务A恰好切换到B。。这岂不是坑爹了。。。不太明白，看看查查资料有没有详细的介绍。 还有就是return不能随便用，在开发系统的时候，还有函数调用时对EFLAG寄存器的影响，都是应该注意的。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC15%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》多任务\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，多任务\u003c/p\u003e","title":"【30天自制操作系统】多任务 I"},{"content":"Abstract: 《30天自制操作系统》多任务 Keywords: 《30天自制操作系统》，Macbook，环境\n多任务 今天是多任务第二讲，比昨天的层次更高，更加接近实际使用的设计。\n首先是任务的自动化，所谓任务就是正在执行的程序，程序成千上万，可以自己随意安装，所以，管理就不能靠人工修改代码，而要自动管理，自动分配空间，自动注册相关设置，所以就要建立一个结构体来管理众多任务，记录当前任务的信息。 任务的休眠，有些任务需要等待数据，或者等待中断程序，等待的时候CPU等就会跟着闲着，这可不好，我们必须让CPU尽可能的处于运行状态，所以就有了任务的休眠，让一个等待的任务不再处于运行状态，而让那些高计算量的任务过来运行，而当任务休眠时，数据到来，马上唤醒任务，让他继续执行。就是这样的。 接下来就涉及到任务的优先级了，谁应该有更多的系统资源，比如你听音乐和写程序，一输入代码音乐就卡这可不好，分成1，2，3，4，5\u0026hellip;\u0026hellip;12各种等级，我们可以音乐12级，文本编辑5级，这样优先执行音乐，音乐不会卡，文本就算卡也没办法。。自找的。。跟我们音乐大哥抢资源是抢不过滴。。。。。 其实这并不是科学的划分，如果超过12个任务，必然有同级任务存在，那到时候怎么办呢，音乐和电影抢资源，谁应该赢呢，即使这个问题解决了，优先级运行时间划分也存在问题，如果安装1级1s（假设），2级2秒。。。。10级十秒这种划分也不是很科学的，因为2级和1级只差一级，但2级运行时间是1级的一倍，而3级和2级之间也只差一级，但却只有1.5倍所以这也是个问题。。这个姑且放一下，以后再考虑。 作者解决音乐和电影的方法是把等级再分层，类似与金字塔结构，每层有不同的任务，任务永远只在顶层切换，而不会轮到下层，知道上层都运行结束，也就是顶层消灭了，才会轮到现在的顶层，之前的第二层。而如果高层来人了，当前层当前运行任务结束后，无条件切换到上层。 都是些理论，不知道现在实际种工作的操作系统是怎么弄得，查到资料了再写出来。。。谢谢观看。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC16%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 《30天自制操作系统》多任务\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，环境\u003c/p\u003e","title":"【30天自制操作系统】多任务 II"},{"content":"Abstract: 汇编调用C语言 Keywords: 《30天自制操作系统》，Macbook，汇编调用C语言，C语言调用汇编\n汇编语言函数用c语言调用 其实我们可以把这些在完成操作系统编写时写的函数称为库函数（注意：不是标准库函数），但是c在调用库函数（普通函数），压栈的顺序是从右向左的，这个是肯定，所以，参数出栈的时候先出来的是后面的参数，明白了这个问题，就很好理解io_out()函数参数的出栈和参数的使用了，还有函数如果有返回值，保存在eax（32位返回值），64位返回值保存在 eax 和edx中，edx保存高32位，eax保存低32位。 知道了这些，参数和返回值就已经搞定了，剩下的就是定义和具体代码的实现了，什么利用中断啊，MOV给你MOV给我啊的什么就可以自由发挥了。。看起来已经很酷了。。 函数定义，搜了一下，发现定义都不同，但都是写global声明下函数名（名字前面加下划线），然后以函数名为标号写下函数体，返回用ret（但有些函数好像不能用RET返回。。具体以后再说）。 还有就是函数指针，由于博主以前研究了好长时间指针，虽然不能说特别明白，但是看这个书上的指针还是没什么障碍，像什么2【p】这种访问方式，以前也都见过，如果有人这里有疑问推荐几本书：《c专家编程》《c陷阱与缺陷》《c和指针》都有详细描述，自己写个程序试试就行了，这里不再赘述。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/30-days/30%E5%A4%A9%E8%87%AA%E5%88%B6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--%E7%AC%AC4%E5%A4%A9.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 汇编调用C语言\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 《30天自制操作系统》，Macbook，汇编调用C语言，C语言调用汇编\u003c/p\u003e","title":"【30天自制操作系统】汇编与C语言"},{"content":"Abstract: 数字图像处理：第1天 Keywords: 灰度级数\n本文最初发表于csdn，于2018年2月17日迁移至此\n结论一：对于细节较多的图像，当图像大小（N）不变的情况下，灰度级别对于感官质量相对独立； 解释：如果图像细节较多，降低灰度级别，视觉上感觉差别不大 代码编写比较随意，未进行性能优化。只为观察效果：\n#define K 1 //灰度级别 #define STEP (256/(1\u0026lt;\u0026lt;K)) int main(){ //char name[50];  IplImage * image = cvLoadImage(\u0026#34;e:\\\\OpenCV_Image\\\\airplane.jpg\u0026#34;,0); //IplImage * result = cvCreateImage(cvGetSize(image),8,1);  cvNamedWindow(\u0026#34;原图\u0026#34;); cvNamedWindow(\u0026#34;变换\u0026#34;); cvShowImage(\u0026#34;原图\u0026#34;,image); for(int i=0;i\u0026lt;image-\u0026gt;width;i++) for(int j=0;j\u0026lt;image-\u0026gt;height;j++){ int value=cvGetReal2D(image,i,j); int step=STEP; while(value\u0026gt;step){ step+=STEP; } value=step; cvSetReal2D(image,i,j,value); } cvShowImage(\u0026#34;变换\u0026#34;,image); //sprintf(name,\u0026#34;%s%d%s\u0026#34;,\u0026#34;e:\\\\OpenCV_Image\\\\airplane_\u0026#34;,K,\u0026#34;.jpg\u0026#34;);  //cvSaveImage(name,image);  cvWaitKey(); cvReleaseImage(\u0026amp;image); return 0; } 以上三张图从左到右从上到下，分别是：原始图像，K=1,2,3,4,5,6,7,8（1阶为二值化图像，8阶为原始图像灰度图）\n问题：对于同一分辨率下的图片如何评价其细节的多少。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-1-1-%E4%B8%8D%E5%90%8C%E7%81%B0%E5%BA%A6%E7%BA%A7%E7%9A%84%E5%9B%BE%E5%83%8F.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第1天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 灰度级数\u003c/p\u003e","title":"【数字图像处理】1.1:灰度级"},{"content":"Abstract: 数字图像处理：第2天 Keywords: 灰度变换，gama变换，对数，反对数变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度变换，及按照一定规则对像素点的灰度值进行变换，变换的结果可以增强对比度，或者达到其他的效果（例如二值化，或者伽马变换），由于灰度变换为针对单个像素点的灰度值进行变换，素以算法复杂度一般为O（W*H）（图像宽和高）\n#include \u0026lt;cv.h\u0026gt;#include \u0026lt;highgui.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;math.h\u0026gt;#define CONTRASTFUNC0 -1\t//翻转 #define CONTRASTFUNC1 0\t//分段 #define CONTRASTFUNC2 1\t//对数 #define CONTRASTFUNC3 2\t//反对数 #define CONTRASTFUNC4 3\t//n次幂 #define CONTRASTFUNC5 4\t//n次根 #define CONTRASTGAMA 5 //gama #define GRAYLEVEL 8 #define MAX_PVALUE (1\u0026lt;\u0026lt;GRAYLEVEL)  #define GETPIX(image,x,y) ((unsigned char) (image-\u0026gt;imageData)[(x)*image-\u0026gt;width+(y)]) #define SETPIX(image,x,y,value) (((image-\u0026gt;imageData)[(x)*image-\u0026gt;width+y])=((unsigned char)value))  unsigned char ContrastTable[MAX_PVALUE];//映射表  void ContrastStretch(IplImage *src,IplImage *dst,int method, double p0,double p1,int p2,int p3){ if(method==CONTRASTFUNC0){//图像翻转  for(int i=0;i\u0026lt;MAX_PVALUE;i++) ContrastTable[i]=MAX_PVALUE-1-i; } else if(method==CONTRASTFUNC1){//分段拉伸  for(int i=0;i\u0026lt;MAX_PVALUE;i++) ContrastTable[i]=i\u0026lt;=p0?i*p1/p0 : i\u0026lt;=p2?(i-p0)*(p3-p1)/(p2-p0)+p1: (i-p2)*(MAX_PVALUE-1-p3)/(MAX_PVALUE-1-p2)+p3; } else if(method==CONTRASTFUNC2){//对数  for(int i=0;i\u0026lt;MAX_PVALUE;i++) ContrastTable[i]=46*log(double(1+i));//46*log(256)近似于256  } else if(method==CONTRASTFUNC3){//反对数  for(int i=0;i\u0026lt;MAX_PVALUE;i++) ContrastTable[(int)(46*log(double(1+i)))]=i; for(int i=0;i\u0026lt;MAX_PVALUE;i++) if(ContrastTable[i]==0) ContrastTable[i]=ContrastTable[i-1]; } else if(method==CONTRASTFUNC4){//N次方  double coef=255/pow(255.,(double) p0);//coef为系数，即255要映射到255  for(int i=0;i\u0026lt;MAX_PVALUE;i++) ContrastTable[i]=coef*pow((double)i,(double)p0); } else if(method==CONTRASTFUNC5){//N次根  double coef=255/pow(255.,(double) p0);//coef为系数，即255要映射到255  for(int i=0;i\u0026lt;MAX_PVALUE;i++) ContrastTable[(int)(coef*pow((double)i,(double)p0))]=i; for(int i=0;i\u0026lt;MAX_PVALUE;i++) if(ContrastTable[i]==0) ContrastTable[i]=ContrastTable[i-1]; } else if(method==CONTRASTGAMA){//gama  double gama=p0; double coef=255/pow(255.,gama);//coef为系数，即255的gama次幂要映射到255  coef=(p1\u0026lt;=coef\u0026amp;\u0026amp;p1\u0026gt;0.0)?p1:coef; for(int i=0;i\u0026lt;MAX_PVALUE;i++) ContrastTable[i]=coef*pow((double)i,gama); } ///////////////////////////////重新映射/////////////////////////////////////////////  for(int i=0;i\u0026lt;256;i++) printf(\u0026#34;%d-\u0026gt;%d\\n\u0026#34;,i,ContrastTable[i]); for(int i=0;i\u0026lt;src-\u0026gt;width;i++) for(int j=0;j\u0026lt;src-\u0026gt;height;j++) SETPIX(dst,i,j,ContrastTable[GETPIX(src,i,j)]); } int main(){ IplImage * image = cvLoadImage(\u0026#34;e:\\\\OpenCV_Image\\\\lena.jpg\u0026#34;,0); IplImage * test =cvCreateImage(cvSize(512,512),image-\u0026gt;depth,1); //meanfilter(image,test,3);  ContrastStretch(image,test,0,100,0,100,255); cvNamedWindow(\u0026#34;原图\u0026#34;); cvNamedWindow(\u0026#34;变换\u0026#34;); cvShowImage(\u0026#34;原图\u0026#34;,image); cvShowImage(\u0026#34;变换\u0026#34;,test); cvSaveImage(\u0026#34;e:\\\\OpenCV_Image\\\\lena_thr100.jpg\u0026#34;,test); cvWaitKey(); cvReleaseImage(\u0026amp;image); } 灰度翻转： 分段线性变换：（50,40）（150,180） 分段线性变换（50,70）（150,130） 对数变换 反对数： 二次方： 二次根： 分段线性变换，当参数设置为（100,0）（100,255）时，函数为二值化函数： ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-1-2-%E7%81%B0%E5%BA%A6%E5%8F%98%E6%8D%A2-gama%E5%8F%98%E6%8D%A2-%E5%AF%B9%E6%95%B0-%E5%8F%8D%E5%AF%B9%E6%95%B0%E5%8F%98%E6%8D%A2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第2天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 灰度变换，gama变换，对数，反对数变换\u003c/p\u003e","title":"【数字图像处理】1.2:灰度变换，gama变换，对数，反对数变换"},{"content":"Abstract: 数字图像处理：第70天 Keywords: 彩色图像直方图\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-图像增强 直方图增强 继续简单的介绍一下彩色图像处理相关的知识，今天来简单的说下直方图增强在彩色图像中的应用，灰度图像直方图增强在此处做了相关介绍，包括其数学原理。 对于灰度图像中的一些算法适合直接用到彩色图像的各个通道，也有一些不适合，直方图均衡就属于不适合的一种，如果直接将其作用在各个通道上，将引起图像色相的变化因此也就是图像变的不是其原来的样子了，今天我们将直方图均衡用到HSI色彩空间的I分量上，直方图对亮度分量进行均衡，是图像在亮度上得到增强，在饱和度和色向上保持不变。\n算法原理 算法的原理就是利用HSI色彩空间的特点，I分量代表图像亮度，处理后不会改变图像色相。\n算法步骤：\n 从RGB转换到HSI 分离HSI空间，I分量形成一个单独的灰度图像$f_i$ 对$f_i$进行直方图均衡 用均衡后的数据代替原I分量数据 HSI转换回RGB  代码 void HistEqualRGB(RGB *src,RGB *dst,int width,int height){ HSI *temp=(HSI*)malloc(sizeof(HSI)*width*height); double *chanel_i=(double *)malloc(sizeof(double)*width*height); RGB2HSI(src, temp, width, height); for(int i=0;i\u0026lt;width*height;i++){ chanel_i[i]=(double)((int)temp[i].c3); } HistogramEqualization(chanel_i, chanel_i, width, height); for(int i=0;i\u0026lt;width*height;i++){ temp[i].c3=chanel_i[i]; } HSI2RGB(temp, dst, width, height); free(temp); free(chanel_i); } 效果分析 下面对一些图片进行上述算法操作，来观察效果。 原图： 原图I分量： 原图I分量直方图： 直方图均衡后结果： 直方图均衡后直方图： 处理后结果：  原图： 原图I分量： 原图I分量直方图： 直方图均衡后结果： 直方图均衡后直方图： 处理后结果：  原图： 原图I分量： 原图I分量直方图： 直方图均衡后结果： 直方图均衡后直方图： 处理后结果：  原图： 原图I分量： 原图I分量直方图： 直方图均衡后结果： 直方图均衡后直方图： 处理后结果： 总结 总体来说算法效果稳定，计算速度快，但这只是一种最简单的彩色图像增强的方法，因为已经决定以后做识别方向，所以彩色图像的相关更深入的彩色变换，平滑，锐化，分割等只做简单介绍，大家多多交流。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-10-1-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%A2%9E%E5%BC%BA.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第70天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 彩色图像直方图\u003c/p\u003e","title":"【数字图像处理】10.1:彩色图像-图像增强 直方图增强"},{"content":"Abstract: 数字图像处理：第71天 Keywords: 高斯滤波,均值滤波,双边滤波,中值滤波\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-图像增强 图像平滑 最近博客数量变多，访问量也一路上升，虽然目的并不是要多少访问量，但看着知识一点点的积累，而且有人认可，心情相当不错的。\n算法原理 对灰度图像进行平滑的文章： 1、均值，高斯平滑 2、双边滤波 3、中值滤波\n这些算法在彩色图像中应用有两种，一种是对RGB全部分量进行分别处理，然后将个处理过的分量进行合并，得到彩色图像。另一种是对HSI的I分量进行处理，同样能得到平滑效果，对彩色图像的平滑这里只是最简单的介绍，如果想写个美图秀秀，需要在找几篇论文来学下。\n代码 void SmoothRGB(RGB *src,RGB *dst,int width,int height,int m_width,int m_height,double param1,double param2,int Smooth_type){ double *chanel_r=(double*)malloc(sizeof(double)*width*height); double *chanel_g=(double*)malloc(sizeof(double)*width*height); double *chanel_b=(double*)malloc(sizeof(double)*width*height); double *chanel_r_dst=(double*)malloc(sizeof(double)*width*height); double *chanel_g_dst=(double*)malloc(sizeof(double)*width*height); double *chanel_b_dst=(double*)malloc(sizeof(double)*width*height); Split(src, chanel_r, chanel_g, chanel_b, width, height); switch (Smooth_type) { case SMOOTH_GAUSSIAN:{ GaussianFilter(chanel_r, chanel_r_dst, width, height, m_width, m_height, param1); GaussianFilter(chanel_g, chanel_g_dst, width, height, m_width, m_height, param1); GaussianFilter(chanel_b, chanel_b_dst, width, height, m_width, m_height, param1); break; } case SMOOTH_MEDIAN:{ MedianFilter(chanel_r, chanel_r_dst, width, height, m_width, m_height); MedianFilter(chanel_g, chanel_g_dst, width, height, m_width, m_height); MedianFilter(chanel_b, chanel_b_dst, width, height, m_width, m_height); break; } case SMOOTH_BILATERAL:{ BilateralFilter(chanel_r, chanel_r_dst, width, height, m_width, m_height, param1, param2); BilateralFilter(chanel_g, chanel_g_dst, width, height, m_width, m_height, param1, param2); BilateralFilter(chanel_b, chanel_b_dst, width, height, m_width, m_height, param1, param2); break; } case SMOOTH_MEAN:{ MeanFilter(chanel_r, chanel_r_dst, width, height, m_width, m_height); MeanFilter(chanel_g, chanel_g_dst, width, height, m_width, m_height); MeanFilter(chanel_b, chanel_b_dst, width, height, m_width, m_height); break; } default: break; } Merge(chanel_r_dst, chanel_g_dst, chanel_b_dst, dst, width, height); free(chanel_r); free(chanel_g); free(chanel_b); free(chanel_r_dst); free(chanel_g_dst); free(chanel_b_dst); } void SmoothHSI(HSI *src,HSI *dst,int width,int height,int m_width,int m_height,double param1,double param2,int Smooth_type){ double *chanel_i=(double*)malloc(sizeof(double)*width*height); double *chanel_i_dst=(double*)malloc(sizeof(double)*width*height); Split(src, NULL, NULL, chanel_i, width, height); switch (Smooth_type) { case SMOOTH_GAUSSIAN:{ GaussianFilter(chanel_i, chanel_i_dst, width, height, m_width, m_height, param1); break; } case SMOOTH_MEDIAN:{ MedianFilter(chanel_i, chanel_i_dst, width, height, m_width, m_height); break; } case SMOOTH_BILATERAL:{ BilateralFilter(chanel_i, chanel_i_dst, width, height, m_width, m_height, param1, param2); break; } case SMOOTH_MEAN:{ MeanFilter(chanel_i, chanel_i_dst, width, height, m_width, m_height); break; } default: break; } for(int i=0;i\u0026lt;width*height;i++){ dst[i].c1=src[i].c1; dst[i].c2=src[i].c2; dst[i].c3=chanel_i_dst[i]; } free(chanel_i); free(chanel_i_dst); } 处理效果 原图： RGB三通道5x5均值滤波： RGB三通道5x5高斯滤波： RGB三通道5x5双边滤波： RGB三通道5x5中值滤波： HSI色彩空间 I 通道5x5均值滤波： HSI色彩空间 I 通道5x5高斯滤波： HSI色彩空间 I 通道5x5双边滤波： HSI色彩空间 I 通道5x5中值滤波： 总结 因为是从灰度图像移植过来的算法，所以具体原理不用废话了。 待续。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-10-2-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%9B%BE%E5%83%8F%E5%B9%B3%E6%BB%91.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第71天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 高斯滤波,均值滤波,双边滤波,中值滤波\u003c/p\u003e","title":"【数字图像处理】10.2:彩色图像-图像增强 图像平滑"},{"content":"Abstract: 数字图像处理：第72天 Keywords: 拉普拉斯算子,Sobel算子,Robert算子\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-图像增强 图像锐化 今天写的博客有点多，因为这里的理论只是在前面已经详细介绍过了，所以多写点，好有时间来学习模式分类和图像特征点部分，请大家多多指教。 #算法说明 彩色图像锐化用到的算法还是灰度图像中所用的算法，在彩色图像中的应用可以直接对RGB三通道分别锐化然后合并，也可以对HSI中的I分量进行锐化，然后再得到锐化图像。 具体算法知识： 1、拉普拉斯算子 2、Sobel算子 3、Robert算子\n代码 废话不多说，直接上代码：\nvoid SharpenRGB(RGB *src,RGB *dst,int width,int height,double c,int Sharpen_type){ double *chanel_r=(double*)malloc(sizeof(double)*width*height); double *chanel_g=(double*)malloc(sizeof(double)*width*height); double *chanel_b=(double*)malloc(sizeof(double)*width*height); double *chanel_r_dst=(double*)malloc(sizeof(double)*width*height); double *chanel_g_dst=(double*)malloc(sizeof(double)*width*height); double *chanel_b_dst=(double*)malloc(sizeof(double)*width*height); Split(src, chanel_r, chanel_g, chanel_b, width, height); switch (Sharpen_type) { case SHARPEN_LAPLACE:{ LaplaceSharpen(chanel_r, chanel_r_dst, width, height, c); LaplaceSharpen(chanel_g, chanel_g_dst, width, height, c); LaplaceSharpen(chanel_b, chanel_b_dst, width, height, c); break; } case SHARPEN_SOBEL:{ SobelSharpen(chanel_r, chanel_r_dst, width, height, c); SobelSharpen(chanel_g, chanel_g_dst, width, height, c); SobelSharpen(chanel_b, chanel_b_dst, width, height, c); break; } case SHARPEN_ROBERT:{ RobertSharpen(chanel_r, chanel_r_dst, width, height, c); RobertSharpen(chanel_g, chanel_g_dst, width, height, c); RobertSharpen(chanel_b, chanel_b_dst, width, height, c); break; } default: break; } Merge(chanel_r_dst, chanel_g_dst, chanel_b_dst, dst, width, height); free(chanel_r); free(chanel_g); free(chanel_b); free(chanel_r_dst); free(chanel_g_dst); free(chanel_b_dst); } void SharpenHSI(HSI *src,HSI *dst,int width,int height,double c,int Sharpen_type){ double *chanel_i=(double*)malloc(sizeof(double)*width*height); double *chanel_i_dst=(double*)malloc(sizeof(double)*width*height); Split(src, NULL, NULL, chanel_i, width, height); switch (Sharpen_type) { case SHARPEN_LAPLACE:{ LaplaceSharpen(chanel_i, chanel_i_dst, width, height, c); break; } case SHARPEN_SOBEL:{ SobelSharpen(chanel_i, chanel_i_dst, width, height, c); break; } case SHARPEN_ROBERT:{ RobertSharpen(chanel_i, chanel_i_dst, width, height, c); break; } default: break; } for(int i=0;i\u0026lt;width*height;i++){ dst[i].c1=src[i].c1; dst[i].c2=src[i].c2; dst[i].c3=chanel_i_dst[i]; } free(chanel_i); free(chanel_i_dst); } 效果 原图： RGB三通道Laplace： RGB三通道Sobel： RGB三通道Robert： HSI中 I 通道Laplace： HSI中 I 通道Sobel： HSI中 I 通道Robert： 月球图像锐化： RGB三通道Sobel锐化： HSI I 通道图像锐化： 总结 彩色图像锐化如此，具体数学知识可以去查看以前的博文。 待续。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-10-3-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%9B%BE%E5%83%8F%E9%94%90%E5%8C%96.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第72天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 拉普拉斯算子,Sobel算子,Robert算子\u003c/p\u003e","title":"【数字图像处理】10.3:彩色图像-图像增强 图像锐化"},{"content":"Abstract: 数字图像处理：第73天 Keywords: 彩色分割\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-图像分割 彩色空间分割 本系列最后一篇，后续的文章将在其他系列中编写，本想把小波也放在这个系列，但感觉最近写小波有点困难，这里学习环境不好，所想去写点好玩的，比如模式识别，图像特征点什么的，希望大家继续关注。\n算法原理 多通道图像的简单分割，可以给定阈值向量，然后给定范围，可以是三维的球形，或者立方体，这个就要看具体的设计了，比如举个简单的例子，给定RGB中心阈值为 $\\vec T(R_0,G_0,B_0)$ ，阈值为100，那么对于像素点 $(x,y)$ 处的色彩向量 $\\vec I(R_{xy},G_{xy},B_{xy})$ 那么只要满足 $|\\vec T- \\vec I |\u0026lt;100$\n的点满足要求，为目标点，否则为背景点。 算法同样适用于其他色彩空间，但要根据具体情况来设计，所以灵活性很强。\n代码 double Chanel3Distance(RGB point1,RGB point2){ return sqrt((point1.c1-point2.c1)*(point1.c1-point2.c1)+ (point1.c2-point2.c2)*(point1.c2-point2.c2)+ (point1.c3-point2.c3)*(point1.c3-point2.c3)); } void SegmentRGB(RGB* src,RGB *dst,int width,int height,RGB * color_center,double threshold){ double distance=0.0; for(int i=0;i\u0026lt;width*height;i++){ distance=Chanel3Distance(src[i], *color_center); if(distance\u0026lt;=threshold){ dst[i].c1=src[i].c1; dst[i].c2=src[i].c2; dst[i].c3=src[i].c3; }else{ dst[i].c1=0.; dst[i].c2=0.; dst[i].c3=0.; } } } 结果分析 下面简单的分割和平滑，然后进行合并。 原图： 分割（使用简单的RGB模型，肤色分割点简单选取，所以效果不是很好）： 平滑后合并 原图： 分割结果： 总结 至此用了70几天简单的介绍了图像处理的基础知识，特定的应用因为不具有行业通用性，所以将会有选择的进行学习，感谢自己一路坚持了几个月，感谢大家一直的支持，希望大家的技术进步。 本系列结束。 技术，待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-10-4-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E5%BD%A9%E8%89%B2%E7%A9%BA%E9%97%B4%E5%88%86%E5%89%B2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第73天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 彩色分割\u003c/p\u003e","title":"【数字图像处理】10.4:彩色图像-图像分割 彩色空间分割"},{"content":"Abstract: 数字图像处理：第3天 Keywords: DFT, 离散傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n傅里叶变换是一个很大的话题，今天实现了下一维的DFT，后续将完成其他傅里叶系的算法实现和实验； DFT公式： $$ \\hat{x}[k]=\\sum_{n=0}^{N-1}e^{-i\\frac{2\\pi}{N}nk}x[n] \\qquad k = 0,1,\\ldots,N-1 $$ 其中e是自然对数的底数，i是虚数单位。通常以符号 $\\mathcal{F}$ 表示这一变换，即\n$$ \\hat{x}=\\mathcal{F}x $$\nIDFT公式： $$ x\\left[n\\right]={1 \\over N}\\sum_{k=0}^{N-1} e^{ i\\frac{2\\pi}{N}nk}\\hat{x}[k] \\qquad n = 0,1,\\ldots,N-1. $$ 记为： $$ x=\\mathcal{F}^{-1}\\hat{x} $$\nc语言代码： // // main.c // Fourer1D // // Created by Tony on 14/11/16. // Copyright (c) 2014年 Tony. All rights reserved. //  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;time.h\u0026gt;#include \u0026lt;math.h\u0026gt;#define SIZE 1000 #define VALUE_MAX 2000 struct Complex_{ double real; double imagin; }; typedef struct Complex_ Complex; void setInput(double * data,int n){ printf(\u0026#34;Setinput signal:\\n\u0026#34;); srand((int)time(0)); for(int i=0;i\u0026lt;SIZE;i++){ data[i]=rand()%VALUE_MAX; printf(\u0026#34;%lf\\n\u0026#34;,data[i]); } } void DFT(double * src,Complex * dst,int size){ clock_t start,end; start=clock(); for(int m=0;m\u0026lt;size;m++){ double real=0.0; double imagin=0.0; for(int n=0;n\u0026lt;size;n++){ double x=M_PI*2*m*n; real+=src[n]*cos(x/size); imagin+=src[n]*(-sin(x/size)); } dst[m].imagin=imagin; dst[m].real=real; if(imagin\u0026gt;=0.0) printf(\u0026#34;%lf+%lfj\\n\u0026#34;,real,imagin); else printf(\u0026#34;%lf%lfj\\n\u0026#34;,real,imagin); } end=clock(); printf(\u0026#34;DFT use time :%lf for Datasize of:%d\\n\u0026#34;,(double)(end-start)/CLOCKS_PER_SEC,size); } void IDFT(Complex *src,Complex *dst,int size){ //Complex temp[SIZE];  clock_t start,end; start=clock(); for(int m=0;m\u0026lt;size;m++){ double real=0.0; double imagin=0.0; for(int n=0;n\u0026lt;size;n++){ double x=M_PI*2*m*n/size; real+=src[n].real*cos(x)-src[n].imagin*sin(x); imagin+=src[n].real*sin(x)+src[n].imagin*cos(x); } real/=SIZE; imagin/=SIZE; if(dst!=NULL){ dst[m].real=real; dst[m].imagin=imagin; } if(imagin\u0026gt;=0.0) printf(\u0026#34;%lf+%lfj\\n\u0026#34;,real,imagin); else printf(\u0026#34;%lf%lfj\\n\u0026#34;,real,imagin); } end=clock(); printf(\u0026#34;IDFT use time :%lfs for Datasize of:%d\\n\u0026#34;,(double)(end-start)/CLOCKS_PER_SEC,size); } int main(int argc, const char * argv[]) { double input[SIZE]; Complex dst[SIZE]; setInput(input,SIZE); printf(\u0026#34;\\n\\n\u0026#34;); DFT(input, dst, SIZE); printf(\u0026#34;\\n\\n\u0026#34;); IDFT(dst, NULL, SIZE); } ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-2-1-%E4%B8%80%E7%BB%B4dft.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第3天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e DFT, 离散傅里叶变换\u003c/p\u003e","title":"【数字图像处理】2.1:一维DFT"},{"content":"Abstract: 数字图像处理：第4天 Keywords: DFT, 离散傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n傅里叶变换数学原理会在后续完整介绍，目前只实现代码，观察下结果，公式在上一篇博客中已经描述\n上代码：\n// // main.c // Fourer2D // // Created by 谭升 on 14/11/17. // Copyright (c) 2014年 谭升. All rights reserved. //  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;math.h\u0026gt;#include \u0026lt;time.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#define VALUE_MAX 255 #define WIDTH 5 #define HEIGHT 5  struct Complex_{ double real; double imagin; }; typedef struct Complex_ Complex; int Initdata(double (*src)[WIDTH],int size_w,int size_h){ srand((int)time(0)); for(int i=0;i\u0026lt;size_w;i++){ for(int j=0;j\u0026lt;size_h;j++){ src[i][j]=rand()%VALUE_MAX; printf(\u0026#34;%lf \u0026#34;,src[i][j]); } printf(\u0026#34;;\\n\u0026#34;); } return 0; } int DFT2D(double (*src)[WIDTH],Complex (*dst)[WIDTH],int size_w,int size_h){ for(int u=0;u\u0026lt;size_w;u++){ for(int v=0;v\u0026lt;size_h;v++){ double real=0.0; double imagin=0.0; for(int i=0;i\u0026lt;size_w;i++){ for(int j=0;j\u0026lt;size_h;j++){ double I=src[i][j]; double x=M_PI*2*((double)i*u/(double)size_w+(double)j*v/(double)size_h); real+=cos(x)*I; imagin+=-sin(x)*I; } } dst[u][v].real=real; dst[u][v].imagin=imagin; if(imagin\u0026gt;=0) printf(\u0026#34;%lf+%lfj \u0026#34;,real,imagin); else printf(\u0026#34;%lf%lfj \u0026#34;,real,imagin); } printf(\u0026#34;;\\n\u0026#34;); } return 0; } int IDFT2D(Complex (*src)[WIDTH],Complex (*dst)[WIDTH],int size_w,int size_h){ for(int i=0;i\u0026lt;size_w;i++){ for(int j=0;j\u0026lt;size_h;j++){ double real=0.0; double imagin=0.0; for(int u=0;u\u0026lt;size_w;u++){ for(int v=0;v\u0026lt;size_h;v++){ double R=src[u][v].real; double I=src[u][v].imagin; double x=M_PI*2*((double)i*u/(double)size_w+(double)j*v/(double)size_h); real+=R*cos(x)-I*sin(x); imagin+=I*cos(x)+R*sin(x); } } dst[i][j].real=(1./(size_w*size_h))*real; dst[i][j].imagin=(1./(size_w*size_h))*imagin; if(imagin\u0026gt;=0) printf(\u0026#34;%lf+%lfj \u0026#34;,dst[i][j].real,dst[i][j].imagin); else printf(\u0026#34;%lf%lfj \u0026#34;,dst[i][j].real,dst[i][j].imagin); } printf(\u0026#34;;\\n\u0026#34;); } return 0; } int main() { double src[WIDTH][HEIGHT]; Complex dst[WIDTH][HEIGHT]; Complex dst_[WIDTH][HEIGHT]; Initdata(src, WIDTH, HEIGHT); printf(\u0026#34;\\n\\n\u0026#34;); DFT2D(src,dst,WIDTH,HEIGHT); printf(\u0026#34;\\n\\n\u0026#34;); IDFT2D(dst,dst_,WIDTH,HEIGHT); } 由于只是为了观察结果，所以使用了固定大小的二维数组，若实际工作中应该根据需要动态分配内存。如下结果：上面为原始数据，中间为DFT后的数据，最下面为IDFT后的结果。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-2-2-%E4%BA%8C%E7%BB%B4dft.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第4天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e DFT, 离散傅里叶变换\u003c/p\u003e","title":"【数字图像处理】2.2:二维DFT"},{"content":"Abstract: 数字图像处理：第5天 Keywords: FFT,快速傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n为什么需要FFT 简单的说，为了速度。我们承认DFT很有用，但是我们发现他的速度不是很快，1D的DFT原始算法的时间复杂度是 $O(n^2)$ ，这个可以通过公式观察出来，对于2D的DFT其时间复杂度是 $O（n^4)$ ，这个速度真的很难接受，也就是说，你计算一幅1024x768的图像时，你将等大概。。。大概。。。我也没试过，反正很久。不信的自己去试试。所以找到一种快速方法的方法计算FFT势在必行。 以下为DFT公式 计算一个4点DFT。计算量如下： 如何得到FFT 通过观察DFT公式，我们发现DFT计算每一项 $X[u]$ 的时候都遍历了完整的 $x[n]$ 所以，我们的想法就是能不能通过其他的 $X[u+m]$（m为一个整数，可正可负）得到 $X[u]$ ，也就是充分利用之间的计算结构来构建现在的结果，这种方法就很容易表现成迭代的形式。本文介绍基2的FFT，及离散信号 $x[n]$ 的个数为2的k次方，即如果完整的离散信号中有N个信分量 ${x_1，x_2，x_3\u0026hellip;.x_N}$ 其中 $N=1\u0026laquo;k$。\n数学基础 FFT的数学基础其实就是：\n旋转因子具有以下性质：\n这些性质使得我们可以利用前面的计算结果来完成出后续的计算。\n最终算法 8-FFT 废话不多说，和上面一样：\n计算过程为：\n结果为：\n$2^n$-FFT 同理，推广到2^n，可以得到类似的结果，不过我们发现，为了使输出结果为顺序结果，输入的顺序经过了一系列的调整，而且每一步WN的幂次参数也是变化，我们必须得到其变化规律才能更好的编写程序：\n观察WN的变化规律为：\n节点距离（设为z）就是从WN的0次幂开始连续加到$W_N$的z次幂。然后间隔为z个式子，再次从0次幂加到z次。重复以上过程：\n例如第二级，间隔z=2，节点为实心黑色点，节点0,1，不做操作，节点$2W_8^0$,节点 $3W_8^2$ ，节点4，5不做操作。。\n其内在规律就是节点i是否乘以WN：\nif（i%z==奇数） 节点 $i*W_N^{step}$\nstep每次增加的数量由当前的所在的计算级决定\nstep=1\u0026lt;\u0026lt;（k（总级数）-i（当前级数）） 输入参数重新排序\ni行表示数组下标，蓝色数字表示实际数据：其内在规律就是，下标为偶数的将被映射到自己本身下标的 $\\frac{1}{2}$ 处，下表为奇数时被映射到数组后半段 $(size_{n/2})$ 的（下标-1）$\\frac{1}{2}$处，将排列后的数据分为前后两个部分，递归次过程，直到只有两个元素。则停止。过程如下：\n观察算法 观察至此，我们已经基本把FFT蝶形算法的所有特征都搞定了，接下来就是使用代码来实现了。\n实现代码 // // main.c // Fourer1D // // Created by Tony on 14/11/16. // Copyright (c) 2014年 Tony. All rights reserved. //  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;time.h\u0026gt;#include \u0026lt;math.h\u0026gt;//mac下M_PI在math.h中有宏定义，所以这里我们选择行的宏定义 #ifndef M_PI #define M_PI 3.14159265358979323846 #endif #define SIZE 1024*16 #define VALUE_MAX 1000  //////////////////////////////////////////////////////////////////// //定义一个复数结构体 /////////////////////////////////////////////////////////////////// struct Complex_{ double real; double imagin; }; typedef struct Complex_ Complex; //////////////////////////////////////////////////////////////////// //定义一个复数计算，包括乘法，加法，减法 /////////////////////////////////////////////////////////////////// void Add_Complex(Complex * src1,Complex *src2,Complex *dst){ dst-\u0026gt;imagin=src1-\u0026gt;imagin+src2-\u0026gt;imagin; dst-\u0026gt;real=src1-\u0026gt;real+src2-\u0026gt;real; } void Sub_Complex(Complex * src1,Complex *src2,Complex *dst){ dst-\u0026gt;imagin=src1-\u0026gt;imagin-src2-\u0026gt;imagin; dst-\u0026gt;real=src1-\u0026gt;real-src2-\u0026gt;real; } void Multy_Complex(Complex * src1,Complex *src2,Complex *dst){ double r1=0.0,r2=0.0; double i1=0.0,i2=0.0; r1=src1-\u0026gt;real; r2=src2-\u0026gt;real; i1=src1-\u0026gt;imagin; i2=src2-\u0026gt;imagin; dst-\u0026gt;imagin=r1*i2+r2*i1; dst-\u0026gt;real=r1*r2-i1*i2; } //////////////////////////////////////////////////////////////////// //在FFT中有一个WN的n次方项，在迭代中会不断用到，具体见算法说明 /////////////////////////////////////////////////////////////////// void getWN(double n,double size_n,Complex * dst){ double x=2.0*M_PI*n/size_n; dst-\u0026gt;imagin=-sin(x); dst-\u0026gt;real=cos(x); } //////////////////////////////////////////////////////////////////// //随机生成一个输入，显示数据部分已经注释掉了 //注释掉的显示部分为数据显示，可以观察结果 /////////////////////////////////////////////////////////////////// void setInput(double * data,int n){ //printf(\u0026#34;Setinput signal:\\n\u0026#34;);  srand((int)time(0)); for(int i=0;i\u0026lt;SIZE;i++){ data[i]=rand()%VALUE_MAX; //printf(\u0026#34;%lf\\n\u0026#34;,data[i]);  } } //////////////////////////////////////////////////////////////////// //定义DFT函数，其原理为简单的DFT定义，时间复杂度O（n^2）, //下面函数中有两层循环，每层循环的step为1，size为n，故为O（n*n）, //注释掉的显示部分为数据显示，可以观察结果 /////////////////////////////////////////////////////////////////// void DFT(double * src,Complex * dst,int size){ clock_t start,end; start=clock(); for(int m=0;m\u0026lt;size;m++){ double real=0.0; double imagin=0.0; for(int n=0;n\u0026lt;size;n++){ double x=M_PI*2*m*n; real+=src[n]*cos(x/size); imagin+=src[n]*(-sin(x/size)); } dst[m].imagin=imagin; dst[m].real=real; /* if(imagin\u0026gt;=0.0) printf(\u0026#34;%lf+%lfj\\n\u0026#34;,real,imagin); else printf(\u0026#34;%lf%lfj\\n\u0026#34;,real,imagin);*/ } end=clock(); printf(\u0026#34;DFT use time :%lf for Datasize of:%d\\n\u0026#34;,(double)(end-start)/CLOCKS_PER_SEC,size); } //////////////////////////////////////////////////////////////////// //定义IDFT函数，其原理为简单的IDFT定义，时间复杂度O（n^2）, //下面函数中有两层循环，每层循环的step为1，size为n，故为O（n*n）, /////////////////////////////////////////////////////////////////// void IDFT(Complex *src,Complex *dst,int size){ clock_t start,end; start=clock(); for(int m=0;m\u0026lt;size;m++){ double real=0.0; double imagin=0.0; for(int n=0;n\u0026lt;size;n++){ double x=M_PI*2*m*n/size; real+=src[n].real*cos(x)-src[n].imagin*sin(x); imagin+=src[n].real*sin(x)+src[n].imagin*cos(x); } real/=SIZE; imagin/=SIZE; if(dst!=NULL){ dst[m].real=real; dst[m].imagin=imagin; } if(imagin\u0026gt;=0.0) printf(\u0026#34;%lf+%lfj\\n\u0026#34;,real,imagin); else printf(\u0026#34;%lf%lfj\\n\u0026#34;,real,imagin); } end=clock(); printf(\u0026#34;IDFT use time :%lfs for Datasize of:%d\\n\u0026#34;,(double)(end-start)/CLOCKS_PER_SEC,size); } //////////////////////////////////////////////////////////////////// //定义FFT的初始化数据，因为FFT的数据经过重新映射，递归结构 /////////////////////////////////////////////////////////////////// int FFT_remap(double * src,int size_n){ if(size_n==1) return 0; double * temp=(double *)malloc(sizeof(double)*size_n); for(int i=0;i\u0026lt;size_n;i++) if(i%2==0) temp[i/2]=src[i]; else temp[(size_n+i)/2]=src[i]; for(int i=0;i\u0026lt;size_n;i++) src[i]=temp[i]; free(temp); FFT_remap(src, size_n/2); FFT_remap(src+size_n/2, size_n/2); return 1; } //////////////////////////////////////////////////////////////////// //定义FFT，具体见算法说明，注释掉的显示部分为数据显示，可以观察结果 /////////////////////////////////////////////////////////////////// void FFT(double * src,Complex * dst,int size_n){ FFT_remap(src, size_n); // for(int i=0;i\u0026lt;size_n;i++)  // printf(\u0026#34;%lf\\n\u0026#34;,src[i]); \tclock_t start,end; start=clock(); int k=size_n; int z=0; while (k/=2) { z++; } k=z; if(size_n!=(1\u0026lt;\u0026lt;k)) exit(0); Complex * src_com=(Complex*)malloc(sizeof(Complex)*size_n); if(src_com==NULL) exit(0); for(int i=0;i\u0026lt;size_n;i++){ src_com[i].real=src[i]; src_com[i].imagin=0; } for(int i=0;i\u0026lt;k;i++){ z=0; for(int j=0;j\u0026lt;size_n;j++){ if((j/(1\u0026lt;\u0026lt;i))%2==1){ Complex wn; getWN(z, size_n, \u0026amp;wn); Multy_Complex(\u0026amp;src_com[j], \u0026amp;wn,\u0026amp;src_com[j]); z+=1\u0026lt;\u0026lt;(k-i-1); Complex temp; int neighbour=j-(1\u0026lt;\u0026lt;(i)); temp.real=src_com[neighbour].real; temp.imagin=src_com[neighbour].imagin; Add_Complex(\u0026amp;temp, \u0026amp;src_com[j], \u0026amp;src_com[neighbour]); Sub_Complex(\u0026amp;temp, \u0026amp;src_com[j], \u0026amp;src_com[j]); } else z=0; } } /* for(int i=0;i\u0026lt;size_n;i++) if(src_com[i].imagin\u0026gt;=0.0){ printf(\u0026#34;%lf+%lfj\\n\u0026#34;,src_com[i].real,src_com[i].imagin); } else printf(\u0026#34;%lf%lfj\\n\u0026#34;,src_com[i].real,src_com[i].imagin);*/ for(int i=0;i\u0026lt;size_n;i++){ dst[i].imagin=src_com[i].imagin; dst[i].real=src_com[i].real; } end=clock(); printf(\u0026#34;FFT use time :%lfs for Datasize of:%d\\n\u0026#34;,(double)(end-start)/CLOCKS_PER_SEC,size_n); } ////////////////////////////////////////////////////////////////////  int main(int argc, const char * argv[]) { double input[SIZE]; Complex dst[SIZE]; setInput(input,SIZE); printf(\u0026#34;\\n\\n\u0026#34;); DFT(input, dst, SIZE); printf(\u0026#34;\\n\\n\u0026#34;); FFT(input, dst, SIZE); //IDFT(dst, NULL, SIZE);  getchar(); } 测试结果 其中时间都为s，FFT优势明显\n原文地址：https://www.face2ai.com/DIP-2-3-FFT算法理解与c语言的实现转载请标明出处\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-2-3-fft%E7%AE%97%E6%B3%95%E7%90%86%E8%A7%A3%E4%B8%8Ec%E8%AF%AD%E8%A8%80%E7%9A%84%E5%AE%9E%E7%8E%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第5天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e FFT,快速傅里叶变换\u003c/p\u003e","title":"【数字图像处理】2.3:FFT算法理解与c语言的实现"},{"content":"Abstract: 数字图像处理：第6天 Keywords: 二维FFT，二维IFFT，图像快速傅里叶\n本文最初发表于csdn，于2018年2月17日迁移至此\n网上关于FFT的实例有很多，具体也可以参照上一篇，其实Matlab，OpenCV都可以很轻松的实现相关操作，但是对于学习其原理，还是自己操作下比较好。 二维FFT的是实现方法是先对行做FFT将结果放回该行，然后再对列做FFT结果放在该列，计算完所有的列以后，结果就是响应的二维FFT。 本次所有操作都是对基2的数据进行的操作。 二维IFFT网上很少见到，操作过程是：上述的傅里叶变换结果，先对每列做一维IFFT，结果放在该列，取复数矩阵其共轭，然后再按照每行做一维IFFT，其结果放在该行，即为最终结果。上代码： 一维情况下：\n// 1D.c // Fourer // // Created by 谭升 on 14/11/25. // Copyright (c) 2014年 谭升. All rights reserved. //  #include \u0026#34;Fourer.h\u0026#34; int isBase2(int size_n){ int k=size_n; int z=0; while (k/=2) { z++; } k=z; if(size_n!=(1\u0026lt;\u0026lt;k)) return -1; else return k; } //////////////////////////////////////////////////////////////////// //复数基本运算 /////////////////////////////////////////////////////////////////// void Add_Complex(Complex * src1,Complex *src2,Complex *dst){ dst-\u0026gt;imagin=src1-\u0026gt;imagin+src2-\u0026gt;imagin; dst-\u0026gt;real=src1-\u0026gt;real+src2-\u0026gt;real; } void Sub_Complex(Complex * src1,Complex *src2,Complex *dst){ dst-\u0026gt;imagin=src1-\u0026gt;imagin-src2-\u0026gt;imagin; dst-\u0026gt;real=src1-\u0026gt;real-src2-\u0026gt;real; } void Multy_Complex(Complex * src1,Complex *src2,Complex *dst){ double r1=0.0,r2=0.0; double i1=0.0,i2=0.0; r1=src1-\u0026gt;real; r2=src2-\u0026gt;real; i1=src1-\u0026gt;imagin; i2=src2-\u0026gt;imagin; dst-\u0026gt;imagin=r1*i2+r2*i1; dst-\u0026gt;real=r1*r2-i1*i2; } void Copy_Complex(Complex * src,Complex *dst){ dst-\u0026gt;real=src-\u0026gt;real; dst-\u0026gt;imagin=src-\u0026gt;imagin; } void Show_Complex(Complex * src,int size_n){ if(size_n==1){ if(src-\u0026gt;imagin\u0026gt;=0.0) printf(\u0026#34;%lf+%lfj \u0026#34;,src-\u0026gt;real,src-\u0026gt;imagin); else printf(\u0026#34;%lf%lfj \u0026#34;,src-\u0026gt;real,src-\u0026gt;imagin); } else if(size_n\u0026gt;1){ for(int i=0;i\u0026lt;size_n;i++) if(src[i].imagin\u0026gt;=0.0){ printf(\u0026#34;%lf+%lfj \u0026#34;,src[i].real,src[i].imagin); } else printf(\u0026#34;%lf%lfj \u0026#34;,src[i].real,src[i].imagin); } } //////////////////////////////////////////////////////////////////// //计算WN /////////////////////////////////////////////////////////////////// void getWN(double n,double size_n,Complex * dst){ double x=2.0*M_PI*n/size_n; dst-\u0026gt;imagin=-sin(x); dst-\u0026gt;real=cos(x); } //////////////////////////////////////////////////////////////////// //随机初始化输入 /////////////////////////////////////////////////////////////////// void setInput(double * data,int n){ printf(\u0026#34;Setinput signal:\\n\u0026#34;); srand((int)time(0)); for(int i=0;i\u0026lt;n;i++){ data[i]=rand()%VALUE_MAX; } } //////////////////////////////////////////////////////////////////// //标准DFT /////////////////////////////////////////////////////////////////// void DFT(double * src,Complex * dst,int size){ for(int m=0;m\u0026lt;size;m++){ double real=0.0; double imagin=0.0; for(int n=0;n\u0026lt;size;n++){ double x=M_PI*2*m*n; real+=src[n]*cos(x/size); imagin+=src[n]*(-sin(x/size)); } dst[m].imagin=imagin; dst[m].real=real; } } //////////////////////////////////////////////////////////////////// //IDT，复原傅里叶 /////////////////////////////////////////////////////////////////// void IDFT(Complex *src,Complex *dst,int size){ for(int m=0;m\u0026lt;size;m++){ double real=0.0; double imagin=0.0; for(int n=0;n\u0026lt;size;n++){ double x=M_PI*2*m*n/size; real+=src[n].real*cos(x)-src[n].imagin*sin(x); imagin+=src[n].real*sin(x)+src[n].imagin*cos(x); } real/=SIZE; imagin/=SIZE; if(dst!=NULL){ dst[m].real=real; dst[m].imagin=imagin; } } } //////////////////////////////////////////////////////////////////// //FFT前，对输入数据重新排序 /////////////////////////////////////////////////////////////////// int FFTReal_remap(double * src,int size_n){ if(size_n==1) return 0; double * temp=(double *)malloc(sizeof(double)*size_n); for(int i=0;i\u0026lt;size_n;i++) if(i%2==0) temp[i/2]=src[i]; else temp[(size_n+i)/2]=src[i]; for(int i=0;i\u0026lt;size_n;i++) src[i]=temp[i]; free(temp); FFTReal_remap(src, size_n/2); FFTReal_remap(src+size_n/2, size_n/2); return 1; } int FFTComplex_remap(Complex * src,int size_n){ if(size_n==1) return 0; Complex * temp=(Complex *)malloc(sizeof(Complex)*size_n); for(int i=0;i\u0026lt;size_n;i++) if(i%2==0) Copy_Complex(\u0026amp;src[i],\u0026amp;(temp[i/2])); else Copy_Complex(\u0026amp;(src[i]),\u0026amp;(temp[(size_n+i)/2])); for(int i=0;i\u0026lt;size_n;i++) Copy_Complex(\u0026amp;(temp[i]),\u0026amp;(src[i])); free(temp); FFTComplex_remap(src, size_n/2); FFTComplex_remap(src+size_n/2, size_n/2); return 1; } //////////////////////////////////////////////////////////////////// //FFT公式 /////////////////////////////////////////////////////////////////// void FFT(Complex * src,Complex * dst,int size_n){ int k=size_n; int z=0; while (k/=2) { z++; } k=z; if(size_n!=(1\u0026lt;\u0026lt;k)) exit(0); Complex * src_com=(Complex*)malloc(sizeof(Complex)*size_n); if(src_com==NULL) exit(0); for(int i=0;i\u0026lt;size_n;i++){ Copy_Complex(\u0026amp;src[i], \u0026amp;src_com[i]); } FFTComplex_remap(src_com, size_n); for(int i=0;i\u0026lt;k;i++){ z=0; for(int j=0;j\u0026lt;size_n;j++){ if((j/(1\u0026lt;\u0026lt;i))%2==1){ Complex wn; getWN(z, size_n, \u0026amp;wn); Multy_Complex(\u0026amp;src_com[j], \u0026amp;wn,\u0026amp;src_com[j]); z+=1\u0026lt;\u0026lt;(k-i-1); Complex temp; int neighbour=j-(1\u0026lt;\u0026lt;(i)); temp.real=src_com[neighbour].real; temp.imagin=src_com[neighbour].imagin; Add_Complex(\u0026amp;temp, \u0026amp;src_com[j], \u0026amp;src_com[neighbour]); Sub_Complex(\u0026amp;temp, \u0026amp;src_com[j], \u0026amp;src_com[j]); } else z=0; } } for(int i=0;i\u0026lt;size_n;i++){ Copy_Complex(\u0026amp;src_com[i], \u0026amp;dst[i]); } free(src_com); } void RealFFT(double * src,Complex * dst,int size_n){ int k=size_n; int z=0; while (k/=2) { z++; } k=z; if(size_n!=(1\u0026lt;\u0026lt;k)) exit(0); Complex * src_com=(Complex*)malloc(sizeof(Complex)*size_n); if(src_com==NULL) exit(0); for(int i=0;i\u0026lt;size_n;i++){ src_com[i].real=src[i]; src_com[i].imagin=0; } FFTComplex_remap(src_com, size_n); for(int i=0;i\u0026lt;k;i++){ z=0; for(int j=0;j\u0026lt;size_n;j++){ if((j/(1\u0026lt;\u0026lt;i))%2==1){ Complex wn; getWN(z, size_n, \u0026amp;wn); Multy_Complex(\u0026amp;src_com[j], \u0026amp;wn,\u0026amp;src_com[j]); z+=1\u0026lt;\u0026lt;(k-i-1); Complex temp; int neighbour=j-(1\u0026lt;\u0026lt;(i)); temp.real=src_com[neighbour].real; temp.imagin=src_com[neighbour].imagin; Add_Complex(\u0026amp;temp, \u0026amp;src_com[j], \u0026amp;src_com[neighbour]); Sub_Complex(\u0026amp;temp, \u0026amp;src_com[j], \u0026amp;src_com[j]); } else z=0; } } for(int i=0;i\u0026lt;size_n;i++){ Copy_Complex(\u0026amp;src_com[i], \u0026amp;dst[i]); } free(src_com); } //////////////////////////////////////////////////////////////////// //IFFT实现 //////////////////////////////////////////////////////////////////// void IFFT(Complex * src,Complex * dst,int size_n){ for(int i=0;i\u0026lt;size_n;i++) src[i].imagin=-src[i].imagin; FFTComplex_remap(src, size_n); int z,k; if((z=isBase2(size_n))!=-1) k=isBase2(size_n); else exit(0); for(int i=0;i\u0026lt;k;i++){ z=0; for(int j=0;j\u0026lt;size_n;j++){ if((j/(1\u0026lt;\u0026lt;i))%2==1){ Complex wn; getWN(z, size_n, \u0026amp;wn); Multy_Complex(\u0026amp;src[j], \u0026amp;wn,\u0026amp;src[j]); z+=1\u0026lt;\u0026lt;(k-i-1); Complex temp; int neighbour=j-(1\u0026lt;\u0026lt;(i)); Copy_Complex(\u0026amp;src[neighbour], \u0026amp;temp); Add_Complex(\u0026amp;temp, \u0026amp;src[j], \u0026amp;src[neighbour]); Sub_Complex(\u0026amp;temp, \u0026amp;src[j], \u0026amp;src[j]); } else z=0; } } for(int i=0;i\u0026lt;size_n;i++){ dst[i].imagin=(1./size_n)*src[i].imagin; dst[i].real=(1./size_n)*src[i].real; } } 二维情况下：\n// // 2D.c // Fourer // // Created by 谭升 on 14/11/25. // Copyright (c) 2014年 谭升. All rights reserved. //  #include \u0026#34;Fourer.h\u0026#34;/* */ int DFT2D(double *src,Complex *dst,int size_w,int size_h){ for(int u=0;u\u0026lt;size_w;u++){ for(int v=0;v\u0026lt;size_h;v++){ double real=0.0; double imagin=0.0; for(int i=0;i\u0026lt;size_w;i++){ for(int j=0;j\u0026lt;size_h;j++){ double I=src[i*size_w+j]; double x=M_PI*2*((double)i*u/(double)size_w+(double)j*v/(double)size_h); real+=cos(x)*I; imagin+=-sin(x)*I; } } dst[u*size_w+v].real=real; dst[u*size_w+v].imagin=imagin; } } return 0; } /* */ int IDFT2D(Complex *src,Complex *dst,int size_w,int size_h){ for(int i=0;i\u0026lt;size_w;i++){ for(int j=0;j\u0026lt;size_h;j++){ double real=0.0; double imagin=0.0; for(int u=0;u\u0026lt;size_w;u++){ for(int v=0;v\u0026lt;size_h;v++){ double R=src[u*size_w+v].real; double I=src[u*size_w+v].imagin; double x=M_PI*2*((double)i*u/(double)size_w+(double)j*v/(double)size_h); real+=R*cos(x)-I*sin(x); imagin+=I*cos(x)+R*sin(x); } } dst[i*size_w+j].real=(1./(size_w*size_h))*real; dst[i*size_w+j].imagin=(1./(size_w*size_h))*imagin; } } return 0; } /* */ void ColumnVector(Complex * src,Complex * dst,int size_w,int size_h){ for(int i=0;i\u0026lt;size_h;i++) Copy_Complex(\u0026amp;src[size_w*i], \u0026amp;dst[i]); } /* */ void IColumnVector(Complex * src,Complex * dst,int size_w,int size_h){ for(int i=0;i\u0026lt;size_h;i++) Copy_Complex(\u0026amp;src[i],\u0026amp;dst[size_w*i]); } /* */ int FFT2D(double *src,Complex *dst,int size_w,int size_h){ if(isBase2(size_w)==-1||isBase2(size_h)==-1) exit(0); Complex *temp=(Complex *)malloc(sizeof(Complex)*size_h*size_w); if(temp==NULL) return -1; for(int i=0;i\u0026lt;size_h;i++){ RealFFT(\u0026amp;src[size_w*i], \u0026amp;temp[size_w*i], size_w); } Complex *Column=(Complex *)malloc(sizeof(Complex)*size_h); if(Column==NULL) return -1; for(int i=0;i\u0026lt;size_w;i++){ ColumnVector(\u0026amp;temp[i], Column, size_w, size_h); FFT(Column, Column, size_h); IColumnVector(Column, \u0026amp;temp[i], size_w, size_h); } for(int i=0;i\u0026lt;size_h*size_w;i++) Copy_Complex(\u0026amp;temp[i], \u0026amp;dst[i]); free(temp); free(Column); return 0; } /* */ int IFFT2D(Complex *src,Complex *dst,int size_w,int size_h){ if(isBase2(size_w)==-1||isBase2(size_h)==-1) exit(0); Complex *temp=(Complex *)malloc(sizeof(Complex)*size_h*size_w); if(temp==NULL) return -1; Complex *Column=(Complex *)malloc(sizeof(Complex)*size_h); if(Column==NULL) return -1; for(int i=0;i\u0026lt;size_w;i++){ ColumnVector(\u0026amp;src[i], Column, size_w, size_h); IFFT(Column, Column, size_h); IColumnVector(Column, \u0026amp;src[i], size_w, size_h); } for(int i=0;i\u0026lt;size_w*size_h;i++) src[i].imagin=-src[i].imagin; for(int i=0;i\u0026lt;size_h;i++){ IFFT(\u0026amp;src[size_w*i], \u0026amp;temp[size_w*i], size_w); } for(int i=0;i\u0026lt;size_h*size_w;i++) Copy_Complex(\u0026amp;temp[i], \u0026amp;dst[i]); free(temp); free(Column); return 0; } 结果:\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-2-4-%E4%BA%8C%E7%BB%B4fft-ifft-c%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第6天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 二维FFT，二维IFFT，图像快速傅里叶\u003c/p\u003e","title":"【数字图像处理】2.4:二维FFT,IFFT,c语言实现"},{"content":"Abstract: 数字图像处理：第7天 Keywords: 傅里叶变换，图像傅里叶谱，二维傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 今天公司的网不知怎么了，死活打不开CSDN，公司有100多架客机，也有极限速度60kb/s的网速，还有3K的工资。\n图像FFT 上篇已经介绍了关于2D FFT的相关知识，这篇只介绍在图像中的应用，对于一幅图像，做二维FFT后，即可得到其傅里叶变换，傅里叶变换后是二维复数矩阵，因为二维数组，如果是实数，是可以通过变换到0~255通过灰度图像显示出来，而变换结果是复数，所以我们通过显示其幅度，即复数的模，来显示傅里叶谱（幅度谱），不废话，上图： 原图: FFT结果: 可以看出，原图为大名鼎鼎的Lenna图，下面的为FFT后的幅度谱，其主要数值分布在四个顶点附近，图像中的位置坐标表示为（u，v），四个顶点分别表示（0，0），（0，max（v）），（max（u），0），（max（u），max（v）），至于为啥是这四个点，我也没研究明白，但是（0，0）附近是可以解释为低频分量较多，但是我们平时看到matlab的图是在图像中心的，这一步需要经过一个简单的变换，只要将原图中（坐标为表示x，y）x+y为偶数时，f（x，y）变成是其相反数，即-f（x，y），我们称之为Shift；之后可以得到中心化后的幅度谱： 此图中图像聚集在图像中心，与Matlab中结果类似，但Matlab中显示的更多，具体原因不清楚，但我感觉是Matlab 对最大值做了处理，因为最大值和最小值之间相差太大，所以拉伸后在变换到0~255，有些结果小于1，无法显示。 中心局部放大: 经过傅里叶逆变换后，再把图像Shift回来，即可得到原图，下面再描述一些变换结果： 原图 频谱 原图 频谱 原图 频谱 未中心化的频谱 #include \u0026#34;Image_FFT.h\u0026#34; /* 中心化，根据傅里叶性质的平移性质 */ void FFT_Shift(double * src,int size_w,int size_h){ for(int j=0;j\u0026lt;size_h;j++) for(int i=0;i\u0026lt;size_w;i++){ if((i+j)%2) src[j*size_w+i]=-src[j*size_w+i]; } } /* 图像快速傅里叶变换，图像大小为2的N次幂 */ void ImageFFT(IplImage * src,Complex * dst){ //FFT_Shift(src, src);  if(src-\u0026gt;depth!=IPL_DEPTH_8U) exit(0); int width=src-\u0026gt;width; int height=src-\u0026gt;height; double *image_data=(double*)malloc(sizeof(double)*width*height); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ image_data[j*width+i]=GETPIX(src, j, i); } FFT_Shift(image_data,width, height);//图像中心化  FFT2D(image_data, dst, width, height); free(image_data); } /* 将幅度值伸缩到0到255，用于频谱显示 */ void Nomalsize(double *src,double *dst,int size_w,int size_h){ double max=0.0,min=DBL_MAX; for(int i=0;i\u0026lt;size_w*size_h;i++){ max=src[i]\u0026gt;max?src[i]:max; min=src[i]\u0026lt;min?src[i]:min; } double step=255.0/(max-min); //printf(\u0026#34;%d\u0026#34;,test);  //printf(\u0026#34;max:%lf min:%lf\\n\u0026#34;,max,min);  for(int i=0;i\u0026lt;size_w*size_h;i++){ dst[i]=(src[i]-min)*step; dst[i]*=45.9*log((double)(1+dst[i])); } } /* 获得频谱 */ void getAmplitudespectrum(Complex * src,int size_w,int size_h,IplImage *dst){ double *despe=(double *)malloc(sizeof(double)*size_w*size_h); if(despe==NULL) exit(0); double real=0.0; double imagin=0.0; for(int j=0;j\u0026lt;size_h;j++) for(int i=0;i\u0026lt;size_w;i++){ real=src[j*size_w+i].real; imagin=src[j*size_w+i].imagin; despe[j*size_w+i]=sqrt(real*real+imagin*imagin); } Nomalsize(despe, despe, size_w, size_h); for(int j=0;j\u0026lt;size_h;j++) for(int i=0;i\u0026lt;size_w;i++){ cvSetReal2D(dst, j, i, despe[j*size_w+i]); } free(despe); } /* 图像傅里叶反变换 */ void ImageIFFT(Complex *src,IplImage *dst,int size_w,int size_h){ Complex *temp_c=(Complex*)malloc(sizeof(Complex)*size_w*size_h); if(temp_c==NULL) exit(0); for(int i=0;i\u0026lt;size_w*size_h;i++) Copy_Complex(\u0026amp;src[i],\u0026amp;temp_c[i]); Complex *temp=(Complex*)malloc(sizeof(Complex)*size_w*size_h); if(temp==NULL) exit(0); double *temp_d=(double *)malloc(sizeof(double)*size_w*size_h); if(temp_d==NULL) exit(0); IFFT2D(temp_c,temp,size_w,size_h); for(int j=0;j\u0026lt;size_h;j++) for(int i=0;i\u0026lt;size_w;i++){ temp_d[j*size_w+i]=temp[j*size_w+i].real; } FFT_Shift(temp_d, size_w, size_h); for(int j=0;j\u0026lt;size_h;j++) for(int i=0;i\u0026lt;size_w;i++){ cvSetReal2D(dst, j, i, temp_d[j*size_w+i]); } free(temp); free(temp_c); free(temp_d); } 下一篇继续研究傅里叶变换的应用；\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-2-5-%E5%9B%BE%E5%83%8F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2-%E5%BF%AB%E9%80%9F%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2fft.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第7天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 傅里叶变换，图像傅里叶谱，二维傅里叶变换\u003c/p\u003e","title":"【数字图像处理】2.5:图像傅里叶变换（快速傅里叶变换FFT）"},{"content":"Abstract: 数字图像处理：第8天 Keywords: 二值图像，形态学，邻域\n本文最初发表于csdn，于2018年2月17日迁移至此\n二值图像 二值图像(Binary Image)，按名字来理解只有两个值，0和1，0代表黑，1代表白，或者说0表示背景，而1表示前景。其保存也相对简单，每个像素只需要1Bit就可以完整存储信息。如果把每个像素看成随机变量，一共有N个像素，那么二值图有2的N次方种变化，而8位灰度图有255的N次方种变化，8为三通道RGB图像有 $255255255$ 的N次方种变化。也就是说同样尺寸的图像，二值图保存的信息更少。\n上图中，彩色图像和灰度图像很好的可以看出右侧的镜子，而二值图像无法看出，根据信息论的相关知识，也可以得出二值图像有很大的信息损失，上图中二值图像由灰度图经过阈值100处理后得到的结构\n邻接 首先注意邻域与邻接的不同，邻域是指某像素p的周围像素，p与他们间的欧式距离不超过“根号2”即对角线的距离。如果q在p的某种邻域中，则p，q为某种邻接\n4-邻域 即对于像素 $(x,y)$ ，其中 $(max-1\u0026gt;x\u0026gt;1,max-1\u0026gt;y\u0026gt;1)$ 。其4-邻域为以下元素：$(x-1,y)(x,y-1)(x+1,y)(x,y+1)$。\nD-邻域 即对于像素 $(x,y)$ ，其中 $(max-1\u0026gt;x\u0026gt;1,max-1\u0026gt;y\u0026gt;1)$ 。其4-邻域为以下元素： $(x-1,y-1)(x+1,y-1)(x-1,y+1)(x+1,y+1)$ 。\n8-邻域 即对于像素 $(x,y)$ ，其中 $(max-1\u0026gt;x\u0026gt;1,max-1\u0026gt;y\u0026gt;1)$ 。其8-邻域为以下元素： $(x-1,y-1)(x-1,y)(x-1,y+1)(x,y-1)(x,y+1)(x+1,y-1)(x+1,y)(x+1,y+1)$\n下面解释下邻接，以下针对二值图像：\n 4-邻接：如果q点在p点的4-邻域集合中，则p，q为4-邻接 8-邻接：如果q点在p点的8-邻域集合中，则p，q为8-邻接 D-邻接：如果q点在p点的D-邻域集合中，则p，q为D-邻接 m-邻接：如果q在p的4-邻域中，或者q在p的D-邻域中，且q的4-邻域与p的4-邻域交集为空，则p，q为m-邻接。  对于m邻接，其主要目的是为了消除8-邻接的二义性。 二义性：在某种情况下，如果邻域中存在多种通路。如下图所示：\n其中点1到点3存在两种通路，1-\u0026gt;p-\u0026gt;3，1-\u0026gt;p-\u0026gt;2-\u0026gt;3。所以点1和点3关系不明确。如果使用m邻接，点3将不会是点p的m-邻接，因为点p和点3的4-邻域存在交集点2。\n补充说明，冈萨雷斯的书中有一个集合v的概念，在二值图中 $v={1}$ ，即元素值为1的集合，这个主要是针对灰度图，多值的，比如对于255个灰度级的图像，我们对 $v={255，254}$ 的集合操作，则不是255和254的灰度被视为0，255，254被视为1。\n连通性 对于集合S存在一条通路的条件是，通路的像素的某种排列，相邻像素满足某种邻接关系。例如点p到点q之间有 $A_1,A_2,A_3\\dots A_n$ 个像素点，且相邻像素点都满足某种邻接。则p和q间存在通路。如果通路首尾相连，则称闭合通路。S集合中的一点p只存在一条通路，则称为一个连通分量，如果S只有一个连通分量，则称为一个连通集。 对于R为一个图像子集，如果R连通的，则称R为一个区域。对于所有不连接的K个区域，其并集Rk构成了图像的前景，Rk的补集称为背景。 二值图像简单的介绍如此，下节介绍形态学。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-3-0-%E4%BA%8C%E5%80%BC%E5%9B%BE%E5%83%8F.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第8天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 二值图像，形态学，邻域\u003c/p\u003e","title":"【数字图像处理】3.0:二值图像"},{"content":"Abstract: 数字图像处理：第9天 Keywords: 形态学\n本文最初发表于csdn，于2018年2月17日迁移至此\n形态学 数学形态学（Mathematical morphology）是一门建立在格论和拓扑学基础之上的图像分析学科，是数学形态学图像处理的基本理论。其基本的运算包括：二值腐蚀和膨胀 (形态学)、二值开闭运算、骨架抽取、极限腐蚀、击中击不中变换、形态学梯度、Top-hat变换、颗粒分析、流域变换、灰值腐蚀和膨胀、灰值开闭运算、灰值形态学梯度等。\n由于二值图像为离散的点集，所以我们将二维离散点定义为栅格，坐标定义为光栅坐标，光栅间的距离为采样间隔。\n形态学目的  图像预处理（去噪声，简化形状） 增强物体结构（抽取骨骼，细化，粗化，凸包，物体标记） 从背景中分隔物体 物体量化描述（面积，周长，投影，Euler-Poincare特征）  定义 形态学变换（Morphological transformation） $\\Psi$ ，由图像和另一个小点集B之间的关系定义，小点集B称为结构元素（structuring element），其中B中含有一个局部原点的定义，对与一种形态学变换，结构元素B起着决定性左右，B的内容主要包括，一些点的坐标，和局部原点坐标o。\n将形态学变换 $\\Psi(X)$ 作用于图像X就是用结构元素B系统地扫描整幅图像。当B处于X的某一位置时，B的局部原点 o 为X当前像素，其计算结果保存到输出图像中。 对偶性(duality)： 对于形态学变换 $\\Psi(X)$ 存在 $\\Psi*X$ 满足：\n$$ \\Psi(X)={\\Psi*(X^c)}^c $$\nc为补集操作 平移(translation)：点集X关于h的评议定义为： $$ Xh={p属于二维空间点集 ，p=x+h，x属于X} $$\n步骤   几何变换 映射 $\\Psi$ 称为几何变换，集合变换是从 $Z \\times Z\\dots\\times Z$ (n个)到 $Z \\times Z\\dots\\times Z$ (n个)的映射，即变换前是二值图像，变换后仍是二值图像，$\\Psi(X)$ 可以使物体边界或者其他被滤出的颗粒。\n  真实测量 测度u，将集合从从 $Z \\times Z\\dots\\times Z$ (n个)维映射到 $\\Re$ (实数)，得到的是一个数值，如体积，重量，表面积。\n  量化原则（Serra，1982）  与平移相容 与尺度缩放相容 局部知识 上部半连通 ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-3-1-%E4%BA%8C%E5%80%BC%E5%9B%BE%E5%83%8F-%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%861-%E6%95%B0%E5%AD%A6%E5%BD%A2%E6%80%81%E5%AD%A6.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第9天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 形态学\u003c/p\u003e","title":"【数字图像处理】3.1:二值图像-形态学处理 数学形态学"},{"content":"Abstract: 数字图像处理：第10天 Keywords: 形态学，腐蚀，膨胀，边缘\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 今天来介绍形态学中最基础也是最重要的两个操作，腐蚀和膨胀，腐蚀和膨胀基本上是所有形态学操作的基础，除此之外还有补集（即二值图全部取反的操作，0变1，1变0），和反射（将所有坐标去反）。\n之前使用过腐蚀和膨胀，仅仅是去噪，那时候连结构元（SE）的概念都没有，其实所有形态学操作，核心都是结构元（包括其形状和中心位置，中心位置可以不在结构元区域中），他的变化可以产生千奇百怪的效果，如果你能很好的设计结构元，那么你将能得到你想要的效果。\n对于写博客，我觉得坚持下来还是不错的，一是可以反思总结一下学习结果，很多收获都是写博客的时候想到的，虽然写起来浪费很多时间，包括作图之类的工作。二是可以作为资料，以后查看，查漏补缺。三是与别人分享知识，如果有问题，可以有人及时指正，良师益友。\n应用 腐蚀和膨胀的应用应该很多，因为其操作简单，属于基础运算，这里简单列举下用途，还是那句话，SE是关键，设计好了可以处理很多二值图像的问题。\n膨胀：\n 桥接缝隙（缝隙点为0，且宽度比SE的宽度小） 消除细小的黑点（二值图像中的0，黑点比SE小）  腐蚀：\n 消除“桥梁”（细线装的白色条纹，值为1，宽度小于SE的宽度） 消除细小的白点（二值图像中的1，白点比SE小）  此外，经过组合，腐蚀和膨胀将完成基本全部的形态学操作，例如后面介绍的，开操作，闭操作，命中与不命中，提取边缘，提取骨骼，裁剪等等。\n数学基础 数学形态学的数学基础是集合论，Milan Sonka etc.的《图像处理、分析与机器视觉》（以下简称IPAMV）中提到的一篇论文【Serra，1982】（此文在结尾附下载地址，版权归IEEE所有，请勿用于商业用途），文中给出了数学形态学的基本操作，和定义，如下：\n上图给出了最基本的腐蚀和膨胀操作，以及平移操作，由于上图中，英文较为简单，这里不再过多的解释，基本操作都是集合操作，如，交并补集操作。 首先，我在看这篇文章之前，对于腐蚀膨胀一直停留在SE划过窗口的模式，也就是，SE在图像上扫描，满足某些条件时进行某些操作，但这么做的问题在于，如果SE比窗口大，按照上述将无法操作，而且，我们无法验证膨胀的交换不变性，A膨胀B=B膨胀A，因为除非A，B大小相等，否者大的无法在小的上滑动。 而文中的标准定义是，对图像X进行移动，包括 $b_1,b_2,b_3,b_4\\dots b_n$ 即集合B中的所有移动的结构的并集，换句话说，就是集合A使用SE,B集合在膨胀，等于B的子集分别腐蚀的并集（子集的并集等于B）。 上图，做了好久的图： STEP1：首先观察左侧SE，红色为中心，和明显，SE具有各向同性，这也是一个结构元的重要特点，即各向同性与各向异性有不同的效果，也有不同的应用。\nSTEP2：SE显示，集合X需要向左移1个单位。\nSTEP3：SE显示，集合X需要向右移1个单位。\nSTEP4：SE显示，集合X需要向上移1个单位。\nSTEP5：SE显示，集合X需要向下移1个单位。\nSTEP last：将所有结果取并集，两种颜色的表示两种操作都能产生那个单元。黑色为原始集合。\n其实这种解释方法比SE滑过窗口的解释更加严谨，也更容易理解，其实SE滑过窗口的原理与着相同，只是用了类似分治的思想。但我实现起来好像两种方法的算法复杂度差不多。\n说道复杂度，由于腐蚀和膨胀属于集合操作，所以，不属于线性操作。\n腐蚀不是膨胀的逆操作，虽然有时可以经过腐蚀后膨胀来恢复原图，但他们并不是一对互逆的操作。其具有对偶性，后续介绍（因为数学公式不好输入）\n腐蚀的具体操作与膨胀类似，但有以下不同： 首先，移动方向，与膨胀不同，如果SE为： $$ \\begin{bmatrix} 0\u0026amp;1\u0026amp;0\\ 1\u0026amp;1\u0026amp;1\\ 0\u0026amp;1\u0026amp;0 \\end{bmatrix} $$ 红色为SE原点，那么绿色代表的位移不是向右移动而是向左移动，即为反方向。 上面数学基础中给出的腐蚀公式并不准确，IPAMV中给出了准确的公式，位移b前应有负号，即-b。 其次，集合操作是交集，这个很关键。\n腐蚀和膨胀的性质 这一节先空着，因为公式不好输入，后续会填上。。敬请期待。（广告：本人找工作，211本科，14年毕业，无工作经验，爱好图像处理，有人要的话随时牵走，工作地点限深圳，联系方式：下面留言）。\n代码 上代码：\n#include \u0026lt;cv.h\u0026gt;#include \u0026lt;highgui.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#define isSIZEEQU(x,y) (((x)-\u0026gt;width)==((y)-\u0026gt;width)\u0026amp;\u0026amp;((x)-\u0026gt;height)==((y)-\u0026gt;height)) typedef int DataType; struct Position_{ int x; int y; }; typedef struct Position_ Position; typedef struct Position_ MoveDirection; //位移操作，将图像整体移动，如果超出边界舍去 void Translation(IplImage *src,IplImage *dst,MoveDirection *direction){ int width=src-\u0026gt;width; int height=src-\u0026gt;height; //printf(\u0026#34;%d,%d\\n\u0026#34;,direction-\u0026gt;x,direction-\u0026gt;y);  IplImage *temp=cvCreateImage(cvSize(width, height), src-\u0026gt;depth, src-\u0026gt;nChannels); cvZero(temp); for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ if(j+direction-\u0026gt;y\u0026lt;height \u0026amp;\u0026amp; i+direction-\u0026gt;x\u0026lt;width \u0026amp;\u0026amp; j+direction-\u0026gt;y\u0026gt;=0 \u0026amp;\u0026amp; i+direction-\u0026gt;x\u0026gt;=0 ) cvSetReal2D(temp, j+direction-\u0026gt;y, i+direction-\u0026gt;x, cvGetReal2D(src, j, i)); } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); } //将小的图像弄到大的黑色图像中间，或者说是给图像加黑色边框 void Zoom(IplImage *src,IplImage *dst){ if(dst-\u0026gt;width\u0026lt;src-\u0026gt;width || dst-\u0026gt;height\u0026lt;src-\u0026gt;height || (dst-\u0026gt;height-src-\u0026gt;height)%2==1|| (dst-\u0026gt;width-src-\u0026gt;width)%2==1){ if(dst-\u0026gt;width\u0026lt;src-\u0026gt;width ) printf(\u0026#34;Zoom wrong:dst\u0026#39;s width too small!\\n\u0026#34;); if(dst-\u0026gt;height\u0026lt;src-\u0026gt;height ) printf(\u0026#34;Zoom wrong:dst\u0026#39;s height too small!\\n\u0026#34;); if((dst-\u0026gt;height-src-\u0026gt;height)%2==1||(dst-\u0026gt;width-src-\u0026gt;width)%2==1) printf(\u0026#34;Zoom wrong:dst-src not a oushu!\\n\u0026#34;); exit(0); } MoveDirection m; m.x=(dst-\u0026gt;width-src-\u0026gt;width)/2; m.y=(dst-\u0026gt;height-src-\u0026gt;height)/2; cvZero(dst); for(int i=m.x,j=0;j\u0026lt;src-\u0026gt;width;i++,j++){ for(int k=m.y,n=0;n\u0026lt;src-\u0026gt;height;k++,n++){ cvSetReal2D(dst, k, i, cvGetReal2D(src, n, j)); } } } //逻辑与操作 void And(IplImage *src0,IplImage *src1,IplImage *dst){ if(!isSIZEEQU(src0,src1)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } if(!isSIZEEQU(src0,dst)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } int width=src0-\u0026gt;width; int height=src0-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(src0, j, i)\u0026gt;100.0\u0026amp;\u0026amp; cvGetReal2D(src1, j, i)\u0026gt;100.0) cvSetReal2D(dst, j, i, 255.0); else cvSetReal2D(dst, j, i, 0.0); } } } //逻辑或操作 void Or(IplImage *src0,IplImage *src1,IplImage *dst){ if(!isSIZEEQU(src0,src1)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } if(!isSIZEEQU(src0,dst)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } int width=src0-\u0026gt;width; int height=src0-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(src0, j, i)\u0026gt;100.0|| cvGetReal2D(src1, j, i)\u0026gt;100.0) cvSetReal2D(dst, j, i, 255); } } } //将所有元素设为1 void One(IplImage *src){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++) for(int j=0;j\u0026lt;src-\u0026gt;height;j++) cvSetReal2D(src, j, i, 255.0); } //膨胀 void Dilate(IplImage *src,IplImage *dst,IplImage *se,Position *center){ if(center==NULL){ Position temp; temp.x=se-\u0026gt;width/2; temp.y=se-\u0026gt;height/2; center=\u0026amp;temp; } //printf(\u0026#34;%d,%d\u0026#34;,center-\u0026gt;x,center-\u0026gt;y);  MoveDirection m; IplImage *temp=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *tempdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *realdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); cvZero(realdst); Zoom(src,temp); int width=se-\u0026gt;width; int height=se-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(se, j, i)\u0026gt;100.0){ m.x=i-center-\u0026gt;x; m.y=j-center-\u0026gt;y; Translation(temp,tempdst, \u0026amp;m); Or(tempdst, realdst, realdst); } } } cvCopy(realdst, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;realdst); cvReleaseImage(\u0026amp;tempdst); } //腐蚀 void Erode(IplImage *src,IplImage *dst,IplImage *se,Position *center){ if(center==NULL){ Position temp; temp.x=se-\u0026gt;width/2; temp.y=se-\u0026gt;height/2; center=\u0026amp;temp; } MoveDirection m; IplImage *temp=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *tempdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *realdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); One(realdst); Zoom(src,temp); int width=se-\u0026gt;width; int height=se-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(se, j, i)\u0026gt;100.0){ m.x=center-\u0026gt;x-i; m.y=center-\u0026gt;y-j; Translation(temp,tempdst, \u0026amp;m); And(tempdst, realdst, realdst); } } } cvCopy(realdst, dst, NULL); cvReleaseImage(\u0026amp;tempdst); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;realdst); } //开操作 void Open(IplImage *src,IplImage *dst,IplImage *se,Position *center){ Erode(src, dst, se, center); Dilate(dst, dst, se, center); } //关操作 void Close(IplImage *src,IplImage *dst,IplImage *se,Position *center){ Dilate(src, dst, se, center); Erode(dst, dst, se, center); } int main(){ IplImage *se=cvLoadImage(\u0026#34;/Users/Tony/Binary_Image/mask6.jpg\u0026#34;,0); IplImage *src=cvLoadImage(\u0026#34;/Users/Tony/lena/lena_BW.jpg\u0026#34;, 0); IplImage *dst=cvCreateImage(cvGetSize(src), 8, 1); IplImage *subdst=cvCreateImage(cvGetSize(src), 8, 1); Close(src, dst, se, NULL); cvSub(dst,src,subdst, NULL); cvNamedWindow(\u0026#34;SRC\u0026#34;, 1); cvShowImage(\u0026#34;SRC\u0026#34;, src); cvNamedWindow(\u0026#34;DST\u0026#34;, 1); cvShowImage(\u0026#34;DST\u0026#34;, dst); cvNamedWindow(\u0026#34;SUB\u0026#34;, 1); cvShowImage(\u0026#34;SUB\u0026#34;, subdst); cvSaveImage(\u0026#34;/Users/Tony/Binary_Image/lena_close_sub.jpg\u0026#34;, subdst, 0); cvSaveImage(\u0026#34;/Users/Tony/Binary_Image/lena_close.jpg\u0026#34;, dst, 0); cvWaitKey(0); return 0; } 结果 膨胀：第一行SE各向同性，后两行SE各向异性 膨胀结果,结构元,与原图的差 腐蚀：第一行SE各向同性，后两行SE各向异性 腐蚀结果,结构元,原图的差 同一结构元不同中心位置的不同结果： 结构元为简单，各向同性： $$ \\begin{bmatrix} 0\u0026amp;1\u0026amp;0\\ 1\u0026amp;1\u0026amp;1\\ 0\u0026amp;1\u0026amp;0 \\end{bmatrix} $$ 实际中的应用，lena图，灰度图100为阈值后的二值图： 原图 腐蚀与原图的差,膨胀与原图的差 ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-3-2-%E4%BA%8C%E5%80%BC%E5%9B%BE%E5%83%8F-%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%862-%E8%85%90%E8%9A%80%E5%92%8C%E8%86%A8%E8%83%80.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第10天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 形态学，腐蚀，膨胀，边缘\u003c/p\u003e","title":"【数字图像处理】3.2:二值图像-形态学处理 腐蚀和膨胀"},{"content":"Abstract: 数字图像处理：第11天 Keywords: 形态学，开操作，闭操作，二值图像\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 简单来说所谓开操作和闭操作就是把腐蚀和膨胀结合起来，先腐蚀后膨胀就是开，膨胀后腐蚀就是关，至于为什么是开为什么是关，我一开始也记不住，记得老师好像也没告诉我为啥叫开，为啥叫闭，不过在下面的介绍中，会给出叫开和关的原因。\n数学原理 额，公式还没准备好。。。\n性质 开操作，一般会平滑物体轮廓，断开较窄的狭颈（细长的白色线条），所以叫开，并消除细小的突出物。 闭操作，一般也会平滑物体轮廓，但与开操作相反，弥合较窄的间断和细长的沟壑，所以叫闭，消除小的空洞，填补轮廓线的中的断裂。 上述中所有所谓的细，窄都是与结构元SE相比的，所以，关键还是SE。 幂等性（idempotent）：就是当对同一SE对图像做开（闭）操作，做一次和做多次，结果是一样的。\n代码 代码，完全是基于腐蚀和膨胀的操作，其中腐蚀和膨胀的具体实现，见上一节：\n//开操作 void Open(IplImage *src,IplImage *dst,IplImage *se,Position *center){ Erode(src, dst, se, center); Dilate(dst, dst, se, center); } //关操作 void Close(IplImage *src,IplImage *dst,IplImage *se,Position *center){ Dilate(src, dst, se, center); Erode(dst, dst, se, center); } 结果 以下所有结果的SE均为3x3的全为1，中心为中间元素，即SE为各向同性的。 开操作：左上为原图，左下为开操作结果，右上是开操作结果与原图的差，可以看出，细小的白色突起，细长的线条被处理掉了，而狭长的黑色沟壑被保留： 闭操作：左上为原图，左下为闭操作结果，右上为原图与闭操作结果的差。可以看出，原图中的黑色细条（即黑色沟壑）被填充，但白色突起点被保留。 下面对以100为阈值处理的lenna图进行操作: lena的open结果： lena的close结果 ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-3-3-%E4%BA%8C%E5%80%BC%E5%9B%BE%E5%83%8F-%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%863-%E5%BC%80%E6%93%8D%E4%BD%9C%E5%92%8C%E9%97%AD%E6%93%8D%E4%BD%9C.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第11天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 形态学，开操作，闭操作，二值图像\u003c/p\u003e","title":"【数字图像处理】3.3:二值图像-形态学处理 开操作和闭操作"},{"content":"Abstract: 数字图像处理：第12天 Keywords: 二值图像，形态学，击中，边界提取，孔洞填充，连通分量提取，凸壳，细化，骨架，形态学重建\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 其实写博客是个很痛苦的过程，要准备一些东西，还怕写错会误导别人，但是在总结和准备相关资料的时候会更深入的理解其中的一些算法原理，然后再根据自己的实际操作把过程表述出来，可以发现一些错误，也能理顺思路。 我百度找资料的时候（大牛都用google你怎么还用百度？我找不到上Google的方法![）发现，我写的前几篇博文被一个论坛转载了，但图片有些只能显示一半，也没有标明出处，对此我表示很蛋疼，首先，写出来的东西就是给别人看的，我很希望自己的博客被转载，但是，出于礼貌，应该写上原文出处，其次应该尽量把文章保持原状，不要误导别人。 开始正题，形态学基本操作：腐蚀和膨胀，进而演化出了开操作和闭操作，后来有演化出了一些其他操作，能够应用于各种不同的场景，比如最简单的边界提取，稍微复杂的孔洞填充，与空洞填充类似的连通分量提取，凸壳不知道在哪方面应用，但是细化和骨架在很多手势识别，步态识别中应用广泛，形态学重建可以算作一种很简单的图像分割，但高速有效。\n数学公式 击中： 边界提取： 孔洞填充： 连通分量提取： 凸壳： 细化： 骨架： 测地腐蚀： 测地膨胀： （2018年补充：详情可以参考刚萨雷斯数字图像处理，或等作者后续补上）\n算法介绍  击中：击中其实是一种模板匹配，如果一个结构元（模板）与图像中的一个连通区域完全相等，那么腐蚀的结果将是一个点，这就算是“击中”了，因为找到了完全一致的模式（模式的含义去查一下吧，这里的模式和模式识别的模式含义相同），但如果模板与联通区域去不完全相同，但腐蚀后的结果还是一个点，我们也称为一种“击中”，但不是严格的击中，比如我们的模板是一个直径为10的实心圆形，连通区域是一个直径为10的实心圆和一个直径为1的实心圆的并列排布，其腐蚀结果也是一个点，但其不是完美的击中，要找到完美的击中，必须在模板外面加个边框（或者是背景），这样的结果就不会出现不完美击中，我们可以选模板外一圈黑色的背景（红色背景）作为边框，与上述连通区域做击中操作，很明显。。击不中。。完美击中和不完美的击中都有很多用途，后面的操作中会继续用到。 上图，看图更明确：  红色表示背景，击中结果是只能有一个像素被点亮\n一般性的匹配，不关心背景，只寻找一个模式 2. 孔洞填充：最简单的理解，一个白色的圆环，背景为黑，想要的结果是一个白色的实心圆，这就用到了孔洞填充，孔洞填充的一个缺点在于，必须已知一个种子点，从种子点向外膨胀至整个孔洞。 下图中红色为种子点，黑色为背景 种子点和原图 填充后结果 3. 连通分量提取：与孔洞填充类似，孔洞处理的是被连通分量包围的背景，连通分量提取是提取的连通分量，而非中间的孔洞。所以从公式上看也非常相似。连通分量提取也需要种子点，所以也不是很智能。 四个连通分量，红点为示例种子点 4. 凸壳：是为了找到一个凹陷的物体外壳，以不完美命中（腐蚀）为主要操作，通过调整结构元，加上原图，得到相关结果。 原图 凸壳添加结果 添加的凸壳 细化：细化和骨架有点类似，但涉及到一个是否同伦操作，来贴一下同伦的含义，拓扑学的一个概念，是连通性的一个概念定义，如果变换不改变连通结构，则视为同伦，或者同伦树不变，以下来自百度百科对同伦的解释，wiki的数学有点复杂，不好理解：  这个解释的很形象，尤其是终结者的实例，很形象。 上图： 原图放大好多倍以后，每个方框是一个像素 结果 6. 骨架：使用形态学腐蚀减去结果的开运算的骨架与细化相比，缺少的就是同伦性，即骨架操作得到的并不是原图像的同伦变换，而且这种骨架有些地方并不是一个像素，而是多个像素，使得这种骨架算法应用不是很广泛，但骨架却应用相当广泛 7. 形态学重建：提取原始图像中包含某些特定特征的连通区域，需要一个模板和一些种子点，比如重建开操作提取文字中有长竖“I”这种结构的，先用长竖对图像进行几次腐蚀（命中），使之得到一些点，在膨胀这些点，并以原图作为模板，最终得到包含长竖的字母。\n代码 #include \u0026lt;cv.h\u0026gt; #include \u0026lt;highgui.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #define isSIZEEQU(x,y) (((x)-\u0026gt;width)==((y)-\u0026gt;width)\u0026amp;\u0026amp;((x)-\u0026gt;height)==((y)-\u0026gt;height))  typedef int DataType; struct Position_{ int x; int y; }; typedef struct Position_ Position; typedef struct Position_ MoveDirection; //位移操作，将图像整体移动，如果超出边界舍去  int isEqual(IplImage *src1,IplImage *src2){ if(!isSIZEEQU(src1, src2)) return 0; int width=src1-\u0026gt;width; int height=src1-\u0026gt;height; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ int v0=cvGetReal2D(src1, j, i); int v1=cvGetReal2D(src2, j, i); if(v0!=v1) return 0; } return 1; } //检测图像是否为空  int isEmpty(IplImage *src){ int width=src-\u0026gt;width; int height=src-\u0026gt;height; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ int v=cvGetReal2D(src, j, i); if(v!=0.0) return 0; } return 1; } void Translation(IplImage *src,IplImage *dst,MoveDirection *direction){ int width=src-\u0026gt;width; int height=src-\u0026gt;height; //printf(\u0026#34;%d,%d\\n\u0026#34;,direction-\u0026gt;x,direction-\u0026gt;y);  IplImage *temp=cvCreateImage(cvSize(width, height), src-\u0026gt;depth, src-\u0026gt;nChannels); cvZero(temp); for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ if(j+direction-\u0026gt;y\u0026lt;height \u0026amp;\u0026amp; i+direction-\u0026gt;x\u0026lt;width \u0026amp;\u0026amp; j+direction-\u0026gt;y\u0026gt;=0 \u0026amp;\u0026amp; i+direction-\u0026gt;x\u0026gt;=0 ) cvSetReal2D(temp, j+direction-\u0026gt;y, i+direction-\u0026gt;x, cvGetReal2D(src, j, i)); } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); } //将小的图像弄到大的黑色图像中间，或者说是给图像加黑色边框  void Zoom(IplImage *src,IplImage *dst){ if(dst-\u0026gt;width\u0026lt;src-\u0026gt;width || dst-\u0026gt;height\u0026lt;src-\u0026gt;height || (dst-\u0026gt;height-src-\u0026gt;height)%2==1|| (dst-\u0026gt;width-src-\u0026gt;width)%2==1){ if(dst-\u0026gt;width\u0026lt;src-\u0026gt;width ) printf(\u0026#34;Zoom wrong:dst\u0026#39;s width too small!\\n\u0026#34;); if(dst-\u0026gt;height\u0026lt;src-\u0026gt;height ) printf(\u0026#34;Zoom wrong:dst\u0026#39;s height too small!\\n\u0026#34;); if((dst-\u0026gt;height-src-\u0026gt;height)%2==1||(dst-\u0026gt;width-src-\u0026gt;width)%2==1) printf(\u0026#34;Zoom wrong:dst-src not a oushu!\\n\u0026#34;); exit(0); } MoveDirection m; m.x=(dst-\u0026gt;width-src-\u0026gt;width)/2; m.y=(dst-\u0026gt;height-src-\u0026gt;height)/2; cvZero(dst); for(int i=m.x,j=0;j\u0026lt;src-\u0026gt;width;i++,j++){ for(int k=m.y,n=0;n\u0026lt;src-\u0026gt;height;k++,n++){ cvSetReal2D(dst, k, i, cvGetReal2D(src, n, j)); } } } //逻辑与操作  int And(IplImage *src0,IplImage *src1,IplImage *dst){ int isChanged=0; if(!isSIZEEQU(src0,src1)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } if(!isSIZEEQU(src0,dst)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } int width=src0-\u0026gt;width; int height=src0-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(src0, j, i)\u0026gt;100.0\u0026amp;\u0026amp; cvGetReal2D(src1, j, i)\u0026gt;100.0) cvSetReal2D(dst, j, i, 255.0); else cvSetReal2D(dst, j, i, 0.0); } } return isChanged; } //逻辑或操作  void Or(IplImage *src0,IplImage *src1,IplImage *dst){ if(!isSIZEEQU(src0,src1)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } if(!isSIZEEQU(src0,dst)){ printf(\u0026#34;And wrong !\\n\u0026#34;); exit(0); } int width=src0-\u0026gt;width; int height=src0-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(src0, j, i)\u0026gt;100.0|| cvGetReal2D(src1, j, i)\u0026gt;100.0) cvSetReal2D(dst, j, i, 255); } } } //取反  void Not(IplImage *src,IplImage *dst){ if(!isSIZEEQU(src,dst)){ printf(\u0026#34;Not wrong !\\n\u0026#34;); exit(0); } int width=src-\u0026gt;width; int height=src-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ cvSetReal2D(dst, j, i, 255.0-cvGetReal2D(src, j, i)); } } } //将所有元素设为1  void One(IplImage *src){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++) for(int j=0;j\u0026lt;src-\u0026gt;height;j++) cvSetReal2D(src, j, i, 255.0); } //膨胀  void Dilate(IplImage *src,IplImage *dst,IplImage *se,Position *center){ if(center==NULL){ Position temp; temp.x=se-\u0026gt;width/2; temp.y=se-\u0026gt;height/2; center=\u0026amp;temp; } //printf(\u0026#34;%d,%d\u0026#34;,center-\u0026gt;x,center-\u0026gt;y);  MoveDirection m; IplImage *temp=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *tempdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *realdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); cvZero(realdst); Zoom(src,temp); int width=se-\u0026gt;width; int height=se-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(se, j, i)\u0026gt;100.0){ m.x=i-center-\u0026gt;x; m.y=j-center-\u0026gt;y; Translation(temp,tempdst, \u0026amp;m); Or(tempdst, realdst, realdst); } } } cvCopy(realdst, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;realdst); cvReleaseImage(\u0026amp;tempdst); } //腐蚀  void Erode(IplImage *src,IplImage *dst,IplImage *se,Position *center){ if(center==NULL){ Position temp; temp.x=se-\u0026gt;width/2; temp.y=se-\u0026gt;height/2; center=\u0026amp;temp; } MoveDirection m; IplImage *temp=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *tempdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); IplImage *realdst=cvCreateImage(cvGetSize(dst), dst-\u0026gt;depth,dst-\u0026gt;nChannels); One(realdst); Zoom(src,temp); int width=se-\u0026gt;width; int height=se-\u0026gt;height; for(int i=0;i\u0026lt;width;i++){ for(int j=0;j\u0026lt;height;j++){ if(cvGetReal2D(se, j, i)\u0026gt;100.0){ m.x=center-\u0026gt;x-i; m.y=center-\u0026gt;y-j; Translation(temp,tempdst, \u0026amp;m); And(tempdst, realdst, realdst); } } } cvCopy(realdst, dst, NULL); cvReleaseImage(\u0026amp;tempdst); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;realdst); } //开操作  void Open(IplImage *src,IplImage *dst,IplImage *se,Position *center){ Erode(src, dst, se, center); Dilate(dst, dst, se, center); } //关操作  void Close(IplImage *src,IplImage *dst,IplImage *se,Position *center){ Dilate(src, dst, se, center); Erode(dst, dst, se, center); } //击中与击不中  void HitorMiss(IplImage *src,IplImage *se1,IplImage *se2,IplImage *dst,Position *se1center,Position *se2center){ IplImage *temp1=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp2=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Erode(src, temp1, se1, se1center); Not(src, temp2); Erode(temp2, temp2, se2, se2center); And(temp1, temp2, dst); cvReleaseImage(\u0026amp;temp1); cvReleaseImage(\u0026amp;temp2); } //二值图像，边缘检测  void BinaryEdge(IplImage *src,IplImage* dst){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Erode(src, temp, NULL, NULL); cvSub(src, temp, dst, NULL); cvReleaseImage(\u0026amp;temp); } //孔洞填充  void FillHole(IplImage *src,IplImage *dst,IplImage *se,Position *seed){ IplImage * temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); cvZero(temp); IplImage * lasttemp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage * nsrc=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Not(src, nsrc); cvSetReal2D(temp, seed-\u0026gt;y, seed-\u0026gt;x, 255.0); while(!isEqual(lasttemp, temp)){ cvCopy(temp, lasttemp, NULL); Dilate(temp, temp, se, NULL); And(temp, nsrc, temp); } Or(temp, src, dst); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;lasttemp); cvReleaseImage(\u0026amp;nsrc); } //连通分量获取  void GetConComponent(IplImage *src,IplImage *dst,IplImage *se,Position *seed){ IplImage * temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); cvZero(temp); IplImage * lasttemp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); cvSetReal2D(temp, seed-\u0026gt;y, seed-\u0026gt;x, 255.0); while(!isEqual(lasttemp, temp)){ cvCopy(temp, lasttemp, NULL); Dilate(temp, temp, se, NULL); And(temp, src, temp); } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;lasttemp); } //骨架  void FrameWork(IplImage *src,IplImage *dst,IplImage *se){ cvZero(dst); IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth,src-\u0026gt;nChannels); IplImage *temp_open=cvCreateImage(cvGetSize(src), src-\u0026gt;depth,src-\u0026gt;nChannels); cvCopy(src, temp, NULL); while(!isEmpty(temp)){ Erode(temp, temp, se, NULL); cvCopy(temp, temp_open, NULL); Open(temp_open, temp_open, se, NULL); cvSub(temp, temp_open, temp_open,NULL); Or(temp_open, dst, dst); } cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_open); } //凸壳生成结构元  IplImage* CreateConvexhullSE(int num){ IplImage *se=cvCreateImage(cvSize(3, 3), 8, 1); cvZero(se); switch (num) { case 0: { cvSetReal2D(se, 0, 0, 255.0); cvSetReal2D(se, 1, 0, 255.0); cvSetReal2D(se, 2, 0, 255.0); } break; case 1: { cvSetReal2D(se, 0, 0, 255.0); cvSetReal2D(se, 0, 1, 255.0); cvSetReal2D(se, 0, 2, 255.0); } break; case 2: { cvSetReal2D(se, 0, 2, 255.0); cvSetReal2D(se, 1, 2, 255.0); cvSetReal2D(se, 2, 2, 255.0); } break; case 3: { cvSetReal2D(se, 2, 0, 255.0); cvSetReal2D(se, 2, 1, 255.0); cvSetReal2D(se, 2, 2, 255.0); } break; default: break; } return se; } //凸壳  void Convexhull(IplImage *src,IplImage *dst){ cvCopy(src, dst, NULL); IplImage * se[4]; IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); //cvCopy(src, temp, NULL);  for(int i=0;i\u0026lt;4;i++){ cvCopy(src, temp, NULL); se[i]=CreateConvexhullSE(i); while (!isEqual(temp, temp_last)) { cvCopy(temp, temp_last, NULL); Erode(temp, temp, se[i],NULL); Or(temp, dst, temp); } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;se[i]); } cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } //生成细化结构元  IplImage* CreateThinningSE(int num){ IplImage *se=cvCreateImage(cvSize(3, 3), 8, 1); cvZero(se); switch (num) { case 0: { cvSetReal2D(se, 2, 0, 255.0); cvSetReal2D(se, 2, 1, 255.0); cvSetReal2D(se, 2, 2, 255.0); cvSetReal2D(se, 1, 1, 255.0); } break; case 1: { cvSetReal2D(se, 1, 1, 255.0); cvSetReal2D(se, 1, 0, 255.0); cvSetReal2D(se, 2, 0, 255.0); cvSetReal2D(se, 2, 1, 255.0); } break; case 2: { cvSetReal2D(se, 1, 1, 255.0); cvSetReal2D(se, 1, 0, 255.0); cvSetReal2D(se, 2, 0, 255.0); cvSetReal2D(se, 0, 0, 255.0); } break; case 3: { cvSetReal2D(se, 1, 1, 255.0); cvSetReal2D(se, 0, 0, 255.0); cvSetReal2D(se, 0, 1, 255.0); cvSetReal2D(se, 1, 0, 255.0); } break; case 4: { cvSetReal2D(se, 1, 1, 255.0); cvSetReal2D(se, 0, 0, 255.0); cvSetReal2D(se, 0, 1, 255.0); cvSetReal2D(se, 0, 2, 255.0); } break; case 5: { cvSetReal2D(se, 0, 1, 255.0); cvSetReal2D(se, 0, 2, 255.0); cvSetReal2D(se, 1, 1, 255.0); cvSetReal2D(se, 1, 2, 255.0); } break; case 6: { cvSetReal2D(se, 1, 1, 255.0); cvSetReal2D(se, 0, 2, 255.0); cvSetReal2D(se, 1, 2, 255.0); cvSetReal2D(se, 2, 2, 255.0); } break; case 7: { cvSetReal2D(se, 1, 1, 255.0); cvSetReal2D(se, 1, 2, 255.0); cvSetReal2D(se, 2, 1, 255.0); cvSetReal2D(se, 2, 2, 255.0); } break; default: break; } return se; } IplImage* CreateThinningUSE(int num){ IplImage *se=cvCreateImage(cvSize(3, 3), 8, 1); cvZero(se); switch (num) { case 0: { cvSetReal2D(se, 0, 1, 255.0); cvSetReal2D(se, 0, 2, 255.0); cvSetReal2D(se, 0, 0, 255.0); } break; case 1: { cvSetReal2D(se, 0, 1, 255.0); cvSetReal2D(se, 0, 2, 255.0); cvSetReal2D(se, 1, 2, 255.0); } break; case 2: { cvSetReal2D(se, 0, 2, 255.0); cvSetReal2D(se, 1, 2, 255.0); cvSetReal2D(se, 2, 2, 255.0); } break; case 3: { cvSetReal2D(se, 1, 2, 255.0); cvSetReal2D(se, 2, 1, 255.0); cvSetReal2D(se, 2, 2, 255.0); } break; case 4: { cvSetReal2D(se, 2, 0, 255.0); cvSetReal2D(se, 2, 1, 255.0); cvSetReal2D(se, 2, 2, 255.0); } break; case 5: { cvSetReal2D(se, 1, 0, 255.0); cvSetReal2D(se, 2, 0, 255.0); cvSetReal2D(se, 2, 1, 255.0); } break; case 6: { cvSetReal2D(se, 0, 0, 255.0); cvSetReal2D(se, 1, 0, 255.0); cvSetReal2D(se, 2, 0, 255.0); } break; case 7: { cvSetReal2D(se, 0, 0, 255.0); cvSetReal2D(se, 0, 1, 255.0); cvSetReal2D(se, 1, 0, 255.0); } break; default: break; } return se; } //细化操作  void Thinning(IplImage *src,IplImage *dst){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_com=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); cvZero(temp_last); cvCopy(src, temp, NULL); while(!isEqual(temp, temp_com)){ cvCopy(temp, temp_com, NULL); for(int i=0;i\u0026lt;8;i++){ cvCopy(temp, temp_last, NULL); IplImage *se1=CreateThinningSE(i); IplImage *se2=CreateThinningUSE(i); HitorMiss(temp, se1, se2, temp, NULL, NULL); cvSub(temp_last, temp, temp, NULL); cvReleaseImage(\u0026amp;se1); cvReleaseImage(\u0026amp;se2); } } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_com); cvReleaseImage(\u0026amp;temp_last); } //重建开操作  void reBuildOpen(IplImage *src,IplImage *dst,IplImage *ground,IplImage *dilateSE,IplImage *erodeSE,int eroden){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); cvCopy(src, temp, NULL); for(int i=0;i\u0026lt;eroden;i++){ Erode(temp, temp, erodeSE, NULL); } while(!isEqual(temp, temp_last)){ cvCopy(temp, temp_last, NULL); Dilate(temp, temp, dilateSE, NULL); And(temp, ground, temp); } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } int main(){ return 0; } 结果 击中：\n边界提取： 孔洞填充： 连通分量： 凸壳： 细化： 骨架： 形态学重建：\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-3-4-%E4%BA%8C%E5%80%BC%E5%9B%BE%E5%83%8F-%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%864-%E5%85%B6%E4%BB%96.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第12天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 二值图像，形态学，击中，边界提取，孔洞填充，连通分量提取，凸壳，细化，骨架，形态学重建\u003c/p\u003e","title":"【数字图像处理】3.4:二值图像-形态学处理4 其他操作"},{"content":"Abstract: 数字图像处理：第13天 Keywords: 灰度图像\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 二值图像涉及到的操作多为形态学操作，前几篇已经对二值图像的形态学操作做了简单的介绍，如果以后有更深入的研究或者有新的发现，将会继续补充，按照进度，接下来将要学习灰度图像的一些基本算法。我主要学习的教材是“冈萨雷斯的第三版”，《图像处理、分析与机器视觉》，以及《图像处理与计算机视觉算法及应用》。这几本书算是经典的入门级教材，也算深入教材，据大牛们说，这几本书超级给力，做完项目看会更有体会，因为本人小菜，所以大牛的话要慢慢来自己体会。 2014年最后一天，给灰度图开个好头，希望2015年能找到图像处理的相关工作。。。。。\n灰度图像 灰度图像是二值图像的进化版本，是彩色图像的退化版，也就是灰度图保存的信息没有彩色图像多，但比二值图像多，灰度图只包含一个通道的信息，而彩色图通常包含三个通道的信息，单一通道的理解可以理解为单一波长的电磁波，所以，红外遥感，X断层成像等单一通道电磁波产生的图像都为灰度图，而且在实际中灰度图易于采集和传输等性质的存在导致基于灰度图像开发的算法非常丰富。 我们可以通过下图来大致看一下彩色图像，灰度图像，二值图像在冈萨雷斯第三版中相关知识数量：\n中间部分为灰度图像的相关算法知识，左侧为彩色图像，右侧为二值图像。\n算法分类 下面来分类下各个分支，有些分支有些二义性，因为其可以属于不同的父节点，所以学习过程中会有交叉部分。 形态学 增强 滤波 滤波分为空域滤波和频域滤波两类： 空域： 频域： 复原 分割 总结 学习是没有尽头的，当我们只是单纯的喜欢一种技术，一门知识，并愿意花时间学习的时候，总会有所收获，无论是学习的过程还是结果，都会使我们终身受益。2015与诸君共勉。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-0-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第13天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 灰度图像\u003c/p\u003e","title":"【数字图像处理】4.0:灰度图像"},{"content":"Abstract: 数字图像处理：第14天 Keywords: 形态学，腐蚀，膨胀，开闭操作\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 新年第一篇博客，希望大家新年里能都收获更多的知识和技术，今天说的是灰度图像的形态学处理，灰度图像与二值图像的区别在于其记录了灰度信息，所以，形态学处理的定义与二值图像有些不同，因为二值图像可以用一系列的二维坐标来表示图像信息，而灰度图需要一个三维坐标表示，而且二值图像中结构元SE是平坦的，没有灰度信息的，但灰度图中，结构元是可以带有第三维信息的，即结构元也是灰度的，这就带来了一些问题，因为二值图像中，形态学的输出结果完全由输入图像产生，但是结构元一旦引入灰度信息，那么输出结果将不再由输入图像唯一确定。所以，一般情况下，结构元都使用平坦的结构元。\n腐蚀与膨胀 腐蚀与膨胀是形态学的基本操作，在灰度图像中也是如此，在二值图像中腐蚀和膨胀定义为对图像进行translation以后的“与”和“或”的逻辑操作结果，在灰度图像中，为了保存灰度信息，“与”和“或”操作被对应的替换成了“最大值”和“最小值”操作这样就给出了灰度图像中腐蚀和膨胀的操作定义。 另一种定义《图像处理、分析与机器视觉》中给出的是这样的，首先给出了个本影的概念，如果假设给出函数y=f(x)，那么f产生的曲线到x轴的阴影叫做本影，曲线叫做顶面（三维的叫面，二维的叫线，三维以上叫超平面），结构元表示为函数v=k(x)，一维函数f，k的腐蚀和膨胀等效于其本影（二值图像）的腐蚀与膨胀，并以此为基础推广到二维灰度图。\n我们以512x512的lena图中的第256行为例，展示下其操作结果，结构元为横向的1x5的结构元SE（1行，5列），中心为SE中心。下图为灰度图剖面示意图，其中蓝色为原图数据，绿色为膨胀后数据，红色为腐蚀后数据： 我们将局部放大，可以看到其具体情况： 开操作与闭操作 与二值图像中的定义相同，开、闭操作也又腐蚀膨胀操作衍生出来的，依旧先腐蚀后膨胀是开操作，先膨胀后腐蚀是闭操作。 具体如下图，蓝色为原图，绿色为闭操作，红色为开操作：\n局部放大，可以看出，开操作是将跨度小于结构元的峰顶消去，闭操作是将跨度小于结构元的谷底填平，冈萨雷斯的书中层有个滚动圆形结构元的例子也很生动，结论一致：\n顶帽和底帽 顶帽操作和底帽操作是灰度图像所特有的，其原理是开操作将使峰顶消去，具体消去了多少呢，可以用原图减去开操作结果，这样就能得到其消去的部分，而这个过程成为顶帽操作，顶帽就是开操作消去的峰顶，这一部分对应于图像中较亮的部分，也叫白色顶帽。 同理，底帽操作是用闭操作的结果减去原图就得到被闭操作填充的谷底部分，这一部分对应于图像中较暗的部分，也叫黑色底帽。\n测地腐蚀与测地膨胀 测地腐蚀与测地膨胀在二值图像中的操作是将腐蚀与膨胀结果与ground做“与”和“或”操作，与灰度膨胀中膨胀的推广一样，“与”和“或”被取代为最大值和最小值。\n重建操作 重建操作分为很多种，包括重建开操作、重建顶帽操作等。其根本原理是通过腐蚀找到SE的模式，然后迭代膨胀或者迭代顶帽操作直到图像收敛。\n数学表达 腐蚀：\n膨胀：\n开操作：\n闭操作：\n顶帽操作：\n底帽操作：\n测地腐蚀：\n测地膨胀：\n嘿嘿。。公式还在生产中。。公式太多。。哈哈哈哈。。。。。\n代码 #include \u0026lt;cv.h\u0026gt;#include \u0026lt;highgui.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#define TOFINDMAX 0 #define TOFINDMIN 1 #define isSIZEEQU(x,y) (((x)-\u0026gt;width)==((y)-\u0026gt;width)\u0026amp;\u0026amp;((x)-\u0026gt;height)==((y)-\u0026gt;height)) struct position{ int x; int y; }; typedef struct position Position; //判断结构元是否平滑 int isSmooth(IplImage *src){ int width=src-\u0026gt;width; int height=src-\u0026gt;height; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ int v=cvGetReal2D(src,j,i); if(v!=255.0\u0026amp;\u0026amp;v!=0.0) return 0; } return 1; } //判断两幅图像是否相等 int isEqu(IplImage *src1,IplImage *src2){ if(!isSIZEEQU(src1, src2)) return 0; for(int i=0;i\u0026lt;src1-\u0026gt;width;i++) for(int j=0;j\u0026lt;src1-\u0026gt;height;j++){ double v1=cvGetReal2D(src1, j, i); double v2=cvGetReal2D(src2, j, i); if(v1!=v2) return 0; } return 1; } //将图像全部设置为1 void One(IplImage *src){ int width=src-\u0026gt;width; int height=src-\u0026gt;height; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++) cvSetReal2D(src, j, i, 255.0); } //位移，如果非平滑SE将加上sevalue，即对应的灰度值 void Translation(IplImage *src ,IplImage *dst,double SEvalue,Position *d,int istoFindMin){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); int srcwidth=src-\u0026gt;width; int srcheight=src-\u0026gt;height; int dstwidth=dst-\u0026gt;width; int dstheight=dst-\u0026gt;height; if(istoFindMin) One(temp); else cvZero(temp); for(int i=0;i\u0026lt;srcwidth;i++){ for(int j=0;j\u0026lt;srcheight;j++){ int target_x=i+d-\u0026gt;x; int target_y=j+d-\u0026gt;y; if(target_x\u0026gt;=0\u0026amp;\u0026amp;target_y\u0026gt;=0\u0026amp;\u0026amp; target_x\u0026lt;dstwidth\u0026amp;\u0026amp;target_y\u0026lt;dstheight){ double value=cvGetReal2D(src, j, i)+SEvalue; value=(value\u0026gt;=255.0?255.0:value); cvSetReal2D(temp, target_y, target_x, value); } } } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); } //找出两幅等大图像中同一位置中相对较大的像素值 void MaxPix(IplImage *src1 ,IplImage *src2,IplImage *dst){ if(!isSIZEEQU(src1, src2)||!isSIZEEQU(src1, dst)){ printf(\u0026#34;MaxPix wrong: src size not equ!\\n\u0026#34;); exit(1); } int width=src1-\u0026gt;width; int height=src1-\u0026gt;height; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ double value1=cvGetReal2D(src1, j,i); double value2=cvGetReal2D(src2, j,i); value1\u0026gt;value2?cvSetReal2D(dst, j,i,value1):cvSetReal2D(dst, j, i, value2); } } //找出两幅等大图像中同一位置中相对较小的像素值 void MinPix(IplImage *src1 ,IplImage *src2,IplImage *dst){ if(!isSIZEEQU(src1, src2)||!isSIZEEQU(src1, dst)){ printf(\u0026#34;MaxPix wrong: src size not equ!\\n\u0026#34;); exit(1); } int width=src1-\u0026gt;width; int height=src1-\u0026gt;height; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ double value1=cvGetReal2D(src1, j,i); double value2=cvGetReal2D(src2, j,i); value1\u0026lt;value2?cvSetReal2D(dst, j, i, value1):cvSetReal2D(dst, j, i, value2); } } //灰度图像膨胀 void Dilate_Gray(IplImage *src,IplImage *dst,IplImage *se,Position *center){ int SEissmooth=isSmooth(se); IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Position centerde; centerde.x=se-\u0026gt;width/2; centerde.y=se-\u0026gt;height/2; if(center==NULL){ center=¢erde; } int sewidth=se-\u0026gt;width; int seheight=se-\u0026gt;height; cvCopy(src,temp_last,NULL); for(int i=0;i\u0026lt;sewidth;i++) for(int j=0;j\u0026lt;seheight;j++){ cvCopy(src,temp,NULL); double value=cvGetReal2D(se, j, i); if(value!=0.0){ Position d; d.x=center-\u0026gt;x-i; d.y=center-\u0026gt;y-j; if(SEissmooth) Translation(temp, temp, 0.0, \u0026amp;d,TOFINDMAX); else Translation(temp, temp, value, \u0026amp;d,TOFINDMAX); MaxPix(temp, temp_last, temp_last); } } cvCopy(temp_last, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } //灰度图像腐蚀 void Erode_Gray(IplImage *src,IplImage *dst,IplImage *se,Position *center){ int SEissmooth=isSmooth(se); IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Position centerde; centerde.x=se-\u0026gt;width/2; centerde.y=se-\u0026gt;height/2; if(center==NULL){ center=¢erde; } int sewidth=se-\u0026gt;width; int seheight=se-\u0026gt;height; cvCopy(src,temp_last,NULL); for(int i=0;i\u0026lt;sewidth;i++) for(int j=0;j\u0026lt;seheight;j++){ cvCopy(src,temp,NULL); double value=cvGetReal2D(se, j, i); if(value!=0.0){ Position d; d.x=i-center-\u0026gt;x; d.y=j-center-\u0026gt;y; if(SEissmooth) Translation(temp, temp, 0.0, \u0026amp;d,TOFINDMIN); else Translation(temp, temp, -1.0*value, \u0026amp;d,TOFINDMIN); MinPix(temp, temp_last, temp_last); } } cvCopy(temp_last, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } //开操作 void Open_Gray(IplImage *src,IplImage *dst,IplImage *se,Position *center){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Erode_Gray(src, temp, se, center); Dilate_Gray(temp, temp, se, center); cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); } //闭操作 void Close_Gray(IplImage *src,IplImage *dst,IplImage *se,Position *center){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Dilate_Gray(src, temp, se, center); Erode_Gray(temp, temp, se, center); cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); } //灰度梯度形态学提取 void Gard_Gray(IplImage *src,IplImage *dst,IplImage *se,Position *center){ IplImage *temp_dilate=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_erode=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Dilate_Gray(src, temp_dilate, se, center); Erode_Gray(src, temp_erode, se, center); cvSub(temp_dilate, temp_erode, dst, NULL); cvReleaseImage(\u0026amp;temp_erode); cvReleaseImage(\u0026amp;temp_dilate); } //顶帽操作 void TopHat(IplImage *src,IplImage *dst,IplImage *se,Position *center){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Open_Gray(src, temp, se, center); cvSub( src,temp, dst, NULL); cvReleaseImage(\u0026amp;temp); } //底帽操作 void BottomHat(IplImage *src,IplImage *dst,IplImage *se,Position *center){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Close_Gray(src, temp, se, center); cvSub(temp,src, dst, NULL); cvReleaseImage(\u0026amp;temp); } //测地腐蚀 void Erode_Gray_g(IplImage *src,IplImage *ground,IplImage *dst,IplImage *se,Position *center){ int SEissmooth=isSmooth(se); IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Position centerde; centerde.x=se-\u0026gt;width/2; centerde.y=se-\u0026gt;height/2; if(center==NULL){ center=¢erde; } int sewidth=se-\u0026gt;width; int seheight=se-\u0026gt;height; cvCopy(src,temp_last,NULL); for(int i=0;i\u0026lt;sewidth;i++) for(int j=0;j\u0026lt;seheight;j++){ cvCopy(src,temp,NULL); double value=cvGetReal2D(se, j, i); if(value!=0.0){ Position d; d.x=i-center-\u0026gt;x; d.y=j-center-\u0026gt;y; if(SEissmooth) Translation(temp, temp, 0.0, \u0026amp;d,TOFINDMIN); else Translation(temp, temp, -1.0*value, \u0026amp;d,TOFINDMIN); MinPix(temp, temp_last, temp_last); } } MaxPix(temp_last,ground,temp_last); cvCopy(temp_last, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } //测地膨胀 void Dilate_Gray_g(IplImage *src,IplImage *ground,IplImage *dst,IplImage *se,Position *center){ int SEissmooth=isSmooth(se); IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); Position centerde; centerde.x=se-\u0026gt;width/2; centerde.y=se-\u0026gt;height/2; if(center==NULL){ center=¢erde; } int sewidth=se-\u0026gt;width; int seheight=se-\u0026gt;height; cvCopy(src,temp_last,NULL); for(int i=0;i\u0026lt;sewidth;i++) for(int j=0;j\u0026lt;seheight;j++){ cvCopy(src,temp,NULL); double value=cvGetReal2D(se, j, i); if(value!=0.0){ Position d; d.x=center-\u0026gt;x-i; d.y=center-\u0026gt;y-j; if(SEissmooth) Translation(temp, temp, 0.0, \u0026amp;d,TOFINDMAX); else Translation(temp, temp, value, \u0026amp;d,TOFINDMAX); MaxPix(temp, temp_last, temp_last); } } MinPix(temp_last, ground, temp_last); cvCopy(temp_last, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } //重建开操作 void Rebuild_Open(IplImage *src,IplImage *dst,IplImage *ground,IplImage *erodeSE,IplImage *dilateSE,int eroden){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); cvCopy(src, temp, NULL); for(int i=0;i\u0026lt;eroden;i++){ Erode_Gray(temp, temp, erodeSE, NULL); } while(!isEqu(temp, temp_last)){ cvCopy(temp, temp_last, NULL); Dilate_Gray_g(temp, ground, temp, dilateSE, NULL); } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } //重建闭操作，这段没测试 void Rebuild_Close(IplImage *src,IplImage *dst,IplImage *ground,IplImage *dilateSE,IplImage *erodeSE,int dilaten){ IplImage *temp=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage *temp_last=cvCreateImage(cvGetSize(src), src-\u0026gt;depth, src-\u0026gt;nChannels); cvCopy(src, temp, NULL); for(int i=0;i\u0026lt;dilaten;i++){ Dilate_Gray(temp, temp, dilateSE, NULL); } while(!isEqu(temp, temp_last)){ cvCopy(temp, temp_last, NULL); Erode_Gray(temp, temp, erodeSE, NULL); MinPix(temp, ground, temp); } cvCopy(temp, dst, NULL); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;temp_last); } //重建顶帽操作 void Rebuild_Tophat(IplImage *src,IplImage *dst,IplImage *ground,IplImage *dilateSE,IplImage *erodeSE,int eroden){ Rebuild_Open(src,dst,ground,erodeSE,dilateSE,eroden); cvSub(src, dst, dst, NULL); } //重建底帽操作 void Rebuild_Bottomhat(IplImage *src,IplImage *dst,IplImage *ground,IplImage *dilateSE,IplImage *erodeSE,int dilaten){ Rebuild_Close(src,dst,ground,dilateSE,erodeSE,dilaten); cvSub(src, dst, dst, NULL); } int main(){ return 1; } 结果图片 以下结果原图为lena 512x512的图像产生： 腐蚀： 膨胀： 开操作： 闭操作： 顶帽操作： 底帽操作： 重建操作示意（冈萨雷斯 中文第三版 P437）： 去除横向亮条：重建顶帽操作 重建开操作，去除纵向亮纹： 上图横向膨胀： 膨胀结果与重建顶帽操作的最小操作： 原图对比： ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-1-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%86.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第14天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 形态学，腐蚀，膨胀，开闭操作\u003c/p\u003e","title":"【数字图像处理】4.1：灰度图像-形态学处理"},{"content":"Abstract: 数字图像处理：第23天 Keywords: 频域滤波，频域滤波器，振铃，卷积\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 这两天写了一下频域滤波的代码，并且发现以前博客里代码的一个BUG，产生BUG的原因是一维存储的二维矩阵行宽和列长被写反了，而且测试没有测出来，看来测试是必要的，而且还学习了下使用github，托管代码，因为是小菜，托管HelloWorld也不丢人，频域滤波打算写两到三篇，然后结束，进行下一大块问题的研究。 频域滤波的主要知识框架作了一个结构图，完全个人理解，如有理论上的问题，请及时指出：\n频域滤波基础 对于空域滤波和时域滤波，其最重要的理论基础就是，频率域的高频部分对应于空域中变化陡峭的细节，而频域的低频部分对应于空域变化换面的平坦区域，为了得到平滑或陡峭的部分，我们就想到了频域滤波，在空域中对细节和非细节的提取和定义并没有频域那么直接与明确，通过频域这个实验室，可以得出很多针对不同问题，特殊的滤波器，然后将其转换到空域，生成小模板，使用卷积运算来完成快速的滤波，达到我们的目的。基本关系示意图： 没错，冰冰和凤姐就差一个滤波器！（此为示意图，没有这样的滤波器，请勿模仿0.0） 我们的基本原理就是高频对应变化陡峭的部分，低频对应变化缓慢的部分。低频和高频在频率域以数字度量，滤波器对频域的数字进行不同程度的增强或抑制，之后进行逆变换，得到想要的图像，频域滤波器必须在频域关于原点对称，不然对称部分频率的逆变换将是原图产生伪影效果。 影响频域滤波效果的只有输入和滤波器，对于未处理的输入，如果直接进行DFT和滤波的话，会产生纠缠效果，我们来看数学效果: 卷积定理表明：对于3x3的两个矩阵a和b的卷积，应该等于a的DFT和b的DFT对位相乘（对位相乘的意思就是对于a和b对位相乘的结果c，c(x,y)=a(x,y)*b(x,y)，而非一般的矩阵相乘），再进行IDFT变换，就可以得到和空域的卷积结果，但我们得到的结果是：\nclear all;clc; a=[1 2 1;1 1 1 ;0 2 1]; fa=fft2(a); b=[3 2 1;3 1 1 ;2 0 1]; fb=fft2(b); temp=eye(3); for R=1:3 for C=1:3 temp(R,C)=fa(R,C)*fb(R,C); end end res=ifft2(temp); 卷积 等于结果： 通过我们用手计算，发现结果不对，这是什么原因呢，我们暂时不说原因，我们把矩阵用0扩充到5x5：\nclear all;clc; a=[1 2 1 0 0;1 1 1 0 0;0 2 1 0 0;0 0 0 0 0;0 0 0 0 0]; fa=fft2(a); b=[3 2 1 0 0;3 1 1 0 0;2 0 1 0 0;0 0 0 0 0;0 0 0 0 0]; fb=fft2(b); temp=eye(5); for R=1:5 for C=1:5 temp(R,C)=fa(R,C)*fb(R,C); end end res=ifft2(temp); 卷积 等于 没错，看红色框框，和我们用笔算的3x3的结果一样； 原因是，如果对于一个有限宽度为A的信号使用宽度为B的窗进行卷积，那么结果将会是一个宽度为A+B-1的信号，但如果信号宽度为A但周期也是A即信号尾部和下一个周期的头是紧挨着的话，那么进行卷积的时候尾部的信号就会和头部的信号相互纠缠，使结果的头部和尾部受到污染，中间部分保持正确，但如果信号比较短，就像上面3x3的话就完全被污染了。 但上面的信号是并不是周期的，而是有限宽度的，问题就出在DFT上，因为DFT把信号扩充了，而且是周期性的扩充，DFT时信号其实是这样的： 所以DFT的两个信号就是周期的而不是我们之前输入的信号，所以就会产生纠缠 避免这个问题很简单，对两个矩阵都用用0进行填充，填充成5x5后，得到正确的卷积结果。 滤波器的特性，滤波器是对傅里叶谱的实部和虚部进行等比增强或抑制，也就是傅里叶谱经过滤波器后，相位是没有变化的，主要原因之前提到过，就是一个谱的相角决定了图片的主要结构特征，所以，如果改变相角，那图片将产生巨大问题，所以使用的滤波器都是0相移的，但如果使用非0相移可以得到其他目的的结果，比如让一幅图面目全非。 对于理想滤波器，我们必须考虑其扩充问题，原因是，离线理想滤波器是频域的，其扩充应该在空域进行，而理想滤波器的IDFT在空域不是有限的而是一个无限的sinc函数，进行阶段后再填充，返回频率域后填充后的滤波器将产生严重的振铃现象，这里我们的办法是只对图像进行填充，填充到原图的4倍大小，然后使用填充后大小的滤波器，来减轻缠绕问题，同时时使问题简单化，但只是缓解缠绕问题，振铃问题依然存在。 上图a为频率域一维理想滤波器，b为IDFT后的空间波形，c是进行填充，d是填充后DFT的结果 下面我来解释下振铃现象，来个官方的解释： 振铃效应（Ringingeffect）是影响复原图像质量的众多因素之一，其典型表现是在图像灰度剧烈变化的邻域出现类吉布斯（Gibbs）分布\u0026ndash;(满足给定约束条件且熵最大的分布)的振荡。在图像盲复原中，振铃效应是一个不可忽视的问题，其严重降低了复原图像的质量，并且使得难于对复原图像进行后续处理。振铃效应是由于在图像复原中选取了不适当的图像模型造成的；在图像盲复原中如果点扩散函数选择不准确也是引起复原结果产生振铃效应的另一个原因，特别是选用的点扩散函数尺寸大于真实点扩散函数尺寸时，振铃现象更为明显；振铃效应产生的直接原因是图像退化过程中信息量的丢失，尤其是高频信息的丢失。 官方解释可能不好理解，在频域理想滤波器情况下，其实就是sinc函数产生的，只考虑上图的a和b，a是频域理想滤波器，b就是空域与之对应的滤波器，图像如果和b卷积，结果必然产生类似波浪一样的噪声，图像边缘将会产生抖动，边缘变宽，这就是振铃效果，我们来观察不同截止频率的理想滤波器以及其IDFT效果： 可以看出越宽（截止频率大）的滤波器频域振铃越步明显，相反，越窄的滤波器，空域振铃效果越大。\n频域滤波过程 频率域滤波的过程：因为我们要得到的是图像，输入是图像，输出也是图像，这个过程叫图像处理，如果输出的时频谱或其他的，我们叫图像分析，滤波器的主要参数是类型和截止频率。 下面来看一下算法的完整计算过程： 原图： 空域填充： 用0对图片进行填充，得到4倍原图的新图像 频谱中心化 这是为了配合滤波器，因为滤波器一般被设计为中间为低频，四周为高频的模式 DFT： 过度到频域 设计滤波器 FIR滤波器，有限长单位冲激响应滤波器，又称为非递归型滤波器，图像处理中都使用这种滤波器产生一个高斯低通滤波器： 对位相乘： IDFT： 将滤波结果重建为图像 空域取消填充： 将填充过的图像恢复 代码：\n#include \u0026#34;filter.h\u0026#34; static void showfilter(double *filter,int width,int height){ IplImage *show=cvCreateImage(cvSize(width, height),8,1); for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ cvSetReal2D(show, j, i, filter[i*width+j]*255.0); } cvNamedWindow(\u0026#34;Filter\u0026#34;, 1); cvShowImage(\u0026#34;Filter\u0026#34;, show); cvWaitKey(0); cvSaveImage(\u0026#34;/Users/Tony/DIPImage/step.jpg\u0026#34;, show, 0); cvReleaseImage(\u0026amp;show); } void MatrixMulti_R_C(double *src1,Complex *src2,Complex *dst,int size){//m(1,1)=a(1,1)*b(1,1);  for(int i=0;i\u0026lt;size;i++){ dst[i].real=src2[i].real*src1[i]; dst[i].imagin=src2[i].imagin*src1[i]; } } int ChangtoPower2(int size){ size--;//避免为2的幂的size会被扩大  int i=0; while ((size/=2)\u0026gt;0) { i++; } return 2\u0026lt;\u0026lt;i; } //将图像伸缩到2的幂次大小，并填充 void ResizeMatrix4FFT(IplImage *src,IplImage **dst){ int width=src-\u0026gt;width; int height=src-\u0026gt;height; int re_width=ChangtoPower2(width); int re_height=ChangtoPower2(height); IplImage *temp=cvCreateImage(cvSize(re_width, re_height), src-\u0026gt;depth, src-\u0026gt;nChannels); cvResize(src, temp, 0); *dst=cvCreateImage(cvSize(re_width*2, re_height*2), src-\u0026gt;depth, src-\u0026gt;nChannels); cvZero(*dst); for(int i=0;i\u0026lt;re_width;i++) for(int j=0;j\u0026lt;re_height;j++) cvSetReal2D(*dst, j, i, cvGetReal2D(temp, j, i)); cvReleaseImage(\u0026amp;temp); } //将扩充后的图像还原为左上角的 void CutImage421(IplImage *src,IplImage *dst){ //IplImage *temp=cvCreateImage(cvSize(src-\u0026gt;width/2, src-\u0026gt;height/2), src-\u0026gt;depth, src-\u0026gt;nChannels);  int width=dst-\u0026gt;width; int height=dst-\u0026gt;height; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++) cvSetReal2D(dst, j, i, cvGetReal2D(src, j, i)); } //频域滤波 void FrequencyFiltering(IplImage *src,IplImage *dst,int filter_type,double param1,int param2){ IplImage *temp=NULL; //调整至2的幂，并用黑色填充，防止周期缠绕  ResizeMatrix4FFT(src, \u0026amp;temp); int fft_width=temp-\u0026gt;width; int fft_height=temp-\u0026gt;height; //产生滤波器  double *filter=(double *)malloc(sizeof(double)*fft_height*fft_width); if(filter==NULL){ printf(\u0026#34;frequency filter malloc faile\u0026#34;); exit(0); } //生成滤波器  switch(filter_type){ case ILPF: IdealLPFilter(filter, fft_width, fft_height, param1); break; case BLPF: if(param2\u0026lt;0) param2=2; ButterworthLPfilter(filter, fft_width, fft_height, param1, param2); break; case GLPF: GaussianLPFilter(filter, fft_width, fft_height, param1); break; case IHPF: IdealHPFilter(filter, fft_width, fft_height, param1); break; case BHPF: if(param2\u0026lt;0) param2=2; ButterworthHPfilter(filter, fft_width, fft_height, param1, param2); break; case GHPF: GaussianHPFilter(filter, fft_width, fft_height, param1); break; } //FFT  Complex *temp_complex=(Complex*)malloc(sizeof(Complex)*fft_height*fft_width);//fft结果  if(temp_complex==NULL){ exit(0); } ImageFFT(temp, temp_complex); //相乘  MatrixMulti_R_C(filter,temp_complex,temp_complex,fft_width*fft_height); //IFFT  ImageIFFT(temp_complex, temp, temp-\u0026gt;width, temp-\u0026gt;height); //还原图像  IplImage *result2=cvCreateImage(cvSize(temp-\u0026gt;width/2, temp-\u0026gt;height/2), temp-\u0026gt;depth, temp-\u0026gt;nChannels); CutImage421(temp, result2); cvResize(result2, dst, 0); free(filter); free(temp_complex); cvReleaseImage(\u0026amp;temp); cvReleaseImage(\u0026amp;result2); } 空域滤波与频域滤波 空域和频域的桥梁是傅里叶变换，而纽带是卷积定理，对于频域的特性，我们将其看做一个实验室，进行试验并产生小的滤波模板，将小的滤波模板对空域图像进行循环卷积，得到我们想要的结果，空域小模板多半是奇数大小的（3x3，5x5）的，其计算复杂度低，过程简单，用时较短，这就是之前提到的那个面试的问题的答案，如果我当时把这个过程给面试官讲清楚，估计我现在就是一名图像工作者了。\n总结 这就是频域滤波的基础，后面的事情就是设计滤波器，满足不同的问题，有了基础，设计滤波器就要自己发挥了。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-10-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E6%A6%82%E8%AE%BA.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第23天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 频域滤波，频域滤波器，振铃，卷积\u003c/p\u003e","title":"【数字图像处理】4.10:灰度图像-频域滤波 概论"},{"content":"Abstract: 数字图像处理：第24天 Keywords: 频域滤波，高斯滤波器，布特沃斯滤波器\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 依然是废话开始，滤波器的起源就是频域来的，针对频域特性，滤波器被设计成各种各样的功能，但是频域滤波器都是线性的，转换到空域生成的卷积模板也是线性的，而有些从空域出发设计的滤波模板并不是线性的，判断是不是线性可以使用以下判断方法，也是信号与系统中常用的方法，$F(ax+by)=aF(x)+bF(y)$ 如果满足就是线性的，不满足则不是。常见的滤波模板中涉及到排序的，都不是线性模板，而通过频域设计然后转换为空域模板的，都是线性的模板。 由于滤波原理上一篇中已经做了详细的介绍，本篇主要记录几种常见的滤波器，并分析其滤波特点。主要介绍内容如下： 由于同态滤波的特殊性，将在下一篇中介绍，对于每种滤波器，都会分析其功率和振铃现象。 现在给出一个常用的距离函数，将在所有下面所有滤波器中使用，使用欧氏距离，因为频率域的距离代表的是带宽，所以，截止频率，带宽意义重大 $(P,Q)$ 为频谱大小，也是上篇中填充后的空域图像大小，并且我们使用的频谱都是中心化了的： 测试图片为： 平滑-低通滤波器 低通滤波器，顾名思义，只通过低频信号截断高频成分，在频谱中，中间部分为低频，四周为高频，截断点由滤波器的宽度决定，也就是截止频率，不同的截止频率对应于不同的效果和不同的剩余功率。\nILPF 理想低通滤波器，就是简单的截断，或者说设置一个频率阈值，大于阈值的频谱置零，小于等于阈值的不变： 这个效果就是频域的一个圆形，我们来观察，频域截止频率为50，P，Q为512的ILPF： 下面来观察滤波器在截止频率为10，20，30的振铃和频率特性： 截止频率10，滤波后剩余功率50.196378%，有振铃： 截止频率20，滤波后剩余功率78.559714%，有振： 截止频率30，滤波后剩余功率88.736089%，有振： BLPF 布特沃斯低通滤波器，采用布特沃斯公式，产生一种低阶时转折平滑，高阶时转折尖锐的滤波器： 示意图如下： 下面观察2阶和10阶布特沃斯滤波器在截止频率10，20，30时的表现： 2阶： 截止频率10，滤波后剩余功率40.044113%，无振铃： 截止频率20，滤波后剩余功率59.692304%，无振铃： 截止频率30，滤波后剩余功率70.964773%，无振铃： 10阶： 截止频率10，滤波后剩余功率46.72560%，有振铃： 截止频率20，滤波后剩余功率74.527332%，有振铃： 截止频率30，滤波后剩余功率86.566573%，有振铃： GLPF  由高斯公式产生的滤波器，由于高斯的傅里叶变换还是高斯的，所以变换无振铃效应： 滤波器示意图： 观察高斯滤波器在截止频率10，20，30时的表现： 截止频率10，滤波后剩余功率45036072%，无振铃： 截止频率20，滤波后剩余功率66.908677%，无振铃： 截止频率30，滤波后剩余功率77.419008%，无振铃： 锐化-高通滤波器 与低频对应的就是高频滤波器，同样我们介绍理想高通，布特沃斯高通，高斯高通，并提出钝化高提升高频强调滤波器；\nIHPF 理想高通的滤波器示意图： 同样观察截止频率为10，20，30的滤波效果和振铃效应： 截止频率10，滤波后剩余功率51.407872%，有振铃： 截止频率20，滤波后剩余功率23.044536%，有振铃： 截止频率30，滤波后剩余功率12.868162%，有振铃： BHPF 与低通布特沃斯相似，高通布特沃斯滤波器也是 一种低阶时转折平滑，高阶时转折尖锐的滤波器 我们同样观察2阶和10阶的布特沃斯在截止频率为10，20，30时的效果和振铃现象： 2阶： 截止频率10，滤波后剩余功率34.679332%，无振铃： 截止频率20，滤波后剩余功率18.029713%，无振铃： 截止频率30，滤波后剩余功率11.967571%，无振铃： 10阶： 截止频率10，滤波后剩余功率46.760049%，有振铃： 截止频率20，滤波后剩余功率20.167635%，有振铃： 截止频率30，滤波后剩余功率12.362897%，有振铃： GHPF 与低通一致，高斯高通滤波器为： 示意图： 观察截止频率为10，20，30的滤波效果和振铃效应： 截止频率10，滤波后剩余功率34.706478%，无振铃： 截止频率20，滤波后剩余功率16.325098%，无振铃： 截止频率30，滤波后剩余功率11.140692%，无振铃： 部分代码 低通：\n#include \u0026#34;lowpassfilter.h\u0026#34;static double Distance(int x,int y,int c_x,int c_y){ return sqrt((x-c_x)*(x-c_x)+(y-c_y)*(y-c_y)); } void IdealLPFilter(double *Filter,int width,int height,double cut_off_frequency){ int center_x=width/2; int center_y=height/2; double distance=0.0; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ distance=Distance(i,j,center_x,center_y); if(distance\u0026lt;=cut_off_frequency) Filter[j*width+i]=1.0; else Filter[j*width+i]=0.0; } } void ButterworthLPfilter(double *Filter,int width,int height,double cut_off_frequency,int n){ int center_x=width/2; int center_y=height/2; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ double value=1.0; for(int k=0;k\u0026lt;n;k++) value*=(Distance(i, j, center_x, center_y)/cut_off_frequency); Filter[j*width+i]=1/(1+value); } } void GaussianLPFilter(double *Filter,int width,int height,double cut_off_frequency){ int center_x=width/2; int center_y=height/2; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ double value=Distance(i, j, center_x, center_y); Filter[j*width+i]=exp(-value*value/(2*cut_off_frequency*cut_off_frequency)); } } 高通：\n#include \u0026#34;highpassfilter.h\u0026#34;static double Distance(int x,int y,int c_x,int c_y){ return sqrt((x-c_x)*(x-c_x)+(y-c_y)*(y-c_y)); } void IdealHPFilter(double *Filter,int width,int height,double cut_off_frequency){ int center_x=width/2; int center_y=height/2; double distance=0.0; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ distance=Distance(i,j,center_x,center_y); if(distance\u0026lt;=cut_off_frequency) Filter[j*width+i]=0.0; else Filter[j*width+i]=1.0; } Filter[width*(height+1)/2]+=1.0; } void ButterworthHPfilter(double *Filter,int width,int height,double cut_off_frequency,int n){ int center_x=width/2; int center_y=height/2; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ double value=1.0; for(int k=0;k\u0026lt;n;k++) value*=(Distance(i, j, center_x, center_y)/cut_off_frequency); Filter[j*width+i]=1.0-1.0/(1.0+value); } Filter[width*(height+1)/2]+=1.0; } void GaussianHPFilter(double *Filter,int width,int height,double cut_off_frequency){ int center_x=width/2; int center_y=height/2; for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++){ double value=Distance(i, j, center_x, center_y); Filter[j*width+i]=1.0-exp(-value*value/(2*cut_off_frequency*cut_off_frequency)); } Filter[width*(height+1)/2]+=1.0; } 钝化，高提升，高频强调 将高通滤波后的结果与原图进行一些加减，将得到钝化，高提升高频强调滤波器 对于下面公式k1控制距离原点的偏移量，k2控制高频贡献。\n k1=1时k2=1为钝化模板 k1=1时k2\u0026gt;1为高提升滤波器 k1=1时，统称高频强调滤波器 具体性质据定于所选用的滤波模板，与上面叙述的模板性质和截止频率有关，在这里不详细叙述。 代码：  #include \u0026#34;Homomorphicfilter.h\u0026#34; static double Distance(int x,int y,int c_x,int c_y){ return sqrt((x-c_x)*(x-c_x)+(y-c_y)*(y-c_y)); } void HomorphicFilter(double *filter,int width,int height,double cut_off_frequency,double lambda_l,double lambda_h,double c){ int center_x=width/2; int center_y=height/2; double distance; double distance_2; double cut_off_frequency_2=cut_off_frequency*cut_off_frequency; for(int i=0;i\u0026lt;height;i++) for(int j=0;j\u0026lt;width;j++){ distance=Distance(j, i, center_x, center_y); distance_2=distance*distance; filter[i*width+j]=(lambda_h-lambda_l)*(1.0-exp(-c*distance_2/cut_off_frequency_2))+lambda_l; } } 总结  总结一下，这篇的理论在前面已经介绍了，所以更多是验证前面的结论，观察滤波效果。滤波中值得注意的是振铃现象，对于不能容忍人工缺陷的应用中，如医学图像处理，不能使用带有振铃现象的滤波器，在实际物理中，理想滤波器无法实现，所以更多的使用高斯和其他一些无振铃的滤波器\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-11-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E6%BB%A4%E6%B3%A2%E5%99%A8.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第24天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 频域滤波，高斯滤波器，布特沃斯滤波器\u003c/p\u003e","title":"【数字图像处理】4.11:灰度图像-频域滤波 滤波器"},{"content":"Abstract: 数字图像处理：第25天 Keywords: 同态滤波\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 今天的废话是，图像处理是个很大的很混乱的科目，因为任何内容都至少属于两个大框架，比如通态滤波，听起来就是频域滤波，但它的另一个身份还属于图像增强。如果单纯的想把内容归类，这个很困难，因为图像的知识结构是图，而不是简单的树。 新买了一个书《图像处理基础》，像是一本字典，或者是题库，里面的标题像十万个为什么，要参加校招的同学可以临时复习下，当然，当我们深入学习时这本书也是一本不错的指导手册，像一本纸质的wiki，一本不错的工具书。 本文结构图： 成像原理 成像原理就是我们为什么能看到东西的原理，和光学有关，但这里我们对其进行抽象，如果完整建模成像的原理，将会是很复杂的模型，暂时不考虑复杂的情况，而只考虑两种主要参数，入射分量和反射分量。 例如对于一幅图像，对于像素点(x,y)我们设其值为f(x,y)那么可以确定的是f(x,y)的是大于0且有限的： 成像的物体受到光源的照射，反射到成像元件或者人眼里的是入射光照反射的部分，换个理解反射分量类似于一个衰减系数，例如入射光是1，反射分量是0.7，那么我们看到的是$1\\times 0.7=0.7$ ，如果物体是黑洞，那么反射分量是0，如果绝对无减少的反射，那么反射分量是1；入射分量为 $i(x,y)$,反射分量（我感觉叫反射系数更准确） $r(x,y)$ ，那么成像可以被描述为： 并且存在关系： 对于单色图像，图像的像素值表示该点的灰度强度 灰度强度介于最大和最小值之间，并且最小值大于0，最大值有限，实际上满足，，称为灰度级通常是【0,max】,对于8bit灰度像素，灰度级为【0，255】。\n同态滤波 介绍同态滤波之前，必须要了解下为什么叫同态，因为这个问题我也研究了很久，根据wiki上的介绍，同态的本质是一种映射，但是这种映射拥有特殊的性质，这种性质就是保持相关的属性不变，相关属性包括，幺元，逆元，二元运算等。看个例子： 典型的例子就是： 其中f可以为： 那么f(2+4)=f(2)*f(4)。这个映射f就是一个同态映射。加法运算的幺元0映射结果是乘法的幺元1。满足之前我们提到的性质。\n数学原理 根据前面的成像原理，和同态定义，我们提出了同态滤波，同态滤波，同态滤波的特点是，压缩灰度范围，同时增强对比度，增强对比度类似于增加像素灰度的方差，而压缩灰度值一定程度上限制了方差的大小，所以同态滤波有点类似于，给你减工资，还要让你工作积极性高涨。。。。。 根据上文成像原理的假设，图像 $f(x,y)=i(x,y)*r(x,y)$ ,但对于傅里叶变换（DFT）不存在下面关系，因为乘法的傅里叶变换不是傅里叶变换的乘法： 所以我们引进了自然对数函数，一个使乘法变成加法的神奇运算，我们定义： 那么就可以利用傅里叶的线性得到： 或者： $F_i$ 和 $F_r$ 分别是 $ln (i(x,y))$ 和 $ln (r(x,y))$ 的傅里叶变换。我们设计一个滤波器对Z进行滤波，就能得到： 进行傅里叶逆变换，就得到空间图像： 其中，我们根据前面定义能得到： 所以处理后的图像s可以表示为： 因为之前我们做了取自然对数运算，作为还原，我们计算： 其中： $i_0$ 和 $r_0$ 是滤波处理后的入射分量和反射分量。 整个过程用下面流程图表示： 更进一步，在图像中，变换缓慢的部分为照射分量，和发生突变通常是由反射分量组成，特别是物体连接处，对应于频谱就是高频和低频部分，虽然只是粗略的近似，但结果在图像中是很有用的。 数学公式： 滤波器的形状： 应用 对照明变化明显的图像进行预处理，减少图像照明变化的特征，增强较暗部分的细节\n代码 #include \u0026#34;Homomorphicfilter.h\u0026#34;static double Distance(int x,int y,int c_x,int c_y){ return sqrt((x-c_x)*(x-c_x)+(y-c_y)*(y-c_y)); } void HomorphicFilter(double *filter,int width,int height,double cut_off_frequency,double lambda_l,double lambda_h,double c){ int center_x=width/2; int center_y=height/2; double distance; double distance_2; double cut_off_frequency_2=cut_off_frequency*cut_off_frequency; for(int i=0;i\u0026lt;height;i++) for(int j=0;j\u0026lt;width;j++){ distance=Distance(j, i, center_x, center_y); distance_2=distance*distance; filter[i*width+j]=(lambda_h-lambda_l)*(1.0-exp(-c*distance_2/cut_off_frequency_2))+lambda_l; } } 示例 原图： 下图：截止频率20，$\\lambda_l=1.5$ , $\\lambda_h=10$ , $c=1$ 下图：截止频率20，$\\lambda_l=0.1$ , $\\lambda_h=10$ , $c=2$ 总结 总结，同态滤波可以有效的解决因光照不平衡引起的黑暗处细节消除的问题，并且平衡光照，频率域滤波可以处理各种问题，但归结起来有以下：1.图像要保留的部分和要去除的部分在频率上必须区别明显；2滤波器的性质决定了处理的结果；3分析频谱是一重要的环节。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-12-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%90%8C%E6%80%81%E6%BB%A4%E6%B3%A2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第25天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 同态滤波\u003c/p\u003e","title":"【数字图像处理】4.12:灰度图像-频域滤波 同态滤波"},{"content":"Abstract: 数字图像处理：第15天 Keywords: 卷积，傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 依旧是废话，傅里叶变换大学学了等于白学，首先问题在自己，自己当时就没把心思投入到学习中，第二，老师讲的真的是，现在回想，看看斯坦福的公开课“傅里叶变换及其应用”感觉差距不是一般的大。不是对老师的不尊，也不是崇洋媚外，事实摆在那里，一看就知道。把复杂的讲简单了，讲出自己的理解的老师，我上学的时候应该只遇到过一个，可惜是教大学物理的而不是专业课，而且本人大一的时候不好好学习，错过了唯一一位讲课用心的老师（略表惭愧，老师姓弓，西安电子科技大学，在此对弓老师表示敬意）。\n卷积 卷积，大部分老师是说，对于离散序列，两个序列的卷积就是，一个序列翻转，顺次划过另一个，然后对应的乘积再求和，没错，公式上写的就是那样。而且一般的课程都是先讲连续形式的卷积，即把乘积求和变成乘积的积分。其实我觉得，卷积一般用在离散信号，所以先讲离散的应该比较好理解，再说积分会打击很多人的信心。毕竟小学就会加法，而积分到了大学才学，应该先学简单的。来看官方的（wiki）定义： 两个序列卷积，完成的仅仅是一个计算过程，没有物理意义，也没有工程意义，所以，我要是老师我会这么跟大家说。 首先，我们有一个阶梯，每个阶梯上都有一个秤，所有秤都会连接到服务器（额，这个有点扯），每节台阶上的秤得到的结果会加不同的权，然后到服务器求和，也就是说，如果我们有且只有一个标准重量（例如1kg）物体，把它放到第一台阶的秤上，系统输出就是其重量（1kg）加权（例如：权重为2）的结果2。假设系统有三节台阶，每个台阶的权分别为 ${2，3，4}$ ，那么标准重量物体随等间隔时间t依次上台阶，系统输出就是 ${2，3，4}$ ，哈哈，恭喜你，标准物体就是冲击（自行翻阅信号与系统的书籍），输出的 ${2，3，4}$ 就是系统（这个阶梯秤系统）的冲击响应。可能不严谨，但是这个绝对比书上的形象。 还没看懂的话下面更形象： 上图是我们的完整系统，output为输出，其中s1到s4为各个阶梯上的秤的度数，每个阶梯下面对应的权重，也就是system{1，2，3，4}，输入是我们一群已经排好队的球，每个球的位置已经确定，即有序的，每个球的重量一定，不随时间变化。所以input为 ${1，2，4，2，2，2}$ 下面就开始卷积了，卷积过程就是一个系统对一个输入的响应过程； STEP0:调转队列（翻转的过程），准备上楼梯 STEP1:上楼开始，头先上 STEP2:这是一个球的一小步，却是输入信号的一大步 STEP3: STEP4: STEP5: STEP6: STEP7: STEP8: STEP9: STEP10: 结论 卷积可以按照以上来理解，之前网上有对卷积通俗的解释，是打人，先打一顿肿了，第二天还没好又打了一顿，感觉这种解释太血腥，而且不直观，不够数据化，所以想出来上面的解释，但数学是数学，理解归理解，数学还是更严谨些。希望大家能对卷积有更好的认识。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-2-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E5%8D%B7%E7%A7%AF.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第15天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 卷积，傅里叶变换\u003c/p\u003e","title":"【数字图像处理】4.2:灰度图像-频域滤波 傅里叶变换之卷积"},{"content":"Abstract: 数字图像处理：第16天 Keywords: 傅里叶级数\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，故事是这样的，当我上大学的时候，学过信号与系统，当时已经学了高的数学，也知道了傅里叶变换的公式，但是，公式是怎么来的，有什么用，不清楚，学信号与系统的时候，知道傅里叶用在什么地方，但是不清楚为什么可以用在这些地方，书中的记忆是：傅里叶变换或者其家族的变换，主要目的是把信号转换到频域，然后巴拉巴拉巴拉。。。其实事情确实是像书中描述的那个样子，但是，可以完全换一种能让人接受的方式告诉大家，可是写书的人不肯，于是苦了看书的人。 知乎上有一篇文章叫做“ 傅里叶分析之掐死教程（完整版） ” 大家可以自行搜索，整片文章没什么数学推导，完全从感性上介绍了傅里叶变换，有很多图，还有动图，可以供大家理解，但感性过后应该从数学的角度理性的思考傅里叶变换家族，对于图像处理，傅里叶是必须要使用和掌握的工具，而图像处理又是出自信号处理，所以我选择看奥本海姆的《信号与系统》，从中重新理解下傅里叶变换家族。 这一篇，我们从最简单的傅里叶级数开始讲起。大概的知识结构如下图： 傅里叶级数 傅里叶级数应该是由很多人提出来的，但大家都只是用到了类似的结构，但是没有系统的证明，其实傅里叶也没证明，他只是把傅里叶级数正式的用到了他的研究中，后来一群人前赴后继的证明出了一个有一个的傅里叶级数的准确性和存在条件，于是傅里叶级数和后续的傅里叶变换才开始推动人类进步。\n信号分解 研究信号的人都有这样一种经历，有个信号摆在你面前，乱七八糟一堆东西，你根本就不知道那是个什么东西，拿一个简单的周期信号来说： 这样一坨信号 $f(t)$ ，一般人看不出来什么信息，周期可以看出来，但是别的基本看不出什么，但如果将这个周期信号分解来看，那么信号是由 $sin(t)，cos(t)，sin(2t)，cos(2t)，sin(3t)，cos(3t)，sin(4t)，cos(4t)$ 组合出来的（称为基），有了这些，我们可以很容易的预测未来某个时刻信号的准确值，这些三角函数的性质，组成了完整信号的性质。 将大的，复杂的东西分解成小的，简单的东西，便于理解和掌握，这是一类信号问题的研究方法，于是，产生了信号分解这样一个分支。\n目标信号 一个信号可以分解成无数种其他信号的组合，但是哪一种是我们需要的，这需要我们人为的加入一些控制目标性质。\n 性质1：由这些基本信号可以构成相当广泛的一类有用的信号 性质2：线性时不变系统对每一个基本信号的响应应该十分简单，以使系统对任意输入信号的响应有一个方便的表示式  正交性 官方的解释： “正交性”是从几何中借来的术语。如果两条直线相交成直角，他们就是正交的。用向量术语来说，这两条直线互不依赖。沿着某一条直线移动，该直线投影到另一条直线上的位置不变，该术语还用于表示某种不相依赖性或者解耦性。如果两个或者更多事物种的一个发生变化，不会影响其他事物。这些事物就是正交的。 将信号分解成一组信号的组合，而且这组信号内任意两个之间都是正交的，那么其中每个基保存的信息互不影响。 其实除了上面两点性质，正交性也是一个重要的性质，对于一个向量，其具有基向量，基向量如果是正交的，那么其相关性为0，也就是说由基向量组成的一个向量，各个基向量是互不相关的，在简单的二维笛卡尔坐标系中，向量（a，b），其中两组基为（1，0）和（0，1）其正交，因为其垂直，或者内积为0都能证明其正交，a，b的值对彼此都没有影响。 函数正交的含义：如果两个函数ψ1(r)和ψ2(r)满足条件：$\\int \\Psi_1(r)*\\Psi2(r)d\\tau=0$，则称这两个函数相互正交。\n复指数信号 继续1.1中我们要寻找的目标函数，前辈们发现复指数信号具有1.1中所需要的性质，而且发现，对于一个线性时不变系统来说输入一个复指数信号，输出仍然是一个复指数信号，只是幅度上发生变化。（手写体公式，因为公式太多） 对于连续信号：对于离散信号： 对于一个连续线性时不变系统，系统单位冲击响应是h(t)，输入x(t)为连续信号，输出由其卷积来定义（不懂卷积的看上一篇博客） 将 $e^st$ 移到前面去： 假定上面式子中积分收敛，于是系统对输入的响应就是： 其中H(s)为一个复常数。 对于一个系统，如果输出等于输入乘以常数，那么这个输入叫做该系统的特征函数，这个常数叫做特征值，可以看出，复指数函数是系统的特征函数。 对于离散时间线性不变系统，单位脉冲响应为h【n】，输入序列为:$x[n]=z^n$其中z为复数，由卷积可以确定系统输出： 同样，假定上述求和收敛，输出为： 其中： 为一个复常数。 与连续的结果一致，复指数是离散时间不变线性系统的特征函数。 对于一个线性时不变系统来说，可以把一个更一般的输入分解成特征函数的组合。 令x(n)为三个复指数信号的线性组合： 根据特征函数的性质，系统的每一个分量的响应是： 根据线性系统的叠加性质： 上述结果表明，如果将一个信号分解成其特征函数的组合： 那么其响应可以非常容易的表达： 对于离散序列，将其分解成特征复指数线性组合： 结果与连续情况下完全相同 谐波 对于一个周期信号： 基波的周期就是满足这个式子的最小非零正值T，而w0=2*Pi/T，称为基波频率。基本周期函数\u0026ndash;复指数函数信号： ，与这个信号的有关的成谐波关系（harmonically related）的复指数信号集就是： k为整数。这些信号每一个都有一个基波频率为w0的整数倍，T是每一个信号的周期，对于k\u0026gt;=2后k\u0026lt;=-2其基波周期为T的约数，所以用成谐波关系的复指数线性组合来形成原始信号： 当k=0时，为常数，k=1，-1时，基波频率等于w0，称为基波分量，或一次基波分量，k=2，-2时，基波波长为T的一半，频率为2*w0。而且，互为谐波的信号彼此正交。 傅里叶级数 到这里还没晕的同学。。恭喜你。。上面这个式子就是傅里叶级数，也就是它： 对于傅里叶级数，还有另外两种表示，但不如上式常用。 推导过程：\n 1：因为x(t)是实信号x*(t)=x(t)于是如图中1的式子。 2：用-k代替k，式子结果不变，形式如图中2的式子。 3：比较式子1和2发现式子3的结果。 4：将求和重写划定求和范围 5：用ak的共轭代替a-k 6：中括号内互为共轭，所以得到图中式子6 分别是ak表示为极坐标和笛卡尔坐标的形式时： 极坐标： 笛卡尔坐标：   系数推导过程 既然傅里叶级数形式已经确定，剩下的任务就是确定每项的系数即ak的值，我们将用到前面提到的正交的性质，即在一个周期内，互为谐波的信号相乘后积分为0，因为傅里叶级数中的谐波集合都是互相正交的，所以只需让级数求和式子乘以ak的项，然后求周期内积分即可，数学推导如下： 对于求第n想的系数，首先级数两边同时乘以第n项的信号分量，整理求和和积分的顺序 将上面结果做欧拉变换，得到下面结果，得到了实部和虚部分别的积分，容易看出k=n时积分结果不为0，其他项由于相互正交，积分结果必然为0，所以为k!=n的时候为0； 于是我们得到上面an的最终结果，系数{ak}往往称为x(t)的傅里叶级数系数，或者x(t)的频谱系数: 收敛性 关于级数收敛性的问题，这里不再详细描述，因为详细的证明收敛性，闭推导过程复杂的更多，这里只给出存在傅里叶级数的条件（狄里赫利条件）：\n 任何周期内，x(t)必须绝对可积。 在任意有限区间内，x(t)具有有限个起伏变化；也就是说，在任何单个周期内，x(t)的最大值和最小值个数有限。 在x(t)的任意有限区间内，只有有限个不连续点，而且在这些不连续点上，函数是有限值。 下图a,b,c分别对应1，2，3图的反面例子：   性质 性质上传张图片吧，奥本海姆的书里面拍的，还算清楚 总结 傅里叶级数是傅里叶家族中最简单的成员，到这里我们从数学角度分析了一下傅里叶级数的来历，后面我们介绍连续非周期的傅里叶变换，离散傅里叶变换，和离散时间傅里叶变换\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-3-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E8%BF%9E%E7%BB%AD%E5%91%A8%E6%9C%9F%E4%BF%A1%E5%8F%B7%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第16天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 傅里叶级数\u003c/p\u003e","title":"【数字图像处理】4.3：灰度图像-频域滤波 傅里叶变换之连续周期信号傅里叶级数"},{"content":"Abstract: 数字图像处理：第17天 Keywords: 离散周期序列，傅里叶级数\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，本来以为奥本海姆的书会介绍所有傅里叶家族的变换，但唯独缺少数字图像处理要用的DFT，但感觉理解了傅里叶级数，傅里叶变换，离散时间傅里叶变换以后，DFT也基本没障碍了，这几篇写完之后就可以回归冈萨雷斯的书了，尤其是后面设计图像处理的滤波模板，应该好好深入研究一下。 想起之前去面试一家公司，技术面问我，空域卷积的时间复杂度是多少，我“说不加简化的是$whn*n$ ”，就是最简单的图像大小乘以窗口大小，然后他问为什么不转换到频域，我说傅里叶变换（DFT）很慢，他说“不是啊，FFT也很快啊，只需要 $n log(n)$ ，我顿时有点懵了，因为傅里叶我当时不太明白，然后我说‘这个我不太懂’，很明显最后他们没要我。。。。。想知道具体原因的同学我后面博客会详细说明原因，想想都觉得自己菜。\n离散时间复指数序列的周期性 对于离散复指数序列的周期性，必须要说明一下采样，采样是离散化的关键，从连续信号到离散信号，我们进行的操作是取样，例如对信号 $f(t)$ 进行取样，取样间隔为1，那么得到序列 ${f(0),f(1),f(2),f(3),f(4)\\dotsf(n)}$ ，实际中的例子：我们对sin(pi*x)和sin(3*pi*x)取样，正弦函数是复指数函数的一个对象，取样间隔为0.5，那么我们将得到下面的结果： 可以看出结果是一样的， 从公式上也能看出： 1.1 下图为周期间隔为PI的正弦函数的等间隔（0.5）取样示意图： 对于频率间隔为2*PI的复指数进行等间隔取样，其结果一致，这个原因也将导致离散周期傅里叶级数与连续周期傅里叶级数的重要区别。\n傅里叶级数综合式 离散周期序列的傅里叶级数，与连续周期傅里叶级数的目的一样\u0026ndash;为了分解信号，用成谐波关系的一组基信号来线性合成原始信号，基信号包含频率信息，可以从另一种角度分析信号信息。而离散周期傅里叶级数的特点是其级数项有限，而且不存在收敛问题，具体原因后面会详细介绍。 周期为N的周期信号序列表示为： 2.1 使上式成立的最小正整数N就是上述序列的周期， $w=\\frac{2\\pi}{N}$ ，就是基波的频率，例如下面的复指数序列： 2.2 上式周期为N，谐波集合的所有频率都是2*pi/N的倍数。根据上面我们提到的离散时间复指数序列的周期性，我们能得到： 2.3 当k变化是N的整数倍时，将得到一个一模一样的序列，数学原因是将式子1.1和式子2.2联立，就可以得到数学支持。 所以，一个成基波关系的有限项（N项）的线性组合： 2.4 k只在N个相继的整数区间内变化，不一定是0到N，也有可能是1到N+1，或者m到N+m，所以用改写成下式： 2.5 式2.5就是离散时间傅里叶级数，系数ak就是傅里叶级数的系数。\n傅里叶级数分析式 综合式确定以后，接下来就是确定各个级数的系数，这里采用和连续周期信号傅里叶级数同样的方法，下面式子可以看出，原始信号x共有N项已知，每项有一个N项的级数项组合，每个级数项对应一个未知系数，级数项可以计算，也就是说，有N个方程，N个未知数，且这些方程线性独立，所以系数是有唯一解集的，这也是离散周期信号傅里叶级数不存在收敛问题的愿意，其计算非求极限，而是实实在在的方程的解，而且离散周期信号序列肯定是绝对可和的，所以傅里叶级数一定存在。 但为了与连续形式相对应，我们不采用解方程的方法，而是使用与连续周期函数响应的方法，因为有以下性质： 3.2 推导如下： $f(n)$ 为原始序列，$s(n)$ 为级数集合，$a_n$ 为对应的系数，根据上式可以得到 $s_0$ 的值处处为1（上式虽然值写了N项和为N，但通过复指数原始公式可以得出所有项都为1，因为e的指数是0，或者 $2*\\pi$ 的整数倍），$S^*_k(n)$ 是 $S_k(n)$ 的共轭，所以，他们合体就是 $S_0$ ，所以结合上面公式，得到下面推导的最后结论。 将上面的推导过程综合成两步，使用的关键特性也是3.2： 得到分析结果 所以我们得到离散周期信号傅里叶级数： 性质 离散周期信号的傅里叶级数的性质与连续情况下类似，总结为原书中的下表： 在加一条帕斯瓦尔定理： 总结 离散时间周期信号的傅里叶级数与连续情况下的区别主要有：离散情况下不存在收敛问题，也没有吉伯斯现象，级数项是有限的N，并且离散时间周期序列傅里叶级数的系数是周期的，且周期为N。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-4-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E7%A6%BB%E6%95%A3%E5%91%A8%E6%9C%9F%E4%BF%A1%E5%8F%B7%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第17天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 离散周期序列，傅里叶级数\u003c/p\u003e","title":"【数字图像处理】4.4：灰度图像-频域滤波 傅里叶变换之离散周期信号傅里叶级数\""},{"content":"Abstract: 数字图像处理：第18天 Keywords: 连续信号傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 这两天博客写的有点多，感觉博客应该是知识的总结和理解，而是单纯的为了写博客而写博客，不过这两天的内容连续性很强，所以一口气下来也未尝不是好事，这两三篇文章一直在研究理论，已经好久没写代码了，哈哈，后面准备写下DFT和采样相关的，然后就回归冈萨雷斯，看图像比看数学有意思多了。 看奥本海姆的《信号与系统》发现看到最后竟然没有DFT，因为在图像处理里面用到的主要是DFT，有点郁闷，不过感觉傅里叶家族的东西基本相通，一个能推出另一个，另一个的特化就是前一个，等等，希望把FS（傅里叶级数），FT（傅里叶变换），DTFT（离散时间傅里叶变换）看完以后能触类旁通，学明白图像频域滤波的基础知识。 本篇文章介绍FT-傅里叶变换，DTFT-离散时间傅里叶变换将在后面介绍，下篇将介绍采样定理，原因，采样定理是FT的直接产物，DTFT在图像中应用不大，但作为DFT的近亲，还是要简单介绍下。\n傅里叶变换 我们平时所说的傅里叶变换实际上说的就是FT，其针对的原始信号是连续的信号，包括周期与非周期的（因为其涉及到收敛问题，所以连续信号的傅里叶变换是否存在要使用判断条件的），当然周期性的连续信号变换结果与傅里叶级数的变换结果是一致的。\n从连续周期级数推导FT 下面我们来推导下傅里叶变换，看下面一个周期方波信号，宽2T1，周期为T： 图像如下 根据前面的知识，其存在傅里叶级数，且该信号的级数系数为： 由于并未给出T1和T的具体值，这里我们设T=4*T1，其图像是： 黑色包络线并非级数结果，因为结果是离散的，是红色点标出的值，红色点的纵坐标就是级数的系数值，横坐标为对应的频率。 可以看出，决定形状的变量有k，T1，和T，我们对ak的表达式做简单的变形，将w0与k合体，因为w0为基波频率，其根据周期T唯一确定，如下： 这样一来，如果式子右边以w为变量，且w连续，那么就是一种很常见的波形了，我们暂且称之为SA函数波形，也就是上图外面的包络波形，如果w是离散的，也就是求级数的情况，相当于对包络函数的等间隔取样。 接下来我们设定T为T1的整数倍，而w0由T唯一确定，而且，最重要的是，我们发现上面式子中，右侧公式的值不受T的影响（虽然w0受到T影响，但可以用k来抵消掉，而使w保持不变 ）所以我们来调整T的大小，随之T的增大，w0不断变小，而包络线不变，这样当T逐渐增大，w0=2*pi/T逐渐变小当如下图： 当T逐渐增大的时候 T进一步增加 我们来看级数系数在对应的w0的变化在SA函数取样 当T=8*T1的时候： 当T=16*T1的时候： 当T趋近于无穷大，级数系数取样间隔变得无穷小，周期方波变成只有一个方波的非周期绝对可积信号。 以上通过扩大一个周期信号的周期，给出一个周期无穷大的绝对可积的信号的“傅里叶级数”形式，当我们原始信号为一个周期无穷大，或者是非周期的绝对可积信号，我们将使用相同的思想，先将原始信号按照一定的周期复制成周期信号，然后求解周期信号的傅里叶级数，然后将周期扩展到无穷大： 上面图中是x(t)周期复制的结果，的傅里叶级数为： 分析公式为： 由于x(t)在-T/2到T/2内与相同，在区间-T/2到T/2外，x(t)为0，所以将换成x(t)： 定义Tak的包络函数 所以，系数ak可以写为： 把ak带回来式子中得到 因为，改写为： 当T趋近于无穷大的时候，w0趋近于无穷小上式子变成积分形式，上式求和内容为一个矩形面积，当w0趋近于无穷小的时候，收敛于x（t）。 至此，我们得出傅里叶完整公式对： 因为傅里叶变换是无限的“级数”，所以存在收敛问题：\n 条件1：信号绝对可积  条件2：任何区间内，x（t）具有有限个最大值与最小值 条件3：任何区间内，x（t）具有有限个不连续点，并且每个不连续点的值是有限值。 对于周期函数的傅里叶变换，其结果与傅里叶级数相同，我们可以理解为其傅里叶变换的一串冲击函数。 傅里叶变换的性质： 至此，对连续函数的傅里叶变换进行了简要的推导，由于其在图像处理里面很少用到，但有可能在模式识别中有用到，所进行简要的介绍。 ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-5-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E8%BF%9E%E7%BB%AD%E4%BF%A1%E5%8F%B7%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2ft.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第18天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 连续信号傅里叶变换\u003c/p\u003e","title":"【数字图像处理】4.5:灰度图像-频域滤波 傅里叶变换之连续信号傅里叶变换（FT）"},{"content":"Abstract: 数字图像处理：第19天 Keywords: 采样定理，奈奎斯特采样定理\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 采样定理，大名鼎鼎，早有耳闻，就是不知道为啥，但很好记，采样频率一定要大于原始信号最大频率的两倍，为啥是两倍，不知道，怎么从离散恢复出连续信号，也不知道，但前几天学过傅里叶了以后在看采样定理，就会发现其实采样定理是傅里叶的一种应用，或者叫做推广。 采样定理是连接连续信号与离散信号的桥梁，就行傅里叶是连接时域（空域）与频域的桥梁一样，他们的共同点是带我们从另外的角度看问题，也许正是这种原因，这样的定理都格外引人注意，因为他们让我们以不同的角度看世界，也会从另外的角度看到一些之前没看到的角落。 上图： 采样定理 设x(t)是某一个带线信号，在 $|w|\u0026gt;w_m$ 时，$X(jw)=0$ ；如果 $w_s\u0026gt;2w_m$ ，其中$w_s=\\frac{2\\pi}{T}$，那么 $x(t)$ 就唯一由其样本 $x(nT)$ , $n=\\dots -2,-1,0,1,2,\\dots$ 所确定。\n 上一句的话的注释：wm是信号的最高频率，大于 $w_m$ 的频率幅值为0；采样频率为 $w_s$ ，采样周期为 $T$ ，也就是冲击串的周期是T。采样频率一定要大于某个值，也就是采样周期要小于某个值，定性的说法就是采样越密集越好，这与我们平时的感觉也是一样的，相当于单位面积像素越多的显示器显示效果越好一样的道理。大于2倍最高频率称为奈奎斯特率。\n混叠现象 先来看一下，不满足采样定理会出现什么情况，如果采样间隔很大（采样周期很大）将会产生混叠现象。 先看一个实际例子，一个圆盘以恒定速度旋转，旋转方向逆时针，转一圈的时间为T。 为了识别圆盘的位置，我们画出任意一条半径，来记录我们在不同时间间隔下拍下来的圆盘位置。拍摄到半径的旋转位置为信号，旋转方向为与上一次拍到图片夹角最小的方向。\n  情况一：我们拍照的间隔是T/8，也就是采样频率是旋转频率的8倍（由于旋转速度恒定，即信号频率恒定，所以最高频率就是恒定频率）。我们来看t0到t11的旋转情况，将每一时刻拆分出来： 在一幅图上表示：   情况二：拍照间隔为(1/2)T的，采样频率是原始频率的2倍，我们来看t0到t11的旋转情况，将每一时刻拆分出来： 合并到一张图像： 现在已经分不清旋转的方向了，混叠现象出现，这也是为啥要大于原信号最高频率的两倍而不是大于等于原信号最高频率的两倍。\n  情况三：拍照间隔为(7/8)T的，采样频率是原始频率的（8/7）倍，我们来看t0到t11的旋转情况，将每一时刻拆分出来： 同样，放到一张图上的： 这幅图我们已经完全把旋转方向感觉反了，这就是混叠现象。 上面的例子从常识上给出了采样定律的正确性，下面从数学的角度证明采样定理。\n  采样定理数学原理 利用冲击函数的采样特性，我们可以将一个连续时间信号与冲击串相乘得到一个有幅度变化的离散序列，这个过程就是一个采样的过程，冲击串的周期称为采样周期，就是采样定理里面提到的ws，得到下图： 在时域中： 因为冲击函数的取样特性带入上面式子，得到： 根据时域内乘法后的傅里叶变换等于频域卷积的性质，我们有： 式子中： 因为一个信号与冲击函数的卷积等于该信号的移位，所以： 因为原信号是带限的，所以其频谱与冲击串卷积后将变成一个周期信号： 可以看到原信号与冲击串卷积后，其原始结构并未改变，而只是复制成了周期形式，这是采样定理最关键的地方，首先必须确定原信号的频谱的频率有限，第二是频谱和冲击串卷积后复制的频谱之间不能相互影响，为了满足这一点，就要求采样频率大于原信号最大频率的两倍，如果相互影响就会产生混叠现象： 为了恢复原始信号，我们只需要对离散信号的频谱进行低通滤波，理想情况下将得到完整的原始信号，条件是滤波截止频率在下图的红色位置，即wm和ws-wm之间： 即可恢复原始信号。 一个完整的采样到恢复的过程总结为以下： 混叠的数学原理 当采样频率小于信号最大频率的两倍时发生的现象，称之为混叠，混叠只对那些没有被采样到的点，从离散恢复回来后会有差异，但被采样的点不会变化， xr(t)为从离散序列中恢复出来的连续信号。 从频域的角度看混叠，我们以信号为例： 可以看出当采样频率小于信号最大频率时出现混叠，当虚线和实线标志位置对换时这个结果导致恢复后信号的相位发生了倒置，称为相位倒置。 再看一个时域的例子： 采样间隔相同，频率高的信号采样后失真。 离散信号的采样定理与上述相同，因此不再赘述。\n总结 至此对采样定理进行了简要介绍，其实前面介绍离散周期信号的傅里叶级数的时候，为什么离散周期信号傅里叶级数项是有限的，就是因为采样的原因，上图已经给出了原因。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-6-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E9%87%87%E6%A0%B7%E5%AE%9A%E7%90%86.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第19天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 采样定理，奈奎斯特采样定理\u003c/p\u003e","title":"【数字图像处理】4.6:灰度图像-频域滤波 傅里叶变换之采样定理"},{"content":"Abstract: 数字图像处理：第20天 Keywords: 离散时间傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 本来是不想写DTFT的，原因1，与前面傅里叶变换（FT）推导过程相似，原因2，在图像处理中DTFT应用不是很广泛，但后来想想还是写出来，原因1，不写出来我觉得心里不踏实，原因2，DTFT是DFT的近亲，不写的话家族不完整，下一篇写DFT，其实写到这个阶段，要写的东西就少了许多，因为很多都是引用前面的结论和一些性质。但还是写出来吧，为了心里踏实。 忘了哪位中国上一辈的科学家说过，“搞科研不能糊弄，你糊弄它，它就糊弄你。”我这算不上科研，但感觉还是学踏实点心里有底，不至于以后哪里出了问题总是会怀疑自己知识基础有问题。还有，希望我们当代的科研工作者能好好搞研究，都不好好教学了还搞不好研究，那就真是一群废物了。\n从离散周期信号的傅里叶级数推导离散时间傅里叶变换  一个某一个有限序列 $x[n]$ ,其在某一个阶段 $N(N_1\u0026lt;=n\u0026lt;=N_2)$ 内不为0，其外部，全部为0，用这个信号构造一个周期信号 $x\u0026rsquo;[n],x[n]$ 是$x\u0026rsquo;[n]$ 的一个周期。 $x\u0026rsquo;[n]$ 是周期信号，所以其有傅里叶级数，并且其傅里叶级数是周期的：\n在 $N$ 内 $x\u0026rsquo;[n]=x[n]$ ，替换求和内容为：\n在N外 $x[n]=0$ ；所以定义函数： 所以系数$a_k$ 正比于 $X(e^{jw})$ ： 其中 $w0=\\frac{2\\pi}{N}$ ，上面式子和第一个式子结合起来就有： 因为 $w_0N=2\\pi$ ，所以有： 随着 $N$ 的增加，$w_0$不断减少，当 $N$ 趋近于无穷大的时候， $w_0$ 趋近于无穷小，$x\u0026rsquo;[n]=x[n]$ ，此时，上式变成一个积分式，积分变量为w0，因为 $w_0=\\frac{2*pi}{N}$ ，所以积分区间为 $2\\pi$ ，就有： 因为 $X(e^jw)e^{jw}$ 的周期是 $2\\pi$ ，所以积分区间可以去任何长度的 $2\\pi$ 区间，得出以下变换对： 这就是离散时间傅里叶变换对，同样，转换到频域的叫分析公式，转换到时域的叫综合公式。\n性质 总结 至此，傅里叶家的四种主要变换已经全部推导了以下，下一篇写下DFT是什么，然后介绍几个常见问题，并给出傅里叶家谱和之间的相互关系。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-7-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2dtft.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第20天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 离散时间傅里叶变换\u003c/p\u003e","title":"【数字图像处理】4.7:灰度图像-频域滤波 傅里叶变换之离散时间傅里叶变换(DTFT)"},{"content":"Abstract: 数字图像处理：第21天 Keywords: 离散傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 一如既往的开篇废话，今天介绍离散傅里叶变换（DFT），学习到这，不敢说对傅里叶有多了解，但起码把他家家谱算是捋顺了，这个家族不关系相当复杂，让多少人都迷糊了好久，估计上大学学明白的人不多，工作了用不到也就不会可以去学习了，不过很庆幸，感觉自己又学会了一点东西，也解决了之前心中的一个疑惑，后面希望能够将学到的傅里叶的数学知识用到解决实际问题的算法中。\n离散傅里叶变换 首先说一下DFT的存在的意义，我们必须明确一点，计算机只能处理离散的有限的序列，不论是时域，还是频域如果计算得到的时连续的公式，那么计算机都无法实现，所以纵观前几篇的傅里叶，没有人满足这个条件，傅里叶级数虽然离散但是时域和频域都是无限的，傅里叶变换时域和频域都是无限的，而且是连续的，DTFT时域是离散有限的，但频域是连续的无限的；所以都不满足计算机对数据的要求，但通过观察我们可以发现，对DTFT稍加改造，或者对离散周期序列的傅里叶级数进行裁剪将能得到满足我们需求的数据类型，即有限的且离散的\u0010\u0010\u0010数据。 我们对DTFT的改造方式，根据其时域的采样方式，以等价的方式在频域进行等间隔采样，便得到频域离散的序列。 或者我们可以将原信号，即时域离散的有限信号，进行周期复制，周期要大于序列的长度，以保证信号形状不变，此离散周期序列存在傅里叶级数，但级数是周期离散的，也就是无限的，我们截取其中的一个完整的周期，便得到了频域离散有限的信号。 离散傅里叶变换公式上与离散周期傅里叶级数极其相似，对于序列 ${x[n]}$ 其中 $0\\leq n\u0026lt;N$ ：\n戴帽子的x就是频域序列；对应的从频域到时域的转换： 上面两个式子就是离散傅里叶变换。由公式可以看出，时域和频域的信号都是长度为N的离散序列。\n数学推导 下面对DFT进行简要推导，假设 $x(t)$ 其中t在 $[0,L]$ 区间上，现在对时域进行取样，取样周期为 $T$ 即新序列 $$ x[n]=x(k\\times T),k=0,1,2,3,\\dots N-1 $$ 共 $N$ 个采样点，其中 $N=\\frac{L}{T}$ ，为离散后的序列： 其傅里叶变换为: 这个式子其实就是前面所说的离散时间傅里叶变换。下面对其的频域进行采样，我们知道根据采样定律，采样频率必须大于原信号的最大频率的2倍，所以采样周期必须小于原信号最小周期的二分之一，所以，一个离散的有限的序列，其频率最大不超过其取样周期T的二倍的倒数，即频率域的最大频率 $w\u0026lt;\\frac{(2\\times\\pi)}{(2\\times T)}$ ,对应的负频率就应该是\n$-w\u0026gt;-\\frac{2\\times\\pi}{2\\times T}$ ，所以离散序列对应的频率宽度为 $\\frac{2\\times\\pi}{T}$ 。因为原序列的长度为L所以其最小频率间隔为 $\\frac{2\\times\\pi}{L}$ 。那么频率域的离散序列长度为 $\\frac{\\frac{2\\times\\pi}{T}}{\\frac{2\\times\\pi}{L}}=\\frac{L}{T}=N$ ，与原序列对应，顾频率域的采样间隔为 $\\frac{2\\pi}{N}$ 。 顾采样序列为：： 将上面式子归一化，令 $T=1$ ，便得到DFT公式。 所以，完整的DFT的过程是：对于连续非周期信号，现将其离散化，得到DTFT，然后将频率域进行再次离散化，得到DFT。 或者将有限离散序列进行复制，使原信号变成离散周期信号，求其傅里叶级数，求主值，也就是一个周期内的连续的点，就是对应的DFT。\n性质 总结 总结下，DFT进行了粗略的介绍，因为其计算和离散周期信号的傅里叶级数计算相似，而且公式很易于接受，所以就只进行了简单的描述，之前后面一篇总结下所有的傅里叶的基础知识，然后介绍二维傅里叶，在外面绕了一圈就要回归图像了。 下面给出傅里叶家族的一些基本关系，灰色粗箭头表示可以由什么推导出什么，其他箭头表示连线： ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-8-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E7%A6%BB%E6%95%A3%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2dft.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第21天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 离散傅里叶变换\u003c/p\u003e","title":"【数字图像处理】4.8:灰度图像-频域滤波 傅里叶变换之离散傅里叶变换(DFT)"},{"content":"Abstract: 数字图像处理：第22天 Keywords: 卷积，图像傅里叶变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 今天要记录的是二维离散傅里叶变换的一些性质，也是傅里叶在图像处理中要用到的一些性质，所以要重点学习下，并总结下。重点在性质。\n二维DFT数学公式 根据一维离散傅里叶变换，直接给出二维DFT的公式，这个公式的具体代码实现在前面文章中有提到，但是实际中由于DFT计算速度过慢，普遍使用FFT作为快速算法，DFT和IDFT的公式如下： 二维DFT公式的推导过程与一维相同，对连续傅里叶变换进行采样或者是将原始数据周期复制后计算傅里叶级数，然后取主值，在此不再赘述。\n性质 空间与频率关系 对于MxN的图像，和是空间的取样间隔，所以原函数的总长度是Mx和 $Nx$ ，频域间隔u，v为以下关系： 所以，频率域样本间隔与空域样本间隔成反比。\n平移，旋转与中心化 将原图像进行平移或者对频域信号进行移动时，与其对应的频率和空域也会有相应的变换，变换关系如下： 可以看出，频率域原点移动到（u0，v0），对应的空域为原信号与后面复指数信号的乘积，这也是所有书上都要说的频谱中心化，也就是（u0，v0）=（M/2,N/2），这样就能得到： 上面的 $(-1)^{x+y}$ ，是 $\\frac{M}{2},\\frac{N}{2}$ 带入得到的。 对于旋转特性，我们选择用极坐标表示二维笛卡尔坐标系，于是有以下变换性质： 得出结论，频谱与空域具有旋转同步性，即空域旋转多少频域就旋转多少，频域旋转空域就旋转多少。 证明实验如下： 原图 傅里叶变换后的频谱 旋转后的图片 旋转后的频谱\n微分性质 微分性质，即一个信号的微分的傅里叶变换具有对高频成分的放大作用 $$ f\u0026rsquo;(x,y)\u0026lt;=\u0026gt;j\\times u\\times v\\times F(u,v) $$ 所以得出微分可以计算边缘-这种高频成分的功能\n周期性与对称性 DFT的周期性与离散周期信号的傅里叶级数一致，即原信号周期，频谱也周期，图像大小为MxN： $k_1，k_2$ 为整数。 下面给出原始频谱和中心化后的频谱的图示，来自冈萨雷斯： 可以看出，频谱是周期排布的。 下图总结了其他一些对称的变换性质： 对应几个特殊的性质的解释： 傅里叶谱与相角 图像傅里叶转换后的结果是复数矩阵，因为复数没法用在图像中显示，所以在图像中我们看到的都是幅度，也就是结果实部和虚部的平方和的平方根，也就是复数的长度，但是复数还有另一个性质，也就是其相位，根据实验显示，相位存储了图像的轮廓信息，或者叫做机构信息，而幅度存储了灰度信息。下面来看一下实验： 首先，我们通过FFT得到lena的傅里叶变换结果，也就是等大的复数矩阵，我们将所有元素的长度归一化，也就是用每个像素的实部和虚部除以其距离，这样将得到幅度为1，但相角不同的复数矩阵，然后用IFFT进行逆运算，得到的计算结果在0.001左右，为了显示，将其扩大1000倍进行显示： 原图 相角重建 接下来考察幅度的影响，我们将所有元素的相角设为45，135，225，315度，也就是对所有的实部和虚部设为其距离除以根号2，这样得到的IFFT结果如下： 可见，幅度对结构信息没有影响。 代码：\n//将幅度归一，相角保持不变，图中Complex为复数数据  void OneRange(Complex *src,Complex *dst,int width,int height){ double realv=0.0,imaginv=0.0; for(int i=0;i\u0026lt;width*height;i++){ realv=src[i].real; imaginv=src[i].imagin; double distance=sqrt(realv*realv+imaginv*imaginv); dst[i].real=realv/distance; dst[i].imagin=imaginv/distance; } } //将相角归一，相角保持不变，图中Complex为复数数据  void OneAngel(Complex *src,Complex *dst,int width,int height){ double realv=0.0,imaginv=0.0; for(int i=0;i\u0026lt;width*height;i++){ realv=src[i].real; imaginv=src[i].imagin; double distance=sqrt(realv*realv+imaginv*imaginv); dst[i].real=distance/sqrt(2.0); dst[i].imagin=distance/sqrt(2.0); } } 二维卷积定理 卷积定义： 频域和空域的卷积关系： 卷积的一个问题在于卷积以后会对原始信号的周期进行一定长度的扩展，比如长度为A和B的信号的卷积，卷积结果的信号长度为A+B-1，所以对于周期信号，卷积后的结果有可能造成信号纠缠： 解决这个问题的方法很简单，就是对周期进行用零扩展，长度为A和B的信号卷积，将A和B都扩展成A+B-1，用0填充。f和h的图像进行卷积，首先进行扩展： 总结 至此对图像傅里叶变换的一些性质进行了简要介绍，并总结出下表，方便以后出现问题进行查阅： 下一篇介绍图像混淆\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-4-9-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2-%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E4%B9%8B%E4%BA%8C%E7%BB%B4%E7%A6%BB%E6%95%A3%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第22天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 卷积，图像傅里叶变换\u003c/p\u003e","title":"【数字图像处理】4.9:灰度图像-频域滤波 傅里叶变换之二维离散傅里叶变换"},{"content":"Abstract: 数字图像处理：第35天 Keywords: 灰度变换,伽马变换\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，灰度变化早在学习DIP刚开始的几篇中已经介绍了，准确的说是实现了，但没有讲原理，提供了代码和处理结果，今天主要介绍下原理，对于灰度变换，一定要明确一点，灰度变换是针对灰度的，与图像中的位置无关，也就是说，灰度变换是用新灰度代替对应的旧灰度，而新灰度与旧灰度的映射关系，根据相关模型确定，今天我们学习的模型有：\n 图像反转 对数变换 幂律（伽马）变换 分段线性变换  灰度变换数学 图像反转：用互补灰度代替原灰度（互补灰度，是我自己编出来的，对与灰度a假设灰度级别一共有255，那么互补灰度就是255-a，式子中L-1为灰度级） 对数变换：r为原始灰度，1+r为了log函数为正值，参数c为了保证r属于【0,L-1】并且新的灰度值s也属于【0,L-1】. 幂律变换：也叫伽马变换或伽马校正，阴极射线管CRT设备，存在灰度和电压的幂律响应，gama值1.7到2.5 分段线性：与分段函数相似，不同的定义域有不同的变换函数 代码与效果 代码在这里，用力戳：灰度变换，Gama\n总结 灰度变换的主要目的是改变对比度，而且针对灰度进行操作，一般的做法是建立一个映射表，大小为灰度级大小，然后根据不同的变换函数生成映射，将输入图片根据映射表产生输出，计算速度很快，时间复杂度为图像总的像素个数。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-9-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E7%81%B0%E5%BA%A6%E5%8F%98%E6%8D%A2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第35天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 灰度变换,伽马变换\u003c/p\u003e","title":"【数字图像处理】5-9:灰度图像--图像增强 灰度变换"},{"content":"Abstract: 数字图像处理：第26天 Keywords: 卷积，相关，空域滤波\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，博客已经写到第28篇了，但书已经看了两个月了，每天虽然很累，但感觉收获还是很多的，一个是感觉自己找到门路，学习基础是很无聊的，因为不会直接做出结果，没有成就感，但回头看看写的博客和代码，感觉确实有提高，群里好多硕士也都是被老板抓过来就做图像类的项目，有些人根本不知道是啥，现学编程，找篇论文，抓起OpenCV一顿找大牛求教，最后做出来个结果，其实之前我也是这样，做出来东西以后感觉自己收获很多，也有成就感，不过现在看看有点空中楼阁的意思，就像很多人做了很多图像项目，却连基本的卷积，相关，平滑，锐化都说不明白，他们同样能把工作干的很出色，但对基础并不是很熟悉，更多的人做图像的初衷并不是爱好，而是单纯的工作，或者被动的学习。兴趣是最好的老师，值得庆幸的是，虽然每天工作一天，我还是很有激情的学了这么久，而且热情更加高涨。 相声四门功课，说学逗唱，唱是太平歌词，但很多人不会唱太平歌词，因为那个东西很枯燥并且很难，而且现在没人要求你必须学全了四门功课才能上岗，所以，现在相声不好听，不是市场不景气，是做的不够好，为啥郭德纲就能卖出票，为啥十几年前老郭都吃不上饭了还坚持着，他后来说过，就是爱这行。同样，图像的基础很庞杂，综合学科，线性代数，概率论，微积分，信号与系统，数字信号处理，数论等一大堆涉及的知识，如果是真爱，慢慢来，一点点的学扎实，一定会有用武之地，虽然公司要硕士博士去做图像算法，但我相信，只要学的扎实，学的深入，会有人要我的\n空域滤波基础介绍 今天废话多了点，因为内容不是很多，滤波的概念其实是频域概念，即对信号频率进行处理，高于或低于截止频率的将被干掉，或者带通带限，就有了高通滤波器，低通滤波器。频域的相乘对应于时域的卷积，于是，空域滤波器（空间滤波器也叫卷积核，空间掩膜，核，模板，窗口等）和图像的卷积能达到和频域相同或相近的效果，所以我们要说先图像空域的卷积，值得注意的是空间滤波器只有线性滤波器和频域对应有关联，非线性滤波器在频域无法实现。\n空间滤波器组成：\n 一个邻域（典型的是以某一点为中心的矩形） 对该邻域做规定的运算，得到结果赋值到邻域中心位置，特别的，赋值到邻域对应的中心并不是原图上的，而是结果上的，因为本步计算的结果将不会影响下一步计算，所以原图像不能被改变  如果执行线性操作，将可以在频域找到对应操作，如果采用非线性操作，频域则无对应。一般掩膜使用奇数x奇数大小，因为这样中心为整数坐标： 线性计算如下，w为模板，f为图像： 示意图： 与卷积相似的计算，叫做相关，相关时模板从图像上划过，计算模板与图像每个位置对应的乘积并求和，赋值给中心位置，卷积类似但是要将模板旋转180°，如果将模板按照线性存储，简单的做饭就是将数组倒置，最后变最前。对于图像边缘的像素，卷积无法覆盖的地方采用外围补0的方法。 截个图，来解释下上面的话，或者看前面关于卷积的理解的博客： 需要注意一下几点：\n 卷积或相关时关于位移的函数 滤波模板w与只有一个1其他都是0的函数相关，得到一个旋转180°的拷贝，这个函数叫离散单位冲激。 如果模板关于原点对称，那么相关和卷积结果相同 滤波时使用卷积，相关可以用于模板匹配 卷积是线性系统计算的基础  接下来就是卷积模板的定义了，之前在频域中说过，一种线性模板可以是频域设计的然后ifft到空域，生成小模板，还有就是直接在空域设计，模板系数决定模板的用途，对于线性模板，唯一能做的是相乘后求和，所以计算方式规定了以后就只能有系数是可以设计的了。例如使用一个二维高斯函数生成模板。 空域中定义一个滤波器以后，一般下需要到频域观察下其是否有其他频率影响，例如对于一个平滑滤波器，我们应该观察下其频域特性，是否在高频部分有旁瓣。不过对于一个离散模板来说，只要有截断，就会产生旁瓣。\n空域滤波与频域比较 这里我们只比较算法的时间复杂度，因为之前我曾说过一道面试题，为什么使用空间滤波模板，而不是用频域滤波。下图给出频域计算过程和算法计算量： 最后得出的计算量是除去赋值操作，结果： $(4+8log2N)N^2$ 而空间卷积（相关）如果不加优化，使用最普通的算法，时间复杂度是 $N^2M^2$ （其中M是窗宽），所以当 $M^2\u0026lt;(4+8log2N)$ 时，频域滤波速度没有优势，更主要的一点是扩展图像以后引入的高频干扰，观察下图 填充后的图像边缘将引入高频分量，使频域滤波后的一周有高频干扰，如图： 原图： 上面是一副512x512的纯白图片 采用高斯高通处理后： 周围的泛白就是高频成分被遗留的结果。 特别的，如果空预处理高通时，改变周围的填充策略（例如复制边缘）将不会有这种效果。\n总结 总结，空域处理将占据处理的大部分，而频域更适合做分析，就像matlab一般用作模拟和实验算法，真正的工程中一般要用c，c++这些工具，所以，各有各的用途，分析和工程，都是重要的应用\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-0-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E7%A9%BA%E5%9F%9F%E6%BB%A4%E6%B3%A2%E5%9F%BA%E7%A1%80-%E5%8D%B7%E7%A7%AF%E5%92%8C%E7%9B%B8%E5%85%B3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第26天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 卷积，相关，空域滤波\u003c/p\u003e","title":"【数字图像处理】5.0:灰度图像-空域滤波 基础：卷积和相关"},{"content":"Abstract: 数字图像处理：第27天 Keywords: 灰度拉伸\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 昨天前天写了两天代码，写代码久了就不会写博客了，昨天前天把高斯滤波，中值滤波，中值滤波快速算法，双边滤波，均值滤波实现了一下，并且进行了测试，学到了不少调试的技巧，其中中值滤波快速算法调了好久，不过还不错的是，结果都正确了，并且和OpenCV的结果进行了比较，验证了算法实现的正确性，至于代码速度，感觉追求速度是每个程序设计人员都追求的，但目前我们属于学习算法阶段，所以我们应该先把功能实现，观察算法结果，当把大部分算法实现完成后，根据需要进行优化，和改进。我觉得这是一个正确的道路。\n知识结构 接下来几天的任务是图像增强，图像增强并没有严格数学上的定义，也就是没有说明处理后达到什么样的指标后算是完成，同样，增强是针对人的，如果你觉得处理后观察结果更显然了，那就是达到了增强的目的，增强分好多方法，大方向可以分为，灰度拉伸和空间滤波，灰度拉伸是简单的像素灰度到像素灰度的变换，（1）比如灰度值a经过f（a）（f个人给出）后得到了b，这样处理整幅图后看起来更能满足目的了，这也就算是增强了。（2）空间滤波利用的则是所处理像素周围像素的灰度，按照一定方法计算出该处的值，得到平滑或锐化的效果，这算是区域算法。（3）再有就是利用整幅图像的信息，如直方图，生成（1）中的f，然后对灰度进行计算。 大体知识结构给出：\n解释下图像增强和去噪的区别，增强属于无先验知识的处理，也就是说，拿来一幅图，我就可以通过增强算法进行处理，不需要知道噪声模型等其他信息，也就是说，系统输入就是一张图，按照相关参数和算法，给出一副更能满足观察者需求的图片。图像去噪则属于图像复原一类的知识，需要知道噪声模型，根据相关的先验知识进行针对性的处理，增强也有去噪的效果，但增强的目的并不是为了去噪，而是改善图像，降低观察者观察相关信息的难度，当然对于平滑图像来说，高频噪声被处理掉了，达到了一部分去噪的目的，这只是衍生的一种效果，根本目的不是降噪或修复图片，因为干掉噪声的同时，高频细节也被干掉了，而去噪只要求去除噪声修复图片，当然，增强也有降低噪声并保留边缘的需求，但增强和去噪是不同的，最好不要混淆。 因为平滑和锐化对应于频域就是低通和高通操作，所以频域相关的增强后面就不再描述，具体可参见前面的博客。\n总结 因为是综述性的介绍，所以本文较短，只想记录下路线，并解释下去噪和增强的关系。具体理论基础和相关代码实现请关注后续文章。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-1-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E7%BB%BC%E5%90%88%E4%BB%8B%E7%BB%8D.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第27天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 灰度拉伸\u003c/p\u003e","title":"【数字图像处理】5.1:灰度图像-图像增强 综合介绍"},{"content":"Abstract: 数字图像处理：第36天 Keywords: 直方图均衡\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，图像处理这些代码已经有三千多行了，不多，但是感觉多加练习以后对算法理解和写代码的能力上都有很大提高，毕竟对于算法来说想明白了一定要用一下才会真正掌握，但不能靠记忆去记住一个算法，这就需要我们懒人的天性，不愿意记住完整的公式，更愿意记住一个简单的起始，通过自己的理解和数学推导算法，这是个很不错的 方法，而写代码属于一种技术工作，熟能生巧，要多加练习，并且也要思考其中的技术细节，总之，做一切事情思考下还是不错的。 废话完成，说说直方图均衡，在冈萨雷斯的书里面直方图均衡化在第三章提出，因为之前想按照书上目录上的顺序来写着一些列的博客，后来发现还是自己总结下的学习思路，按照自己理解的知识网络来走，所以刚要写直方图均衡的时候就是转向自己的节奏开始按照二值图像，灰度图像，彩色图像的知识结构介绍。 直方图均衡的目的和前面灰度变换一样，为了增强对比度，使图像的灰度分布在整个灰度范围内更加均衡，其中直方图需要来解释下，直方图是个统计概念，比如我们有十种颜色的球，每种颜色的球有不同的数量，假设颜色分布为a0~a9，数量为n（x）（x取值为a0~a9）那么直方图就是以a0~a9为横坐标，n为纵坐标的统计直方图：\n如果将上图中的数据归一化（每个分量除以球数总和），也就是使得各分量总和为1，各个分量就表示这种颜色的球出现的频率，就得到频率直方图。 如果将各种颜色换成各灰度值，球的个数等效的换成具有该灰度的像素数量，或者换成该灰度出现的频率，就成了图像的直方图，对于彩色图像和灰度图像，直方图具有重要的统计意义，而对于二值图像来说，该意义不大，因为二值图像就两个灰度，所以其只能反映黑白面积比例。 直方图均衡的目的是为了使灰度分布的更广泛，从而来拉伸对比度：\n事实表明当灰度的直方图范围从上面的左边变换成右边后，图像对比度得到提升，也就达到了我们增强图像的目的\u0026ndash;更便于观察，更容易区分不同灰度间细节。\n数学原理 直方图变换的最终操作和前面提到的灰度操作是一样的，即： 这是一种从灰度到灰度的映射，并且该映射与前面伽马变换对数变换的映射不同的是，它不具有确定的表达公式，而是根据原始图像的灰度分布不同而“自适应”的产生映射，并且必须具有以下两个性质：\n 该函数必须单调递增（因为要从s反射回r所以该函数必须是严格的单调递增，即r和s为一对一的关系） $0\\leq r\\leq L-1$ 时，必须满足$0\\leq s \\leq L-1$ 这是两点约束，即我们找到的T必须使得上面成立，而且还要达到均衡直方图的目的。即完成下面的转化： 上面的是直方图的原始分布和目标分布，下面是响应的T。  推导过程： 先提出一个概率密度函数pdf的概念，就是每个灰度值对应出现的概率，用p表示，pr（r）表示原始灰度r出现的概率，其计算是用灰度值为r的像素总个数除以图像的像素总个数。同理变换后的ps（s）表示变换后灰度为s的像素的概率。\n r到s是一对一的映射，所以若r0映射到s0那么pr（r0）=ps（s0）这个式子是因为像素个数不会改变，只是对应的灰度值改变了，这个就是我们接下来要用到的最基本的原理。 我们的目标灰度分布是均匀分布，也就是上图右上的概率分布图，因为绿色部分面积必然为1，所以，ps的目标分布为： 根据上述的基本原理和假设存在： 根据积分定理，w为积分假变量，那么： 所以： 将上面离散化： 上面为大概的公式推导，如有不严谨之处还请指出。 所以我们将按照上面得出的结论进行编程：   代码 /******************************************************************************************** 直方图基本操作 *******************************************************************************************/ void InitMappingTable(void * arry,int size,int Data_type){ if(Data_type==TABLE_INT) for(int i=0;i\u0026lt;size;i++) ((int*)arry)[i]=0; else if(Data_type==TABLE_CHAR) for(int i=0;i\u0026lt;size;i++) ((char*)arry)[i]=0; else if(Data_type==TABLE_DOUBLE) for(int i=0;i\u0026lt;size;i++) ((double*)arry)[i]=0; } void InitHistogram(int *hist){ for(int i=0;i\u0026lt;GRAY_LEVEL;i++) hist[i]=0; } void setHistogram(double *src,int *hist,int width,int height){ InitHistogram(hist); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ int tempv=src[j*width+i]; hist[tempv]++; } } int findHistogramMax(int *hist){ for(int i=GRAY_LEVEL-1;i\u0026gt;=0;i--){ if(hist[i]!=0) return i; } return -1; } int findHistogramMin(int *hist){ for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ if(hist[i]!=0) return i; } return -1; } void fillMaptable(double * map){ for(int i=1;i\u0026lt;GRAY_LEVEL;i++){ if(map[i]==0) map[i]=map[i-1]; } } /******************************************************************************************** 直方图均衡 *******************************************************************************************/ //均衡直方图，将原图直方图，经过公式得到目标直方图 void EqualizationHist(int *src_hist,double *dst_map){ int temphist[GRAY_LEVEL]; InitHistogram(temphist); int max=findHistogramMax(src_hist); int min=findHistogramMin(src_hist); temphist[min]=src_hist[min]; for(int i=min+1;i\u0026lt;=max;i++) temphist[i]=temphist[i-1]+src_hist[i]; for(int i=max;i\u0026gt;=min;i--)//感谢pymess同学指正，之前的有问题  temphist[i]-=temphist[min]; int total=temphist[max]; for(int i=min;i\u0026lt;=max;i++){ dst_map[i]=((double)GRAY_LEVEL-1.0)*temphist[i]/total; } } //直方图均很，用输入图像得到输出图像 void HistogramEqualization(double *src,double *dst,int width,int height){ int hist[GRAY_LEVEL]; setHistogram(src, hist, width, height); double GrayMappingTable[GRAY_LEVEL]; InitMappingTable(GrayMappingTable,GRAY_LEVEL,TABLE_DOUBLE); EqualizationHist(hist, GrayMappingTable); for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++) dst[j*width+i]=GrayMappingTable[(int)src[j*width+i]]; } 结果 原图： 原图直方图： 直方图均衡后图片： 直方图均衡后直方图： 总结 上面给出的结果为经典结果，很多文章都使用的这幅图片，值得解释的是，虽然我们从r到s的映射是一对一的，但r和s是离散的整数，如果s被映射到非整数，将就近取整，所以有些灰度值会被合并。 直方图运算速度快，效果好，应用范围很广，故总结如上。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-10-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96histogramequalization.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第36天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 直方图均衡\u003c/p\u003e","title":"【数字图像处理】5.10:灰度图像--图像增强 直方图均衡化（Histogram Equalization)"},{"content":"Abstract: 数字图像处理：第37天 Keywords: 直方图匹配,直方图规定化,增强对比度\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 开篇废话，本文应该是图像增强部分的最后一篇，直方图匹配（规定化）通俗一点说，就是人为规定输出图像的直方图，根据上文的说的均衡化的推倒过程，其中我们设定输出直方图为 $\\frac{1}{L-1}$ 其实这就是规定化的，只是规定为一个常数，如果想要实现输出图像要根据输入的直方图产生，我们就需要使用直方图规定化，或直方图均衡，但如果直方图使用恒定，比如我们不想用常数，而是想用高斯，可以直接根据上文改一个高斯出来，这就省去了每次调用时都要人工产生直方图，更官方一点的话就是直方图匹配是直方图均衡的一般化，直方图均衡是直方图规定化的特例（当规定直方图为常数，例如都是1）。\n数学原理 看一下原理，直方图匹配使用了直方图均衡做中间环节，将原图直方图和目标直方图进行均衡，然后互射，从原始图像直接映射到目标直方图均衡的结果，然后根据目标直方图均衡的逆映射，得到目标灰度值，示意图如下： 上面的示意图完整的表示了整个算法过程：\n 计算原图的直方图Hr，输入目标直方图Hs 均衡Hr，Hs得到映射 $G(r)$ 和 $Z(s)$ 得到最终映射为 $Z^{-1}(G(r))$ 对于原图灰度r计算T为最终要得到的映射关系：  我们输入（规定）一个随机变量，具有如下性质： 根据上面两个式子，我们有： 那么就必须有： 这就是上面的算法过程的数学过程。 需要说明的是，实际操作因为离散的原因G(z)有可能不是满射的，也就是说G^-1可能会出现对应空值的情况，比如原始灰度a-\u0026gt;均衡后灰度b-\u0026gt;逆映射到目标是为空。为了防止这种情况产生大量0灰度结果，我们可以使用填充技术，如果逆映射为空，就用附近的灰度结果来填充。\n代码 /******************************************************************************************** 直方图基本操作 *******************************************************************************************/ void InitMappingTable(void * arry,int size,int Data_type){ if(Data_type==TABLE_INT) for(int i=0;i\u0026lt;size;i++) ((int*)arry)[i]=0; else if(Data_type==TABLE_CHAR) for(int i=0;i\u0026lt;size;i++) ((char*)arry)[i]=0; else if(Data_type==TABLE_DOUBLE) for(int i=0;i\u0026lt;size;i++) ((double*)arry)[i]=0; } void InitHistogram(int *hist){ for(int i=0;i\u0026lt;GRAY_LEVEL;i++) hist[i]=0; } void setHistogram(double *src,int *hist,int width,int height){ InitHistogram(hist); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ int tempv=src[j*width+i]; hist[tempv]++; } } int findHistogramMax(int *hist){ for(int i=GRAY_LEVEL-1;i\u0026gt;=0;i--){ if(hist[i]!=0) return i; } return -1; } int findHistogramMin(int *hist){ for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ if(hist[i]!=0) return i; } return -1; } void fillMaptable(double * map){ for(int i=1;i\u0026lt;GRAY_LEVEL;i++){ if(map[i]==0) map[i]=map[i-1]; } } /******************************************************************************************** 直方图归一化 *******************************************************************************************/ void HistogramSpecification(double *src,double *dst,int* hist,int width,int height){ int src_hist[GRAY_LEVEL]; setHistogram(src, src_hist, width, height); double srcMap[GRAY_LEVEL]; double histMap[GRAY_LEVEL]; InitMappingTable(srcMap,GRAY_LEVEL,TABLE_DOUBLE); EqualizationHist(src_hist, srcMap); EqualizationHist(hist, histMap); int histMap_[GRAY_LEVEL]; InitHistogram(histMap_); for(int i=0;i\u0026lt;GRAY_LEVEL;i++) histMap_[(int)histMap[i]]=i; double dstMap[GRAY_LEVEL]; for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ dstMap[i]=histMap_[(int)srcMap[i]]; } fillMaptable(dstMap); for(int i=0;i\u0026lt;width;i++) for(int j=0;j\u0026lt;height;j++) dst[j*width+i]=dstMap[(int)src[j*width+i]]; }  结果对比 原图： 原图直方图： 直方图匹配1： 目标直方图： 实际操作结果直方图： 直方图匹配2： 目标直方图： 实际操作结果直方图： 直方图匹配3： 目标直方图： 实际操作结果直方图： 直方图匹配4： 目标直方图： 实际操作结果直方图： 总结 直翻图匹配交直方图均衡使用更灵活，更能控制输出的灰度特性，主要优点就是更加自由可以自己设计目标，所以应用范围交直方图均衡更加广泛，当目标直方图设计为常数是，直方图匹配就是直方图均衡。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-11-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%8C%B9%E9%85%8D-%E8%A7%84%E5%AE%9A%E5%8C%96-histogramspecification.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第37天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 直方图匹配,直方图规定化,增强对比度\u003c/p\u003e","title":"【数字图像处理】5.11:灰度图像-图像增强 直方图匹配（规定化）Histogram Specification\""},{"content":"Abstract: 数字图像处理：第28天 Keywords: 平滑，均值滤波，高斯平滑\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 今天的废话是，我早上来了开始写博客，写了大概和下文差不多的内容，写了很多，就在发表以后，我震惊了，博客是空的，没有内容，我就表示呵呵了，不知道是网速的问题还是CSDN的问题，总之，我就不说啥了。 今天的内容是是平滑，先介绍均值和高斯平滑，高斯平滑是均值平滑的一个扩展，或者是一个进化版本。均值的原理是，一个规定的邻域内，所有像素的平局值作为最终计算的结果，每个像素的权值相同，为总像素的倒数，而高斯平滑是上述的升级版本，邻域内每个像素的权值不同，而权值是由高斯函数确定的。 均值平滑和高斯平滑都是线性的，也就是，一旦参数给定，模板就确定下来，不会因为位置和像素分布不同而改变，而线性模板的基本运算是卷积。 线性模板的另一个性质就是可以进行频域分析，比如高斯平滑的频域仍然是高斯的，而且是低通的，这与前面讲到的平滑是消除尖锐的噪声（高频）的操作相互证明了其正确性，均值平滑是一个盒状滤波器，其频域为sinc函数，也是一个近似的低通，但sinc函数有旁瓣，所以，模板宽度选择不好可能会有一些不好的效果，比如有些高频会被保留，这一点也比较好理解。 比较下两种均值（加权和不加权）。比如一维的一个序列 ${0，0，0，0，0，1000，0，0，0，0}$ ，明显1000是个边缘，如果使用3个宽度的均值平滑，结果是 ${0，0，0，0，333，333，333，0，0，0}$ ，边缘被完全模糊掉了。但如果使用 ${1，2，1}$ 的近似高斯平滑模板，结果是 ${0，0，0，0，250，500，250，0，0，0}$ ，边缘被保留。所以，加权平均（高斯）可以保留一定的细节。 对于设计的线型滤波器，其效果可以先是由傅里叶变换，到频域进行观察，便可大致推测出其效果，测试图片（灰度输入）： 均值滤波 数学 基本数学原理公式，以3x3为例： 得到滤波模板： 应用于图像的计算公式： 上式中的w(s,t)恒等于1。\n效果 观察下用3x3，5x5和7x7的均值模板与图像卷积的结果： 3x3模板： 5x5模板： 7x7模板： 可以得出结论，均值滤波的模板越大，图像越模糊。\n代码 void MeanMask(double *mask,int width,int height){ double w=width; double h=height; double meanvalue=1.0/(w*h); for(int i=0;i\u0026lt;width*height;i++) mask[i]=meanvalue; } void MeanFilter(IplImage *src,IplImage *dst,int width,int height){ double * pixarry=(double *)malloc(sizeof(double)*src-\u0026gt;width*src-\u0026gt;height); double * dstarry=(double *)malloc(sizeof(double)*src-\u0026gt;width*src-\u0026gt;height); double * mask=(double *)malloc(sizeof(double)*width*height); for(int j=0;j\u0026lt;src-\u0026gt;height;j++){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++){ pixarry[j*src-\u0026gt;width+i]=cvGetReal2D(src, j, i); } } MeanMask(mask, width, height); RealRelevant(pixarry,dstarry,mask,src-\u0026gt;width,src-\u0026gt;height,width,height); for(int j=0;j\u0026lt;src-\u0026gt;height;j++){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++){ cvSetReal2D( dst,j,i,dstarry[j*src-\u0026gt;width+i]); } } free(pixarry); free(dstarry); free(mask); } 这个代码并不是最快速的，只是为了和高斯平滑相互保持一致。所以，如果要使用，需要先优化。\n高斯滤波 高斯平滑，就是一种加权的均值，权值由高斯函数确定。\n数学 高斯函数： 在三维中的形状如下： 生成上图的matlab代码：\n% 公式： p(z) = exp(-(z-u)^2/(2*d^2)/(sqrt(2*pi)*d) X = 0 : 1 : 100; Y = 0 : 1: 100; % 方差 d= 49; Z = zeros(101, 101); for row = 1 : 1 : 101 for col = 1 : 1 : 101 Z(row, col) = (X(row) - 50) .* (X(row)-50) + (Y(col) - 50) .* (Y(col) - 50); end end Z = -Z/(2*d); Z = exp(Z) / (sqrt(2*pi) * sqrt(d)); surf(X, Y, Z); 当坐标（u，v）远离原点时，函数值越小。这与概率的知识相符，越远离中心（原点）的像素灰度值，对中心像素灰度值的相关性越小，所以被赋予的权值越小。\n效果 观察效果，使用不同标准差，和模板大小的结果： 结论：\n 同等模板大小，标准差越大越模糊 标准差相同，模板越大图像越模糊。  代码 void GaussianMask(double *mask,int width,int height,double deta){ double deta_2=deta*deta; int center_x=width/2; int center_y=height/2; double param=1.0/(2*M_PI*deta_2); for(int i=0;i\u0026lt;height;i++) for(int j=0;j\u0026lt;width;j++){ double distance=Distance(j, i, center_x, center_y); mask[i*width+j]=param*exp(-(distance*distance)/(2*deta_2)); } double sum=0.0; for(int i=0;i\u0026lt;width*height;i++) sum+=mask[i]; for(int i=0;i\u0026lt;width*height;i++) mask[i]/=sum; } void GaussianFilter(IplImage *src,IplImage *dst,int width,int height,double deta){ double * pixarry=(double *)malloc(sizeof(double)*src-\u0026gt;width*src-\u0026gt;height); double * dstarry=(double *)malloc(sizeof(double)*src-\u0026gt;width*src-\u0026gt;height); double * mask=(double *)malloc(sizeof(double)*width*height); for(int j=0;j\u0026lt;src-\u0026gt;height;j++){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++){ pixarry[j*src-\u0026gt;width+i]=cvGetReal2D(src, j, i); } } GaussianMask(mask, width, height, deta); RealRelevant(pixarry,dstarry,mask,src-\u0026gt;width,src-\u0026gt;height,width,height); for(int j=0;j\u0026lt;src-\u0026gt;height;j++){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++){ cvSetReal2D( dst,j,i,dstarry[j*src-\u0026gt;width+i]); } } free(pixarry); free(dstarry); free(mask); } 总结 之前写了很多结论，现在都忘完了，因为理论很简单，主要是观察结果，线性平滑基本就介绍这些，下篇介绍非线性的。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-2-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%B9%B3%E6%BB%91%E4%B9%8B%E5%9D%87%E5%80%BC%E6%BB%A4%E6%B3%A2-%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第28天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 平滑，均值滤波，高斯平滑\u003c/p\u003e","title":"【数字图像处理】5.2:灰度图像-图像增强 平滑之均值滤波、高斯滤波"},{"content":"Abstract: 数字图像处理：第29天 Keywords: 双边滤波\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，话说昨天写博客写完了，发表以后居然刷出来的是空白，顿时很生气，因为写了一上午的东西瞬间就没了，于是在微博上吐槽了csdn，于是csdn的官方微博和客服微博都跟我进行了沟通和道歉，感觉态度还是不错的，作为用户没有付给他们钱，但还是受到了不小的重视，感觉还是不错。学习是一个被分享之后经过自己加工后再分享的过程，一个分享的平台是很重要的选择，好的平台能够学到知识，并能分享知识，和别人讨论知识，收集资源分享资源。希望大家共同进步。 图像增强，平滑第二天，虽然说是第二天，但要学习和研究包括写程序，都不是一天完成的。上一篇写的是线性滤波模板，此类模板我们可以叫他们静态模板，因为其只依赖于我们的选择，我们一旦选择完成，模板就唯一确定，不会在卷积的过程中产生变换，所以这类模板具有线性性质，但缺点是不灵活，不能根据不同灰度变化情况来实时的调整权重，双边滤波就是一种非线性模板，能够根据像素位置和灰度差值的不同产生不同的模板，得到不同的滤波结果。\n基本思路 双边滤波器是针对高斯平滑的提升版本，高斯平滑根据像素邻域的距离决定权重，生成权重的函数为高斯函数，所以叫高斯平滑或者高斯滤波，效果是使图像模糊，并一定程度上的保存边缘，双边滤波的改进是增加了灰度值的影响，也就是邻域的像素灰度值如果和中心像素的灰度值越接近，那么权值在高斯权值的基础上在加上一个相对较大的权值，相反，如果灰度差很大，将会给已生成的高斯模板对应的位置加上一个小的权值，以此类推，并将模板系数归一化（和为1，其目的是完全平滑的图像结果不变），因此模板的系数不再单纯的依赖位置关系，更依赖于灰度关系，因此边缘将能够被有效的保存。\n数学基础 数学开始，数学公式可能看的比较难懂，但是懂了以后就会彻底理解整个算法，上面的描述只需要几个公式就能准确的表示出来。 来看基础版本的公式，这个公式通用均值滤波和高斯滤波： 上式中：将上式中的积分脑补成求和，求和范围是模板覆盖的范围，就代表模板的坐标位置（x，y），X就可以表示参数，比如高斯里的标准差参数。1/kd(x)为归一化参数，保证绝对平滑的位置灰度值不变，c的选择比较灵活，如果选择高斯函数，那么就产生了高斯滤波模板。 上面是只利用位置（距离）产生模板系数的公式。下面我看一下只利用灰度信息产生一个模板（非线性）： f表示灰度分布，也就是模板内被覆盖的灰度值，x（向量）代表原点位置，向量代表当前求和位置，同样积分改成求和，求和区间为模板覆盖的区间，同样1/kr（x）是归一化参数，保证平滑图片的不变性。 根据上面的描述，函数c只根据位置来平滑，平滑效果好，但边缘保存弱，函数s只根据灰度差值产生模板，边缘保存效果好，平滑效果差。 为了得到一个边缘保持性好，同时平滑能力强的方法，我们决定将他们合体： 这个公式总和了上面两种处理方法，同时根据距离和灰度差值产生模板系数，产生了一种新的非线性模板，归一化的k计算如下： 对于图像，脑补成求和而不是积分，这个就是Bilateral Filter的形式，也就是说，c和s并没有规定为高斯函数，如果你有更好的，可以自己开发，当然最一般的情况下，c和s我们选择高斯函数： 其中d表示距离，这里用欧氏距离来计算（欧氏距离就是初中学的最简单的那个）： $\\sigma_d$为距离的标准差，由我们手动决定，但要注意的是，标准差对于一个高斯函数来说，决定的是它的“胖瘦”，也就是图形是宽还是窄，如果过窄，其中心权重接近1，其他权重会很小，极限情况下退化成冲击，则只有中心位置元素，如果标准差选择过大，高斯函数会过胖，也就是趋于一条直线，这时高斯平滑接近于均值，每个位置权重过于接近，并且，距离超过3倍的deta，那么权重也会很小，所以这个性质在我们选择参数和后面观察结果也是很有用的参考。 同理我们选择高斯函数作为s函数： 式子中灰度差为： 也就是模板内不同位置的灰度与中心灰度的差。 观察下处理结果，英文不难，不再翻译： 来看原始论文的效果： 主要观察点在猫咪的胡须，看到即使右下角特别模糊的情况下，猫咪的胡须还是可以识别出来的。\n代码 与前面文章同样，代码未经过优化，知识原始公式的翻译，如果应用于工程，需要使用快速算法或将算法进行优化：\n//高斯函数 double gaussian(double x,double deta){ return exp(-0.5*(x*x)/(deta*deta)); } //计算当前模板系数 double BilateralWindow(double *window,int width,int height,double deta_d,double deta_r){ double *mask=(double *)malloc(sizeof(double)*width*height); if(mask==NULL){ printf(\u0026#34;bilateral window malloc wrong\\n\u0026#34;); exit(0); } GaussianMask(mask,width,height,deta_d); double detavalue=0.0; double center_value=window[height/2*width+width/2]; double k=0.0; double result=0.0; for(int j=0;j\u0026lt;height;j++){ for(int i=0;i\u0026lt;width;i++){ detavalue=center_value-window[j*width+i]; mask[j*width+i]*=gaussian(detavalue,deta_r); k+=mask[j*width+i]; } } for(int i=0;i\u0026lt;width*height;i++){ result+=mask[i]*window[i]; } free(mask); return result/k; } //双边滤波 void BilateralFilter(IplImage *src,IplImage *dst,int width,int height,double deta_d,double deta_r){ double *window=(double *)malloc(sizeof(double)*width*height); for(int j=height/2;j\u0026lt;src-\u0026gt;height-height/2;j++){ for(int i=width/2;i\u0026lt;src-\u0026gt;width-width/2;i++){ for(int m=-height/2;m\u0026lt;height/2+1;m++){ for(int n=-width/2;n\u0026lt;width/2+1;n++) window[(m+height/2)*width+n+width/2]=cvGetReal2D(src, j+m, i+n); } double value=BilateralWindow(window,width,height,deta_d,deta_r); cvSetReal2D(dst, j, i, value); } } free(window); } 观察效果 下面来观察我们的效果，具体参数已经标在了图像上：\n观察结论：$\\sigma_d$（距离标准差）越大会导致图像更加模糊，因为使用高斯函数，$\\sigma_r$（灰度标准差）越大会导致细节变得更模糊，所以可以根据3倍 $\\sigma$ 原则来选取合适的模板大小和deta大小，灰度差范围-255到255，距离差范围根据模板大小确定。\n总结 双边滤波，可以很好的保存边缘并产生平滑效果，比高斯滤波和均值滤波效果更好，但计算量也更大 参考论文：\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-3-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A2bilateralfiltering.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第29天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 双边滤波\u003c/p\u003e","title":"【数字图像处理】5.3:灰度图像-图像增强 双边滤波 Bilateral Filtering"},{"content":"Abstract: 数字图像处理：第30天 Keywords: 图像平滑，中值滤波\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 开篇废话是中值滤波原理和基础代码都很好理解和编写，但是快速算法有点不好写，上周日看了下算法大概的思路然后就开始写，写了一天也不好，周一又调试了一上午，后来发现整个算法其实原理上没有任何难点，只是自己还没完全清楚每个细节就开始写代码，而且算法又是迭代的，一步有问题后面结果都不对，所以，下次再实现算法的时候，要先想一段时间，然后再实现，这样不仅不浪费时间反而可以节约时间，并且不会受到打击。\n中值滤波介绍 中值滤波时典型的非线性方法，与前面介绍的方法不同，中值滤波更接近于灰度图像的腐蚀和膨胀，是在一定区域内比较大小，找出中值，也就是排序后中间那个数，也就是中学的中位数，平均数用于均值滤波，中位数用于中值滤波，要是专家就可以写本书：统计学在图像处理中的二三事（这句话属于扯淡）。 中值滤波会产生对原始图像的人为因素破坏，所以在医疗成像等对人为因素引起误差不能够接受的时候不能是用中值滤波。 中值滤波对椒盐噪声和斑点噪声效果显著，而且中值滤波具有较好的边缘保持特性，所以在图像处理中知名度很高。\n数学原理 中值滤波的数学公式就是: $$ g(x,y)=Median{f(x-m/2,y-n/2)\\dots f(x+m/2,y+n/2)} $$ 翻译成自然语言就是在当前模板覆盖范围内需找中位数作为结果。 此计算中涉及到排序，所以，如果使用比较的方法排序，最快速度的复杂度是 $O(m\\times n\\times log(m\\times n)\\times W\\times H)$ ，如果使用非比较型排序，算法最大时间复杂度是 $255\\times W\\times H$ 也就是 $O(W\\times H)$ 但要使用更多的存储空间，最直观的方法是使用计数排序，建立一个255大小的空间，或者理解为一个直方图，来查找中值。 快速方法是使用计数排序，但存在一个类似于游标的指针，指向当前的中值，并记录当前模板覆盖范围内小于中值的数据的个数，当模板滑动的时候，观察移出数据和移入数据对小于中值个数的影响确定移动游标的方向，以此来减少直方图搜索范围，降低运算量。\n快速算法  初始化 $T=模板覆盖区域元素个数/2$ 首先，初始化直方图，将该行第一组模板覆盖的元素排序，找出中值 mid_value ，记录小于此中值的元素个数c。 移出模板覆盖区域最左侧一列元素，对于每一个元素如果小于 mid_value ，$c=c-1$ ; 移入模板覆盖外最右侧一列元素（相当于模板右移）， 对于每一个元素如果小于 mid_value ，$c=c+1$ ; 比较c和T的大小：   如果 $c\u0026lt;T$ :从 mid_value 开始，包括mid_value，向更大的方向检索，如果有搜索到元素，c加上对应的个数，直到 $c\\geq T$ ，当前元素为新的 mid_value； 如果 $c==T$ ：从mid_value开始，包括mid_value，向更大的方向检索，如果有搜索到元素，该元素为新的mid_value; 如果 $c\u0026gt;T$ ：从mid_value开始，包括mid_value，向更小的方向检索，如果有搜索到元素，c减去对应的个数，直到c\u0026lt;=T，当前元素为新的mid_value，c的值加上新mid_value的个数；  如果模板右边无数据，到下一行，回到第2步否则回到第3步，继续；  原型算法代码 //以下为低速普通中值滤波，排序使用计数排序  void initHist(int *hist,int size){ for(int i=0;i\u0026lt;size;i++) hist[i]=0; } int sort(int *src,int size){ int hist[GRAY_LEVEL]; int m=0; initHist(hist, GRAY_LEVEL); for(int i=0;i\u0026lt;size;i++) hist[src[i]]++; for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ m+=hist[i]; if(m\u0026gt;size/2) return i; } return 0; } void MedianFilter(IplImage *src,IplImage *dst,int width,int height){ IplImage* temp=cvCreateImage(cvSize(src-\u0026gt;width+width, src-\u0026gt;height+height), src-\u0026gt;depth, src-\u0026gt;nChannels); IplImage* dsttemp=cvCreateImage(cvSize(src-\u0026gt;width+width, src-\u0026gt;height+height), src-\u0026gt;depth, src-\u0026gt;nChannels); cvZero(temp); for(int j=0;j\u0026lt;src-\u0026gt;height;j++){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++){ double value=cvGetReal2D(src, j, i); cvSetReal2D(temp, j+height/2, i+width/2, value); } } int *window=(int *)malloc(sizeof(int)*width*height); if(window==NULL){ printf(\u0026#34; \u0026#34;); exit(0); } for(int j=height/2;j\u0026lt;temp-\u0026gt;height-height/2-1;j++){ for(int i=width/2;i\u0026lt;temp-\u0026gt;width-width/2-1;i++){ for(int n=-height/2;n\u0026lt;height/2+1;n++) for(int m=-width/2;m\u0026lt;width/2+1;m++){ window[(n+height/2)*width+m+width/2]=cvGetReal2D(temp, j+n, i+m); } double pix=sort(window,width*height); cvSetReal2D(dsttemp, j, i, pix); } } for(int j=height/2;j\u0026lt;temp-\u0026gt;height-height/2-1;j++){ for(int i=width/2;i\u0026lt;temp-\u0026gt;width-width/2-1;i++){ double value=cvGetReal2D(dsttemp, j, i); cvSetReal2D(dst, j-height/2, i-width/2, value); } } free(window); } 快速算法代码 int findMedian(int *hist,int *movein,int *moveout,int movesize,int *cursor,int median,int t){ for(int i=0;i\u0026lt;movesize;i++){ hist[movein[i]]++; hist[moveout[i]]--; if(movein[i]\u0026lt;median) (*cursor)++; if(moveout[i]\u0026lt;median) (*cursor)--; } if((*cursor)\u0026lt;t){ for(int i=median;i\u0026lt;GRAY_LEVEL;i++){ (*cursor)+=hist[i]; if(*cursor\u0026gt;=t+1){ (*cursor)-=hist[i]; return i; } } }else if((*cursor)\u0026gt;t){ for(int i=median-1;i\u0026gt;=0;i--){ (*cursor)-=hist[i]; if(*cursor\u0026lt;=t){ return i; } } } else if ((*cursor)==t){ for(int i=median;i\u0026lt;GRAY_LEVEL;i++){ if(hist[i]\u0026gt;0) return i; } } return -1; } //初始化一行 int InitRow(IplImage *src,int *hist,int row,int *cursor,int win_width,int win_height){ int t=win_width*win_height/2+1; *cursor=0; for(int i=0;i\u0026lt;GRAY_LEVEL;i++) hist[i]=0; for(int j=-win_height/2;j\u0026lt;win_height/2+1;j++) for(int i=0;i\u0026lt;win_width;i++){ int pixvalue=cvGetReal2D(src, j+row, i); hist[pixvalue]++; } for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ *cursor+=hist[i]; if(*cursor\u0026gt;=t){ *cursor-=hist[i]; return i; } } return -1; } void MedianFilter(IplImage *src,IplImage *dst,int width,int height){ int hist[GRAY_LEVEL]; int median; int *movein=(int *)malloc(sizeof(int)*height); int *moveout=(int *)malloc(sizeof(int)*height); double *dsttemp=(double *)malloc(sizeof(double)*src-\u0026gt;width*src-\u0026gt;height); int t=width*height/2; for(int j=height/2;j\u0026lt;src-\u0026gt;height-height/2-1;j++){ int cursor=0; median=InitRow(src, hist, j, \u0026amp;cursor, width, height); dsttemp[j*src-\u0026gt;width+width/2]=median; for(int i=width/2+1;i\u0026lt;src-\u0026gt;width-width/2-1;i++){ for(int k=-height/2;k\u0026lt;height/2+1;k++){ movein[k+height/2]=cvGetReal2D(src, j+k, i+width/2); moveout[k+height/2]=cvGetReal2D(src, j+k, i-width/2-1); } median=findMedian(hist, movein, moveout, height, \u0026amp;cursor, median, t); dsttemp[j*src-\u0026gt;width+i]=median; } } for(int j=0;j\u0026lt;src-\u0026gt;height;j++){ for(int i=0;i\u0026lt;src-\u0026gt;width;i++){ cvSetReal2D(dst, j, i, dsttemp[j*src-\u0026gt;width+i]); } } free(dsttemp); free(movein); free(moveout); } 效果 来观察下lena图矩阵原版的中值滤波结果： 原图数据： 我们的慢速结果： 我们的快速结果： OpenCV的结果： 下面看加了椒盐噪声的lena图的中值滤波和高斯滤波的效果： 3x3中值： 3x3高斯： 5x5中值： 5x5高斯： 7x7中值： 7x7高斯： 观察结果：对于椒盐噪声影响严重的图片，中值滤波效果远远好于高斯滤波，中值滤波的模板越大图像被模糊的越严重\n总结 图像增强基础的平滑算法就介绍到这里，下一篇开始介绍锐化相关\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-4-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E4%B8%AD%E5%80%BC%E6%BB%A4%E6%B3%A2.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第30天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 图像平滑，中值滤波\u003c/p\u003e","title":"【数字图像处理】5.4:灰度图像-图像增强 中值滤波"},{"content":"Abstract: 数字图像处理：第31天 Keywords: 锐化\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 这篇作为基础篇，所以废话不多，但感觉很多人对锐化有误解，尤其是和边缘提取，其实这两个概念完全是两回事，当然有相关的地方，下面详细说说自己的理解。\n锐化的理解 首先说说锐化，锐化是图像增强的一部分，前面说过了，增强的目的是使观察者看起来更容易识别某些模式，要观察的模式从频率域来分就有低频模式和高频模式，低频模式，也就是之前一直在讲的相对变化缓慢的部分，或者根本没有灰度变化的大片区域，高频部分，就是接下来讲的，图像的细节，细节的定义不知道是什么，但边界，轮廓，一些变化强烈的部分算是细节，当然噪声也被归类到了细节，但细节的具体定义我还不知道，是否有相关算法能够对细节进行定义，还不知道。锐化的目的就是在图像中强调这些细节，而不是提取这些细节。也就是说对原图中的细节进行一定的提升，使得图像对于观察者来说更容易识别或者看起来更舒服。 根据上面的解释，我们就能得到图像锐化的一般步骤：\n因为要突出细节，所以第一步就是找到细节，至于如何找到细节，冈萨雷斯书中给出了三种方法，一阶微分，二阶微分，非锐化掩蔽（unsharpen mask）。 突出细节的方法：将提取的细节，或细节乘以预定的系数后与原图像相加（或相减）。这样就得到了锐化后的图像。\n数学基础 一阶微分 因为图像是离散信号，所以其一阶微分并不像连续函数按照极限方式的定义，而且对数字函数的一阶微分定义有不同理解，但所有的定义都满足一下三点：\n 恒定灰度值区域一阶微分为零 灰度台阶或者灰度斜坡起始处一阶微分非零 在灰度斜坡上一阶微分非零  对于函数f(x)的一阶微分或导数，将展开为关于x的泰勒级数，令，只保留泰勒级数的线性项结果为数字差分： 使用偏导数是为了与图像函数对应，因为图像函数是二维函数，显然当函数只有一个变量的时候：于是我们得到一阶微分的基本定义是差值。\n二阶微分 对于二阶微分，我们对上面式子关于x再次求导，得到二阶导数表达式： 上面第二行用到了一阶微分的定义，所以得到了二阶微分的表达式。 与一阶微分类似，二阶微分也必须保证下面几点\n 在恒定灰度区域为零 在灰度台阶和灰度斜坡起始处为非零 沿灰度斜坡微分为零 看一个一维的例子： 上面是一幅图的一行数据，数据中包含了斜坡，孤立点，线条，和台阶，可以看到下面对应的原始数据，一阶导数，二阶导数，一阶导数是原始数据的差，二阶导数是一阶导数的差。当然减数和被减数的顺序可以从左到右也可以从右到左。 可以对照上面的实际数据，证明必须要满足的三点都成立。所以我们可以用此来定义一阶和二阶微分。 上面数据中，我们可以看到在斜坡处，二阶微分会出现一个零交叉，这对于边界定位来说是非常有用的，因为在图像中边缘一般情况下是以斜坡方式存在，所以，一阶微分给出的边缘会相对较粗，二阶微分给出较准确的位置，也正是因为这一点，二阶微分比一阶微分在锐化细节方面表现更好。  总结 因为以上为纯理论知识，所以今天做一个简单的总结，相关的实验和图像处理结果会在后面的相关算法介绍中给出。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-5-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E9%94%90%E5%8C%96%E5%9F%BA%E7%A1%80.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第31天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 锐化\u003c/p\u003e","title":"【数字图像处理】5.5:灰度图像-图像增强 锐化基础"},{"content":"Abstract: 数字图像处理：第32天 Keywords: 拉普拉斯算子\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 今天的废话是，锐化和后面的分割有很大的关系，所以决定把图像增强总结完以后开始说分割，分割中有很多又去的课题，比如边缘提取，形状识别等，具有挑战性，图像增强除了平滑和锐化还有灰度变换，但在很早以前已经写过灰度变换的一些例子了，所以，后面只简要的写下原理，周一就可以开始研究分割了，每当结束一个大分支的时候总有一种踏上新的征途的感觉，但不得不承认，之前所学习的一切只是基础知识，如果想深入的话可能要花费很长时间，尤其是图像处理这么复杂的多学科领域，新技术日新月异，但只要打好基础，一定会在以后从事的领域中大展拳脚，希望与因为爱好而从事图像处理，计算机视觉的攻城狮们共同进步。\n数学基础 拉普拉斯算子，二阶微分线性算子，为什么上来就学二阶微分算子，前文说过，与一阶微分相比，二阶微分的边缘定位能力更强，锐化效果更好，所以我们来先学习二阶微分算子，使用二阶微分算子的基本方法是定义一种二阶微分的离散形式，然后根据这个形式生成一个滤波模板，与图像卷积。 各向同性滤波器，图像旋转后响应不变，这就要求滤波模板自身是对称的，如果不对称，结果就是，当原图旋转90°时，原图某一点能检测出细节（突变）的，现在却检测不出来，这就是各向异性的原因。我们更关心的是各向同性滤波模板，对图像的旋转不敏感。 对于二维图像 $f(x,y)$ ，二阶微分最简单的定义\u0026ndash;拉普拉斯算子定义为：\n对于任意阶微分算子都是线性算子，所以二阶微分算子和后面的一阶微分算子都可以用生成模板然后卷积的方式得出结果。 根据前面对二阶微分的定义有： 根据上面的定义，与拉普拉斯算子的定义相结合，得到： 也就是一个点的拉普拉斯的算子计算结果是上下左右的灰度的和减去本身灰度的四倍。同样，可以根据二阶微分的不同定义，所有符号相反，也就是上式所有灰度值全加上负号，就是-1，-1，-1，-1，4。但要注意，符号改变，锐化的时候与原图的加或减应当相对变化。上面是四邻接的拉普拉斯算子，将这个算子旋转45°后与原算子相加，就变成八邻域的算子了，也就是一个像素周围一圈8个像素的和与中间像素8倍的差，作为拉普拉斯计算结果。 因为要强调图像中突变（细节），所以平滑灰度的区域，无响应，即模板系数的和为0，也是二阶微分必备条件。 最后的锐化公式： g是输出，f为原始图像，c是系数，也就是要加上多少细节的多少，与上一篇的锐化过程是一致的，先提取细节，然后再加（或者减去负细节）到原图中。 得到滤波模板，此模板尺寸恒定，就是3x3，不像平滑模板，尺寸可变： 代码 void Laplace(double *src,double *dst,int width,int height,int mask_type){ double LaplaceMask0[9]={0,1,0,1,-4,1,0,1,0}; double LaplaceMask1[9]={1,1,1,1,-8,1,1,1,1}; double LaplaceMask2[9]={0,-1,0,-1,4,-1,0,-1,0}; double LaplaceMask3[9]={-1,-1,-1,-1,8,-1,-1,-1,-1}; switch(mask_type){ case SHARPEN_LAP_0: RealRelevant(src, dst, LaplaceMask0, width, height, LAPLACE_MASK_SIZE,LAPLACE_MASK_SIZE); break; case SHARPEN_LAP_1: RealRelevant(src, dst, LaplaceMask1, width, height, LAPLACE_MASK_SIZE,LAPLACE_MASK_SIZE); break; case SHARPEN_LAP_2: RealRelevant(src, dst, LaplaceMask2, width, height, LAPLACE_MASK_SIZE,LAPLACE_MASK_SIZE); break; case SHARPEN_LAP_3: RealRelevant(src, dst, LaplaceMask3, width, height, LAPLACE_MASK_SIZE,LAPLACE_MASK_SIZE); break; default: printf(\u0026#34;wrong mask type\\n\u0026#34;); matrixCopy(src, dst, width, height); break; } } void LaplaceSharpen(double *src,double *dst,int width,int height,int mask_type,double c){ Laplace(src,dst,width,height,mask_type); matrixMultreal(dst,dst,c,width,height); switch(mask_type){ case SHARPEN_LAP_0: case SHARPEN_LAP_1: matrixSub(src,dst,dst,width,height); break; case SHARPEN_LAP_2: case SHARPEN_LAP_3: matrixAdd(src,dst,dst,width,height); break; default: printf(\u0026#34;wrong mask type\\n\u0026#34;); matrixCopy(src, dst, width, height); break; } } 结果 来看对一副月球图片的锐化效果，分别使用四种模板，其结果是四邻接的两个模板结果相同，八邻接的结果相同，观察两个不同的系数，分别c=0.5和c=0.8，具体系数已经标注在图片上了： 原图： 原图灰度图像： 细节提取： 锐化图像： 总结 总结就是，二阶微分拉普拉斯算子对图像的锐化效果很不错，但对噪声敏感，后面介绍非锐化掩蔽，和一阶微分算子\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-6-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%AE%97%E5%AD%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第32天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 拉普拉斯算子\u003c/p\u003e","title":"【数字图像处理】5.6:灰度图像--图像增强 拉普拉斯算子"},{"content":"Abstract: 数字图像处理：第33天 Keywords: 非锐化掩蔽(USM)\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，今天写了两篇博客，为了加快学习进度，而且是周末，相当于给自己加个班，而且理论之前已经研究明白了，所以写起来也比较自如。有人在群里问，学习图像处理，要看书还是要写代码还是要作项目，我觉得，看书像是内功心法，写代码相当于招式练习，项目就像实战一样，所以如果只写代码或者一上来就去跟别人做项目而完全不看书研究基础算法就像空中楼阁，美丽但不牢固，更重要的是容易迷失自己，也就是走火入魔，个人观点，本人也在基础练习阶段，所以说这些没什么经验依据，只是自己的理解。 非锐化掩蔽，一开始感觉这个词好难接受，不知道要干嘛，google之发现线索不多，经过一番研究发现这个词这么理解：非锐化\u0026ndash;锐化的相反的操作是平滑，所以非锐化就是平滑操作；掩蔽\u0026ndash;字面意思是隐藏，其实我们可以把它理解成为减去除去，所以这个过程就是减去平滑后的图像得到的结果。而实际算法的思路是，原图减去平滑后的图像，得到被削弱的边缘部分，然后按照一定比例和原图相加，如果比例为1，那么就是非锐化掩蔽，如果大于1就是高提升滤波，和前面频率域的高提升，高频强调思路一致，只是那部分用的是频率域方法。\n数学原理 数学原理与前面的锐化原理基本保持一致，只是在确定细节的方法上有些不同：\n 生成模板，$\\bar{f}$ 表示 $f$ 的平滑结果：  钝化模板按照一定比例与原图相加： 其中 $k=1$ 时为非锐化掩蔽。 $k\u0026gt;1$时为高提升滤波。  示意图 观察示意图： 算法基本步骤：\n 平滑图像，边缘和突变被消减，而平滑部分不变（此处不适合采用边缘保持性好的平滑算法，代码中使用了均值滤波和高斯滤波）。 原图像与平滑后图像相减，得到钝化模板。 钝化模板的k倍与原图相加得到锐化结果。 值得注意的是，因为钝化模板有负值，所以，如果k选择过大的时候有可能图像产生负值   代码 void UnsharpMasking(double *src,double *dst,int width,int height,int smooth_type,int smooth_mask_width,int smooth_mask_height,double gaussian_deta,double k){ switch (smooth_type) { case SMOOTH_GAUSSIAN: GaussianFilter(src, dst,width,height, smooth_mask_width, smooth_mask_height,gaussian_deta); break; case SMOOTH_MEAN: MeanFilter(src, dst,width,height, smooth_mask_width, smooth_mask_height); break; default: break; } matrixSub(src, dst, dst, width, height); matrixMultreal(dst, dst, k, width, height); matrixAdd(src, dst, dst, width, height); } 结果 实验结果，分别采用高斯滤波和均值滤波，作为平滑算法。 原图： 高斯滤波： $5\\times 5$ ，$\\sigma=1$ ，$k=1$ ： 钝化模板： 锐化结果： $5\\times 5$ ，$\\sigma=1$ ，$k=2$： 锐化结果： $5\\times 5$ ，$\\sigma=2$ ，$k=1$： 钝化模板： 锐化结果： $5\\times 5$ ，$\\sigma=2$ ，$k=2$ 锐化结果： $7\\times 7$ ，$\\sigma=1$ ，$k=1$ 钝化模板： 锐化结果： $7\\times 7$ ，$\\sigma=1$ ，$k=2$ 锐化结果： $7\\times 7$ ，$\\sigma=2$ ，$k=1$ 钝化模板： 锐化结果： $7\\times 7$ ，$\\sigma=2$ ，$k=2$： 锐化结果： 均值 5x5，k=1： 钝化模板： 锐化结果： 5x5，k=2： 锐化结果： 7x7，k=1： 钝化模板： 锐化结果： 7x7，k=2： 锐化结果： 总结 观察上面结果，发现：\n k越大对细节增强越明显。 模板越大会导致增细节被增强的越宽，所以模板大小要适度。 被平滑消除的边缘越多，锐化后增强的越多。 下一篇介绍下一阶微分算子 待续。。。 ","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-7-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E9%9D%9E%E9%94%90%E5%8C%96%E6%8E%A9%E8%94%BDunsharpeningmask.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第33天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 非锐化掩蔽(USM)\u003c/p\u003e","title":"【数字图像处理】5.7:灰度图像-图像增强 非锐化掩蔽 （Unsharpening Mask）"},{"content":"Abstract: 数字图像处理：第34天 Keywords: Sobel算子,Robert算子,\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 继续废话，之前介绍了二阶微分，和非锐化掩蔽，按照顺序该说一阶微分了，一阶微分与二阶微分一样，是线性算子，线性算子的计算方法多半是生成模板，然后与图像卷积，一阶微分同样，几天简单的介绍两个一阶微分算子，Robert算子和Sobel算子，这两个算子应该算是大名鼎鼎了，因为这两个算子在后面的边缘检测中都是里程碑似的算法，在增强部分，他们也主要用在边缘增强，本文只简要介绍下两个算子的大概使用和增强效果，具体的数学原理推导和其他性质，将在图像分割部分完整介绍。\n图像梯度介绍 首先介绍下梯度，梯度并非是一个数值，梯度严格意义上是一个向量，这个向量指向当前位置变化最快的方向，可以这么理解，当你站在一个山上，你有360°的方向可以选择，哪个方向下降速度最快（最陡峭），便是梯度方向，梯度的长度，表示为向量的长度，表示最大的变化速率。 梯度的数学表达：\n其中表示微分算子。梯度在三维坐标中表示为： 同样在二维中只取前两项，也就是由x方向的偏微分和y方向的偏微分组成。对于图像f中点(x,y)处的梯度，定义为： 与上面所述保持一致，图像梯度方向给出图像变化最快方向，当前点的梯度长度为： 次长度计算中有平方和开平方，所以将不再是现行操作。 为了简单计算，将上面求距离简化成： 然而上面式子最大的问题在于不具有旋转不变形，也就是不是各向同性的，具体原因是三角形三遍关系原理，因为梯度方向和长度对于旋转是不变的，所以，x轴和y轴发生旋转的时候，直角三角形两边发生变化，但保持斜边长度和方向不变，因此两个直角边的长度和必然发生改变，也就是上面的M值必然会改变，顾其不具有旋转不变形。 为了表达方便先重新来定义下模板位置，如下图 其中z5表示模板中心。\nRobert算子 奇葩算子Robert，说它奇葩确实奇葩，因为不知道Robert哪来的勇气或者推导过程，使用一个2x2的模板，而且是对角线做差，其差分为： 因为向量无法在图像中显示，我们要计算梯度向量的长度： 简化为绝对值方法： 这个就是Robert交叉算子。模板： Sobel算子 因为Robert算子是2x2的模板，不是对称的奇数模板，我们更喜欢3x3的模板，所以，要根据上面的Robert算子改造出来一个3x3模板，提出了下面这个计算方法： 怎么来的？说实话我一开始也不知道，只是说根据上面Robert算子，搞出来一个等价的，其数字模板为： 并且其下降速率（梯度的长度）计算公式： 所有上面的疑惑就是这个公式： 到底是怎么来的，为什么中间会有2，以及为什么是隔行相减，下面的过程是我自己发明的，没有数学依据，只是自己的猜测，根据Robert算子的两个式子，横向划过3x3的所有位置，然后相加，就得到了Sobel算子： 这个过程就用Robert产生了Sobel，同样的纵向移动就会产生x轴方向的算子。Sobel算子原理的论文不多，但都说这是个很好的边缘检测算子。\n代码 Robert：\nvoid Robert(double *src,double *dst,int width,int height){ double RobertMask_x[9]={0,0,0,0,-1,0,0,0,1}; double RobertMask_y[9]={0,0,0,0,0,-1,0,1,0}; double *dst_x=(double *)malloc(sizeof(double)*width*height); double *dst_y=(double *)malloc(sizeof(double)*width*height); RealConvolution(src, dst_x, RobertMask_x, width, height, ROBERT_MASK_SIZE,ROBERT_MASK_SIZE); RealConvolution(src, dst_y, RobertMask_y, width, height, ROBERT_MASK_SIZE,ROBERT_MASK_SIZE); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ dst[j*width+i]=abs(dst_x[j*width+i])+abs(dst_y[j*width+i]); } free(dst_x); free(dst_y); } void RobertSharpen(double *src,double *dst,int width,int height,double c){ Robert(src,dst,width,height); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ dst[j*width+i]=src[j*width+i]+c*dst[j*width+i]; } } Sobel：\nvoid Sobel(double *src,double *dst,int width,int height){ double SobelMask_x[9]={-1,-2,-1,0,0,0,1,2,1}; double SobelMask_y[9]={-1,0,1,-2,0,2,-1,0,1}; double *dst_x=(double *)malloc(sizeof(double)*width*height); double *dst_y=(double *)malloc(sizeof(double)*width*height); RealRelevant(src, dst_x, SobelMask_x, width, height, SOBEL_MASK_SIZE,SOBEL_MASK_SIZE); RealRelevant(src, dst_y, SobelMask_y, width, height, SOBEL_MASK_SIZE,SOBEL_MASK_SIZE); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ dst[j*width+i]=abs(dst_x[j*width+i])+abs(dst_y[j*width+i]); } free(dst_x); free(dst_y); } void SobelSharpen(double *src,double *dst,int width,int height,double c){ Sobel(src,dst,width,height); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ dst[j*width+i]=src[j*width+i]+c*dst[j*width+i]; } } 结果 原图： Robert： Robert Sharpen： Sobel： Sobel Sharpen： 可以观察出Sobel边缘较宽，来观察简单图形的Robert和Sobel局部放大图： Robert： Sobel： Robert局部放大1，2，3： Sobel局部放大图1，2，3： 总结 Sobel和Robert都能对边缘有较强的响应，而且Sobel对边缘的响应较宽而且更加强烈，Robert算子对边缘响应较弱，而且对弯曲的边缘敏感度第（Robert1中圆形弧形部分亮度低）。 待续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-5-8-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-robert%E7%AE%97%E5%AD%90-sobel%E7%AE%97%E5%AD%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第34天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Sobel算子,Robert算子,\u003c/p\u003e","title":"【数字图像处理】5.8:灰度图像-图像增强 Robert算子、Sobel算子"},{"content":"Abstract: 数字图像处理：第38天 Keywords: 图像分割\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话  废话开始，因为图像锐化中使用到了一些分割中的基础知识，所以为了比较连贯，决定跳过图像修复开始图像分割，分割的目的是为了区分图像中不同的区域或物体，人类视觉可以利用多种模型同时计算，也就是我们平时可以轻易的分辨出餐盘里的肉和青菜，并且可以准确的把肉夹到嘴里，而在灰度图中，信息没有人眼捕捉到的多，灰度图像分割多半依靠区域的灰度变化，产生一系列的算法和应用。 本文作为分割的基本介绍，首先会给出分割的基本知识结构，此结构不唯一，以下为各人观点，非严格定义： 分割介绍 分割是将图像分为构成它的子区域或物体，其中有些是我们关注的部分有些是不关注的，由于灰度图像中只保存有灰度信息。所以灰度图像的分割依靠的是灰度的两个基本性质之一，不连续性或相似性。后面将要介绍的算法主要分类为：\n 基于边缘 基于阈值 基于区域 基于形态学方法 基于模糊理论 基于微分方程 基于人工神经网络 其他方法  其中我们主要介绍基础方法，基于边缘，阈值，和区域的方法。基于形态学的方法和其他方法中将介绍主流算法原理和实现。\n基础知识 对于去不图像区域R，图像分割将满足下面几点： 解释下d和e，分割出来的每个区域都会满足一个不同性质，这也是分割他们依据，相邻的两个区域的分类属性必须不同，否则其必然为一个区域。 灰度图像分割主要依据的是不连续性和相似性，其中边缘检测作为基于边缘的分割方法，其主要依靠的是不连续性，而区域分割方法主要依靠的是相似性。\n总结 分割是图像分析的关键步骤，是从图像处理过渡到计算机视觉的部分，分割以后，我们对关心的目标进行相关计算，得出我们希望的数据，对背景记性舍弃或者保存，降低图像整体目标的复杂性，好的分割可以提高信噪比，使后续运算结果更加准确，降低识别难度。 待续。。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-0-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E7%BB%BC%E5%90%88%E4%BB%8B%E7%BB%8D.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第38天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 图像分割\u003c/p\u003e","title":"【数字图像处理】6.0:灰度图像-图像分割 综合介绍"},{"content":"Abstract: 数字图像处理：第39天 Keywords: 边缘模型\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，上午写了下分割的综述，可能会有一些遗漏，后面会补充修正，学习分割首先拿边缘检测入手，以前一直认为分割是把一片区域提取出来，边缘和这个没啥关系，不过现在想想，边缘的作用就是分开不同的区域，也就是把区域分割成很多份，所以，边缘就是分割用的，而且边缘可以反映形状信息，在应用中很广泛。为了分割边缘，首先我们必须了解下边缘在灰度图像中的存在方式。\n边缘模型 图像中像素灰度值以离散方式存在，边缘可以用数字表示为以下模型：\n台阶形：${0,0,0,0,0,255,255,255,255,255}$\n斜坡形：${0,0,0,50,100,150,200,255,255,255}$\n屋顶形：${0,0,80,160,255,160,80,0,0,0}$\n来看示意图，当观察一阶微分和二阶微分的时候请注意坐标轴，冈萨雷斯的树上没有给出坐标值，所以会产生一些误解，认为微分结果很大。 台阶形边缘： 斜坡形： 屋顶形： 产生这几种模型的原因对于台阶模型和斜坡模型比较好理解，屋顶模型的产生可以是一条比较细的线，距离镜头较近，而离开背景较远产生的现象。\n噪声的影响 前面在介绍锐化的时候讲到提取细节使用一阶微分和二阶微分算子，边缘属于细节，所以一阶微分和二阶微分也用于边缘检测。向前面说的一样，一阶微分和二阶微分算子对噪声敏感，二阶微分尤其严重。 边缘噪声的来源，由于镜头对焦的原因，边缘会产生模糊现象，而电子线路噪声是产生图像噪声的根本原因，下面我们来简单模拟，标准差为0.1，1，10，均值为0的加性噪声。\n 斜坡形： 标准差0.1的加性噪声： 一阶微分： 二阶微分：   标准差1.0的加性噪声： 一阶微分： 二阶微分： 标准差10的加性噪声： 一阶微分： 二阶微分：  台阶形： 标准差0.1的加性噪声： 一阶微分： 二阶微分：   标准差1的加性噪声： 一阶微分： 二阶微分： 标准差10的加性噪声： 一阶微分： 二阶微分：  屋顶形：  标准差0.1的加性噪声： 一阶微分： 二阶微分： 标准差1.0的加性噪声： 一阶微分： 二阶微分： 标准差10.0的加性噪声： 一阶微分： 二阶微分： 观察上面的效果可以发现，斜坡形和屋顶形对标准差为10的噪声微分已经无法使用，但台阶形似乎影响不大。并且我们可以观察出来，二阶微分受噪声影响更大。\n总结 根据上述，我们发现去噪处理对于边缘提取是必要的，而用有检测算子检测到细节以后如何刷选也是一个和关键的步骤：\n 平滑处理，目的降噪。 边缘点检测，使用一阶微分或二阶微分，检测候选点。 边缘定位，筛选边缘点。  待续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-1-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E8%BE%B9%E7%BC%98%E6%A8%A1%E5%9E%8B.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第39天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 边缘模型\u003c/p\u003e","title":"【数字图像处理】6.1:灰度图像-图像分割 边缘模型"},{"content":"Abstract: 数字图像处理：第48天 Keywords: 霍夫变换,直线检测\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 霍夫变换(Hough Transform)\u0026ndash;直线\u0026quot; 废话开始，要过年了，到处人心惶惶，沉下心写篇博客，下一篇就等农历新年以后了。马上新年了，希望自己在新年能提高技术，找到一份图像处理的好工作，也希望大家都能学习到更多的知识，做自己喜欢做的事情。 以前基本每天都写博客，坚持了三个月感觉确实有提高，也能把知识总结分享出来，看着每天博客的访问量不断增长，心里很有成就感，共同学习，共同进步，喜欢分享的人，才能获得别人的分享，好多优秀的同学并不喜欢分享知识，或者用很高深的话显示出自己的知识，现在想想，能用俗话说清相对论的人才是高手，让那些故弄玄虚，作假，抄袭欺骗国家欺骗人民的院士，专家都去shi吧。 废话稍微多了一点点，说说霍夫变换，Hough Transform，由Hough提出，问题原型是如何找到图像中的直线，后来延伸到可以检测出任何可以表示成方程的图形，霍夫变换的检测可以检测不完整的图形，也就是中间有间断的，并且霍夫变换对旋转具有不变性，对噪声不敏感，但是霍夫变换的缺点是需要的存储量较大，标准的霍夫变换运算量大，相对较慢。 本篇只介绍标准霍夫变换对直线的检测，放在图像分割这部分是因为冈萨雷斯书中将霍夫变换用于连接边缘。 #数学原理 我们本篇只介绍检测直线，对于图像中直线，一般用方程 $y=kx+b$ ，式子中 $k$ 表示直线的斜率， $b$ 表示直线相对于y轴的截距，如图中所示： 如果按照正常思维，搜索图中的直线，使用穷举的方法，假设图像一共有N个像素任意两点可以构成一条直线，所以过一点应该有(N-1)/2条，所以全图像存在N(N-1)/2条直线，如果要确定一点是否是直线上的点，一共需要至少 $N^2(N-1)/2$ 次计算，这个代价的算法在实际中基本没有价值，于是，Hough提出了一种巧妙的方法，将直线表示成 $-b=xk-y$ 这种变换的意义在于自变量不是x而是k，y也不再是因变量，b变成了因变量，所以图像上任意两点，可以从（x，y）坐标系映射到（k，b）坐标系，系数选取点 $(x_1=1.8,y_1=7.6)$ 和点 $(x_2=3.4,y_2=10.8)$ ，绘制出两条曲线： 可以根据上图得出k=2，b=4，那么在$y=kx+b$的点的在k，b坐标系上的交点都在(2,-4)处， 根据上面我们可以利用这个特点，将一幅图像从(x,y)坐标系，转换到(k,b)坐标系，假设原图中有k个亮点，可以在(k,b)坐标系画出k条直线，那么越多的直线交于一点，说明该点的坐标为斜率和截距的直线点在原图中出现频率越大，那么我们就能根据这个特点找出这些点。 问题来了，当原图直线和x轴垂直时，斜率k趋近于无穷大，所以在(k,b)坐标系内无法表示，所以我们换一种方法，使用直线 $y=kx+b$ 的法线，截距为-b，那么这条直线为 $y=-\\frac{1}{k}x-b$ ，如果使用参数方程，原直线为 $cos(\\theta)y-sin(\\theta)*x=cos(\\theta)b$ ，令 $\\varrho=cos(\\theta)b$ 原坐标的直线为 $cos(\\theta)y-sin(\\theta)*x=\\varrho$ ，那么法线的参数方程为 $sin(\\theta)y+cos(\\theta)*x=-\\varrho$ ，方程坐标系为 $(\\theta，\\varrho)$ 。 将上面的思想应用到 $(\\theta，\\varrho)$ 坐标系，我们将得到，如下的信息，原图： 转换到参数坐标系： 交点出就是两条对角线的参数。 我们来观察一条直线的参数坐标系： 水平直线： 对应参数坐标系： 垂直直线： 对应参数坐标系： 45°直线： 对应参数坐标系： -45°直线： 对应参数坐标系： 五条直线交于一点： 对应参数坐标系： 上图中越明亮的点说明重叠参数方程越多，我们来观察两条直线对应参数坐标系的立体情况： 原图： 平面的参数方程坐标系： 参数方程坐标系的立体显示： #代码\nvoid SHT(int x,int y,int zero,double * polar){ double angle_step=POLARSTEP; double angle=-M_PI_2; for(int i=0;i\u0026lt;POLARWIDTH;i++){ int p_y=(int)(((sin(angle)*y+cos(angle)*x)+0.5)*POLARHEIGHT_ZOOM)+zero; polar[p_y*POLARWIDTH+i]++; angle+=angle_step; } } ///////////////////////////////////////////////////////////////////////////// void HoughLine(double *src,double *dst,int width,int height,int lineLength){ int polar_height=2*POLARHEIGHT_ZOOM*(int)(sqrt(width*width+height*height)+1); int polar_width=POLARWIDTH; double *polar=(double *)malloc(sizeof(double)*polar_height*polar_width); Zero(polar,polar_width,polar_height); for(int j=0;j\u0026lt;height;j++){ for(int i=0;i\u0026lt;width;i++){ if(src[j*width+i]==255.0) SHT(i, j,polar_height/2,polar); } } for(int j=0; j\u0026lt;polar_height;j++) for(int i=0;i\u0026lt;polar_width;i++){ if(polar[j*polar_width+i]\u0026gt;lineLength){ double theta=i*POLARSTEP; if(theta==M_PI_2) DrawLine(dst, width, height, theta, abs(j-polar_height/2)/POLARHEIGHT_ZOOM); else if (theta==0) DrawLine(dst, width, height, theta, abs(j-polar_height/2)/POLARHEIGHT_ZOOM); else{ DrawLine(dst, width, height, theta, -(int)((j-polar_height/2)/cos(i*POLARSTEP))/POLARHEIGHT_ZOOM); } } } free(polar); } 效果 Hough变换输入图像因为边缘图像，或是二值图像效果如下图： 原图： 边缘检测： 霍夫直线检测： 原图： 边缘检测： 霍夫直线检测： 结论 霍夫变换可以有效的检测出图像中的直线，但需要设定一定的参数，比如定位参数坐标中极大值的方法，以避免错误的检出结果，广义的霍夫变换可以检测任何$g(\\vec x,\\vec c)=0$表示的形状，只是计算难度根据方程的复杂度决定，$\\vec c$表示方程参数，参数越多需要的存储空间越大，需要的计算量也越大。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-10-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2houghtransform-%E7%9B%B4%E7%BA%BF.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第48天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 霍夫变换,直线检测\u003c/p\u003e","title":"【数字图像处理】6.10:灰度图像-图像分割 霍夫变换(Hough Transform)--直线\""},{"content":"Abstract: 数字图像处理：第40天 Keywords: 边缘检测,算法\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 本来想这篇就开始介绍算子，但想想觉得如果只是散乱的介绍，可能效果不好，所以先来一篇综述，讲讲这些算子的发展过程，和他们之间的相互推导关系，可能有点教科书的风格，但是学算法最主要的就是了解一个算法所解决的问题，算法的优点和缺点，以及改进方法。所以，一定要来一篇综述。 首先，介绍下边缘检测算子的家族关系： 之所以一直在强调是边缘检测算子，是因为这些算子只能检测出图像突变的部分，突变的部分包括边缘，非边缘的其他细节，噪声，等等，所以这些被检测出来的点成为边缘候选点，之后再经过其他方式筛选出边缘点，这就是基本的边缘检测过程：\n 使用边缘检测算子检测到候选点 使用筛选算法得到边缘。 大家常说的Canny其实应该属于第二步的算法，而不是Canny算子。。。。  算子介绍 介绍下算子间的关系，强调一点，这些算子出现的时间没有考证先后顺序，以下顺序为逻辑顺序，对于一阶微分算子，鼻祖应该是最简单 $[-1,1]$ 和 $[-1；1]$ 就是一个横向差分和纵向差分，然后进化出来了Robert算子和Prewitt算子。Robert算子继而生出来Sobel算子标准版，后来的计算标明，在 $3\\times 3$ 的算子，Scharr算子的表现比 $3\\times 3$ 的Sobel算子好，并且Sobel算子出现之前的算子都是未经过数学论证的，也就是说，这些算子是根据经验或实验弄出来的，接下来的文章里，会逐一介绍各个算子。 二阶算子的鼻祖是Laplace算子。主要使用的是Marr-Hildreth算子，通常叫LoG，就是Laplace和高斯平滑模板结合的结果。为了计算方便，通常使用DoG来近似LoG，DoG的全称是高斯差分，两个高斯模板相减产生，能够近似LoG却可以降低计算量。\n总结 简短的介绍，使整个脉络相对清楚，下一篇开始从Robert开始介绍。 待续\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-2-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E7%AE%97%E5%AD%90-%E7%BB%BC%E8%BF%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第40天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 边缘检测,算法\u003c/p\u003e","title":"【数字图像处理】6.2:灰度图像-图像分割 边缘检测算子 综述"},{"content":"Abstract: 数字图像处理：第41天 Keywords: Robert算子,边缘检测\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，Robert算子，之前被用到了图像增强中的锐化，原因是作为一阶微分算子，Robert简单，计算量小，对细节反应敏感，之前说过算子对边缘检测的贡献是提供边缘候选点，Robert算子相比于其他3x3算子，在不经过后处理时，可以给出相对较细的边缘，有看一个博客，博主说Robert给出的边缘较粗，但根据我的测试结果，一阶微分中Robert给出的候选点是最细的，相关Robert基础知识参考前面博文“[灰度图像\u0026ndash;图像增强 Robert算子、Sobel算子]”。这里我们不在介绍重复知识。\n算子比较 与标准一阶差分不同，Robert采用对角线差分，前面博文我曾说我懂为什么要使用对角线，现在有了答案，假设我们采用标准的一阶微分算子，对下面一个数字化的矩形进行横向和纵向的差分，并得出结果，红色表示算子模板中心： 可以看出，得到的边缘一部分是在内边界，一部分是外边界，并且，黄色像素点并未有计算结果，也就是，边缘候选点丢失了一个。 但是如果我们采用Robert算子计算，结果如下： 右下角为得到的结果，虽然边缘候选点依然有外边缘和内边缘，但没有遗漏边缘候选点，这就是Robert由于普通差分的地方，也就是对角线差分的好处。 检验完候选点后，接下来的任务是筛选，筛选算法有很多，但最简单的是阈值处理，即超过阈值的为边缘，否则为噪声，或非边缘，这样做的缺点是不准确，有点是速度极快。计算量相当小，在速度要求较高的但准确度要求不高的地方，可以使用Robert加阈值的简单处理。得到边缘。\n代码实现结果 代码之前已经给出，这里只现实下阈值处理后的样子。 原图： Robert+阈值（阈值给出方式为：边缘候选点的最大值的百分比）： 总结 Robert算子可以检测到全部的边缘候选点，边界定位能力一般，原因是一阶微分对于斜坡型边缘定位都不准确，使用简单的阈值后可以去掉一些非边缘点，但检测结果准确性一般，但计算速度非常快。\n待续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-3-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-robert%E7%AE%97%E5%AD%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第41天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Robert算子,边缘检测\u003c/p\u003e","title":"【数字图像处理】6.3:灰度图像-图像分割 Robert算子"},{"content":"Abstract: 数字图像处理：第42天 Keywords: 边缘检测,Sobel算子\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，Sobel我们并不陌生，之前在图像增强的时候也已经介绍了它的作用，并且还杜撰了一下它的来历，也就是用Robert平移相加（类似于相关或卷积），下面可以给出Sobel的另一个来源，因为Sobel数学推导的过程和资料很少，而且当时提出Sobel的时候应该也是没有数学论证的，而只是简单的实验后，发现效果非常好。 我们还要介绍下扩展Sobel算子，Sobel原始模型为标准3x3模板，但可以扩展成5x5到任意奇数x奇数的大小，而模板系数的确定可以根据帕斯卡三角来计算，真的很神奇。Sobel之后延伸出了Scharr算子，这个算子也为3x3算子，但是效果据说比3x3的Sobel好，后面文章将会给出具体对比。\n算子形式 数学形式的标准Sobel为： 此模板为最早提出的Sobel模板，由于模板的对称性，我们可以将它分解一下，并根据卷积的运算性质，可以得到： 也就是说，图像对Sobel 的响应等于，对模板分解后的小模板分别卷积，而观察小模板我们可以发现，其中 $[1,0,-1]$ 或其转置为差分，也就是用于寻找边缘候选点的，而另一个 $[1,2,1]$ 是一个标准平滑算子，这也就是很多书上说，Sobel具有平滑和微分的功效，原因就是这里了，也就是说，算子先将图像横向或纵向平滑，然后再纵向或横向差分，得到的结果是平滑后的差分结果。 或者，也可根据以下方式得到分解到的两个模板，其中星号表示卷积： 另一种得到模板的方法是通过帕斯卡三角，得到，并且帕斯卡三角的奇数行是最有高斯模板的整数系数的逼近，也就是说，高斯模板可以通过帕斯卡三角查询到其整数系数的近似，来观察帕斯卡三角： 其中标注的就可以用来生成扩展的Sobel算子，其中较常用的有5x5和7x7的模板。 用两个小模板分别卷积的另一个好处是减少计算量，对于使用大小为nxn的模板，卷积计算量为 $O(nnwidthheight$ 而分开成小模板卷积计算量是 $O(2nwidthheight$ 也就是 $O(nwidthheight$ 减少了一项，当n相对较大的时候，计算量明显减少。 帕斯卡三角的计算是通过组合公式给出，具体不在这里描述，所以Sobel算子的模板计算方法我们就有了大概的了解。 opencv文档中给出了关于sobel算子的下面信息： 和上面描述的方法类似，更直观，可以用来理解sobel的模板结构，不同的差分方向带来的问题就是边缘方向的确定，由于算子属于一阶微分，也就是梯度算子之一，所以梯度方向信息也显得很重要，比如后面要说的canny就是用到了梯度方向的信息，所以，在确定方向时要注意算子的差分方向。 对于阶梯型边缘，计算过程及结果如下，红色为模板中心： 可以看到，相比于Robert算子，Sobel得到的边界候选位置相对较宽，而且包括全部的内边界和外边界。并且差分被放大了，也就是说，用Sobel算子处理后的图片有可能超过原图像灰度级别，对于这个问题，处理方法是将平滑分算子（分解后的平滑部分，例如【1，2，1】）归一化，得到的差值仍在原始灰度级范围内。\n代码效果 代码：\ndouble Sobel(double *src,double *dst,double *edgedriction,int width,int height,int sobel_size){ //double SobelMask_x[3]={-1,-2,-1,0,0,0,1,2,1};  double *dst_x=(double *)malloc(sizeof(double)*width*height); double *dst_y=(double *)malloc(sizeof(double)*width*height); if(sobel_size==3){ double SobelMask1[3]={0.25,0.5,0.25}; double SobelMask2[3]={1,0,-1}; RealConvolution(src, dst_x, SobelMask1, width, height, 1, 3); RealConvolution(dst_x, dst_x, SobelMask2, width, height, 3, 1); RealConvolution(src, dst_y, SobelMask2, width, height, 1, 3); RealConvolution(dst_y, dst_y, SobelMask1, width, height, 3, 1); }else if(sobel_size==5){ double SobelMask1[5]={0.0625,0.25,0.375,0.25,0.0625}; double SobelMask2[5]={1/3.0,2/3.0,0,-2/3.0,-1/3.0}; RealConvolution(src, dst_x, SobelMask1, width, height, 1, 5); RealConvolution(dst_x, dst_x, SobelMask2, width, height, 5, 1); RealConvolution(src, dst_y, SobelMask2, width, height, 1, 5); RealConvolution(dst_y, dst_y, SobelMask1, width, height, 5, 1); }else if(sobel_size==7){ double SobelMask1[7]={0.015625,0.09375,0.234375,0.3125,0.234375,0.09375,0.015625}; double SobelMask2[7]={0.1,0.4,0.5,0,-0.5,-0.4,-0.1}; RealConvolution(src, dst_x, SobelMask1, width, height, 1, 7); RealConvolution(dst_x, dst_x, SobelMask2, width, height, 7, 1); RealConvolution(src, dst_y, SobelMask2, width, height, 1, 7); RealConvolution(dst_y, dst_y, SobelMask1, width, height, 7, 1); } if(edgedriction!=NULL) //getEdgeDirection(dst_x, dst_y, edgedriction, width, height);  getEdgeAngle(dst_x, dst_y, edgedriction, width, height); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ dst[j*width+i]=abs(dst_x[j*width+i])+abs(dst_y[j*width+i]); } free(dst_x); free(dst_y); return findMatrixMax(dst,width,height); } 下面对比3x3，5x5，7x7算子的效果： 原图： 局部放大： 阈值后 结论 Sobel算子的效果相比于其他算子，效果较好，而且计算量不大，可以用于实时系统，其结合简单的阈值可以得到较好的效果，但得到的边缘较宽，可以使用形态学细化。 待续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-4-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-sobel%E7%AE%97%E5%AD%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第42天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 边缘检测,Sobel算子\u003c/p\u003e","title":"【数字图像处理】6.4:灰度图像-图像分割 Sobel算子"},{"content":"Abstract: 数字图像处理：第43天 Keywords: Prewitt算子,边缘检测\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 废话开始，发现CSDN有了新的博客写作方式-MarkDown看起来很腻害的样子，有空试一下，希望以后能有更好的知识分享总结出来，当然要用更好的方式，更鲜明的表达出自己对知识的理解和观点，翻了下以前写的博客，感觉自己写博客的调理更清楚了，而且发现博客最好别写太长，当然，大牛除外，因为太长了可能有点驾驭不住。 今天来学习Prewitt算子，这个算子也是一阶微分算子，所以和前面说的Sobel有些类似，但不同的是平滑模板和不同情况下的效果。\nPrewitt算子 来看prewitt算子，这个算子形式简单，基本形式如下： 一排1减去另一排1，差分被它体现的淋漓尽致，当然我们观察它的性质还是要看分解形式，也就是前两个小模板，$[1，0，-1]$ 不用解释，差分的形式，为什么不用 $[1，-1]$ 进行差分？首先对于2x3的模板和3x3的模板，我们更倾向于3x3，因为3x3的模板中心落在实数上 ，其次$[1，0，-1]$的差分结果能够在一定程度上减少噪声影响。这个差分的性质，Sobel，Prewitt以及后面的Scharr都是一样的，所以这里并不是他们的差异，他们的差异主要集中在平滑算子上。Sobel算子的平滑算子是一个接近高斯的小模板，而Prewitt的平滑算子则是一个均值模板，也就是 $\\frac{1}{3}[1，1，1]$ ，其原理与Sobel也保持一致，横向平滑，纵向差分产生Y方向的一阶微分，或者纵向平滑横向差分，产生X方向一阶微分，当然要注意按照这个模板做出的梯度方向是左手坐标系，也就是和图像坐标系一致的，即 $(0,0)$ 在左上角，x轴向右为正，y轴向下为正，为了使用习惯，可以把y轴取负，就能得到传统的右手坐标系了。。。 关于扩展，没有见过有人扩展prewitt，但是按照理论是绝对可行的，我猜想，可以扩展成 代码和结果 代码：\n///////////////////////////////////////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////////////////////////////////////// double Prewitt(double *src,double *dst,int width,int height){ double PrewittMask1[3]={1.0/3.0,1.0/3.0,1.0/3.0}; double PrewittMask2[3]={-1.0,0.0,1.0}; double *dst_x=(double *)malloc(sizeof(double)*width*height); double *dst_y=(double *)malloc(sizeof(double)*width*height); RealRelevant(src, dst_x, PrewittMask1, width, height, 1, 3); RealRelevant(dst_x, dst_x, PrewittMask2, width, height, 3, 1); RealRelevant(src, dst_y, PrewittMask2, width, height, 1, 3); RealRelevant(dst_y, dst_y, PrewittMask1, width, height, 3, 1); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ dst[j*width+i]=abs(dst_x[j*width+i])+abs(dst_y[j*width+i]); } free(dst_x); free(dst_y); return findMatrixMax(dst,width,height); } 结果如下： 原图： prewitt算子的处理结果： 局部放大： 在观察下阈值 结论 结论是prewitt会使灰度值相对集中，相比于Sobel并不会凸显出边界响应，整体边缘候选点区域接近，不适合做阈值后处理，但优点是速度快，计算简单。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-5-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-prewitt%E7%AE%97%E5%AD%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第43天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Prewitt算子,边缘检测\u003c/p\u003e","title":"【数字图像处理】6.5:灰度图像-图像分割 Prewitt算子"},{"content":"Abstract: 数字图像处理：第44天 Keywords: 边缘检测,scharr算子\n本文最初发表于csdn，于2018年2月17日迁移至此\n开篇废话 感受下markdown的写博客的感觉，好像在写程序一样，果然是程序员的好工具，不过开头怎么没有空格。。。一空格就自动变成代码了，这让我情何以堪，好吧，以后的文章开头不空格了。本来打算上一篇直接介绍Scharr算子，但是发现Prewitt也能占很大篇幅，为了保证每一篇的内容不过长，所以拆了一篇出来，下一篇写Sobel，Prewitt，Scharr的对比。\nScharr算子介绍 果然没有空格，好吧，不空格就不空格吧，OpenCV的Canny算法介绍中提到了Scharr算子，并且说 $3\\times3$ 的Scharr算子比Sobel算子准确性要强，后面一篇会给出一些具体的数据，以及具体的实验步骤以及数据，先看看Scharr长什么样子吧： 与Sobel的不同点也是在平滑部分，这里所用的平滑算子是 $\\frac{1}{16}$ *[3,10,3] ，相比于 $\\frac{1}{4}$ *[1,2,1] ，中心元素占的权重更重，这可能是相对于图像这种随机性较强的信号，邻域相关性不大，所以邻域平滑应该使用相对较小的标准差的高斯函数，也就是更瘦高的模板 #代码\ndouble Scharr(double *src,double *dst,double *edgedriction,int width,int height){ double ScharrMask1[3]={0.1875,0.625,0.1875}; double ScharrMask2[3]={1,0,-1}; double *dst_x=(double *)malloc(sizeof(double)*width*height); double *dst_y=(double *)malloc(sizeof(double)*width*height); RealConvolution(src, dst_x, ScharrMask1, width, height, 1, 3); RealConvolution(dst_x, dst_x, ScharrMask2, width, height, 3, 1); RealConvolution(src, dst_y, ScharrMask2, width, height, 1, 3); RealConvolution(dst_y, dst_y, ScharrMask1, width, height, 3, 1); for(int i=0;i\u0026lt;width*height;i++) dst_y[i]=-dst_y[i]; if(edgedriction!=NULL) getEdgeAngle(dst_x, dst_y, edgedriction, width, height); for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ dst[j*width+i]=abs(dst_x[j*width+i])+abs(dst_y[j*width+i]); } free(dst_x); free(dst_y); //matrixMultreal(dst, dst, 1.0/16.0, width, height);  return findMatrixMax(dst,width,height); } 效果 原图： scharr算子结果： 按顺序局部放大： 注意7中具有细小噪声点，放大后观察： 阈值处理后： 总结 Scharr作为一阶微分算子，与其他微分算子具有相同的基本特点，即对突变有较强的响应，但缺点也是使用Scharr后处理时，阈值无法很好的分离边缘候选点中边缘点与非边缘点，其优点是速度极快，而且Scharr大小固定，也就是只有$3 \\times 3$，第一篇markdown的博客，待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-6-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-scharr%E7%AE%97%E5%AD%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第44天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 边缘检测,scharr算子\u003c/p\u003e","title":"【数字图像处理】6.6:灰度图像-图像分割 Scharr算子"},{"content":"Abstract: 数字图像处理：第45天 Keywords: Sobel算子,Scharr算子,Prewitt算子\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 Sobel算子，Prewitt算子和Scharr算子平滑能力比较 依然是废话，这篇主要想对比下Sobel，Prewitt和Scharr算子的平滑能力，由于一阶微分对噪声响应强，进行微分之前进行降噪是非常必要的，这里我们进行的实验是，以lena图作为实验原图，取其中一行数据作为无噪声的原始信号，分别加上不同的强度的高斯白噪声，对噪声的分类和噪声具体性质的研究将在图像恢复中描述。但这里我们使用不同强度的高斯白噪声。\n数学原理 数学原理主要介绍下衡量噪声强度的方法-均方根误差root-mean-square error，第 $i$ 个测量值与真实值差的平方 $d^2$ ，对 $d^2_i$ 进行求和后平均 $$ Re=\\frac{1}{n}\\sum_{i=1}^nd^2_i $$\n代码 %matlab代码 clear all;clc; noise_ratio=3;%噪声的标准差3% x=imread(\u0026#39;/Users/Tony/DIPImage/lena\u0026#39;,\u0026#39;jpg\u0026#39;); signal=double(x(250,:)); noiseImage=uint8(randn(512,512)*2.55*noise_ratio); dst=x+noiseImage; figure(1); imshow(dst); for m=1:100 noise=randn(1,512)*2.55*noise_ratio; signal_noise=signal+noise; for n=2:511 scharr(n)=signal_noise(n-1)*3./16.+signal_noise(n)*10./16.+signal_noise(n+1)*3./16.; sobel(n)=signal_noise(n-1)*0.25+signal_noise(n)*0.5+signal_noise(n+1)*0.25; prewitt(n)=signal_noise(n-1)*1./3.+signal_noise(n)*1./3.+signal_noise(n+1)*1./3.; end d_noise(m)=0; d_scharr(m)=0; d_sobel(m)=0; d_prewitt(m)=0; for n=2:511 d_scharr(m)=(d_scharr(m)+(scharr(n)-signal(n))^2); d_sobel(m)=(d_sobel(m)+(sobel(n)-signal(n))^2); d_prewitt(m)=(d_prewitt(m)+(prewitt(n)-signal(n))^2); d_noise(m)=d_noise(m)+noise(m)^2; end end x=1:100; figure(2); plot(x,d_scharr,\u0026#39;-r\u0026#39;,x,d_sobel,\u0026#39;.-b\u0026#39;,x,d_prewitt,\u0026#39;-g\u0026#39;,x,d_noise,\u0026#39;-k\u0026#39;); Matlab写程序写的不多，所以将就看。 #实验结果 下面我们分别使用不同强度的高斯加性白噪声叠加到图像上，并计算 $\\frac{1}{4}\\times[1,2,1]$，$\\frac{1}{3}\\times[1,1,1]$，$\\frac{1}{16}\\times[3,10,3] $ 对叠加了噪声的lena图的第250行数据进行平滑，叠加的噪声的标准差分别是当前信号的$0.5%,1%,2%,3%,4%,5%$，均值为$0$下面我们来观察效果。 下面折线图中，为了观察清楚，均方误差未乘以$\\frac{1}{n}$，因为所有信号使用的n都相同，图中的黑色线为未处理信号的噪声强度，红色为Scharr算子的结果，绿色为Prewitt算子的结果，蓝色为Sobel算子结果，下面我们来观察结果： 总结 当噪声强度超过标准差为信号的 $4%$ 时，Sobel和Scharr的性能开始接近，超过 $5%$ 的时候Prewitt，Sobel，Scharr性能基本相同，但小于 $3%$ 时候Scharr的性能明显强于Sobel，并且其性能排名为$Scharr \u0026gt; Sobel \u0026gt; Prewitt$当噪声标准差为 $0.5%$ 时，误差大概为1个像素左右，此时不进行平滑的结果更好，此时均值平滑的效果最差。 自此简单的评估了下各算子的噪声平滑效果，在小噪声情况下，$3\\times3$的算子Scharr算子性能强于Sobel。 待续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-7-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E7%AE%97%E5%AD%90%E5%B9%B3%E6%BB%91%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%83.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第45天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Sobel算子,Scharr算子,Prewitt算子\u003c/p\u003e","title":"【数字图像处理】6.7:灰度图像-图像分割 Sobel算子，Prewitt算子和Scharr算子平滑能力比较"},{"content":"Abstract: 数字图像处理：第46天 Keywords: canny,边缘检测\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 Canny边缘检测 废话开始，Canny大名鼎鼎，大家都称之为Canny算子，包括wiki上也是写的Canny detector，但是按照我的理解，我觉得叫做Canny算法比较合适，但如果叫做算子，那也应该叫做复合算子，因为Canny本身并不是一个线性模板（像Sobel那样）或者一个局部比较类的算法（像中值滤波那样的），Canny是一套完整的理论，并实现出了完整的算法。 Canny是目前已知的最好的边缘检测算法，是不是之一我不确定，但可以肯定的是，它的应用非常广泛，基本用到边缘检测的，大家永远第一个想到它。Canny算法的复杂度比前面的检测加阈值的算法计算复杂度更高，空间复杂度也要高一些，但现在的计算设备，对于Canny基本可以实现实时，并且有人用GPU来实现，所以从86年Canny提出了这个算法到现在，在边缘检测方面，其地位还是比较稳固的。 Canny算法的另一个显著特征是它有完整的数学推导过程，能够证明这个算法能给出最好的边缘。后面我们将会简单的看一下数学过程。\n 算法原理 算法原理，Canny首先提出了三种基本条件，来定义一个边缘。来看原文【Canny1986】：\n  Good detection. There should be a low probability of failing to mark real edge points, and low probability of falsely marking nonedge points. Since both these probabilities are monotonically decreasing functions of the output signal-to-noise ratio, this criterion corresponds to maximizing signal-to-noise ratio. Good localization. The points marked as edge points by the operator should be as close as possible to the center of the true edge. Only one response to a single edge. This is implicitly captured in the first criterion since when there are two responses to the same edge, one of them must be considered false. However, the mathematical form of the first criterion did not capture the multiple response requirement and it had to be made explicit.   翻译一下：\n 好的检测：一定要尽可能少的遗漏边缘点，尽可能少的添加非边缘点。正确的边缘点为信号，被错误检测出来的非边缘点为噪声，所以第一点归结为提高信噪比。 准确的位置：检测出的边缘点一定要与真正的边缘中心，尽可能的近。 边缘单一响应：对于一个边缘，只能产生一个响应，如果对于一个边缘产生两个响应，第一点的数学求解过程不能保证这一点，所以要单独明确，如果对于一边有两个响应，必须去掉一个。  这就是Canny的指导思想，并且根据这一思想进行建模。\n 数学原理 发表在VOL. PAMI-8, NO. 6, NOVEMBER 1986上的文章给出了明确的求解过程，包括建模上面的理论，并求解最优解，因为本人数学功底一般，后面的求解过程有兴趣的同学可以自行查看论文，这里只给出建模上面三个基本原理的过程，以一维下的情况给出。 首先我们设滤波器的单位冲击响应为： $f(x)$ ，定义边缘本身为 $G(x)$ ，边缘中心位置为$x=0$处，滤波器响应范围 $[-w,+w]$ 边缘对滤波器的响应为： $H_G(x)=\\int_{-w}^{+w} G(-x)f(x),dx$ (1)\n噪声 $n(x)$ 相应的均方根为： $H_n=n_0[\\int_{-w}^{+w}f^2(x),dx]^{1/2}$ (2)\n其中 $n^2_0$ 是单位长度的均方噪声幅度 由上面两个式子，我们定义信噪比SNR： $SNR=\\frac{\\int_{-w}^{+w} G(-x)f(x),dx}{n_0[\\int_{-w}^{+w}f^2(x),dx]^{1/2}}$ (3)\n这便是原理1的建模，使信噪比尽可能大，来满足原理1。 为了度量检测出的边缘与实际边缘的位置关系，我们使用均方根误差距离。对于检测滤波器 $f(x)$ 我们一般认为滤波结果给出的局部最大值为检测到的边缘，所以如果没有噪声加入时响应结果的一阶导数在$x=0$处应该为0，即 $x=0$ 为对滤波器响应的局部最大值，也就是计算边界点。 设 $H_n(x)$ 只是噪声对滤波器的响应， $H_G(x)$ 只是边缘对滤波器的响应，根据上面的设想，一定有 $x=x_0$ 处满足：\n$H^{\u0026rsquo;}_n(x_0)+H^{\u0026rsquo;}_G(x_0)=0$ (4)\n泰勒展开 $H^{\u0026rsquo;}_G(x_0)$ 可以得出：\n$H^{\u0026rsquo;}_G(x_0)=H^{\u0026rsquo;}_G(0)+H^{\u0026rsquo;\u0026rsquo;}_G(0)x_0+o(x_0^2)$ (5)\n因为可以确定 $H^{\u0026rsquo;}_G(0)=0$ 结合(4)(5)可以得出：\n$H^{\u0026rsquo;\u0026rsquo;}_G(0)x_0 \\approx {-H^{\u0026rsquo;}_n(x_0)}$(6)\n$H^{\u0026rsquo;}_n(x_0)$ 为高斯随机量，并且其方差是 $H^{\u0026rsquo;}_n(x_0)$ 的均方值，并且给出其期望为：\n$E[H^{\u0026rsquo;}n(x_0)^2]=n_0^2\\int^{+w}{-w}f^{\u0026lsquo;2}(x),dx$(7)\n将(7)和(6)结合，可以得到：\n$E[x^2_0]\\approx {\\frac{n_0^2\\int^{+w}{-w}f^{\u0026lsquo;2}(x),dx}{[{\\int{-w}^{+w} G^{\u0026rsquo;}(-x)f^{\u0026rsquo;}(x),dx}]^2}}=\\delta x^2_0$ (8)\n$\\delta x^2_0$ 是 $x_0$ 标准差的近似，位置由其倒数给出：\n$Localization =\\frac{|\\int^{+w}{-w}G^{\u0026rsquo;}(-x)f^{\u0026rsquo;}(x),dx|}{n_0[\\int^{+w}{-w}f^{\u0026lsquo;2}(x),dx]^{-1/2}}$(9)\n这便是原理2的建模，使信噪比尽可能大，来满足原理1。 结合原理1，原理2，我们得出为了满足前面两个基本原理，需要最大化：\n$\\frac{\\int_{-w}^{+w} G(-x)f(x),dx}{n_0[\\int_{-w}^{+w}f^2(x),dx]^{1/2}}\\frac{|\\int^{+w}{-w}G^{\u0026rsquo;}(-x)f^{\u0026rsquo;}(x),dx|}{n_0[\\int^{+w}{-w}f^{\u0026lsquo;2}(x),dx]^{-1/2}}$(10)\n下面就是第三个原理的过程了，其与前两个原理使用的只是不太相同，根据Schwarz不等式，SNR（3）中给出的式子上边界为：\n$n^{-1}0[\\int^{+w}{-w}G^{2}(x)dx]^{1/2}$\nLocalization(9)上边界于：\n$n^{-1}0[\\int^{+w}{-w}G\u0026rsquo;^{2}(x)dx]^{1/2}$\n最大值出现在 $f(x)=G(-x)$ 其中 $x$ 属于 $[-w,+w]$ 检测点到真实点的平均距离为：\n$x_{ave}=\\pi(\\frac{-R(0)}{R^{\u0026rsquo;\u0026rsquo;}(0)})^{1/2}$(11)\n其中R是函数g的自相关：\n$R(0) = \\int ^{+\\infty}_{-\\infty}g^{2}(x),dx$\n$R^{\u0026rsquo;\u0026rsquo;}(0) = -\\int ^{+\\infty}_{-\\infty}{g\u0026rsquo;}^{2}(x),dx$\n所以 $f\u0026rsquo;$ 的零交叉平均距离为：\n$x_{zc}(f)=\\pi(\\frac{\\int ^{+\\infty}{-\\infty}f\u0026rsquo;^{2}(x),dx}{\\int ^{+\\infty}{-\\infty}{f\u0026rsquo;\u0026rsquo;}^{2}(x),dx})^{1/2}$(12)\n对于 $f$ 的最大噪声响应 $x_{max}$ 定义为两倍 $x_{zc}$ 我们定义其为滤波器宽度的k倍\n$x_{max}(f)=2x_{zc}(f)=kW$(13)\n所以噪声最大的数量在 $N_n$ 区域内：\n$N_n=\\frac{2W}{x_{max}}=\\frac{2}{k}$(14)\n数学建模大概如上，可能看不懂，但是没关系，因为这并不影响我们对算法过程的理解。 算法过程 算法过程比较容易：\n 高斯平滑，采用$5 \\times5$的高斯滤波器对图像进行平滑。 使用Sobel算子检测边缘候选点，计算梯度方向，得到简化的梯度方向。 非极大值抑制，减少多重响应。 边缘跟踪，采用双阈值处理候选点。 形态学细化，对于有些较粗的边界采用形态学方法处理。  具体解析： 第一步：高斯平滑，Canny的核心检测算子是Sobel算子，所以。Canny如果按照算子划分的话属于一阶微分算子，所以它具有一阶微分算子的特性，对噪声敏感，所以，我们的第一步就是降低噪声，使用高斯平滑降低噪声，冈萨雷斯书上介绍说Canny证明高斯平滑是最好的平滑算子，这一点我并未查证，但根据高斯函数的频域特性可知，它没有振铃，而且当频率值超过 $3\\times \\delta$ 时基本趋近于零，所以可以当做一种完美的滤波器（以上这句话是我猜测的，没有理论根据），此步骤的主要目的\u0026ndash;降噪 第二步：Sobel边缘检测，此处为检测边缘候选点，并计算出候选点梯度的过程，其中Sobel可选大小为3x3，5x5，7x7或者-1，其中-1表示为Scharr算子，这个在前文已经比较过了Sobel和Scharr的性能，在这里不再解释 第三步：局部非极大值抑制，俗话就是找局部最大值，寻找方法是得出每个候选点的梯度方向，沿着梯度方向和梯度反方向比较相邻的元素，如果候选点是三个中最大的，则保留，否则，置零。 第四步：双阈值滞后滤波，这里的滞后滤波应该是比较难以理解的，我们分别设置两个阈值，大阈值和小阈值之间比例大于为3：1或2：1，这里的方法是:\n 1. 输入候选点灰度值 2. 判断是否大于大阈值，如果大于则到第4步 2. 判断是否大于小阈值，如果小于则到第5步 3. 判断该点是否连通于边缘点，不是则到第5步 4. 此点为边缘点，到第6步 5. 此点非边缘点 6. 如果图像还有未处理候选边缘像素，返回第1步，否则结束 双阈值滞后处理是Canny的核心部分，下面给出双阈值的阈值传递函数： 上图说明只有的时滞后阈值的传递函数，解释下就是如果一个候选点大于高阈值，那么它肯定是边缘点，如果大于小阈值，则需要它连通到边缘点。 我们在算法实现时使用方法是，先找到所有大于大阈值的点的集合H，H全部为边缘点，然后找出所有大于小阈值的点的集合L，其中L包含所有的H，那么以H中的每个点为种子点，以八邻域遍历L，被遍历到的为边缘点，未被遍历的为非边缘点。这里的遍历与图的遍历相同，可以使用深度优先或广度优先。\n第五步：细化结果，有时一个边缘会产生两个等价的边缘点，使用细化可以得到单像素边缘，但需注意，这两个点都是正确的点，选其中任意一个都是正确的。\n代码实现 /* * 四个角度对应编号 * 0 1 2 * 3 * 5 * 6 7 8 * edgedirction */ void getEdgeDirection(double *edgedirection,double *sample_direction,int width,int height){ double angle=0.0; for(int i=0;i\u0026lt;width*height;i++){ angle=edgedirection[i]; if(angle\u0026lt;22.5||angle\u0026gt;=337.5) sample_direction[i]=5.0; else if(angle\u0026lt;67.5\u0026amp;\u0026amp;angle\u0026gt;=22.5) sample_direction[i]=2.0; else if(angle\u0026lt;112.5\u0026amp;\u0026amp;angle\u0026gt;=67.5) sample_direction[i]=1.0; else if(angle\u0026lt;157.5\u0026amp;\u0026amp;angle\u0026gt;=112.5) sample_direction[i]=0.0; else if(angle\u0026lt;202.5\u0026amp;\u0026amp;angle\u0026gt;=157.5) sample_direction[i]=3.0; else if(angle\u0026lt;247.5\u0026amp;\u0026amp;angle\u0026gt;=202.5) sample_direction[i]=6.0; else if(angle\u0026lt;292.5\u0026amp;\u0026amp;angle\u0026gt;=247.5) sample_direction[i]=7.0; else if(angle\u0026lt;337.5\u0026amp;\u0026amp;angle\u0026gt;=292.5) sample_direction[i]=8.0; else if(angle==-1.0) sample_direction[i]=-1.0; } } /* * 四个角度对应编号 * 0 1 2 * 3 * 5 * 6 7 8 * */ void Non_MaxSuppression(double *src,double *dst,double *dirction,int width,int height){ double *temp=(double*)malloc(sizeof(double)*width*height); int dir; int y; int x; double value_c; Zero(temp, width, height); for(int j=1;j\u0026lt;height-1;j++) for(int i=1;i\u0026lt;width-1;i++){ if(dirction[j*width+i]!=-1.0){ dir=(int)dirction[j*width+i]; y=dir/3-1; x=dir%3-1; value_c=src[j*width+i]; if(value_c\u0026lt;=src[(j+y)*width+i+x]||value_c\u0026lt;src[(j-y)*width+i-x]) temp[j*width+i]=0.0; else temp[j*width+i]=value_c; } } matrixCopy(temp, dst, width, height); free(temp); } void EdgeTrack(double *src,int width,int height,Position *seed){ int x=seed-\u0026gt;x; int y=seed-\u0026gt;y; if(x\u0026gt;=0\u0026amp;\u0026amp;x\u0026lt;width\u0026amp;\u0026amp;y\u0026gt;=0\u0026amp;\u0026amp;y\u0026lt;height\u0026amp;\u0026amp;src[y*width+x]==1.0){ src[y*width+x]=2; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++){ if(!(j==0\u0026amp;\u0026amp;i==0)){ Position seed_next; seed_next.x=x+i; seed_next.y=y+j; EdgeTrack(src,width,height,\u0026amp;seed_next); } } } } void NonZeroSetOne(double *src,double *dst,int width,int height){ for(int i=0;i\u0026lt;width*height;i++) dst[i]=src[i]!=0.0?1.0:0.0; } void Canny(double *src,double *dst,int width,int height,int sobel_size,double threshold1,double threshold2){ double *temp=(double *)malloc(sizeof(double)*width*height); double *edge_a=(double *)malloc(sizeof(double)*width*height);//边缘幅度  double *edge_d=(double *)malloc(sizeof(double)*width*height);//边缘方向  double *threshold_max=(double *)malloc(sizeof(double)*width*height); double *threshold_min=(double *)malloc(sizeof(double)*width*height); /* *step1:gaussian smooth */ double gaussianmask[25]={ 2, 4, 5, 4, 2, 4, 9,12, 9, 4, 5,12,15,12, 5, 4, 9,12, 9, 4, 2, 4, 5, 4, 2}; RealConvolution(src, temp, gaussianmask, width, height, 5, 5); matrixMultreal(temp, temp, 1.0/159.0, width, height); /* *step2:sobel */ if(sobel_size==3) Scharr(temp, edge_a, edge_d, width, height); else if(sobel_size==5||sobel_size==7) Sobel(temp, edge_a, edge_d, width, height,sobel_size); /* *step3:Non_MaxSuppression */ getEdgeDirection(edge_d, edge_d, width, height); Non_MaxSuppression(edge_a, temp, edge_d, width, height); /* *step4:double threshold */ Threshold(temp, threshold_max, width, height, threshold1, MORETHAN); Threshold(temp, threshold_min, width, height, threshold2, MORETHAN); NonZeroSetOne(threshold_max,threshold_max,width,height); NonZeroSetOne(threshold_min,threshold_min,width,height); for(int j=0;j\u0026lt;height;j++){ for(int i=0;i\u0026lt;width;i++){ if(threshold_max[j*width+i]==1.0\u0026amp;\u0026amp;threshold_min[j*width+i]!=2.0){ Position p; p.x=i; p.y=j; EdgeTrack(threshold_min, width, height, \u0026amp;p); } } } /* *step5:result */ Zero(dst, width, height); for(int i=0;i\u0026lt;width*height;i++) if(threshold_min[i]==2.0) dst[i]=255.0; free(temp); free(threshold_max); free(threshold_min); free(edge_d); free(edge_a); } 实现结果 实验每步结果： 原图： STEP1： STEP2： Sobel梯度幅度结果： Sobel梯度方向结果： STEP3： STEP4：  原图： STEP1： STEP2： Sobel梯度幅度结果：\nSobel梯度方向结果： STEP3： STEP4： 总结 总结，Canny实现起来算法过程并不难，可能进一步优化加速就需要一些难度了，冈萨雷斯书中提到，第四步滞后阈值可以和第三步非极大值抑制放在一起。 待续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-8-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-canny%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第46天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e canny,边缘检测\u003c/p\u003e","title":"【数字图像处理】6.8:灰度图像-图像分割 Canny边缘检测"},{"content":"Abstract: 数字图像处理：第47天 Keywords: 边缘检测,LoG算子\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 Marr-Hildreth算子（LoG算子） 今天介绍二阶微分算子，二阶微分算子典型的是Laplace算子，LoG可以看成是一个高斯模板的拉普拉斯变换，但是也可以从根源上推导出LoG算子，而后面要介绍的DoG则是为了纯粹的减少计算，模拟LoG的一种方法。 #LoG原理 LoG最底层的原理是二阶微分算子，也就是对原始图像求二次微分的边缘定位算法，前面的边缘模型中可以得知，当使用二阶微分算子的时候，其对边缘的响应是一个零交叉，而且能够判断出高灰度方向，但二阶微分对噪声的敏感度过高，需要平滑预处理。 Marr和Hildreth【Marr和Hildreth，1980】证明了以下两个观点：\n （1）灰度变化与图像尺寸没有关系，因此检测需要不同尺度的算子 （2）灰度的突然变化会在一阶导数中引起波峰和波谷，或者二阶导数中一起零交叉\n 所以可以提出一种能变换尺寸的（当时用的是标准Sobel等那些，固定尺寸的，当时Sobel还没有扩展），在各种大小的图像上都能起作用的，可以检测模糊的相对较大的边缘，也可以检测细小的锐度集中的精细细节，当然这种算子也必须对全图所有像素点其作用。Marr和Hildreth证明LoG算子是满足上述条件的最满意的算子。\n数学原理 LoG算子： $\\nabla^2G$ 其中标准高斯函数，$G(x,y)$ 标准差 $\\delta$ ： $G(x,y)=e^{-\\frac{x^2+y^2}{2\\delta^2}}$\n对一个标准高斯函数（未归一化）进行二次偏微分： $\\nabla^2G(x,y)=\\frac{\\partial^2G(x,y)}{\\partial x^2}+\\frac{\\partial^2G(x,y)}{\\partial y^2}=[\\frac{x^2}{\\delta^4}-\\frac{1}{\\delta^2}]e^{-\\frac{x^2+y^2}{2\\delta^2}}$\n最终表达式： $\\nabla^2G(x,y)=[\\frac{x^2+y^2-2\\delta^2}{\\delta^4}]e^{-\\frac{x^2+y^2}{2\\delta^2}}$\nLoG零交叉出现在$x^2+y^2=2\\delta^2$处LoG函数形状如下图，也被叫做墨西哥草帽算子。 下面全方位无死角观察下，说明，上面公式给出的是倒置的草帽，我们这里给他加了个负号，让它正过来。。。 剖面图，沿着直径切开： 模板性质  性质1，该模板对平坦区域应该无响应，所以算子内系数和应该为0，使用公式计算出来的结果不为零，需要整体上下平移模板 性质2，LoG可以使用laplace算子和高斯模板进行卷积后求得，其等效于使用LoG大小和标准差的高斯平滑后得到laplace结果 性质3，得到的结果要检测零交叉来定位边缘，因为噪声等原因，这里进行判断时可以使用阈值，也就是当出现零交叉的时候还要判断下正负值的差的绝对值是否满足阈值要求。 性质4，LoG模板的大小和标准差的选择关系，遵循高斯分布的$3\\delta$原则，也就是当位置超过均值正负$3\\delta$以外的值很小，也就是说，LoG模板应该选择大于$6\\delta$的最小奇数作为模板大小，过大效果不会有提高反而增加计算量，过小会造成截断，无法得到正确结果。  代码 //3x3的邻域内，如果中心像素大于阈值，切周围存在负像素值，则此点为0交叉点 void findCross(double *src,double *dst,int width,int height,double threshold){ double *dsttemp=(double *)malloc(sizeof(double)*width*height); Zero(dst, width, height); double c_value=0.0; int flag=1; Zero(dsttemp, width, height); for(int j=1;j\u0026lt;height-1;j++){ for(int i=1;i\u0026lt;width-1;i++){ c_value=src[j*width+i]; flag=1; for(int m=-1;m\u0026lt;=1\u0026amp;\u0026amp;flag;m++) for(int n=-1;n\u0026lt;=1;n++){ if(c_value\u0026gt;threshold\u0026amp;\u0026amp;src[(j+m)*width+i+n]\u0026lt;-0.1){ flag=0; dsttemp[j*width+i]=255.0; break; } } } } matrixCopy(dsttemp, dst,width, height); free(dsttemp); } //得到LoG模板，根据公式得出double型模板，然后调整整体位置，使模板内系数和为0 void getLoGMask(double *mask,int width,int height,double delta){ int center_x=width/2; int center_y=height/2; double x,y; for(int j=0;j\u0026lt;height;j++){ for(int i=0;i\u0026lt;width;i++){ x=i-center_x; y=j-center_y; mask[j*width+i]=-((x*x+y*y-2*delta*delta)/pow(delta,4))*(exp(-(x*x+y*y)/(2*delta*delta))); } } double sum=0.0; for(int i=0;i\u0026lt;width*height;i++){ sum+=mask[i]; } double di=sum/(double)(width*height); for(int i=0;i\u0026lt;width*height;i++){ mask[i]-=di; } } //LoG边缘检测完整过程， //生成模板 //与图像卷积 //寻找0交叉 double LoG(double *src,double *dst,int width,int height,int m_width,int m_height,double delta,double threshold){ double *dsttemp=(double *)malloc(sizeof(double)*width*height); double * mask=(double *)malloc(sizeof(double)*m_width*m_height); getLoGMask(mask, m_width,m_height,delta); RealConvolution(src, dsttemp, mask, width, height, m_width, m_height); findCross(dsttemp,dst,width,height,threshold); free(dsttemp); free(mask); return findMatrixMax(dst,width,height); } 操作结果 以下为算子对图像操作的结果 原图： LoG结果： 零交叉检测结果： 原图： 零交叉检测结果： 总结 观察上述结果阈值为0的结果，或出现意大利通心粉效应\u0026ndash;所有边缘会形成一个闭环，典型的是那些圈圈，这是一个严重的缺陷，所以不使用0阈值，而是使用超过0的阈值来解决这个问题，LoG的边缘检测效果还是不错的，当选择 $\\delta$ 的时候可以使用多尺度，然后选择所有尺度都出现的公共边缘（逻辑与），不过这种计算量过大，一般作为考量 $\\delta$ 选择的一种参考。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-6-9-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-marr-hildreth%E7%AE%97%E5%AD%90-log%E7%AE%97%E5%AD%90.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第47天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 边缘检测,LoG算子\u003c/p\u003e","title":"【数字图像处理】6.9:灰度图像-图像分割 Marr-Hildreth算子（LoG算子）"},{"content":"Abstract: 数字图像处理：第49天 Keywords: 阈值处理\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 阈值处理综述 新年第一篇博客，图像分割进行到阈值处理，前面学的边缘检测，然后将边缘连接起来，达到分割区域的目的，用到的基础原理是图像灰度的变化，而阈值处理用到的是阈值的不变，也就是把具有相同或相似的灰度的一类像素定义为一个区域，根据这个特点来定义并区分一个区域。而阈值可以理解为一个界限，或者一个划分，超过这个界限的是一个区域，没超过划分为另一个区域，而核心问题式确定这个阈值。\n阈值概述 阈值处理用到的唯一公式如下，这是阈值处理的核心操作： 式子中T就是我们的阈值，我们将要学习的算法都是来确定这个T的。 当然这是单阈值的方法，如果有多个模式的时候我们也会使用多阈值，但多阈值难度较大，一般很少使用。 阈值根据不同得到阈值的方法可以将阈值分成以下几种类型： 可见阈值可以分成全局阈值，局部阈值和动态阈值，这些不同的阈值要根据不同的图像情况来确定使用不同的方法。 理解阈值我们可以将一副图像想象成一个三维场景，图像长和宽为三维空间中的x轴和y轴，而灰度强度为z轴，对于lena图，用Matlab产生下图，三维图像沿着x轴旋转： 其他角度观察此图： 我们使用一个简单的阈值处理，比如使用128作为阈值，大于128的灰度设为128，小于128的保持不变，可以理解为以横着切一刀，效果就下面： 阈值处理通过观察原图的直方图也可以得出很好的理解，当直方图为双峰，且波谷越深越宽的，使用阈值处理越好。 阈值处理可以产生二值图或者其他灰度图，可以根据需要来产生所需要的结果，一般可以产生下面几种结果，对于单阈值：\nMinvalue最小值，最小灰度值，0 Maxvalue最大值，最大灰度值，255 dst(x,y)=src(x,y)\u0026gt;T?src(x,y):Minvalue; dst(x,y)=src(x,y)\u0026gt;T?Maxvalue:src(x,y); dst(x,y)=src(x,y)\u0026gt;T?Maxvalue:Minvalue; //此处产生二值图像 dst(x,y)=src(x,y)\u0026gt;T?Minvalue:Maxvalue; //此处产生二值图像,将上图反转 影响阈值处理的因素 影响阈值处理的主要两个因素是噪声和光照观察下图为两个灰度的图像。 原图： 原图直方图： 加入标准差为1%（255的1%），均值为0的高斯噪声： 直方图： 加入标准差为3%（255的3%），均值为0的高斯噪声： 直方图： 加入标准差为5%（255的5%），均值为0的高斯噪声： 直方图： 加入标准差为7%（255的7%），均值为0的高斯噪声： 直方图： 加入标准差为10%（255的10%），均值为0的高斯噪声： 直方图： 加入标准差为15%（255的15%），均值为0的高斯噪声： 直方图： 以上直方图的变化可以清楚的发现，对于原图可以很容易使用一个阈值划分成两个部分，当噪声不断增加划分难度越来越大，阈值选择也越来越难，当噪声到15%时，双峰图消失，变成了单峰，对阈值确定产生了巨大的难度。 对于阈值处理第二个严重的影响是不均匀的光照，当图像有偏光，图像直方图会发生漂移如下： 使用偏光照射： 原图直方图： 偏移直方图： 背景系数图： 背景直方图： 对于上述这些影响因素，在阈值处理前需要相应的操作来去噪或者改变偏照的影响，具体方法可以使用顶帽操作或者使用逆光照模板处理，再或者使用可变阈值来处理。\n影响直方图阈值的关键因素是：\n 波峰间间隔，波峰离得越远，分离这些模式的机会会越好 图像中的噪声内容，模式虽噪声的增大而展宽 物体和背景的相对尺寸 光源的均匀性 图像反射特性的均匀性  针对上述这些特征和特殊情况开发出一系列的算法，使阈值处理达到实时和有效分割目标和背景。\n算法分类 根据不同的方法阈值算法可以分为以下几类： 其中包括：\n 均值法 P-tile法 直方图法 边缘最大技术 visual technique法  总结 阈值处理是相对较简单的一种分割方法（对于单个阈值），其关键是确定阈值，后面将详细介绍几种阈值确定方法。 祝大家新年快乐 待续。。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-0-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E7%BB%BC%E8%BF%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第49天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 阈值处理\u003c/p\u003e","title":"【数字图像处理】7.0:灰度图像-图像分割 阈值处理综述"},{"content":"Abstract: 数字图像处理：第50天 Keywords: 阈值处理,平均阈值\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 阈值处理之平均阈值 好久没写博客了，已然不熟练了，过完年整个人都不好了，哈哈，到刚才为止算是把图像分割学习了一下，这两天把学习结果和代码简单总结一下。 前面已经介绍了边缘检测，和边缘修复，阈值处理的基本概念也进行了一定介绍。阈值处理速度快，算法简单，所以应用比较广泛，算法的基本问题在于计算出阈值处理的阈值，在接下来的几篇我可，介绍几种确定阈值的算法，基本数学知识都来自统计学，包括最简单的平均数，p-分位数，方差等数学知识。 下面就从最简单的均值阈值开始介绍。\n平均阈值 均值处理属于阈值中最简单的一种，其使用的一副图像所有灰度的均值作为阈值，分割图像。 根据统计学知识，求均值可以使用概率方法：\n 计算图像灰度直方图 归一化直方图，得出直方图每一项的概率 直方图的横坐标与概率的成绩然后求和 $threshold=mean=\\sum^{L-1}_{i=0}p(i)\\times i$  根据以上均值得到阈值，然后根据阈值处理基本公示处理整幅图像。\n代码 ////计算从start到end的直方图的平均值,hist未归一化 double getMeaninHist(int start,int end,int *hist){ int hist_count=0; double hist_value=0; for(int i=start;i\u0026lt;end;i++){ hist_count+=hist[i]; hist_value+=(double)hist[i]*i; } return hist_value/(double)hist_count; } /*********************************************************************************/ /*********************************************************************************/ //均值法求阈值 //阈值等于全图的像素的平均值 void MeanThreshold(double *src,double *dst,int width,int height,int type){ int hist[GRAY_LEVEL]; double threshold_value=0.0; InitHistogram(hist); setHistogram(src, hist, width,height); threshold_value=getMeaninHist(0, GRAY_LEVEL, hist); Threshold(src,dst, width, height, threshold_value,type); } 结果 观察一下结果：\n 原图： 原图直方图： 平均阈值： 阈值处理结果：  原图加入1%的高斯噪声 未处理： 直方图： 平均阈值： 阈值处理结果：  原图加入5%的高斯噪声 未处理： 直方图： 平均阈值： 阈值处理结果：  原图加入7%的高斯噪声 未处理： 直方图： 平均阈值： 阈值处理结果：  lena图均值阈值处理： 原图： 处理后结果： 阈值： 总结 均值阈值可以完成一些较为简单的的处理，但对目标与背景的大小敏感，也就是直方图那两个峰的面积大小有关，如果这两个峰相对大小相差不多，那么均值相对效果较好，如果两个峰相差太多，也就是背景比目标大很多或相反目标比背景大很多时，结果失效。其次也受到噪声影响，但没有上一因素影响大。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-1-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E4%B9%8B%E5%B9%B3%E5%9D%87%E9%98%88%E5%80%BC.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第50天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 阈值处理,平均阈值\u003c/p\u003e","title":"【数字图像处理】7.1:灰度图像-图像分割 阈值处理之平均阈值"},{"content":"Abstract: 数字图像处理：第59天 Keywords: 分水岭算法\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 区域分割之分水岭算法 今天已经是第60篇博客了，这六十篇每一篇平均要两天左右，所以，在过去的四个月学到了这么多知识，想想挺开心，但学的越多就会发现自己不会的越多。从小学到大学，这么多年一直以学习为主要工作但学习又有很多阶段，对于通用知识，比如小学的语文数学此观点不适用，对于一些专业性较强的知识，感觉会有两个很主要的阶段，感觉自己目前处于入门阶段，由于数字图像涉及数学的知识较多，还有信号和信息论的知识，所以，感觉还是比较难学科，不然招聘公司也不会一年几十万的养着图像处理工程师。 下面的图是本人的一点点见解，只是自己总结的，没有实践，也没有科学依据，不喜勿喷： 我感觉图像处理能分成三个阶段，或者更多，第一阶段的人很多，听说这行前景好或者工资高的人，多半会学习点图像的知识，比如彩色空间啊，了解下OpenCV啊等等，还有一些属于纯属上课被逼无奈的，比如我们学校就对电子信息类专业和通信类开数字图像处理这门课，而且我当时考试还挂了。。。。这部分大家会接触一些名词，一些简单算法，有用心的同学可能会实现下代码，这阶段风景不错，而且好多名词可以拿来忽悠HR或者忽悠导师，得到一份不错的工作，或者做点导师的项目。 这个阶段多半使用现成的函数库，了解了基本算法或者听别人说一些算法，自己来跑结果，这个阶段Matlab的用户量较大。 第1阶段后半期也就是平台期，这个阶段是做了一段时间有一定算法使用基础和代码能力的工程师，多半在各企业负责最底层的图像算法编写，在他们面前是一座大山，和一个路口，继续做图像还是转管理。 如果继续选择图像，就会面临一个峭壁，具体是算法底层的数学，说的数学，总是困难的，爬这个峭壁的动力可以是挣更多的钱，或者爱好，因为一旦到达阶段2，就能成为首席图像处理工程师，到任何需要图像处理的公司，都能够独当一面，这些人已经到了不缺钱的地步，而且在行业内一定有一定名气。 第二阶段的一旦到达，可以说是事业的平稳期，或者巅峰，不缺钱，还能独自决定一些技术层面上的事，指挥手下阶段1的员工工作，这个阶段面临的也是一个选择，就是靠这个吃一辈子饭，绝对没问题，再有就是向更高的境界冲击。 第三阶段没有尽头，因为能促使进入阶段三的动力只有爱好，这部分挣的钱可能没有阶段2挣的多，而且难度更大，看不到尽头，所以这部分属于探索阶段，在这个阶段上看到的一些，都能推动未来学科的发展，所以这个阶段的人多半是在实验室和数学中忙碌一生，然后几十年后出现在各大论文和教材中。 以上属于个人猜想或者愚见，想法幼稚，仅供参考。\n算法描述 今天废话太多了，哈哈，其实上面的可以新开一篇博客单独写出来，但觉得，首先自己年轻视野狭窄，第二水平太低，所以当做废话夹在本文中。 今天说分水岭算法，分水岭算法族是一个思想引出的一些列实现方法。 算法内容：首先将二维灰度图像抽象为三维地形，横纵坐标表示经纬度，灰度表示高度，这样就产生了一个地形图，地形中有高山有盆地，我们的任务就是找到盆地并且给盆地划定地盘。 算法过程，将最低点（灰度）作为起始点，开始从下注水，想泉水一样，水位逐渐上升，所有点都是上下透水的，但不会横向流动，每产生一个新的积水盆地时标记这个盆地使之与原有盆地区别开，当两个盆地内的水要汇集的时候，在此点建立一个水坝，将水分开，此水坝无限高，水永远不会溢出，依次建立所有水坝，最后的水坝就是所谓的分水岭或分水线，也就是得到的分割线。 算法步骤：\n 检测图像中所有的盆地，并依次标记，（此时最终的分割区域数已确定） 将盆地周围邻域的像素按照灰度值大小入队（队列，数据结构，先进先出） 按照灰度值顺序将像素出队，进行判断 判断1：如果像素周围只存在一个盆地标记，将此像素划分到此盆地 判断2：如果像素周围存在两个以及以上盆地，次像素为分水岭。 如果此像素是盆地像素，将该像素邻域像素入队。 重复步骤3，直到队列为空。 得出结果  此算法描述只是分水岭算法之一，冈萨雷斯的书中使用的是形态学的描述方式，但与上述过程原理一致，感兴趣者可自行查阅。 解释下第一步，第一步是本算法的核心，此步骤首先要确定盆地还是非盆地，可以抽象成下面几种模型： 图像中的红色框内为非初始盆地，或叫做盆底，绿色为盆底，对于第二种盆地的确定需要使用图的深度或者广度优先搜索，判断所有候选点，来判断是第二种模型还是第三种。\n代码 // // Watershed // tony.sheng.tan@gmail.com // Created by 谭升 on 15/03/03. // Copyright (c) 2015年 谭升. All rights reserved. // /* typedef struct PriQueueNode_ PriQueueHead; typedef struct NLevelPriQueueNode_ * NLevelPriQueue; typedef struct NLevelPriQueueNode_ NLevelPriNode; typedef struct ExPix_ Pix_Label; * ___ ____________________ * | P |-\u0026gt;| NLevelPriQueue | * |---| |--------------------| * | r |-\u0026gt;| NLevelPriQueue | * |---| |--------------------| * | i |-\u0026gt;| NLevelPriQueue | * |---| |--------------------| * | Q |-\u0026gt;| NLevelPriQueue | * |---| |--------------------| * | u |-\u0026gt;| NLevelPriQueue | * |---| |--------------------| * | e |-\u0026gt;| NLevelPriQueue | * |---| |--------------------| * | u |-\u0026gt;| NLevelPriQueue | * |---| |--------------------| * | e |-\u0026gt;| NLevelPriQueue | * |___| |____________________| struct NLevelPriNode_{ int x; int y; NLevelPriQueue next; }; struct PriQueueNode_{ int nodeNum; NLevelPriQueue head; NLevelPriQueue tail; }; struct ExPix_{ int grayvalue; int label; }; */ #include \u0026#34;watershed.h\u0026#34;#include \u0026lt;stdio.h\u0026gt; //将非极小值且与极小值相邻的元素入队 void inQueue(PriQueueHead* priQueue,int gray_level,int x,int y){ priQueue[gray_level].nodeNum++; ///malloc new node  NLevelPriNode* newNode=(NLevelPriNode *)malloc(sizeof(NLevelPriNode)); newNode-\u0026gt;x=x; newNode-\u0026gt;y=y; newNode-\u0026gt;next=NULL; if(priQueue[gray_level].head==NULL){ priQueue[gray_level].head=newNode; priQueue[gray_level].tail=newNode; }else{ priQueue[gray_level].tail-\u0026gt;next=newNode; priQueue[gray_level].tail=newNode; } } //判断极小值是平底锅型还是台阶形状，使用图的深度优先搜索 int isPan(int *src,int width,int height,double value,int x,int y){ src[y*width+x]=-value; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++) if(j+y\u0026gt;=0\u0026amp;\u0026amp;i+x\u0026gt;=0\u0026amp;\u0026amp;j+y\u0026lt;height\u0026amp;\u0026amp;i+x\u0026lt;width\u0026amp;\u0026amp;(i!=0||j!=0)){ if(src[(j+y)*width+i+x]\u0026lt;value\u0026amp;\u0026amp;src[(j+y)*width+i+x]\u0026gt;0) return 0; } for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++) if(j+y\u0026gt;=0\u0026amp;\u0026amp;i+x\u0026gt;=0\u0026amp;\u0026amp;j+y\u0026lt;height\u0026amp;\u0026amp;i+x\u0026lt;width\u0026amp;\u0026amp;(i!=0||j!=0)){ if(src[(j+y)*width+i+x]==value){ return(isPan(src,width, height, value, i+x, j+y)); } } return 1; } //由于判断平底锅时使部分数据损坏，现进行恢复 void repairPan(int *src ,int width,int height,double value,int x,int y){ src[y*width+x]=-value; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++) if(j+y\u0026gt;=0\u0026amp;\u0026amp;i+x\u0026gt;=0\u0026amp;\u0026amp;j+y\u0026lt;height\u0026amp;\u0026amp;i+x\u0026lt;width\u0026amp;\u0026amp;(i!=0||j!=0)){ if(src[(j+y)*width+i+x]==value){ repairPan(src, width, height, value, x+i, y+j); } } } //平底锅形状，标记极小值为255 void setMinimal(int *src,double *dst,int width,int height,int value,int x,int y){ dst[y*width+x]=255.0; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++) if(j+y\u0026gt;=0\u0026amp;\u0026amp;i+x\u0026gt;=0\u0026amp;\u0026amp;j+y\u0026lt;height\u0026amp;\u0026amp;i+x\u0026lt;width\u0026amp;\u0026amp;(i!=0||j!=0)){ if(src[(j+y)*width+x+i]==value\u0026amp;\u0026amp;dst[(j+y)*width+x+i]==0.0) setMinimal(src,dst,width, height, value, x+i,y+j); } } //台阶形状，标记非极小值127 void setUnMinimal(int *src,double *dst,int width,int height,int value,int x,int y){ dst[y*width+x]=127.0; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++) if(j+y\u0026gt;=0\u0026amp;\u0026amp;i+x\u0026gt;=0\u0026amp;\u0026amp;j+y\u0026lt;height\u0026amp;\u0026amp;i+x\u0026lt;width\u0026amp;\u0026amp;(i!=0||j!=0)){ if(src[(j+y)*width+x+i]==value\u0026amp;\u0026amp;dst[(j+y)*width+x+i]==0.0) setUnMinimal(src,dst,width, height, value, x+i,y+j); } } //数据类型从double到int void Double2Int(double *src,int* dst,int width,int height){ for(int i=0;i\u0026lt;width*height;i++) dst[i]=(int)src[i]+1; } //寻找极小值，包括单点极小值和平底锅 void findMinimal(double *src,double *dst,int width,int height){ int *temp=(int *)malloc(sizeof(int)*width*height); double *dsttemp=(double *)malloc(sizeof(double)*width*height); Zero(dsttemp, width, height); Double2Int(src, temp, width, height); int lessthan=0; int equ=0; double min=findMatrixMin(src, width, height); for(int i=0;i\u0026lt;width*height;i++) if(src[i]==min) dsttemp[i]=255.0; for(int j=0;j\u0026lt;height;j++){ for(int i=0;i\u0026lt;width;i++){ lessthan=0; equ=0; int pix=temp[j*width+i]; if(dsttemp[j*width+i]==0.0){ for(int m=-1;m\u0026lt;2;m++) for(int n=-1;n\u0026lt;2;n++) if(j+m\u0026gt;=0\u0026amp;\u0026amp;i+n\u0026gt;=0\u0026amp;\u0026amp;j+m\u0026lt;height\u0026amp;\u0026amp;i+n\u0026lt;width){ if(m!=0||n!=0){ if(temp[(j+m)*width+i+n]\u0026lt;pix) lessthan=1; if(temp[(j+m)*width+i+n]==pix) equ=1; } } if(equ==1\u0026amp;\u0026amp;lessthan==0){ if(isPan(temp, width, height,pix, i, j)){ //repairPan(temp,width, height, -pix, i,j);  setMinimal(temp,dsttemp, width, height, -pix, i, j); }else { repairPan(temp,width, height, -pix, i,j); setUnMinimal(temp,dsttemp, width, height, pix, i, j); } } if(lessthan==1) dsttemp[j*width+i]=127.0; if(0==lessthan\u0026amp;\u0026amp;0==equ) dsttemp[j*width+i]=255.0; } } } matrixCopy(dsttemp, dst, width, height); free(dsttemp); free(temp); } //标记极小值的label，从-1开始向下增长 -2 -3 -4 -5 -6 -7..... void LabelMinimal(double * src,Pix_Label* dst,int width,int height,int x,int y,int label){ dst[y*width+x].label=label; for(int i=-1;i\u0026lt;2;i++){ for(int j=-1;j\u0026lt;2;j++) if(x+i\u0026gt;=0\u0026amp;\u0026amp;x+i\u0026lt;width\u0026amp;\u0026amp;y+j\u0026gt;=0\u0026amp;\u0026amp;y+j\u0026lt;height\u0026amp;\u0026amp;(i!=0||j!=0)) if(src[(y+j)*width+x+i]==255.0\u0026amp;\u0026amp;dst[(y+j)*width+x+i].label==0){ LabelMinimal(src, dst, width, height, x+i, y+j, label); } } } //初始化label数组，此数组与图像数组多加了label void InitLabelMat(double *src,Pix_Label* dst,double *mask,int width,int height){ for(int i=0;i\u0026lt;width*height;i++){ dst[i].grayvalue=src[i]; dst[i].label=0; } int label_minimal=-1; for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ if(mask[j*width+i]==255.0\u0026amp;\u0026amp;dst[j*width+i].label==0){ LabelMinimal(mask, dst, width,height,i,j, label_minimal); label_minimal--; } } } //初始化队列头数组 void InitPriQueue(PriQueueHead* priQueue,Pix_Label *srclabel,int width,int height){ for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ priQueue[i].head=NULL; priQueue[i].nodeNum=0; priQueue[i].tail=NULL; } int inqueue=0; for(int j=0;j\u0026lt;height;j++) for(int i=0;i\u0026lt;width;i++){ inqueue=0; if(srclabel[j*width+i].label==0){ for(int m=-1;m\u0026lt;2;m++) for(int n=-1;n\u0026lt;2;n++){ if(m+j\u0026gt;=0\u0026amp;\u0026amp;m+j\u0026lt;height\u0026amp;\u0026amp;n+i\u0026gt;=0\u0026amp;\u0026amp;n+i\u0026lt;width\u0026amp;\u0026amp;(m!=0||n!=0)) if(srclabel[(m+j)*width+n+i].label\u0026lt;0) inqueue=1; } if(inqueue){ inQueue(priQueue,srclabel[j*width+i].grayvalue,i,j); srclabel[j*width+i].label=INQUEUE; } } } //for(int i=0;i\u0026lt;GRAY_LEVEL;i++)  // printf(\u0026#34;g:%d NumofNode:%d\\n\u0026#34;,i,priQueue[i].nodeNum);  } int PirQueueisEmpty(PriQueueHead* priqueue){ int sum=0; for(int i=0;i\u0026lt;GRAY_LEVEL;i++) sum+=priqueue[i].nodeNum; return !sum; } NLevelPriNode* outQueue(PriQueueHead* priqueue){ NLevelPriNode* node=NULL; if(!PirQueueisEmpty(priqueue)) for(int i=0;i\u0026lt;GRAY_LEVEL;i++) if(priqueue[i].nodeNum!=0){ node=priqueue[i].head; priqueue[i].head=node-\u0026gt;next; priqueue[i].nodeNum--; break; } return node; } void findWaterShed(Pix_Label * srclabel,PriQueueHead* priqueue,int width,int height){ NLevelPriNode* node=outQueue(priqueue); while(node!=NULL){ int y=node-\u0026gt;y; int x=node-\u0026gt;x; //printf(\u0026#34;x:%d y:%d \\n\u0026#34;,x,y);  int label=0; int isWatershed=0; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++){ if(j+y\u0026gt;=0\u0026amp;\u0026amp;j+y\u0026lt;height\u0026amp;\u0026amp;i+x\u0026gt;=0\u0026amp;\u0026amp;i+x\u0026lt;width\u0026amp;\u0026amp;(j!=0||i!=0)){ if(srclabel[(j+y)*width+x+i].label\u0026lt;0){ if(label==0) label=srclabel[(j+y)*width+x+i].label; else if(label!=srclabel[(j+y)*width+x+i].label){ isWatershed=1; } } } } if(isWatershed) srclabel[y*width+x].label=WATERSHED; else if(label\u0026lt;0){ srclabel[y*width+x].label=label; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++){ if(j+y\u0026gt;=0\u0026amp;\u0026amp;j+y\u0026lt;height\u0026amp;\u0026amp;i+x\u0026gt;=0\u0026amp;\u0026amp;i+x\u0026lt;width\u0026amp;\u0026amp;(j!=0||i!=0)){ if(srclabel[(j+y)*width+x+i].label==0){ inQueue(priqueue, srclabel[(j+y)*width+i+x].grayvalue, i+x, j+y); srclabel[(j+y)*width+i+x].label=INQUEUE; } } } } else if(label==0\u0026amp;\u0026amp;isWatershed==0){ srclabel[y*width+x].label=0; } free(node); node=outQueue(priqueue); } } //meyer分水岭方法 void MeyerWatershed(double *src,double *dst,int width,int height){ double *dst_temp=(double *)malloc(sizeof(double)*width*height); Zero(dst_temp, width, height); Pix_Label * srclabel=(Pix_Label*)malloc(sizeof(Pix_Label)*width*height); PriQueueHead priqueue[GRAY_LEVEL]; double *minimal=(double *)malloc(sizeof(double)*width*height); Zero(minimal, width, height); findMinimal(src, minimal, width, height); InitLabelMat(src, srclabel, minimal, width, height); //for(int j=0;j\u0026lt;height;j++){  // for(int i=0;i\u0026lt;width;i++)  // printf(\u0026#34; l:%3d|\u0026#34;,srclabel[j*width+i].label);  // printf(\u0026#34;\\n\u0026#34;);  //}  InitPriQueue(priqueue, srclabel, width, height); /*for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ NLevelPriNode *node=priqueue[i].head; printf(\u0026#34;%d:\u0026#34;,i); while (node!=NULL) { printf(\u0026#34;x:%d,y:%d \u0026#34;,node-\u0026gt;x,node-\u0026gt;y); node=node-\u0026gt;next; } printf(\u0026#34;\\n\u0026#34;); }*/ findWaterShed(srclabel, priqueue, width, height); for(int i=0;i\u0026lt;width*height;i++) if(srclabel[i].label==WATERSHED) dst_temp[i]=255.0; matrixCopy(dst_temp, dst, width, height); free(dst_temp); free(srclabel); free(minimal); } 实现效果 总结 以上实现目前还有点缺陷，就是盆底面积很大的时候，递归计算会crash，初步设想的解决办法是使用迭代替换递归，分割结果相对来讲对于前面的算法来说相对较好，但运算速度较慢，对噪声敏感。 灰度图像的内容简单的介绍至此，下篇开始介绍彩色基础知识和算法，欢迎收看。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-10-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E5%8C%BA%E5%9F%9F%E5%88%86%E5%89%B2%E4%B9%8B%E5%88%86%E6%B0%B4%E5%B2%AD%E7%AE%97%E6%B3%95.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第59天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 分水岭算法\u003c/p\u003e","title":"【数字图像处理】7.10:灰度图像-图像分割 区域分割之分水岭算法"},{"content":"Abstract: 数字图像处理：第51天 Keywords: 阈值处理,p-tile\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 阈值处理之P-Tile阈值 废话不多说，因为刚才（上一篇）已经说过了，p-tile可能听起来挺可怕，没关系，说个它的对象\u0026ndash;中位数，这个都知道吧，数值排排站，然后选出中间那个，或者说，假如数据一共有N个，那么中位数就是排在第 $N\\times 0.5%$ 的那个数；p位数，也叫p分位，可以理解为数值排排站以后第 $N*p$ 的那个数。\np-tile均值 根据上面对p分位的理解，可以看出这个阈值处理方法是半自动的方法，也就是阈值的生成需要人工控制，就是要手动输入p分位的p，下面代码中p取值$(0 ,1]$\n代码 //阈值法，p分位法 //p分位为统计学方法 //当p为0.5时为中位数 void PtileThreshold(double *src,double *dst,double p_value,int width,int height,int type){/*0\u0026lt;p_value\u0026lt;1*/ int total_pix_count=width*height; int pix_count=0; int hist[GRAY_LEVEL]; double threshold_value=0.0; InitHistogram(hist); setHistogram(src, hist, width,height); for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ pix_count+=hist[i]; if(pix_count\u0026gt;=(int)((double)total_pix_count*p_value)){ threshold_value=(double)i; break; } } Threshold(src,dst, width, height, threshold_value,type); } 效果 原图一个只有两个灰度值的图像，这里使用对其加入5%的高斯噪声， 未处理图像： 未处理时的直方图： 观察直方图，估计出最佳阈值位置： 下面使用不同的p值来测试结果： 可以看出，第二次（我试了好久。。。。。）测试结果能够得出最好结果。\n lena图处理测试： 总结 首先确定p值需要经验或实验，所以P-Tile方法应用于自适应有些困难，其次，影响处理结果的因素是目标与背景大小的比例，目标过大背景过小或者背景过大目标过小，都会对测试结果产生很大影响，其次是噪声，噪声也会对实验结果产生影响。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-2-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E4%B9%8Bp-tile%E9%98%88%E5%80%BC.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第51天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 阈值处理,p-tile\u003c/p\u003e","title":"【数字图像处理】7.2:灰度图像-图像分割 阈值处理之P-Tile阈值"},{"content":"Abstract: 数字图像处理：第52天 Keywords: 迭代阈值,迭代均值\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像\u0026ndash;图像分割 阈值处理之迭代均值阈值 废话开始，本来打算昨天写这篇，半路被几个孙子（大学室友）拉去打Dota，结果输了一晚上，暴雪出的魔兽争霸和魔兽世界可谓游戏中的经典，一个是完美的游戏逻辑设计，其次是游戏画面，然后就有了各路模仿者，有感而发\u0026ndash;做面向用户的应用程序，在满足软件基本要求的基础上，完美的逻辑设计和优秀的人机交互将能使软件经久不衰。\n迭代均值算法 下面开始介绍迭代均值，迭代均值的基本算法如下\n 初始化阈值为$T_0$ 用$T_i$将全部像素值分为两部分$G_1$和$G_2$，计算两部分的均值分别为$m_1$和$m_2$ 用$m_1$和$m_2$产生新的阈值 $T_i=\\frac{m_1+m_2}{2}$ 迭代上面步骤2和步骤3，直到 $|T_i-T_{i-1}|\u0026lt;\\Delta T$  收敛条件是迭代后阈值变化小于一个收敛控制条件，这个条件决定阈值收敛的精确度，当$\\Delta T$设置过大，迭代次数减小，但精确度降低，如果$\\Delta T$设置过小，迭代次数增加，准确度提高。 其次是初始化阈值$T_0$的选择，选择的阈值必须左右都有像素，尽量选择靠近中间的像素，这样可以有效的减少迭代次数。在代码中我使用的初始化阈值是，找出像素最大值和最小值，然后计算出他们的平均值。 #代码 此算法比较简单，上代码：\n//迭代法求阈值，初始化一个阈值 //将直方图分为两部分 //求出两部分的均值 //这两个均值的均值为新的阈值，迭代这些步骤 //deta_t 精确度，当迭代n次以后阈值tn与第n-1次迭代结果tn-1相差小于deta_t时，迭代停止。 void IterativeThreshold(double *src,double *dst,double deta_t,int width,int height,int type){ int hist[GRAY_LEVEL]; InitHistogram(hist); setHistogram(src, hist, width,height); int hist_min=findHistogramMax(hist); int hist_max=findHistogramMin(hist); double threshold_value=(hist_max+hist_min)/2.0; double threshold_last=threshold_value; while (threshold_last-threshold_value\u0026gt;=deta_t|| threshold_last-threshold_value\u0026lt;=-deta_t) { threshold_last=threshold_value; double mean1=getMeaninHist(0, (int)threshold_value, hist); double mean2=getMeaninHist((int)threshold_value,hist_max+1, hist); threshold_value=(mean1+mean2)/2.0; } Threshold(src,dst, width, height, threshold_value,type); } 结果与分析 观察运行结果： 未加噪声的图像，仅有两个灰度值： 直方图：  加入1%的高斯噪声： 直方图：  加入3%的高斯噪声： 直方图：  加入5%的高斯噪声： 直方图：  加入7%的高斯噪声： 直方图：  加入9%的高斯噪声： 直方图：  加入11%的高斯噪声： 直方图：  lena图测试结果： 直方图：  baboon图测试结果： 直方图：  结论 迭代均值能够以较小的计算代价得出相对准确的阈值，只需要输入一个控制精度的参数，所以属于相对自动的算法（与p-tile相比），但与前面提到的一样，算法受到目标大小的影响，当目标和背景的面积相对大小相近的时候算法计算效果较好，当目标比背景大很多的时候，算法基本没有效果，背景比目标大很多的时候同样失效（观察直方图面积可以大概观察出目标与背景的比例）。 另一个问题就是噪声影响，观察上面11%的结果，其受到噪声和目标大小的双重影响：\n所以效果不理想。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-3-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E4%B9%8B%E8%BF%AD%E4%BB%A3%E5%9D%87%E5%80%BC%E9%98%88%E5%80%BC.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第52天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 迭代阈值,迭代均值\u003c/p\u003e","title":"【数字图像处理】7.3:灰度图像--图像分割 阈值处理之迭代均值阈值"},{"content":"Abstract: 数字图像处理：第53天 Keywords: 谷底阈值,峰顶平均\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 阈值处理之谷底阈值、峰顶平均 废话开始，这篇介绍两种基于直方图的方法，前面介绍的几种阈值处理方法，可以使用直方图作为处理工具，也可以不使用直方图，直接操作图像也可以，不过建议使用直方图，因为直方图只进行一次计算，免去后续多次的访问全图像素。 今天介绍的算法有意个前提条件，就是直方图必须是一个双峰图，我们通过找到双峰之间的谷底，或者双峰值的平均值作为阈值。 其次是对直方图进行加工，怎么加工？平滑，通过对直方图的平滑使不标准的直方图变得平滑，最终达到双峰的效果。\n算法描述 算法过程详解\n 计算图像直方图 使用$\\frac{1}{4}[1,2,1]$平滑直方图，直到直方图平滑成双峰图。 计算直方图的一阶导数，按照符号（正负）将一阶导数“格式化”\u0026ndash;一阶导数大于0的设置为1，等于0的设置为0，小于0的设置为-1 寻找峰顶和谷底，从1 -\u0026gt; 0 -\u0026gt; -1的为峰顶从-1 -\u0026gt; 0 -\u0026gt; 1的为谷底 谷底为阈值，或者使用双峰的峰值的平均值作为阈值，进行阈值处理。 解释下第2步，如何判断是否是双峰，同样使用第3步的方法，得出“格式化”的直方图，如果整个“格式化”直方图的模式是： 1，1，1，1，1，1，1，0,（0，0，0，0）-1，-1，-1，-1，-1，-1，-1，0（0，0，0，0）,1，1，1，1，1，1，1，1，1，0（0，0，0，0）-1，-1，-1，-1，-1，-1，-1 满足这种模式的直方图就是一个双峰图，否则不是。粗体处为谷底。 本算法难度不大，所以直接上代码。  代码实现 /*双峰型直方图，经过直方图平滑后呈现出双峰后找出谷底，以此值为阈值划分灰度值 *平滑直方图采用1/4[1 2 1]的模板 *判断是否是双峰采用1阶微分，判断正负性 *直方图非双峰不能使用该方法。 */ //直方图从int转换为double void Hist_int2double(int *hist,double *hist_d){ for(int i=0;i\u0026lt;GRAY_LEVEL;i++) hist_d[i]=(double)hist[i]; } //平滑直方图，是指呈现双峰形状 void SmoothHist(double *hist,double *dsthist){ double *histtemp=(double *)malloc(sizeof(double)*GRAY_LEVEL); histtemp[0]=0.0; histtemp[GRAY_LEVEL]=0.0; for(int i=0;i\u0026lt;GRAY_LEVEL;i++) histtemp[i]=hist[i]; for(int i=1;i\u0026lt;GRAY_LEVEL-1;i++){ histtemp[i]=0.25*histtemp[i-1]+0.5*histtemp[i]+0.25*histtemp[i+1]; } for(int i=0;i\u0026lt;GRAY_LEVEL;i++) dsthist[i]=histtemp[i]; free(histtemp); } //判断是否是双峰直方图，如果是返回谷底，否则返回0 //#define DOUBLEHUMP_BOTTOM 1 //#define DOUBLEHUMP_MEANHUMP 2 int isDoubleHump(double *hist,int returnvalue){ double * diffHist=(double *)malloc(sizeof(double)*GRAY_LEVEL); int * statusHist=(int *)malloc(sizeof(int)*GRAY_LEVEL); diffHist[0]=0.0; diffHist[GRAY_LEVEL-1]=0.0; for(int i=1;i\u0026lt;GRAY_LEVEL-1;i++){ diffHist[i]=hist[i+1]-hist[i]; } for(int i=1;i\u0026lt;GRAY_LEVEL;i++){ if(diffHist[i]\u0026gt;0) statusHist[i]=1; else if(diffHist[i]\u0026lt;0) statusHist[i]=-1; else if(diffHist[i]==0\u0026amp;\u0026amp;statusHist[i-1]\u0026gt;=0) statusHist[i]=1; else if(diffHist[i]==0\u0026amp;\u0026amp;statusHist[i-1]\u0026lt;0) statusHist[i]=-1; } /*1st order: *______________ ________________ * | | * | | * |______________| *status: 1 -1 *hist: *0 0 0 0 0 0 0 1 0 0 0 0 0 0 -1 0 0 0 0 0 0 0 0 */ for(int i=1;i\u0026lt;GRAY_LEVEL-1;i++) if(statusHist[i]*statusHist[i+1]\u0026lt;0){ if(statusHist[i]\u0026gt;0) statusHist[i]=1; else if(statusHist[i]\u0026lt;0) statusHist[i]=-1; }else{ statusHist[i]=0; } statusHist[GRAY_LEVEL-1]=0; /*double hump diff: *______________ _______________ * | | | * | | | * |______________| |______________ *status: 1 -1 1 * top bottom top *0 0 0 0 0 0 0 1 0 0 0 0 0 0 -1 0 0 0 0 0 0 0 1 0 0 0 0 *the arry test store nonzero */ int test[4]={0,0,0,0}; int test_num=0; for(int i=1;i\u0026lt;GRAY_LEVEL-1;i++){ if(statusHist[i]!=0){ test[test_num]=statusHist[i]; if(test_num\u0026gt;=3){ free(diffHist); free(statusHist); return 0; } test_num++; } } if(test_num==3\u0026amp;\u0026amp;test[0]==1\u0026amp;\u0026amp;test[1]==-1\u0026amp;\u0026amp;test[2]==1){ if(returnvalue==DOUBLEHUMP_BOTTOM){ for(int i=1;i\u0026lt;GRAY_LEVEL;i++) if(statusHist[i]==-1){ free(diffHist); free(statusHist); return i; } }else if(returnvalue==DOUBLEHUMP_MEANHUMP){ int hump[2]; for(int i=0,k=0;i\u0026lt;GRAY_LEVEL;i++) if(statusHist[i]==1){ hump[k]=i; k++; } free(diffHist); free(statusHist); return (hump[0]+hump[1])/2; } } free(diffHist); free(statusHist); return 0; } //谷底法阈值分割，适用于直方图是双峰的。 void ValleyBottomThreshold(double *src,double *dst,int width,int height,int type){ int *hist=(int *)malloc(sizeof(int)*GRAY_LEVEL); double *hist_d=(double *)malloc(sizeof(double)*GRAY_LEVEL); setHistogram(src, hist, width, height); Hist_int2double(hist, hist_d); double threshold=0.0; #define MAXLOOP 1000  for(int i=0;i\u0026lt;MAXLOOP;i++){ SmoothHist(hist_d, hist_d); if(0.0!=(threshold = (double)isDoubleHump(hist_d,DOUBLEHUMP_BOTTOM))){ Threshold(src, dst, width, height, threshold, type); printf(\u0026#34;smooth times:%d threshold:%g\\n\u0026#34;,i,threshold); break; } } free(hist); free(hist_d); } //与谷底法类似，不是使用最小谷底值，而是使用峰值位置平均值 void MeanDoubleHumpThreshold(double *src,double *dst,int width,int height,int type){ int *hist=(int *)malloc(sizeof(int)*GRAY_LEVEL); double *hist_d=(double *)malloc(sizeof(double)*GRAY_LEVEL); setHistogram(src, hist, width, height); Hist_int2double(hist, hist_d); double threshold=0.0; for(int i=0;i\u0026lt;MAXLOOP;i++){ SmoothHist(hist_d, hist_d); if(0.0!=(threshold = (double)isDoubleHump(hist_d,DOUBLEHUMP_MEANHUMP))){ Threshold(src, dst, width, height, threshold, type); break; } } free(hist); free(hist_d); } 上面包括谷底阈值和双峰平均值的阈值处理\n结果观察 原图： 直方图和平滑后的直方图，以及格式化的直方图： 阈值处理结果 原图 直方图和平滑后的直方图，以及格式化的直方图：\n阈值处理结果 结论 针对双峰图像的谷底阈值和峰顶平均阈值算法是纯粹的基于直方图的阈值处理方法，本算法对图像的唯一要求是原直方图是双峰的，或者经过平滑可以形成双峰的，这样就可以利用一阶导数得到谷底和峰顶位置，找到合适的阈值进行阈值处理，本算法对噪声不敏感，但鲁棒性不好，容易无法给出阈值。所以需要谨慎使用。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-4-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E4%B9%8B%E8%B0%B7%E5%BA%95%E9%98%88%E5%80%BC-%E5%B3%B0%E9%A1%B6%E5%B9%B3%E5%9D%87.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第53天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 谷底阈值,峰顶平均\u003c/p\u003e","title":"【数字图像处理】7.4:灰度图像-图像分割 阈值处理之谷底阈值、峰顶平均"},{"content":"Abstract: 数字图像处理：第54天 Keywords: OTSU算法\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 阈值处理之OTSU阈值 废话开始，今天介绍OTSU算法，本算法比前面给出的算法更能够给出数学上的最佳阈值，不需要任何输入附加参数、与同样不需要输入附加参数的迭代均值和均值阈值来比较，OTSU给出的阈值能使分类更加均匀。 阈值处理将灰度值分为两类，而对于分类问题，已有的一种最优闭合解\u0026ndash;贝叶斯决策规则。\n贝叶斯决策规则 首先介绍下贝叶斯公式的形象化理解，考虑下图 上面的12幅图中有手枪和弹夹，只有弹夹和手枪出现在同一个盒子的时候才有杀伤力，也就是你拿到一个盒子，你不知道里面是什么，有可能是枪，有可能是弹夹，有可能同时有枪和弹夹。下面来从概率学角度分析 设盒子里有枪为事件A，那么A出现的概率设为 $p(A)$ 。 设盒子里有弹夹为事件B，那么B出现的概率设为 $p(B)$ 。 那么同时出现事件A和事件B的概率为 $p(AB)$ 看图可以知道 $p(A)=\\frac{8}{12}=\\frac{2}{3}$ \u0026hellip;\u0026hellip;\u0026hellip;.(1) $p(B)=\\frac{7}{12}$ \u0026hellip;\u0026hellip;\u0026hellip;.(2) $p(AB)=\\frac{3}{12}=\\frac{1}{4}$ \u0026hellip;\u0026hellip;\u0026hellip;.(3)\n考虑我们随机抽出一个盒子，先拿出一个东西，比如先拿出一把枪，那么也就是事件A发生了，那么我们继续从盒子里拿东西，有可能拿到弹夹，也有可能啥也没有，那么拿到弹夹的概率就如下： $p(B|A)=\\frac{3}{8}$ \u0026hellip;\u0026hellip;\u0026hellip;.(4)\n同理，如果先拿出来的是个弹夹，那么接下来拿出枪的概率是： $p(A|B)=\\frac{3}{7}$ \u0026hellip;\u0026hellip;\u0026hellip;.(5)\n结合(1)(2)(3)(4)(5)，可以得到： $p(AB)=p(A|B)\\times p(B)=p(B|A)\\times p(A)$ \u0026hellip;\u0026hellip;\u0026hellip;.(6)\n 假设下面情形： 已知拿出枪的概率是： $p(A)=\\frac{2}{3}$\n拿出枪以后拿出弹夹的概率 $p(B|A)=\\frac{3}{8}$\n拿出弹夹的概率： $p(B)=\\frac{7}{12}$\n求拿出弹夹以后拿出枪的概率 $p(A|B)=\\frac{p(B|A)\\times p(A)}{p(B)}$\n以上就是贝叶斯公式的一般形式，更复杂的形式会在后面的文章中详细介绍。（更复杂的形式是指盒子里有枪，子弹，弹夹，手榴弹。。。。。。）\n数学原理 OTSU算法可以基于直方图计算，考虑灰度级为{0，1，2\u0026hellip;\u0026hellip;..L-1}大小为 $M \\times N$ 的图像，设 $n_i$ 为灰度级为i的像素的总数量，那么: $M \\times N=\\sum^{L-1}_{i=0}n_i$\n$p(n_i)=\\frac{n_i}{M \\times N}$\n$\\sum^{L-1}_{i=0}p_i=1$\n假设阈值为k将直方图分成两部分。 部分1$(C_1)$的概率为： $p_1(k)=\\sum^{k}_{i=0}p_i$\n部分2$(C_2)$的概率为： $p_2(k)=\\sum^{L-1}_{i=k+1}p_i$\n部分1$(C_1)$的平均数： $m_1(k)=\\sum^{k}{i=0}i\\times P(i|C_1)=\\sum^{k}{i=0}i\\times \\frac{P(C_1|i)\\times P(i)}{P(C_1)}$\n$P(C_1|i)$ 的值为1，因为 $i$ 是属于 $C_1$ 的，所以发生$i$ 以后发生 $C_1$ 的概率是100%，所以 $m_1(k)=\\frac{1}{P_1(k)}\\sum^{k}_{i=0}i\\times p_i$\n部分2$(C_2)$ 的平均数： $m_2(k)=\\frac{1}{P_2(k)} \\sum^{L-1}_{k+1}i\\times p_i$\n全图的均值 $m_G=\\sum^{L-1}_{i=0}iP_i$\n上面的式子可以由下面验证： $P_1m_1+P_2m_2=m_G$\n$P_1+P_2=1$\n下面就是关键部分了，如何评价一个阈值的好坏，提出一个阈值，将像素灰度分为两类，通过以下的公式来评价阈值质量：\n$\\eta=\\frac{\\delta_B^2}{\\delta_G^2}$\n$\\delta_G^2=\\sum^{L-1}_{i=0}(i-m_G)^2\\times p_i$\n$\\delta_B^2$是类间方差，其定义为： $\\delta_B^2=P_1(m_1-m_G)^2+P_2(m_2-m_G)^2$\n公式还可以写成： $\\delta^2_B=P_1P_2(m_1-m_2)^2=\\frac{P_1(m_1-m_G)^2}{1-P_1}$\n于是最佳阈值$k^{}$ 由下面得出： $\\delta^2_B(k^{})=max_{0\\leq k \\leq L-1}\\delta^2_B(k)$\n通过上式可以通过迭代计算出最佳的k值。使用k作为阈值，对图像进行处理。\n代码实现 /* *OTSU 算法 *otsu 算法使用贝叶斯分类原理得到最好聚类 * * */ //归一化直方图  void setHist2One(double *hist_d,double *dst_hist_d){ double sum=0.0; for(int i=0;i\u0026lt;GRAY_LEVEL;i++) sum+=hist_d[i]; if(sum!=0) for(int i=0;i\u0026lt;GRAY_LEVEL;i++) dst_hist_d[i]=hist_d[i]/sum; } //计算公式中最大的deta，并返回直方图灰度 double findMaxDeta(double *hist_d){ double max_deta=-1.0; double max_deta_location=0.0; double m_g=0.0; for(int i=0;i\u0026lt;GRAY_LEVEL;i++) m_g+=i*hist_d[i]; for(int i=0;i\u0026lt;GRAY_LEVEL;i++){ double p1=0.0; double m1=0.0; double deta=0.0; for(int j=0;j\u0026lt;=i;j++){ p1+=hist_d[j]; m1+=j*hist_d[j]; } deta=p1*(m1-m_g)*(m1-m_g)/(1-p1); if(deta\u0026gt;max_deta){ max_deta_location=i; max_deta=deta; } } return max_deta_location; } void OTSUThreshold(double *src,double *dst,int width,int height,int type){ int hist[GRAY_LEVEL]; double hist_d[GRAY_LEVEL]; setHistogram(src, hist, width, height); Hist_int2double(hist, hist_d); setHist2One(hist_d, hist_d); double threshold=findMaxDeta(hist_d); Threshold(src, dst, width, height, threshold, type); } 观察结果 原图： 加入1%的高斯噪声： 加入3%的高斯噪声： 加入5%的高斯噪声： 加入7%的高斯噪声： 加入9%的高斯噪声： 加入11%的高斯噪声： lena: baboon: 总结 OTSU算法产生的阈值是数学角度上的最佳分类，数学基础的贝叶斯公式，但应用也有一定的局限性，比如，前面说过最多的，对全局阈值，目标与背景的大小关系，当目标和背景大小相差很多时，或者噪声很大的时候，对OTSU产生影响较大。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-5-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E4%B9%8Botsu%E9%98%88%E5%80%BC.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第54天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e OTSU算法\u003c/p\u003e","title":"【数字图像处理】7.5:灰度图像-图像分割 阈值处理之OTSU阈值"},{"content":"Abstract: 数字图像处理：第55天 Keywords: 阈值处理\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像\u0026ndash;图像分割 阈值处理之补充说明 在前面的介绍中，说到过，影响阈值处理的两个主要问题是目标和背景的大小关系，和噪声对目标的影响，补充说明就是来解决下这两个问题。 #算法原理 首先来解决噪声影响，在图像增强的时候提到过，低通滤波和平滑能够减少图像噪声，通过减少噪声，可以一定程度上提高阈值处理的结果。例如未去噪的时候直方图如下： 加入11%的高斯噪声的图像直方图： 使用高斯滤波器进行平滑后的直方图： 但是对于相对较小的目标，直方图上基本看不出目标和背景的差别: 原图： 直方图： 我们的方法是使用边缘处理结果边缘作为Mask，得到Mask为1的原图处的灰度值，有这些灰度值做直方图，可以得到下面： 可以看到相对明显的边界，值得注意的是，这里选取边界的算法一定选用检测结果是外边界和内边界结合的边缘图像。所以我们选用Sobel算子进行边缘检测，边缘检测后的阈值处理（对边缘结果的阈值处理）。最后对Mask出来的灰度集合进行OTSU阈值计算，得出最终结果。\n代码 上代码\n/*对于小目标物体 *使用边缘检测结果作为MASK *得到MASK为1处的原图灰度集合 *对这个集合做阈值分割 *的到最终的结果 */ void SobelThreshold(double *src,double *dst,int width,int height,double sobel_threshold,int type){ double *mask=(double *)malloc(sizeof(double)*width*height); double *temp=(double *)malloc(sizeof(double)*width*height); //use 0.05*width and 0.05*height gaussian mask smooth src  GaussianFilter(src, temp, width, height, width/25,height/25, (double)width/150.); double max=Sobel(temp, mask, NULL, width, height, 5); Threshold(mask, mask, width, height, max*sobel_threshold, THRESHOLD_TYPE3); ///////////////////////////////////////////////////////////////////////////  int hist[GRAY_LEVEL]; double hist_d[GRAY_LEVEL]; InitHistogram(hist); for(int i=0;i\u0026lt;width*height;i++) if(mask[i]!=0.0) hist[(int)src[i]]++; Hist_int2double(hist, hist_d); setHist2One(hist_d, hist_d); double threshold=findMaxDeta(hist_d);//  printf(\u0026#34;Threshold:%g \\n\u0026#34;,threshold); Threshold(src, dst, width, height, threshold, type); free(mask); free(temp); } 结果分析 原图，加入3%的高斯噪声 边缘检测后的直方图： 处理结果： 原图，加入7%的高斯噪声 边缘检测的直方图： 处理结果： 平滑后的阈值处理： 加入11%的高斯噪声的图像，平滑后进行阈值处理： 原图直方图： 平滑后直方图： 总结 为了解决前面所说的两个影响阈值处理的两个重要因素，提出的两种解决方法，也可以使用局部阈值或者可变阈值进行处理。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-6-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E4%B9%8B%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第55天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 阈值处理\u003c/p\u003e","title":"【数字图像处理】7.6:灰度图像--图像分割 阈值处理之补充说明"},{"content":"Abstract: 数字图像处理：第56天 Keywords: 局部阈值处理\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 阈值处理之局部阈值 废话开始，今天说下区域阈值（局部阈值）,前面介绍的阈值都是全局阈值，也就是阈值根据全局信息产生，而作用对象也是整幅图像的全部像素，而局部阈值的产生是一个中心像素c(x,y)的邻域的一些属性来计算出一个或多个阈值以及阈值的判别式。这句话比较难懂，举个例子，假设c的邻域R，根据邻域R计算出阈值 $T_1,T_2,T_3\\dots T_n$ 我们可以表示成向量 $T=(T_1,T_2,T_3\\dots T_n)$ ，设计阈值判别式Q(T,pixValue)其中pix_value的值就是像素c(x,y)的灰度值，判别式返回真假，真的话像素设置为亮，否则设置成暗。\n算法内容 该算法的关键点在于设计判别式Q和计算阈值向量T，因为此算法的通用性不是很强，但优点是灵活性强，可以根据不同的图片性质来设计不同的执行方案，比如下面例子中使用最简单的两种统计学参数，均值和标准差，当中心像素大于均值的n倍并且大于标准差的m倍。设置窗口大小，也就是邻域大小，参数n，参数m，最后得到较好的阈值结果。\n代码实现 /*局部阈值 *使用均值和标准差作为判定依据 *输入参数包括邻域大小，均值系数，以及标准差系数 * * */ void LocalThreshold(double *src,double *dst,int width,int height,int w_size,double mean_param,double std_dev_param){ double *temp=(double *)malloc(sizeof(double)*width*height); Zero(temp, width,height); double mean_g=matrixMean(src, width, height); for(int j=w_size/2;j\u0026lt;height-w_size/2;j++){ for(int i=w_size/2;i\u0026lt;width-w_size/2;i++){ double deta=0.0; double mean=0.0; double pix_value=src[j*width+i]; //local mean  for(int m=-w_size/2;m\u0026lt;w_size/2+1;m++){ for(int n=-w_size/2;n\u0026lt;w_size/2+1;n++){ mean+=src[(j+m)*width+i+n]; } } mean/=(double)(w_size*w_size); //local deta  for(int m=-w_size/2;m\u0026lt;w_size/2+1;m++){ for(int n=-w_size/2;n\u0026lt;w_size/2+1;n++){ deta+=(src[(j+m)*width+i+n]-mean)*(src[(j+m)*width+i+n]-mean); } } deta/=(double)(w_size*w_size); deta=sqrt(deta); if(pix_value\u0026gt;mean_param*mean_g\u0026amp;\u0026amp;pix_value\u0026gt;std_dev_param*deta){ temp[j*width+i]=255.0; } } } matrixCopy(temp, dst, width, height); free(temp); } 算法效果 总结 相比于全局阈值，局部阈值对目标大小，以及噪声敏感度强，但其缺点是设计针对性强，没有什么通用的算法，而且输入的参数多半需要分析实验产生，不能实现自动阈值处理，其优点是功能强大。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-7-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E9%98%88%E5%80%BC%E5%A4%84%E7%90%86%E4%B9%8B%E5%B1%80%E9%83%A8%E9%98%88%E5%80%BC.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第56天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 局部阈值处理\u003c/p\u003e","title":"【数字图像处理】7.7:灰度图像-图像分割 阈值处理之局部阈值"},{"content":"Abstract: 数字图像处理：第57天 Keywords: 区域分割,区域生长\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 区域分割之区域生长 继续说废话，昨天写博客被同事看到了，问我，为什么你每一篇开始都是废话，我说凑字数，在一个可以写点轻松的话，天天在算法的海洋里飘荡，偶尔说几句荒山野岭的废话也算活跃气氛了。\n区域分割介绍 今天介绍基于区域的分割方法，前面基于阈值的分割方法暂时告一段落，基于区域的分割运用同样广泛，但和阈值比较，区域分割难度也稍微大了一些，比如后面要讲到的分水岭算法，分水岭算法是个算法族，并不是单一的一个固定算法，比如有基于形态学的，也有基于其他的，但思想都一样，分水岭是那种典型的，看起来很简单，一说原理，小学生都能听懂，但实现起来难度不小，也可能是我代码能力不行，反正写了将近一整天，修改了一天才算看到点结果。 由于基于区域的分割算法已经成为一个专门的研究领域，这几篇博文只介绍一点点最基础的，通用的算法，至于高深的高科技的算法，留到未来的某个时刻。这里只讲最简单的。 今天介绍的区域生长，是其中比较简单的一种。\n区域生长算法 区域生长的算法过程总结如下：\n 选取种子点$c(x,y)$ 以种子点为中心，对其邻域像素进行递归遍历 对于每一个邻域像素$N(x\u0026rsquo;,y\u0026rsquo;)$，设计一个判别式$Q(c(x,y),N(x\u0026rsquo;,y\u0026rsquo;))$。 如果判别式为真，邻域像素N被设置为新的种子点，进入第2步，并将该点加入结果集合（与种子点为同一区域）。否则退出此次递归回到3，检测下一个邻域像素。  整体思路是以种子点为中心，遍历图，深度优先或广度优先没有没有关系，判断中心点和其邻域是否满足判别式，注意，此处最终要的点是判别式，设计判别式可以针对不同的应用，下面代码中设计的判别式是个简单的范围判别式，也就是说如果被判别的像素灰度值，在一定范围内，则为真，否则为假，范围是由种子点和附加参数param一起产生的。 #代码\n/*区域生长，设置一个种子点x（灰度值为x_v），然后以种子点为中心 *向四周进行图搜索，如果点y（灰度值为y_v）,邻域满足param+x_v\u0026gt;y_v\u0026gt;x_v-param（条件1） *则此点与种子点归为一个区域，以此递归，条件1可以根据不同的需要自行设置其他. */ //递归遍历邻域，并判断条件是否成立 void findGrowRegion(double *src,double *dst,int seed_x,int seed_y,int width,int height,int regionNum,double value,double param){ dst[seed_y*width+seed_x]=(double)regionNum; for(int j=-1;j\u0026lt;2;j++) for(int i=-1;i\u0026lt;2;i++){ if(seed_x\u0026gt;=0\u0026amp;\u0026amp;seed_y\u0026gt;=0\u0026amp;\u0026amp;seed_x\u0026lt;width\u0026amp;\u0026amp;seed_y\u0026lt;height\u0026amp;\u0026amp;(j!=0||i!=0)){ if(src[(j+seed_y)*width+i+seed_x]\u0026gt;=value-param \u0026amp;\u0026amp;src[(j+seed_y)*width+i+seed_x]\u0026lt;=value+param \u0026amp;\u0026amp;dst[(j+seed_y)*width+i+seed_x]==0.0) findGrowRegion(src, dst, i+seed_x, j+seed_y, width, height, regionNum, value,param); } } } void RegionGrow(double *src,double *dst,Position * position,int p_size,int width,int height,double param){ double * dsttemp=(double *)malloc(sizeof(double)*width*height); Zero(dsttemp, width, height); int regionNum=100; for(int i=0;i\u0026lt;p_size;i++){ int x=position[i].x; int y=position[i].y; findGrowRegion(src, dsttemp, x,y, width, height, regionNum,src[y*width+x],param); regionNum+=10; } matrixCopy(dsttemp, dst, width, height); free(dsttemp); } 结果分析 总结 区域生长算法实现较简单，但如果递归区域面积过大，可能造成程序卡死，可能是栈空间不够或者别的，这个需要处理下，这个算法的缺点是需要设置种子点，本算法的优点也是可以设置种子点，这样灵活但不够智能，算法执行速度较快。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-8-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E5%8C%BA%E5%9F%9F%E5%88%86%E5%89%B2%E4%B9%8B%E5%8C%BA%E5%9F%9F%E7%94%9F%E9%95%BF.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第57天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 区域分割,区域生长\u003c/p\u003e","title":"【数字图像处理】7.8:灰度图像-图像分割 区域分割之区域生长"},{"content":"Abstract: 数字图像处理：第58天 Keywords: 区域分割\n本文最初发表于csdn，于2018年2月17日迁移至此\n灰度图像-图像分割 区域分割之区域分离 废话开始，今天本来只想写一篇，但晚上觉得还是快把区域分割简单介绍下，后面开始彩色图像类的知识学习和代码实现，下一篇介绍分水岭算法，这才是个头疼的算法，今天的区域分离（合并）相对比较好理解。\n算法原理 首先本算法依然是基于区域的，用到的区域的性质是区域的均值和标准差，简单描述算法，如果一个区域满足设定的均值范围和标准差范围，设置整个区域为亮，否则将次区域分为四份，每一份继续递归进行，直至预先设定的最小区域。 从结构来讲可以抽象成一颗四叉树： 算法最核心的是设计一个判别式，上面说的判别式是均值和均方的联合，也可以使用其他判别式，根据实际情况可以具体设计。\n算法：\n 初始化，输入参数，包括均值上下界m1,m2，标准差上下界d1,d2 计算区域均值和标准差，如果满足条件，输出对应设置为亮 否者将区域分为四份 将其中一份带入步骤2递归进行计算 如果区域分割小于设定的最小值结束递归。  代码 /* *区域分割算法，递归进行判断 *如果区域不符合条件，将区域 *分为四份，递归判断每个区域 *知道区域分为最小设定值 */ void findSplitRegion(double *src,double *dst,int width,int height,int x,int y,int w_width,int w_height,double mean_param1,double mean_param2,double variance_param1,double variance_param2){ double mean=RegionMean(src, width, height, x, y, w_width, w_height); double variance=RegionStdDeviation(src, width, height, x, y, w_width, w_height); if(mean\u0026gt;mean_param1\u0026amp;\u0026amp; mean\u0026lt;=mean_param2\u0026amp;\u0026amp; variance\u0026gt;variance_param1\u0026amp;\u0026amp; variance\u0026lt;=variance_param2){ RegionSetOne(dst, width, height, x, y, w_width, w_height); }else{ #define MINIMAL_CELL 3  if(w_width\u0026gt;=MINIMAL_CELL\u0026amp;\u0026amp;w_height\u0026gt;=MINIMAL_CELL){ findSplitRegion(src, dst, width,height, x, y, w_width/2+1, w_height/2+1, mean_param1, mean_param2, variance_param1, variance_param2); findSplitRegion(src, dst, width,height, x+w_width/2, y, w_width/2+1, w_height/2+1, mean_param1, mean_param2, variance_param1, variance_param2); findSplitRegion(src, dst, width,height, x+w_width/2, y+w_height/2+1, w_width/2+1, w_height/2, mean_param1, mean_param2, variance_param1, variance_param2); findSplitRegion(src, dst, width,height, x, y+w_height/2, w_width/2+1, w_height/2+1, mean_param1, mean_param2, variance_param1, variance_param2); } } } void RegionSplit(double *src,double *dst,int width,int height,double mean_param1,double mean_param2,double variance_param1,double variance_param2){ double *dsttemp=(double *)malloc(sizeof(double)*width*height); findSplitRegion(src, dsttemp, width, height, 0,0, width, height, mean_param1, mean_param2, variance_param1,variance_param2); matrixCopy(dsttemp, dst, width, height); free(dsttemp); } 实验结果 原图： 想要分离周围的星云,参数见图中标注： 原图： 同样分离周围的星云，参数见图中标注： 总结 此算法运行速度很快，但精确度不够高，因为设置的最小区域值决定了区域分割的准确性，所以会有锯齿状的边缘，这是一个缺点。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-7-9-%E7%81%B0%E5%BA%A6%E5%9B%BE%E5%83%8F-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-%E5%8C%BA%E5%9F%9F%E5%88%86%E5%89%B2%E4%B9%8B%E5%8C%BA%E5%9F%9F%E5%88%86%E7%A6%BB.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第58天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 区域分割\u003c/p\u003e","title":"【数字图像处理】7.9:灰度图像-图像分割 区域分割之区域分离"},{"content":"Abstract: 数字图像处理：第60天 Keywords: CIE XYZ,CIE 1931\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色模型，CIE XYZ，CIE RGB 今天学习彩色模型，常用的图像包括彩色图，灰度图，二值图，并且彩色图像-\u0026gt;灰度图像-\u0026gt;二值图像，为一个退化过程，即图像包含的信息逐步减少，根据冈萨雷斯第三版介绍的篇幅来看，对灰度图相关的算法描述较多，因此，可以说，就目前的算法来说，针对灰度图像的图像处理还是比较成熟的，但不能确定是否将来产生的新的理论能够统一现有所有算法，而且创造出更高级的算法。想要深入学习图像处理，个人认为应该对所有算法，所有模型有清晰的认识，了解其来源和完整过程，有助于理解后面的算法。\n正文 颜色的根本来源人类对光的一种心理学感知，而光具有波粒二象性，我们只关心其波性质，原因是人类感官对不同波长的光的反应不同，继而从认知的层次产生了颜色，即光本身没有颜色这个属性，而是人们根据个人需要，把感受到的不同波长定义了不同的颜色。 如图，给出了物理学的对颜色的描述：\n一句话总结，就是人类根据自身对光的感官，主观的定义了颜色。 色彩空间：指的是用一种客观的方式叙述颜色在人眼上的感觉，通常需要三色刺激值。更精确地说，首先先定义三种主要颜色（primary color），再利用颜色叠加模型，即可叙述各种颜色。需要注意的是，三种主要颜色未必需要是真正的颜色（也就是该种颜色无法真的被创造出来）。\n生理因素 首先作为“视觉传感器”，人眼解剖结构表明，对颜色的感知，主要由视锥细胞完成，视锥细胞主要对三种波长敏感，即短（S, 420-440nm）、中（M, 530-540nm）和长（L, 560-580nm）三种波长敏感，视杆细胞支队光强敏感。\nS在大概420纳米处出现峰值，M大概在534纳米处出现峰值，L大概在564处出现峰值，等分布波长谱产生白光，不等分布波长产生颜色。\n条件等色（color metamer）现象 对于条件等色，可以这样解释：颜色由其光的波长（或频率）唯一定义，也就是，一种波不可能由其他波组合出来，因为不可能用多种波长合成一种波长，而人的感官细胞会产生一种错觉，即几种波的混合刺激等效于另一种波的单独刺激，也就是说你平时看到的颜色并不一定是真实的颜色。\nCIE-RGB色彩空间 因为上述现象的存在，前辈们设计了一个实验，首先选定三种颜色作为基础颜色，将其混合，通过调整其混合比例和强度（三种波长），来和另外的纯色（单一波长），当观察者感觉两种结构的效果相同时，记录下混合的比例，这样我们就可以用三种颜色来达成另外一种颜色，测试结构绘制如下：\n这个函数图像有一个有趣的特点，r函数出现了负值，这个在我们的理论描述种是不可能出现的，因为不存在负光强，这里对其进行解释：对于光谱色较暗的段，通过三色叠加不能达到相应的效果，只能提高原色的亮度，才能得到匹配，所以还原为原色时，三色也等量下调，所以出现负波形。 由于其负波形的存在，所以上述模型不能再现实中完全再现所有颜色，也就是说，如果使用该色彩空间，有一段颜色是无法合成的，因为负波段无法产生。 CIE-RGB常被用rg色度来表示： 色彩空间 色彩空间指的是用一种客观的方式叙述颜色在人眼上的感觉，通常需要三色刺激值。更精确地说，首先先定义三种主要颜色（primary color），再利用颜色叠加模型，即可叙述各种颜色。需要注意的是，三种主要颜色未必需要是真正的颜色（也就是该种颜色无法真的被创造出来）。\nGrsassmann定律 由格拉斯曼（Grsassmann）总结的在颜色相加混合时的规律，概括为以下四点：\n 人的视觉只能分辨颜色的三种变化：亮度、色调、饱和度； 两种颜色混合时的补色律和中间色定律； 感觉上相似的颜色，可以互相代替—代替律； 亮度相加定律：由几个颜色组成的混合色的亮度，是各颜色光亮度的总和。  明度与色度 明度：眼睛对光源和物体表面的明暗程度的感觉，主要是由光线强弱决定的一种视觉经验。 色度：色度是不包括亮度在内的颜色的性质，它反映的是颜色的色调和饱和度。\nCIE-XYZ 由于上述RGB空间部分颜色无法再现的原因，前辈们改变方法。 假定Grassmann定律成立，这个新空间通过线性变换而有关于CIE-RGB空间。新空间将以三个新颜色匹配函数来定义：、和。带有频谱功率分布I(λ)的颜色的对应的XYZ三色刺激值为给出为： 值得注意的是，XYZ并不是红绿蓝三种颜色的对应，而是通过这三种颜色导出的一种参数，其颜色并无太大意义，而是具有较高的数学意义。 CIE XYZ色彩空间故意设计得Y参数是颜色的明度或亮度的测量。颜色的色度接着通过两个导出参数x和y来指定，它们是所有三个三色刺激值X、Y和Z的函数所规范化的三个值中的两个： XYZ匹配函数如下： XYZ色度图效果如下： 与CIE-RGB色度图相叠加，如下图： 上图的解释：在CIE rg色度图中展示规定CIE XYZ色彩空间的三角形构造。三角形Cb-Cg-Cr就是在CIE xy色度空间中的xy=(0,0),(0,1),(1,0)三角形。连接Cb和Cr的直线是alychne。注意光谱轨迹通过rg=(0,0)于435.8 nm，通过rg=(0,1)于546.1 nm，通过rg=(1,0)于700 nm。还有，均等能量点（E）位于rg=xy=(1/3,1/3)。 CIE—XYZ色度的性质：\n 色度图展示了对一般人可见的所有色度。这个用颜色展示的区域叫做人类视觉的色域。在CIE绘图上所有可见色度的色域是用颜色展示的马蹄铁形状。色域的曲线边界叫做“光谱轨迹”并对应于单色光，波长用纳米标记。色域底下的直线边界叫做“紫线”，这些颜色尽管在色域的边界上，但没有匹配的单色光。更少饱和的颜色位于图形内部而白色位于中央。 所有可见色度对应于x、y和z的非负值（因此对应于X、Y和Z的非负值）。 如果你在色度图上选择了任何两点，则位于这两点之间直线上任何颜色都可以用这两个颜色混合出来。这得出了色域的形状必定是凸形的。混合三个光源形成的所有颜色都可以在色度图内的源点形成的三角形内找到（对于多个光源也如是）。 两个同等明亮颜色的等量混合一般不位于这个线段的中点。用更一般术语说，在xy色度图上距离不对应于两种颜色之间的差别程度。设计了其他色彩空间（特别是CIELuv和CIELab）来满足这个问题。 给定三个真实光源，这些光源不能覆盖人类视觉的色域。几何上说，在色域中没有三个点可以形成包括整个色域的三角形，更简单的说，人类视觉的色域不是三角形。 平直能量频谱的光对应于点 (x,y) = (1/3,1/3)。  至此我们简单的介绍了下CIE—XYZ空间的来源和相关知识，后续将介绍其他色彩空间知识，但出于知识的路线原因决定，下篇开始介绍二值图像的相关操作，然后介绍灰度图像相关的工作，最后介绍彩色图像时，再介绍其他色彩空间极其应用。欢迎收看。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-0-%E5%BD%A9%E8%89%B2%E6%A8%A1%E5%9E%8B-ciexyz-ciergb.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第60天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CIE XYZ,CIE 1931\u003c/p\u003e","title":"【数字图像处理】8.0:彩色模型，CIE XYZ，CIE RGB"},{"content":"Abstract: 数字图像处理：第61天 Keywords: 彩色图像\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-色彩空间 综述 上一篇结束了灰度图像的基本知识学习，从开始（2014-11-06 10:58）发表第一篇博客到现在，已经过去了大概4个月多了，学了二值图像，灰度图像的一些基础知识，下一步进入彩色图像的基础知识学习，不能说这段时间取得了什么，还是感觉进步了不少，毕竟坚持不懈的学习是因为自己的兴趣。 下面先简单介绍下预计的彩色知识的学习结构： 下面介绍下接下来几篇介绍色彩空间的结构。\n色彩空间学习结构 经过简单的学习，决定按照下面的结构写一些关于色彩空间： 另外关于 RGB和XYZ的简单介绍以及一些基本定理，早前的博客已经有了简单介绍： 彩色模型，CIE XYZ，CIE RGB\n总结 彩色图像的开篇，一些基本的彩色知识在Day8中已经介绍了，下一篇开始按照上图开始介绍色彩空间。 本篇较短，未完待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-1-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4-%E7%BB%BC%E8%BF%B0.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第61天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 彩色图像\u003c/p\u003e","title":"【数字图像处理】8.1:彩色图像-色彩空间 综述"},{"content":"Abstract: 数字图像处理：第62天 Keywords: 色彩空间 RGB\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-色彩空间 RGB系列 心情烦躁，换了一个特别吵的办公室，看来是时候离开了。。本想写完所有冈萨雷斯的学习笔记后去找工作，但时不我待，边找遍边学吧，学习是没有尽头的。\nCIE RGB CIE RGB 详细信息已经在Day8中介绍，详情点击下面链接： CIE XYZ CIE RGB\nsRGB  sRGB色彩空间是惠普与微软于1996年一起开发的用于显示器、打印机以及因特网的一种标准RGB色彩空间。 sRGB最初设计的目的是作为生成在因特网以及万维网上浏览的图像的通用色彩空间，最后选择的是使用Gamma校准系数为2.2的色彩空间，即CRT显示器在这种情况下的平均线性电压响应。\n 下面是从$CIE XYZ$空间转换到$sRGB$的转化公式： 逆向变换： 其中的变换理论基础： 以上图片信息和引用信息全部来自wiki。\n$R_nG_nB_n$ 此空间是一个特殊的RGB空间，用于电视接收机的基色系统（receiver primary color system），它涉及美国标准化NTSC已确认的荧光体，其转换公式： $$ \\left[\\begin{array}{c} R_N\\ G_N\\ B_N\\ \\end{array}\\right] =\\left[ \\begin{array}{ccc} 0.842 \u0026amp; 0.156 \u0026amp; 0.091\\ -0.129 \u0026amp; 1.320 \u0026amp; -0.203\\ 0.008 \u0026amp;-0.069 \u0026amp; 0.897\\ \\end{array}\\right] \\times \\left[\\begin{array}{c} R\\ G\\ B\\ \\end{array}\\right]$$\n总结 本文简单介绍了两种RGB的变种色彩空间，作为一篇概述，色彩空间在图像处理里面的意义在于从不同的角度看颜色，而在视频传输中的意义更为广泛，压缩，传输速度等都是要考虑的因素，在后面的彩色图像处理中肯定会用到不同的色彩空间的转换，这也是一种变换的思想。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-2-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4-rgb%E7%B3%BB%E5%88%97.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第62天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 色彩空间 RGB\u003c/p\u003e","title":"【数字图像处理】8.2:彩色图像-色彩空间 RGB系列"},{"content":"Abstract: 数字图像处理：第63天 Keywords: CMY\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-色彩空间 CMY(K)空间 继续废话，因为色彩空间比较多，所以先大概每个空间都进行一下简介，然后根据后面的应用来回顾这些色彩空间的一些知识，根据不同色彩空间开发出的不同算法会在后面继续介绍，最近生存环境恶劣，但是学习不能停。 看到同志在博客里留言，说博客写的很牛。首先被别人，尤其是陌生表扬心里都会开心，其次写这些博客就是为了记录下自己学习的历程，如果能给别人一些启发那就更好了，希望能继续坚持下去，做自己喜欢的事，无论周围环境如何。\nCMY模型 首先必须要说一下光的彩色和染料的颜色有什么不同，首先看到的光的颜色就是光的颜色，比如看到光是红色的，那么这就是红色的光，但染料颜色和光的颜色不同，我们看到的染料颜色是它反射的颜色，也就是说你看到红色的染料，其实它是非红色的（也就是青色），也就是染料本身的颜色是所看到的相反的颜色。 CMY就是染料的颜色，RGB的补色，RGB是典型的加性色彩空间，而CMY则是典型的减性色彩空间。下面观察CMY的色度图  C：青（Cyan）(G+B) M：洋红或品红（Magenta）(R+B) Y：黄（Yellow）(R+G)  CMY的增强版是CMYK，K表示key，是黑色，加入黑色的原因有很多，比如CMY染料不纯，所以三种颜色混合的时候得不到黑色而是一种暗红，其次如果使用CMY混合得到黑色，那么此处将会被喷了足够多的墨，不容易干，最后，黑色的墨水很便宜，所以在CMY的基础上加入了K。 进一步说明CMY在打印时组合颜色并不是线性的，各种颜色吸收和反射曲线不同，所以具体的比例依据彩色墨水的具体性质而定。 CMY和RGB的转换： $$ \\left[\\begin{array}{c} C\\ M\\ Y\\ \\end{array}\\right] = \\left[\\begin{array}{c} G_{max}\\ G_{max}\\ G_{max}\\ \\end{array}\\right]-\\left[\\begin{array}{c} R\\ G\\ B\\ \\end{array}\\right] $$\n逆变换： $$ \\left[\\begin{array}{c} R\\ G\\ B\\ \\end{array}\\right]= \\left[\\begin{array}{c} G_{max}\\ G_{max}\\ G_{max}\\ \\end{array}\\right]- \\left[\\begin{array}{c} C\\ M\\ Y\\ \\end{array}\\right] $$\n其中 $G_{max}$ 表示单个通道的最大值。\n总结 RGB和CMY的理解较简单，在此不再赘述。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-3-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4-cmy-k%E7%A9%BA%E9%97%B4.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第63天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CMY\u003c/p\u003e","title":"【数字图像处理】8.3:彩色图像-色彩空间 CMY(K)空间"},{"content":"Abstract: 数字图像处理：第64天 Keywords: 色彩空间,YIQ,YUV,YCbCr\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-色彩空间 YIQ 、YUV 、YCbCr 、YC1C2 和I1I2I3 背后有人，今天不说废话。。。。\nYIQ NTSC电视系统指定的色彩空间，为了压缩RGB的传输带宽提高传输速度。 YIQ中Y代表亮度信号（Luminance），IQ作为附加信息，I表示相位（In-phase）色彩从橙色到青色，Q表示正交（Quadrature-phase）色彩从紫色到黄绿色，故YIQ也叫做色差信号（Chrominance signal）。 YIQ中Y信号可以直接用于黑白电视作为灰度信号，进行显示，彩色电视则加入IQ分量与Y一起产生颜色。 转换公式： YUV YUV在德国和法国发展的彩色电视系统PAL和SECAM中使用。 Y分量与YIQ中Y相同。 以下来自wiki：\n 首先YUV是一类信号的总称，$Y\u0026rsquo;UV, YUV, YCbCr，YPbPr$等专有名词都可以称为$YUV$，“Y”表示明亮度（Luminance、Luma），“U”和“V”则是色度、浓度（Chrominance、Chroma），$Y\u0026rsquo;UV, YUV, YCbCr, YPbPr$常常有些混用的情况，其中YUV和Y\u0026rsquo;UV通常用来描述模拟信号，而相反的YCbCr与YPbPr则是用来描述数位的影像信号，例如在一些压缩格式内MPEG、JPEG中，但在现今，YUV通常已经在电脑系统上广泛使用。YUV Formats分成两个格式：\n  紧缩格式（packed formats）：将Y、U、V值储存成Macro Pixels阵列，和RGB的存放方式类似。 平面格式（planar formats）：将Y、U、V的三个分量分别存放在不同的矩阵中。  YIQ和YUV非常适合压缩，因为亮度和色度可以使用不同的比特来编码。 文献中U表示红-蓝色差，V对应绿-品红色差。 YUV可用于彩色图像的高光分析。\nYCbCr YCbCr不是一种绝对色彩空间，是YUV压缩和偏移的版本。\n YCbCr或Y\u0026rsquo;CbCr有的时候会被写作：YCBCR或是Y\u0026rsquo;CBCR，是色彩空间的一种，通常会用于影片中的影像连续处理，或是数字摄影系统中。Y\u0026rsquo;为颜色的亮度（luma）成分、而CB和CR则为蓝色和红色的浓度偏移量成份。Y\u0026rsquo;和Y是不同的，而Y就是所谓的流明（luminance），表示光的浓度且为非线性，使用伽马修正（gamma correction）编码处理。 YCbCr的Y与YUV中的Y含义一致，Cb和Cr与UV同样都指色彩，Cb指蓝色色度，Cr指红色色度，在应用上很广泛，JPEG、MPEG、DVD、摄影机、数字电视等皆采此一格式。因此一般俗称的YUV大多是指YCbCr。 YCbCr格式有： 4∶4∶4 , 4∶2∶2 , 4∶1∶1 , 4∶2∶0 . 四种压缩比.\n YCbCr 4:4:4表示Y，Cb，Cr占同样的比特位 下面的四个像素为: $ [Y0 U0 V0] [Y1 U1 V1] [Y2 U2 V2] [Y3 U3 V3]$\n存放的码流为: $Y0 U0 V0 Y1 U1 V1 Y2 U2 V2 Y3 U3 V3$\n YCbCr 4:2:2表示Cb，Cr占Y比特位的一半 下面的四个像素为: $[Y0 U0 V0] [Y1 U1 V1] [Y2 U2 V2] [Y3 U3 V3]$\n存放的码流为: $Y0V0Y1U1Y2V2Y3U3 $\n解析后为： $[Y0 U0 V1] [Y1 U0 V1] [Y2 U2 V3] [Y3 U2 V3] $\n YCbCr 4:1:1表示Cb，Cr占Y比特位的$\\frac{1}{4}$每个由4个水平方向相邻的像素组成的宏像素需要占用6字节内存。 下面的四个像素为: $ [Y0 U0 V0] [Y1 U1 V1] [Y2 U2 V2] [Y3 U3 V3]$\n存放的码流为: $Y0U0Y1Y2V2Y3 $\n解析后为： $[Y0 U0 V2] [Y1 U0 V2] [Y2 U0 V2][Y3 U0 V2] $\n YCbCr 4:2:0 并不是没有Cr分量，而是采取上下左右分别抽样的方式：\n 如果一行是4:2:0的话，下一行就是4:0:2，再下一行是4:2:0\u0026hellip;以此类推。对每个色度分量来说，水平方向和竖直方向的抽样率都是2:1，所以可以说色度的抽样率是4:1。  对非压缩的8比特量化的视频来说，每个由2x2个2行2列相邻的像素组成的宏像素需要占用6字节内存。 下面八个像素为： $$[Y0 U0 V0] [Y1 U1 V1] [Y2 U2 V2] [Y3 U3 V3]\\ [Y5 U5 V5] [Y6 U6 V6] [Y7U7 V7] [Y8 U8 V8]$$ 存放的码流为： $$Y0 U0 Y1 Y2 U2 Y3\\ Y5 V5 Y6 Y7 V7 Y8$$\n解析后： $$[Y0 U0 V5] [Y1 U0 V5] [Y2 U2 V7] [Y3 U2 V7]\\ [Y5 U0 V5] [Y6 U0 V5] [Y7U2 V7] [Y8 U2 V7]$$\nYC1C2 YC1C2与YCbCr相近，知识YC1C2更接近胶片色阶，而YCbCr更接近荧光粉的色阶。\nI1I2I3 此空间描述了一个特征模型而不是色彩空间I1I2I3可以对图像获得较好的分割结果。 其中I1I2I3如下： $I_1=\\frac{R+G+B}{3}$ $I_2=\\frac{R-B}{2}$ $I_3=\\frac{2G-R-B}{4}$\n总结 简单介绍几个色彩空间，作为彩色图像的基础，色彩空间相对概念性较强，在后面处理中，如需更深入的知识会在后续介绍。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-4-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4-yiq-yuvycbcr-yc1c2%E5%92%8Ci1i2i3.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第64天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 色彩空间,YIQ,YUV,YCbCr\u003c/p\u003e","title":"【数字图像处理】8.4:彩色图像-色彩空间 YIQ 、YUV 、YCbCr 、YC1C2 和I1I2I3"},{"content":"Abstract: 数字图像处理：第65天 Keywords: CIELAB,CIELUV\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-色彩空间 CIELAB、CIELUV 废话开始，昨天没说废话，几天补上，对色彩空间的研究并不是图像处理的主要研究方向，因为色度学作为一门独立的学科博大精深，图像处理只是用了其中一些基本知识，所以如果想真正深入了解色度学，需要潜心研究，而这不是图像处理要研究的，在后续用到有关色度的知识进行图像处理时，可以针对性的学习。 #绝对色彩空间和相对色彩空间 首先要介绍下绝对色彩空间和相对色彩空间，对这个问题的理解，我是这样想的，绝对色彩空间是颜色的绝对描述，例如定义一个n维向量R，其唯一定义了一种颜色，就像下面说的LAB色彩空间，一个向量唯一定义一种颜色，而且定义的时候需要满足条件A，这就表明只要条件A满足，无论在什么设备上看色彩R，其表现出来的颜色都一样。也就是说绝对颜色空间是对颜色的定义。 而相对颜色空间就是在不同的设备上得出的结果不同，比如相机C采集单一颜色L的图像，，使用其内在的传感器生成的RGB值是$r_c，g_c，b_c$，但如果将这个RGB值放在一个另一个显示器M上，其反映出来的并不是之前采集的时候所看到的颜色L（这里的L就是绝对色彩空间）。 上面这两段是的对绝对色彩空间和相对色彩空间的理解，绝对色彩空间是可以转换的，但前提是色域要一致，如果色域不一致，然么色彩将有损耗，也就是如果从绝对色彩A转换到绝对色彩B，而$A-(A \\cap B) \\ne \\emptyset$这样就会产生误差。 绝对色彩空间之间的转换是近似的，而非绝对色彩空间之间的转换，绝对空间到非绝对空间的转换，实际上来讲没有意义。但可以作为图像处理的一种方式，比如从RGB转换到YIQ后可以分析光照，但具体的实现要满足转换前后的现实效果相近似。\nCIELAB色彩空间描述  Lab色彩空间是颜色-对立空间，带有维度L表示亮度，a和b表示颜色对立维度，基于了非线性压缩的CIE XYZ色彩空间坐标。 Hunter 1948 L, a, b色彩空间的坐标是L, a和b。但是，Lab经常用做CIE 1976 (L*, a*, b*)色彩空间的非正式缩写（也叫做CIELAB，它的坐标实际上是L*, a和b）。\n 三个基本坐标意义：\n 颜色的亮度（L*, L* = 0生成黑色而L* = 100指示白色）， 红色／品红色和绿色之间的位置（a*负值指示绿色而正值指示品红） 黄色和蓝色之间的位置（b*负值指示蓝色而正值指示黄色）。  CIELAB是均匀的颜色空间，所谓均匀是当数值均匀变化时，人的感官也是均匀变化； 在Lab* 模型中均匀改变对应于在感知颜色中的均匀改变。所以在Lab* 中任何两个颜色的相对感知差别，可以通过把每个颜色处理为（有三个分量：L*, a*, b* 的）三维空间中一个点来近似，并计算在它们之间的欧几里得距离。在Lab* 空间中的这个欧几里得距离是ΔE（经常叫做“Delta E”，更精确的是ΔE*ab）。\n使用Lab* 中的两个颜色 $({L_1}^,\\ {a_1}^,\\ {b_1}^)和({L_2}^,\\ {a_2}^,\\ {b_2}^)$ : 色彩空间的现实： 得到CIELAB的办法是从XYZ空间进行：\n从RGB到LAB是简单的近似，其本身就没有理论意义，但可以以另一种方式观察图像性质，但必须明确的是LAB是对颜色的定义，而RGB只是这种颜色的一个表达，而且与设备密切相关。设备一旦变化这个RGB值将失去意义。 CIELUV色彩空间描述  LUV色彩空间，也作CIELUV。是于1976年由国际照明委员会CIE 提出，由CIE XYZ空间经简单变换得到，具视觉统一性。\n L表示物体亮度，u和v是色度，其中L与LAB中的L一致。 一般来讲u和v的取值范围为-100到+100，亮度L为0到100。 从XYZ到LUV的转换如下： 总结 今天简单介绍下CIELAB和CIELUV这两种设备无关的色彩空间，下一篇接续色彩空间。 文中内容部分引用自wiki和docin.com 待续。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-5-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4-cielab-cieluv.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第65天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e CIELAB,CIELUV\u003c/p\u003e","title":"【数字图像处理】8.5:彩色图像-色彩空间 CIELAB、CIELUV"},{"content":"Abstract: 数字图像处理：第66天 Keywords: HSI,HSV\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-色彩空间 HSI(HSL)、HSV(HSB) 色彩空间介绍最后两种与人类感知相对较接近的两种空间，彩色图像处理后面的内容大部分用在图像的基础处理，而如果想学习图像分析，应该开始学习一些模式识别和机器学习的算法，打算最近开始学习一些，并且开一个新的博客主题，欢迎讨论。。。 今天介绍下两种相对较接近色彩定义的，人的视觉只能分辨颜色的三种变化：亮度、色调、饱和度，HSI和HSV即表示其相对的变量，其中H（Hue）表示色调，S（Saturation）表示饱和度，I/V（Intensity/Value）表示亮度/明度。HSI和HSV作为相对颜色，但在彩色图像处理中使用广泛，下面来逐一介绍这两种空间，和其与RGB空间的转换。\nHSI色彩空间 色调H(Hue)： 与光波的波长有关，它表示人的感官对不同颜色的感受，如红色、绿色、蓝色等，它也可表示一定范围的颜色，如暖色、冷色等。 饱和度S(Saturation)： 表示颜色的纯度，纯光谱色是完全饱和的，加入白光会稀释饱和度。饱和度越大，颜色看起来就会越鲜艳，反之亦然。 亮度I(Intensity)： 对应成像亮度和图像灰度，是颜色的明亮程度。 RGB和HSI的转换几种转换公式：  HSV色彩空间 HSV模型通常用于计算机图形应用中。在用户必须选择一个颜色应用于特定图形元素各种应用环境中，经常使用HSV 色轮。在其中，色相表示为圆环；可以使用一个独立的三角形来表示饱和度和明度。典型的，这个三角形的垂直轴指示饱和度，而水平轴表示明度。在这种方式下，选择颜色可以首先在圆环中选择色相，在从三角形中选择想要的饱和度和明度。 HSV模型的另一种可视方法是圆锥体。在这种表示中，色相被表示为绕圆锥中心轴的角度，饱和度被表示为从圆锥的横截面的圆心到这个点的距离，明度被表示为从圆锥的横截面的圆心到顶点的距离。某些表示使用了六棱锥体。这种方法更适合在一个单一物体中展示这个HSV色彩空间；但是由于它的三维本质，它不适合在二维计算机界面中选择颜色。 HSV色彩空间还可以表示为类似于上述圆锥体的圆柱体，色相沿着圆柱体的外圆周变化，饱和度沿着从横截面的圆心的距离变化，明度沿着横截面到底面和顶面的距离而变化。这种表示可能被认为是HSV色彩空间的更精确的数学模型；但是在实际中可区分出的饱和度和色相的级别数目随着明度接近黑色而减少。此外计算机典型的用有限精度范围来存储RGB值；这约束了精度，再加上人类颜色感知的限制，使圆锥体表示在多数情况下更实用。 转换公式： RGB-\u0026gt;HSV HSV-\u0026gt;RGB 总结 今天主要介绍HSx模型，这类色彩空间与色彩定义对应，在后面的彩色图像处理中将大量用到。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-6-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4-hsi-hsl-hsv-hsb.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第66天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e HSI,HSV\u003c/p\u003e","title":"【数字图像处理】8.6:彩色图像-色彩空间 HSI(HSL)、HSV(HSB)"},{"content":"Abstract: 数字图像处理：第67天 Keywords: 色彩空间\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-色彩空间 总结 现在把色彩空间进行简单总结，首先推荐一个博客 http://blog.ibireme.com/2013/08/12/color-model/ 这篇博客对整个色彩空间总结的相当全面，而且有GIF图，直观。 下面这个视频连接可以简单的向你介绍颜色的实质： http://v.youku.com/v_show/id_XNTQ5NzAxMTQ0.html 总结起来一张图可以简单的概括下： 色彩空间简单进行介绍，在后面用到相关知识的时候再进行补充。 下篇开始新的内容。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-8-7-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4-%E6%80%BB%E7%BB%93.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第67天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 色彩空间\u003c/p\u003e","title":"【数字图像处理】8.7:彩色图像-色彩空间 总结"},{"content":"Abstract: 数字图像处理：第68天 Keywords: 伪彩色图像\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-伪彩处理 灰度图转伪彩色图像 观察了一下冈萨雷斯的书，发现彩色图像处理只用了一章进行介绍，原因分析了一下，后来发现，好像别的介绍的也不多，得出一个结论，冈萨雷斯这本书只能作为一部纲领性的介绍，它基本涵盖了图像处理的基础知识，但是如果想使用某种方向作为工作的话，需要继续找更多专业的书和开源项目来学习。还是像我之前抱怨的那样，每次看一本书之前都觉得自己看完了会变得超级厉害，但每次看完一本书以后反而会觉得自己像个傻瓜一样，需要更多的书来学习，如此循环，这一生都不会幸福了。盗图一张，与各位共勉 原理 说到伪彩色图像，与其对应的是真彩色，下面介绍下其区别和性质。 我们知道能够观察出颜色的光的波长范围是有限的，只有那么一小段，换句话说也就是说有一大段光，只有一小段有颜色，其他都是灰度的，但人类视觉有一个特点就是，只能分辨出二十几种灰度，也就是说采集到的灰度图像分辨率超级高，有一千个灰度级，但很遗憾，人们只能看出二十几个，也就是说信息损失了五十倍，但人类视觉对彩色的分辨能力相当强，能够分辨出几千种色度。 在从采集的角度说下伪彩和真彩色，伪彩色原始图像是灰度图像 灰度图像的来源：\n 单通道相机或其他传感器（比如CT用的平板）采集到的都是灰度图，这里包括使用单通道采集的频率高于可见光的频率的电磁波，可见光，低于可见光频率的电磁波。 图中红色框内为不可见光，没有颜色，所以他们一定是灰度图，需要时要进行伪彩色处理。 使用多通道采集设备采集的不可见光，这种图像有时候是单通道的，就是1中所说的，也有可能是多通道，不如卫星就有可能，红外，可见光，还有其他不可见光采集设备，这样的多通道灰度图有时候需要进行伪彩处理。  真彩色图的来源： 用多通道采集设备，多为相机来采集可见光，这样得到的是多通道真彩色图像。 #算法分析 对于单通道灰度图转换成伪彩图像的方法是将一种灰度映射为一种颜色，而映射方式不唯一，可以根据需要自行设定，下面的代码使用的算法是我自己想出来的，使用到了HSV色彩空间，并将其中的饱和度和亮度设为1.0，色相使用灰度0到255映射到0°到270°： 代码 void Gray2Color(double *src,RGB* dst,int width,int height,int type){ HSV* temp=(HSV*)malloc(sizeof(HSV)*width*height); for(int i=0;i\u0026lt;width*height;i++){ double gray_value=src[i]; if(type==HIGHVALUE_EQU_RED) temp[i].c1=HSVMAX-GRAY2HSV*gray_value; else if(type==LOWVALUE_EQU_RED) temp[i].c1=GRAY2HSV*gray_value; temp[i].c2=1.0; temp[i].c3=1.0; } HSV2RGB(temp, dst, width, height); free(temp); } #效果分析 灰度渐进图： MacBook Pro x光扫描图 卫星地形图： 星云： 总结 伪彩色图乡相对灰度图像能够识别更多的细节，可分辨性较强。而且转换方式灵活，可以根据需要自行设计转换函数，或者自制映射表。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-9-1-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E4%BC%AA%E5%BD%A9%E5%A4%84%E7%90%86-%E7%81%B0%E5%BA%A6%E5%9B%BE%E8%BD%AC%E4%BC%AA%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第68天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 伪彩色图像\u003c/p\u003e","title":"【数字图像处理】9.1:彩色图像-伪彩处理 灰度图转伪彩色图像"},{"content":"Abstract: 数字图像处理：第69天 Keywords: 色彩变换,补色,色环\n本文最初发表于csdn，于2018年2月17日迁移至此\n彩色图像-彩色变换 补色处理 啊啊啊啊啊。。。办公室好乱。像菜市场那个一样，说好的做一个安静的美男子呢。 彩色图像处理中彩色变换是指 $g(x,y)=T[f(x,y)]$\n其中$f$ , $g$为返回结果是向量的函数，T是$x,y$空间上对f的一个算子。 要顺便说下HSI在0°和360°处是个不连续的点，而且饱和度为0是色相未定义。 今天开始介绍真彩色图像，也包括多通道图像，补色处理就是根据色环，将现有颜色的色相反转180°，得到新的色相，这个变化的用途不知道，一个很一般的也很常见的应用是RGB带CMY的变化，不过，别的好像目前没听说过什么处理要把图像变成补色后才能处理。 #算法原理 算法原理很简单，找到补色，等位替换就好。 色环牛顿老爷子发明的，没有错，就是那个被苹果砸了的牛顿，不得不说，牛顿是位大师级的人物，色环如下： 这个色环可能不是牛顿老爷子的色环，但通过这个来展示下补色就是一条直径上两端的颜色。 对于RGB图像，补色是对应的CMY图像 对于HSI和HSV图像，H分量需要进行相应的旋转，而亮度分量也需要相应的反转，而饱和度不变，能够得到类似的补色效果。\n代码实现 void Complementary_Color(C3 *src,C3 *dst,int width,int height,int color_space_type){ switch (color_space_type) { case COLOR_SPACE_RGB: RGB2CMY(src, dst, width, height); break; case COLOR_SPACE_CMY: CMY2RGB(src, dst, width, height); break; case COLOR_SPACE_HSI:{ for(int i=0;i\u0026lt;width*height;i++){ double h=src[i].c1; if(h\u0026gt;=M_PI) dst[i].c1=h-M_PI; else dst[i].c1=M_PI+h; dst[i].c2=src[i].c2; dst[i].c3=255.-src[i].c3; } break; } case COLOR_SPACE_HSV:{ for(int i=0;i\u0026lt;width*height;i++){ double h=src[i].c1; if(h\u0026gt;=180.0) dst[i].c1=h-180.0; else dst[i].c1=180.0+h; dst[i].c2=src[i].c2; dst[i].c3=1.0-src[i].c3; } break; } default: break; } } 结果观察 原图： RGB： HSI： HSV： Paintbrush： 总结 简单的介绍了最简单的色彩变换，补色变换，可以看出HSI，HSV和RGB处理的结果有些不同，而PaintBrush处理和我处理都不同，不知道它使用的什么算法。 待续。。。\n","permalink":"https://go.face2ai.com/%E7%BC%96%E7%A8%8B/dip/dip-9-2-%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F-%E5%BD%A9%E8%89%B2%E5%8F%98%E6%8D%A2-%E8%A1%A5%E8%89%B2%E5%A4%84%E7%90%86.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 数字图像处理：第69天\n\u003cstrong\u003eKeywords:\u003c/strong\u003e 色彩变换,补色,色环\u003c/p\u003e","title":"【数字图像处理】9.2:彩色图像-彩色变换 补色处理"},{"content":"Abstract: 人脸检测基本算法学习（迁移自CSDN） Keywords: Adaboost，Haar-like，人脸检测，机器学习\n人脸检测分析之 Haar-like，Adaboost，级联(Cascade) 写在前面的话 写在前面的牢骚话，作为一个非主流工程师，我专业与目前工作都与这些知识相隔十万八千里，所以，我所学习和实现的完全是因为兴趣，目前还研究学习的很浅，谈不上高深，所以还是要继续努力学习。希望和大家多交流，也欢迎伪大牛，假专家板砖伺候，也希望真大牛多指点（真大牛不会啰嗦一堆来显得他知道的多，哈哈），总之，本人还在菜鸟阶段，欢迎指教。0.0本文如有错误请及时留言指出，博主会在第一时间修改，确保不会对其他读者产生副作用。\n人脸检测与识别 人脸识别系统主要包括四个组成部分，分别为：人脸图像采集及检测、人脸图像预处理、人脸图像特征提取以及匹配与识别。\n人脸图像采集及检测  人脸图像采集：不同的人脸图像都能通过摄像镜头采集下来，比如静态图像、动态图像、不同的位置、不同表情等方面都可以得到很好的采集。当用户在采集设备的拍摄范围内时，采集设备会自动搜索并拍摄用户的人脸图像 人脸检测：人脸检测在实际中主要用于人脸识别的预处理，即在图像中准确标定出人脸的位置和大小。人脸图像中包含的模式特征十分丰富，如直方图特征、颜色特征、模板特征、结构特征及Haar特征等。人脸检测就是把这其中有用的信息挑出来，并利用这些特征实现人脸检测。 主流的人脸检测方法基于以上特征采用Adaboost学习算法，Adaboost算法是一种用来分类的方法，它把一些比较弱的分类方法合在一起，组合出新的很强的分类方法。 人脸检测过程中使用Adaboost算法挑选出一些最能代表人脸的矩形特征(弱分类器)，按照加权投票的方式将弱分类器构造为一个强分类器，再将训练得到的若干强分类器串联组成一个级联结构的层叠分类器，有效地提高分类器的检测速度。\n人脸图像预处理  人脸图像预处理：对于人脸的图像预处理是基于人脸检测结果，对图像进行处理并最终服务于特征提取的过程。系统获取的原始图像由于受到各种条件的限制和随机干扰，往往不能直接使用，必须在图像处理的早期阶段对它进行灰度校正、噪声过滤等图像预处理。对于人脸图像而言，其预处理过程主要包括人脸图像的光线补偿、灰度变换、直方图均衡化、归一化、几何校正、滤波以及锐化等。\n人脸图像特征提取  人脸图像特征提取：人脸识别系统可使用的特征通常分为视觉特征、像素统计特征、人脸图像变换系数特征、人脸图像代数特征等。人脸特征提取就是针对人脸的某些特征进行的。人脸特征提取，也称人脸表征，它是对人脸进行特征建模的过程。人脸特征提取的方法归纳起来分为两大类：一种是基于知识的表征方法；另外一种是基于代数特征或统计学习的表征方法。 基于知识的表征方法主要是根据人脸器官的形状描述以及他们之间的距离特性来获得有助于人脸分类的特征数据，其特征分量通常包括特征点间的欧氏距离、曲率和角度等。人脸由眼睛、鼻子、嘴、下巴等局部构成，对这些局部和它们之间结构关系的几何描述，可作为识别人脸的重要特征，这些特征被称为几何特征。基于知识的人脸表征主要包括基于几何特征的方法和模板匹配法。 人脸图像匹配与识别 人脸图像匹配与识别：提取的人脸图像的特征数据与数据库中存储的特征模板进行搜索匹配，通过设定一个阈值，当相似度超过这一阈值，则把匹配得到的结果输出。人脸识别就是将待识别的人脸特征与已得到的人脸特征模板进行比较，根据相似程度对人脸的身份信息进行判断。这一过程又分为两类：一类是确认，是一对一进行图像比较的过程，另一类是辨认，是一对多进行图像匹配对比的过程。 以上摘自维基百科，虽然很普通的过程，却不失一般性，顾一个完整的系统，基本要靠这几步来完成，是否能开发出更好更快的而非这种结构的系统也是一个很值得深入的话题，比如一幅包含人脸和复杂背景的图片，是否能不通过分割而直接识别出人脸是谁，或者识别一个物体不是通过现有特征描述而是使用更符合人脑识别机制的其他描述，这些都值得我们深入研究。 其次，我们要区分人脸检测和人脸识别，所谓检测是区分人脸和非人脸，而识别是要得出已知人脸术属于谁，这从意义和用途上都有区别，而现在很多人将人脸识别和人脸检测混为一谈，包括知名专家（上过研究生的人都知道，有些老板根本不干活）。 或者我们可以更简单的把检测当做是分割（segment）的过程，即对物体进行识别最基本的过程：分割+识别。检测过程为输入一张复杂的图像，输出是其中人脸的部分，而识别是输入一张人脸的图像，输出的是这个人的身份信息或其他的只和此人有关的信息。\n检测与识别算法 测算法和识别算法的结构多为：特征描述+分类算法。 特征描述给出不同物体间不同的表示，该描述应该具有一定的性质，包括特异性，即不同物体特征值不同，并对旋转，缩放，光照，形变等不敏感。好的特征描述能够使识别更加准确，而好的分类算法也能提高准确度和计算速度，分类算法多为机器学习算法，即需要训练，才能用于识别工作。 目前比较常用的特征描述有：Haar-like，LBP，SIFT，SURF等常见特征描述。 人脸识别算法分类：\n 基于人脸特征点的识别算法（Feature-based recognition algorithms）。 基于整幅人脸图像的识别算法（Appearance-based recognition algorithms）。 基于模板的识别算法（Template-based recognition algorithms）。 利用神经网络进行识别的算法（Recognition algorithms using neural network）。  人脸识别理论分类：\n 基于光照估计模型理论：提出了基于Gamma灰度矫正的光照预处理方法,并且在光照估计模型的基础上，进行相应的光照补偿和光照平衡策略； 优化的形变统计校正理论：基于统计形变的校正理论，优化人脸姿态； 强化迭代理论：** 强化迭代理论是对DLFA人脸检测算法的有效扩展； 独创的实时特征识别理论：该理论侧重于人脸实时数据的中间值处理，从而可以在识别速率和识别效能之间，达到最佳的匹配效果；   详细可参考网址： http://www.face-rec.org/algorithms/ 所列出的常见算法。\n机器学习 机器学习在人脸识别中使用广泛，其理论和应用都及其有价值，Adaboost就属于一种机器学习算法，直白的理解机器学习就是通过用已知样本来不断的优化算法中的可变系数，最后来建立一种机制，该机制能够达到我们想要分类或者识别的目的。当然这不是官方定义，而是我的理解，但机器学习算法的理论基础多半是优化，寻求最优解，具体知识可以参考其他资料，这里不再赘述。\nHaar-like特征点 Haar-like特征点，是一种简单的特征描述，其理论相当容易理解，就是按照下图的模式来计算白色窗口的像素总和和黑色窗口像素总和的差（C中的计算为白色窗口的像素总和减去黑色窗口的像素总和的2倍），这是常用的几种计算方式。\n4.1\n后来又衍生了很多种Haar-like特征的计算方法，但计算法则不变，都是白色框内的像素总和减去黑色的，如下： .2 以下为图4.1描述的Haar-like特征在不同尺寸图像中的数量（窗口大小就是图像大小） 4.3 优化Haar-like计算速度的方法是使用积分图像，积分图像的简单描述为：对于单通道图像 $F(x,y)$ ,其积分图像 $G(x,y)$ 与 $F(x,y)$ 具有相同尺寸大小，且点 $G(x_0,y_0)$ 处的值为 $SUM(F(i,j))$ 其中 $(i\u0026lt;=x_0\u0026amp;\u0026amp;j\u0026lt;=y_0)$ ，公式描述为下图，ii为积分图像G，i为原始图像F。 所以我们有下列结构，点4的值为ABCD区域像素值的总和，1为A区域的像素值总和，2，3同理： 4.4 所以区域D内像素和的值为：4-2-3+1，这个不难理解，因为2，3都包括1，所以多减了一次，要加回来。这样就很容易的计算局部区域的像素和，从而方便了Haar-like特征值的求解。\nAdaboost 首先我们来介绍强分类器和弱分类器：这类似于两个鉴宝工程师，一个经验丰富，准确率高我们称为老手，一个刚刚毕业准确率低，我们称为新手，但他们和我们这群普通人的区别在于，如果有100万件古董和赝品，老手的正确鉴别率能达到90万件，而新手的鉴别率为70万件，而我们接近以随机方式给出答案，顾准确率在50万件左右。这里的老手对应强分类器，新手对应弱分类器，而我们对应瞎猜分类器（这句掐了别播）。 Adaboost以同样原理工作，其对与已知分类的数据样本给出不同的阈值，各个阈值就是弱分类器，其准确率必须大于50%，其中样本对应的权重不同，其准确率为样本加权后的准确率，例如一百个数据，第一个数据占所有数据权重的百分之60，顾只要这个数据分类正确，其准确率肯定达标，相反肯定不达标。 首先来看阈值选择： 这个式子中，$p_j$ 为 $\\pm 1$ ，其作用就是表示是大于号还是小于号，而西塔为我们给出的阈值，假设为新手A，例如，对于古董的一个特征f值（例如材料密度），我们可以给出其大于阈值是为古董或者小于阈值为古董。当然这个分类并不准确，因为有的赝品也可以用类似真品的材料仿造。这样新手A使用密度测定法给出了一个大于60%的识别率，同时按照A的识别率给A一个发言权重Alpha（ $0\u0026lt;\\alpha\u0026lt;1$ ），为了帮助新手，这100万件古董的主人（对真伪完全确定的人）决定，把新手A分错的真品和赝品调整权重（即调整密度，这都非人力所及的事啊，博主太能扯。。），然新手B来分类，依然以密度分类，然给B一个发言权重，依次迭代新手C，D，E，F，并得到各自的发言权重Alpha。最后当所有新手都有权重时，我们来最后进行表决，拿来另外一件古董（测试样本），让这些新手投票，这件宝贝是还是不是真品，每个人投票后加权（ $\\alpha$ ），如果加权以后超过半数，则这件判断为真品，否则为赝品。这就是完整的Adaboost过程。 先看下实际中的数据运算过程： step 0:原始数据，解释下最左边一列，编号为数据编号，数值为特征值（上例所说的密度），标签为分类（两类，1，0，上例中的正品和赝品），权重（每个样本的“重要性”，或者说对整体的影响，初始化:正样本为1/2m，m为正样本数,负样本为1/2n，n为负样本数）。 step 1：寻找一个阈值 $\\theta=4$ ，使大于（或小于，这里为小于）其的为正样本，另一侧为负样本，这里有阴影的为正样本划分区，另外为负样本区，负样本区中的正样本为分类错误的样本（9，10）这时，我们增加其权重（其实是减小正样本的权重，但归一化后就与增加负样本的效果一致了），并计算这个阈值的误差error=0.142857。 这样我们就得到了第一个弱分类器，其theta=4，方向是小于阈值为正，话语权为 $\\alpha=1.791759$ 。大家会有疑问，4这个阈值哪来的，我是通过穷举所有的阈值，得出error最小的阈值，也有其他算法，可以参考原文【】。 step2:第二部重复上一步，得到error，theta，alpha，更新权重step3:继续重复 step 4: . . . . step n:完成循环要求的次数，或其他推出条件成立时，推出，保存所有theta，alpha和error 下面给出原文中的算法步骤： 上述算法中，表示误差的error求出权重使用了 $\\frac{e}{(1-e)}$ ，这里我们可以知道，如果e大于0.5那么整个算法就会出现问题，因为不是按照思想继续放大错分样本的权重而是减小其权重，所以这里应该有一定的说法。 具体实现代码：\n#include \u0026#34;adaboost.h\u0026#34; void initStrongCl(StrongCl *strongwl,int t){ WeakCl* weakcl=(WeakCl*)malloc(sizeof(WeakCl)*t); if(weakcl==NULL) exit(0); strongwl-\u0026gt;T=t; strongwl-\u0026gt;weakcl=weakcl; } void releaseStrongCl(StrongCl *strongwl){ free(strongwl-\u0026gt;weakcl); } //get data and label from input TrainData* getTrainData(int *Data_Size){ printf(\u0026#34;Input your datasize:\u0026#34;); scanf(\u0026#34;%d\u0026#34;,Data_Size); TrainData *data=(TrainData *)malloc(sizeof(TrainData)*(*Data_Size)); if(data!=NULL) printf(\u0026#34;Memory allocation succeeds\\n\u0026#34;); else printf(\u0026#34;Memory allocation failed\\n\u0026#34;); printf(\u0026#34;input property and label(positive 1 and nagitive 0):\\n\u0026#34;); for(int i=0;i\u0026lt;(*Data_Size);i++){ scanf(\u0026#34;%d,%d\u0026#34;,\u0026amp;data[i].property,\u0026amp;data[i].label); } return data; } //free memory void freeTrainData(TrainData* data){ free(data); } //showdata void showTrainData(TrainData* data,int Data_Size){ printf(\u0026#34;property1 \\n\u0026#34;); for(int i=0;i\u0026lt;Data_Size;i++) printf(\u0026#34;%d\\n\u0026#34;,data[i].property); } // double getBeta(double erro){ return erro/(1.0-erro); } double getAlpha(double beta){ return log(1.0/beta); } void updataWi(TrainData *data,double beta,int Data_Size){ for(int i=0;i\u0026lt;Data_Size;i++){ if(data[i].status==HIT) data[i].w=data[i].w*beta; else if(data[i].status==MISS) data[i].w=data[i].w; } } void nomalization(TrainData *data,int Data_Size){ double sum=0.0; for(int i=0;i\u0026lt;Data_Size;i++){ sum+=data[i].w; } //printf(\u0026#34;Sum of w:%lf\\n\u0026#34;,sum);  for(int i=0;i\u0026lt;Data_Size;i++){ data[i].w=data[i].w/sum; } } void InitWi(TrainData *data,int Data_Size){ double positive=0.0; for(int i=0;i\u0026lt;Data_Size;i++) if(data[i].label==1) positive++; for(int i=0;i\u0026lt;Data_Size;i++){ if(data[i].label==1) data[i].w=1.0/(2.0*positive); else data[i].w=1.0/(2.0*(Data_Size-positive)); } } void InitStatus(TrainData *data,int Data_Size){ for(int i=0;i\u0026lt;Data_Size;i++) data[i].status=HIT; } double getError(TrainData *data,int Data_Size){ double error=0.0; for(int i=0;i\u0026lt;Data_Size;i++) if(data[i].status==MISS) error+=data[i].w; return error; } void Adaboost(TrainData *data,int Data_Size,StrongCl *dst){ int T=dst-\u0026gt;T; InitWi(data,Data_Size); int temptheta=0,theta1=0; double error,beta; int p=0; //p=0 \u0026lt;=\u0026gt; \u0026#39;\u0026lt;\u0026#39; p=0 \u0026lt;=\u0026gt; \u0026#39;\u0026gt;\u0026#39;  double min; //////////////left is positive \u0026amp; right is nagitive//////////////  for(int i=0;i\u0026lt;T;i++){ //get theta first  p=0; min=DBL_MAX;//////Be careful  nomalization(data,Data_Size); for(int j=0;j\u0026lt;Data_Size;j++){ InitStatus(data,Data_Size); temptheta=data[j].property; for(int k=0;k\u0026lt;Data_Size;k++){ if((data[k].property\u0026lt;=temptheta)\u0026amp;\u0026amp;(data[k].label==0)) data[k].status=MISS; if((data[k].property\u0026gt;temptheta)\u0026amp;\u0026amp;(data[k].label)) data[k].status=MISS; } error=getError(data,Data_Size); if(error\u0026lt;=min\u0026amp;\u0026amp;error\u0026lt;0.5){ theta1=temptheta; min=error; } } //////////////right is positive \u0026amp; right is nagitive//////////////  temptheta=0.0; int theta2=0.0; for(int j=0;j\u0026lt;Data_Size;j++){ InitStatus(data,Data_Size); temptheta=data[j].property; for(int k=0;k\u0026lt;Data_Size;k++){ if((data[k].property\u0026gt;=temptheta)\u0026amp;\u0026amp;(data[k].label==0)) data[k].status=MISS; if((data[k].property\u0026lt;temptheta)\u0026amp;\u0026amp;(data[k].label)) data[k].status=MISS; } error=getError(data,Data_Size); if(error\u0026lt;=min){ theta2=temptheta; min=error; p=1; } } //////////////////////////////////////////////////////////////////////////  InitStatus(data,Data_Size); int theta=p?theta2:theta1; if(p) for(int k=0;k\u0026lt;Data_Size;k++){ if((data[k].property\u0026gt;=theta)\u0026amp;\u0026amp;(data[k].label==0)) data[k].status=MISS; if((data[k].property\u0026lt;theta)\u0026amp;\u0026amp;(data[k].label)) data[k].status=MISS; } else for(int k=0;k\u0026lt;Data_Size;k++){ if((data[k].property\u0026lt;=theta)\u0026amp;\u0026amp;(data[k].label==0)) data[k].status=MISS; if((data[k].property\u0026gt;theta)\u0026amp;\u0026amp;(data[k].label)) data[k].status=MISS; } error=getError(data,Data_Size); beta=getBeta(error); updataWi(data, beta,Data_Size); if(p){ printf(\u0026#34;|\u0026gt;=| |Threshold:%9d|error:%9lf |Alpha:%9lf|\\n\u0026#34;,theta,error,getAlpha(beta)); dst-\u0026gt;weakcl[i].p=MORETHAN; dst-\u0026gt;weakcl[i].alpha=getAlpha(beta); dst-\u0026gt;weakcl[i].threshold=theta; } else{ printf(\u0026#34;|\u0026lt;=| |Threshold:%9d|error:%9lf |Alpha:%9lf|\\n\u0026#34;,theta,error,getAlpha(beta)); dst-\u0026gt;weakcl[i].p=LESSTHAN; dst-\u0026gt;weakcl[i].alpha=getAlpha(beta); dst-\u0026gt;weakcl[i].threshold=theta; } if(error\u0026gt;=0.5) break; } ////////let the sum of alpha==1;  double alphasum=0.0; for(int i=0;i\u0026lt;dst-\u0026gt;T;i++){ alphasum+=dst-\u0026gt;weakcl[i].alpha; } for(int i=0;i\u0026lt;dst-\u0026gt;T;i++){ dst-\u0026gt;weakcl[i].alpha/=alphasum; } } 级联cascade 其实我在实现Adaboost时没遇到什么困难，相反，在实现级联时却有点概念不清，首先来看第一张图：\n7.1 其想表明的时将一组（或一个）强分类器作为一个节点，来串联成一个级联结构，类似于逻辑上的“AND”即如果一个子窗口被判定为人脸，那么其必须对于所有分类器都为真，但是，我不懂的问题是，节点内部的那一组强分类器是什么结构，怎么组织的，这个目前还没搞懂，所以级联没有实现出来，也希望大牛指点下。 级联的基本步骤如下： 7.2 翻译成中文为： 7.3 这里的时人脸的Harr-like特征集作为训练样本，识别率为D和误识率F，解释下false positive和false negative，第一个字面翻译为错误的positive就是本来不是目标，却分类成目标，后一个意思是本来是目标，结果当成了非目标。 还有疑问就是选择特征方面，如何选择一个Harr-like特征来训练一个分类器，因为有很多Haar-like，而其中只用了一部分，用哪些？ 每一层都用到Haar-like，但怎么选择用哪个Haar-like，而且上一层用过的Haar-like特征是否还能继续用于这一层。其他部分相对来说比较容易，比如把测试集中的false positive放到下一步的训练集中，减少Adaboost中的阈值个数来提高识别率等。\n相关试验数据 相关试验数据由原论文中截取，因为博主还没实现级联，而且笔记本电脑训练起来也很伤神，所以只给出一个性能图，以供参考： 8.1\n参考 《Rapid_Object_Detection_using_a_Boosted_Cascade_of_Simple_Features》 《Robust Real-Time Face Detection》\n下载附件 http://download.csdn.net/detail/tonyshengtan/8251621\n","permalink":"https://go.face2ai.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/machinelearning-haar-like-adaboost-cascade.zh/","summary":"\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e 人脸检测基本算法学习（迁移自CSDN）\n\u003cstrong\u003eKeywords:\u003c/strong\u003e Adaboost，Haar-like，人脸检测，机器学习\u003c/p\u003e","title":"人脸检测分析之 Haar-like，Adaboost，级联(Cascade)"}]