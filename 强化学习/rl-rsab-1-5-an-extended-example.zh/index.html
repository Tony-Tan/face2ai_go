<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>【强化学习】 1.5 强化学习的一个扩展举例 | 谭升的博客</title><meta name=keywords content="强化学习,强化学习举例,Tic-Tac-Toe"><meta name=description content="Abstract: 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。
Keywords: 强化学习，强化学习举例，Tic-Tac-Toe"><meta name=author content="谭升"><link rel=canonical href=https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-5-an-extended-example.zh/><link crossorigin=anonymous href=../../assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=16x16 href=https://go.face2ai.com/logo.png><link rel=icon type=image/png sizes=32x32 href=https://go.face2ai.com/logo.png><link rel=apple-touch-icon href=https://go.face2ai.com/logo.png><link rel=mask-icon href=https://go.face2ai.com/logo.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-105335860-3","auto"),ga("send","pageview"))</script><meta property="og:title" content="【强化学习】 1.5 强化学习的一个扩展举例"><meta property="og:description" content="Abstract: 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。
Keywords: 强化学习，强化学习举例，Tic-Tac-Toe"><meta property="og:type" content="article"><meta property="og:url" content="https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-5-an-extended-example.zh/"><meta property="article:section" content="强化学习"><meta property="article:published_time" content="2018-09-28T08:42:24+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="【强化学习】 1.5 强化学习的一个扩展举例"><meta name=twitter:description content="Abstract: 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。
Keywords: 强化学习，强化学习举例，Tic-Tac-Toe"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":3,"name":"【强化学习】 1.5 强化学习的一个扩展举例","item":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-5-an-extended-example.zh/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"【强化学习】 1.5 强化学习的一个扩展举例","name":"【强化学习】 1.5 强化学习的一个扩展举例","description":"Abstract: 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。 Keywords: 强化学习，强化学习举例，Tic-Tac-Toe\n","keywords":["强化学习","强化学习举例","Tic-Tac-Toe"],"articleBody":"Abstract: 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。 Keywords: 强化学习，强化学习举例，Tic-Tac-Toe\n强化学习的一个扩展举例 今天我们来讲一个很有趣的例子，英文名字叫\"Tic-Tac-Toe\" 中文名字有翻译成“井字棋”或者什么的，我们这里为了方便就称之为“井字棋”，叫井字棋的原因是因为其棋盘是个“井”字形的，玩法简单，但是这个玩的过程可以使用强化学习的方法来学习，这个简单的棋可以让我们从各个细节审视强化学习的特点，有点，缺点，以及一些小技巧。\n“Tic-Tac-Toe\"简介 规则描述 Tic-Tac-Toe的规则描述如下：\n 使用 $3\\times 3$ 的正方形棋盘，双方各使用 ‘x’和’o’ 作为棋子，轮流下棋， 谁的棋子最先连成一行或者一列，或者对角线，那么获胜，棋局结束 对方完成这个过程，则失败 如果在最终双方都没能连成一行或者一列，或者对角线的情况下，棋局结束，则为平局。  下图来自Wikipedia:“井字棋”\n上面的棋局，使用 ‘x’ 的一方率先完成连成一行的准则，故执’x’一方获胜。\n简单规则下的问题 下面这个视频是我在Google上找到的一个小程序录的一段视频。\nyour browser does not support the video tag  可见，在高级的情况下，双方（我和AI）基本都没法获胜，也就是平局会大量出现，原因是，我们对这种棋的技巧掌握的都很熟练，无法在现行环境（规则）下战胜对方，通过这个观察我们也能发现，在规则相对简单的游戏（“博弈”）中，平局会大量出现。 那么问题就要来了，我们 - 也就是人，在这种简单的规则下，多以平局收场，那么这怎么训练agent呢？如果每局训练总是平局，agent就不知道往什么方向走了。这里我们就要做出一些修改，我们让与我们agent下棋的人或者另一个agent不那么高级，换句话说，我们在简单规则下，降低规则执行者的能力，进而模拟出更高级的博弈（所谓更高级的博弈，无非是我们能力不足才会觉得当前环境，或者规则很困难）。 在后面的训练里，agent会将平局和失败都当做失败来处理，agent的目标明确，就是胜利。\n非强化学习型的解决方法 这个棋局太简单，但是在如此简单的规则下，传统方法（非学习方法）都有诸多问题：\n 使用传统的“极大极小化”（minimax）方法，这个方法会假定对方按照某个既定方案下棋，而事实是对方可能无既定方案，或者既定方案我们永远无法得知。所以这个传统的博弈思想在此不适用。而且“极大极小化”（minimax）方法有个非常大的缺点：如果其认定当前状态必然失败的情况下，即使后面对手出现重大失误（可以逆转取胜的机会），其也不会采取任何致胜招数，而是按照既定失败的套路继续下去。 对于这种连续决策型问题的传统的优化方法，例如动态规划(dynamic programming)，可以计算出在对阵任何对手时候的最优解，但是前提是：对手的全部信息要作为输入提前提交给算法，其中包括在某特定情况（棋局）下，对手下一步棋下在哪的概率。如果我们提前不知道这些信息（大部分情况下，这个信息无法获得） 对于2中的动态规划方法，有另一种改进版就是在训练时和对手多次交手，从而记录学习出对手的信息，或者叫做给对手建模（model）然后再使用动态规划来寻找答案。  上面3中方法中，1和2对解决问题基本没有帮助，3有帮助。3和我们后面会介绍的很多强化学习方法有着非常相似的思路。\n进化方法(Evolutionary Method)学习 “Tic-Tac-Toe” 进化方法 中讲解了进化方法的缺点，就是没有使用agent和环境之间的交互信息来改变agent。而这里如果把进化方法直接使用到“井字棋”中，其表策略（policy）的特点是：直接search全部可能的位置，找到获胜概率最大的位置，然后下棋。也就是，策略要考虑到当前盘面（ $3\\times 3$ 的棋盘上 x和o的分布）并给出获胜概率最大的下棋位置。而这个概率的获得就需要和对手多次下棋，记录，学习出来的。有了这些信息，agent就知道接下来一步或者接下来很多步应该怎么走。 一个典型的进化方法就是在“策略空间”的 hill-climb ，这种方法的策略是逐渐找出能提高表现的策略（并不是一下找到最优的方法，而是像爬山一样，每一步都找到一个能提高agent表现的方案，一步一步的向上爬）。 遗传方法就更加直接暴力了，直接直接评估大量的策略(policies)，去除不合格的，留下好的，然后产生新一代的，以此迭代，直到问题解决。 解决“井字棋”问题，理论上存在很多种不同的优化方法。\n评价函数(Value Function)学习 “Tic-Tac-Toe” 上面我们说了进化方法在井字棋中使用，下面我们就要看看另一个方向的方法 —— 评价函数(value Function)的方法了。\n设计评价函数 我们列一个表，这个表中的每个格子对应一种状态（state），整张表就对应井字棋的全部情况，这个表的每一项都对应一个概率值，这个概率值表示当前状态下最后获胜的期望，注意两点，一个是当前的状态，第二是最终获胜的期望。这个期望，我们就把其当做评价函数的结果，value —— value值。这个表就是我们的评价函数(Value Function)了. 在井字棋中，这个表就包含下一步棋应该怎么走的评估。通过当前状态，我们可以缩小下一步可能出现的状态范围，然后比较所有可能的状态，如果A状态比B状态有更高的获胜期望，那么我们认为A状态比B状态好，所以我们倾向于把棋子走到A状态显示的位置。 对于这个状态表，假设我们执x，那么如果某状态中包含x在一行或者一列或者对角线，那么这个状态的获胜期望是1，相反，如果o在一行或者一列或者对角线，那么这个状态的获胜期望是0；同样，如果对应状态是棋盘下满，而没有获胜方，这时候期望同样是0 。除了上述这些情况，其他状态的初始化值都是0.5，即有一半的可能性会获胜。\n执行(exploitation)和探索(exploration) 当我们有了这张表（上面的评价函数）我们就有了制胜法宝，但是具体执行也是有不同方案的，我们可以查找最大value的状态，然后执行贪心(greedily)选择，这是使得获胜期望最大的策略，也就是完全执行表（value function）中的指示，这个被称为exploitation。 但是我们有时候（偶尔，occasionally）在某些步骤中不选择最大value的状态，而是选择随机状态，因为这种方式可能带我们到一个前所未见的state下。\n上面两段描述的评价函数，以及状态表在井字棋中可以表现为下图：\n学习评价函数（value function） 在下棋的过程中，我们不断修改已有value function对于井字棋，也就是上面我们提到的那张表，我们的目标是试图使其更加准确，为了达到这个目的，我们增加了一步“返回”（back up） 过程（上图中的红色箭头），这个过程在每一步贪心选择后执行，这一步执行的实质是通过当前状态值（value）来适当修改上一步状态的value，使它更接近现在状态的值（评价函数的结果，value），比如图中的红箭头就是让e的值（评价函数的结果，value）更接近g的值（评价函数的结果，value）。 上面的数学表达为，让 $s$ 为贪心选择之前的状态的值（评价函数的结果，value）， $s’$ 为贪心选择后的值（评价函数的结果，value），然后我们的back up更新 $s$ 值，方式是： $$ V(s)\\leftarrow V(s)+\\alpha[V(s’)-V(s)] $$\n其中 $\\alpha$ 是一个小的正小数，叫做“步长”（step-size）参数，这个参数影响学习速率（the rate of learning）注意这里rate是速率的意思，而不是比率的意思，所以学习率这种翻译，我觉得欠妥。 这种基于 $[V(s’)-V(s)]$ 的变化方式（update rule）被称为“时序差分学习”（temporal-difference learning），字面解释：基于两个不同时间点的值（评价函数的结果，value）的差值的方法，这个解释基本能反应这类方法的特点。 这类方法是我们后面要重点学习。\n上述方法对于这个任务可以非常出色的得出不错的结果，例如，在步长参数（step-size）被精确递减后，这个算法对于固定的对手是收敛的，每一步都能给出胜算最高的走法。也就是说，这个方法对于一个固定的对手给出了最优策略。这里的一个关键就是 步长参数（step-size） 的调整，如果这个参数不调整，那么最后的结果也会收敛，但是速度会慢很多。\n“进化方法”与“评价函数”的区别 上面这些细节也佐证了我们前面提到的：“进化方法”和“评价函数学习法”的区别：\n 进化方法的为了学习一个策略，其根本原则是策略不变（不是永久不变，是在目前的短时间内），而去和对手进行多场游戏（就是和环境的交互，interaction），或者使用一个对手的模型，来模拟进行多场游戏。在进行多次游戏后，胜利的次数给出当前策略胜率的无偏估计，然后这个无偏估计被用来优化策略（根据一定的规则从多个策略中淘汰某些，或者其他办法生成新的策略）。 但是问题是每次策略进化都需要多场游戏来计算概率，而每场游戏内的信息被忽略掉了（因为计算概率只关心结果的 —— 胜利出现的次数）而且当一个player（也可以成为agent）获得了胜利，他本场比赛的所有行为都会被认为是正确的，且每一步给予相同的得分，但实际上并不是这样，首先并不是每一步的重要性都一样，其次是并不是每一步都是正确的选择。 对比之下“评价函数学习法”就有不同的表现了，每一步棋和环境的交互信息都会被利用来学习。  总之，进化方法和“评价函数学习法”都是在搜索policy的空间，但是“评价函数学习法”搜索起来更高效，因为他利用了更多的有效信息。\n“Tic-Tac-Toe” 中的强化学习 这个小小的游戏基本展现了强化学习的所有特性：\n 首先强调和环境的交互，这里就是和对手下棋。 \u0008目标明确，正确的行为需要 Planning 或者 Foresight 来处理延迟出现的结果 另一个显著的特征是，RL形成有效的 Planing 以及 lookahead \u0008，而且这个过程不需要使用对手模型（model of opponent），也不会对后面全部的可选操作序列进行搜索（减少policy的搜索空间）  虽然RL的这几个有点很是吸引人，但是这不能掩盖其某些缺点：\n 训练的时的对手，不可能是人，而还是程序，所以这个对手不是Nature的 学习过程也是打碎成不同的步骤（对于井字棋每一局都是一步），而不是连续的进行，只有下完了才会产生reward信号，而不是连续的。 同样对于某些连续的任务，我们也要把它拆成离散形式。  井字棋的搜索范围很小，现在alpha go所面对搜索空间比井字棋大到不知道哪去了~，1992年的时候就有学者研究了\u0008比井字棋更复杂的\u0008游戏，空间大概规模是 $10^{20}$ 他们使用了神经网络作为模型，结合上面的方法得出了很好的结果，具体我们在后面16章学习。 强化学习面对如此巨大的policy空间的行为，主要取决于他对之前学习的信息的理解程度，理解的越好，其行为就更加可靠，反之则不可靠。\n先验知识(Prior Knowledge)与模型(Model) 在井字棋游戏中，RL的agent只知道游戏规则毫无游戏经验或者先验知识，先验知识是锦上添花的元素，也就是，有或者没有这个要靠缘分，就算没有我们也要解决问题，而如果有，那么我们可以利用先验知识节省大量时间，或者大幅度提高结果。RL处理先验知识有多种方式，并且对学习结果。 我们目前处理的情况都是当前环境对agent有明确的反馈，且state明确，在有些问题中state可能是隐藏的或者有大量重复的state这种情况过于复杂，不在我们初级范围内。 Model同样是强化学习中的一个组成要素：当我们的RL（agent）\u0008学习应该怎么样应对不同的状况的时候，他需要思考的就是环境会怎么对他的action进行反应，有些问题确实如此，环境对action的反应就是一个模型，但是有的问题可能比这要更复杂一些，他们有的时候什么模型都没有，不知道环境会做出什么样的反应，面对这样的问题RL也有解决方案。同样的，有模型可以帮助RL更快的学习。 但是我们的井字棋就没有这个model，原因是对手怎么下棋是不可能有模型的，这就是个 Model-Free system。存在精确模型的系统由于模型被精确的使用，所以\u0008做起来相对简单，但是有的时候建立模型的过程会成为这个问题的瓶颈，本书中我们会讨论一些Model-Free的问题，同时组合这些问题成为更复杂的系统。\n总结 本文通过研究Tic-Tac-Toe这个小游戏，从实际应用的角度分析了RL的各个方面，基本涵盖了大部分主要内容，后面我们进入第二章，开始分析具体算法，欢迎大家关注。\nReferences Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.\n 原文地址:https://face2ai.com/RL-RSAB-1-5-An-Extended-Example\n","wordCount":"168","inLanguage":"en","datePublished":"2018-09-28T08:42:24Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"谭升"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://go.face2ai.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-rsab-1-5-an-extended-example.zh/"},"publisher":{"@type":"Organization","name":"谭升的博客","logo":{"@type":"ImageObject","url":"https://go.face2ai.com/logo.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://go.face2ai.com accesskey=h title="谭升的博客 (Alt + H)">谭升的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://go.face2ai.com/math/ title=数学><span>数学</span></a></li><li><a href=https://go.face2ai.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://go.face2ai.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://go.face2ai.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://go.face2ai.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://go.face2ai.com>Home</a></div><h1 class=post-title>【强化学习】 1.5 强化学习的一个扩展举例</h1><div class=post-meta><span title="2018-09-28 08:42:24 +0000 UTC">September 28, 2018</span>&nbsp;·&nbsp;谭升</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%b8%80%e4%b8%aa%e6%89%a9%e5%b1%95%e4%b8%be%e4%be%8b aria-label=强化学习的一个扩展举例>强化学习的一个扩展举例</a><ul><li><a href=#tic-tac-toe%e7%ae%80%e4%bb%8b aria-label=&amp;ldquo;Tic-Tac-Toe&amp;quot;简介>&ldquo;Tic-Tac-Toe"简介</a><ul><li><a href=#%e8%a7%84%e5%88%99%e6%8f%8f%e8%bf%b0 aria-label=规则描述>规则描述</a></li><li><a href=#%e7%ae%80%e5%8d%95%e8%a7%84%e5%88%99%e4%b8%8b%e7%9a%84%e9%97%ae%e9%a2%98 aria-label=简单规则下的问题>简单规则下的问题</a></li><li><a href=#%e9%9d%9e%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e5%9e%8b%e7%9a%84%e8%a7%a3%e5%86%b3%e6%96%b9%e6%b3%95 aria-label=非强化学习型的解决方法>非强化学习型的解决方法</a></li></ul></li><li><a href=#%e8%bf%9b%e5%8c%96%e6%96%b9%e6%b3%95evolutionary-method%e5%ad%a6%e4%b9%a0-tic-tac-toe aria-label="进化方法(Evolutionary Method)学习 &amp;ldquo;Tic-Tac-Toe&amp;rdquo;">进化方法(Evolutionary Method)学习 &ldquo;Tic-Tac-Toe&rdquo;</a></li><li><a href=#%e8%af%84%e4%bb%b7%e5%87%bd%e6%95%b0value-function%e5%ad%a6%e4%b9%a0-tic-tac-toe aria-label="评价函数(Value Function)学习 &amp;ldquo;Tic-Tac-Toe&amp;rdquo;">评价函数(Value Function)学习 &ldquo;Tic-Tac-Toe&rdquo;</a><ul><li><a href=#%e8%ae%be%e8%ae%a1%e8%af%84%e4%bb%b7%e5%87%bd%e6%95%b0 aria-label=设计评价函数>设计评价函数</a></li><li><a href=#%e6%89%a7%e8%a1%8cexploitation%e5%92%8c%e6%8e%a2%e7%b4%a2exploration aria-label=执行(exploitation)和探索(exploration)>执行(exploitation)和探索(exploration)</a></li><li><a href=#%e5%ad%a6%e4%b9%a0%e8%af%84%e4%bb%b7%e5%87%bd%e6%95%b0value-function aria-label="学习评价函数（value function）">学习评价函数（value function）</a></li></ul></li><li><a href=#%e8%bf%9b%e5%8c%96%e6%96%b9%e6%b3%95%e4%b8%8e%e8%af%84%e4%bb%b7%e5%87%bd%e6%95%b0%e7%9a%84%e5%8c%ba%e5%88%ab aria-label=“进化方法”与“评价函数”的区别>“进化方法”与“评价函数”的区别</a></li><li><a href=#tic-tac-toe-%e4%b8%ad%e7%9a%84%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0 aria-label="&amp;ldquo;Tic-Tac-Toe&amp;rdquo; 中的强化学习">&ldquo;Tic-Tac-Toe&rdquo; 中的强化学习</a></li><li><a href=#%e5%85%88%e9%aa%8c%e7%9f%a5%e8%af%86prior-knowledge%e4%b8%8e%e6%a8%a1%e5%9e%8bmodel aria-label="先验知识(Prior Knowledge)与模型(Model)">先验知识(Prior Knowledge)与模型(Model)</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div><div class=post-content><p><strong>Abstract:</strong> 本文介绍强化学习的一个具体例子，Tic-Tac-Toe，作为一种下棋类游戏，Tic-Tac-Toe规则简单，而且问题规模小，容易进行深入分析，了解强化学习在具体例子中的执行过程，进而了解强化学习的性质和特点。
<strong>Keywords:</strong> 强化学习，强化学习举例，Tic-Tac-Toe</p><h1 id=强化学习的一个扩展举例>强化学习的一个扩展举例<a hidden class=anchor aria-hidden=true href=#强化学习的一个扩展举例>#</a></h1><p>今天我们来讲一个很有趣的例子，英文名字叫"Tic-Tac-Toe" 中文名字有翻译成“井字棋”或者什么的，我们这里为了方便就称之为“井字棋”，叫井字棋的原因是因为其棋盘是个“井”字形的，玩法简单，但是这个玩的过程可以使用强化学习的方法来学习，这个简单的棋可以让我们从各个细节审视强化学习的特点，有点，缺点，以及一些小技巧。</p><h2 id=tic-tac-toe简介>&ldquo;Tic-Tac-Toe"简介<a hidden class=anchor aria-hidden=true href=#tic-tac-toe简介>#</a></h2><h3 id=规则描述>规则描述<a hidden class=anchor aria-hidden=true href=#规则描述>#</a></h3><p>Tic-Tac-Toe的规则描述如下：</p><ol><li>使用 $3\times 3$ 的正方形棋盘，双方各使用 &lsquo;x&rsquo;和&rsquo;o&rsquo; 作为棋子，轮流下棋，</li><li>谁的棋子最先连成一行或者一列，或者对角线，那么获胜，棋局结束</li><li>对方完成这个过程，则失败</li><li>如果在最终双方都没能连成一行或者一列，或者对角线的情况下，棋局结束，则为平局。</li></ol><p>下图来自Wikipedia:<a href=https://zh.wikipedia.org/wiki/%E4%BA%95%E5%AD%97%E6%A3%8B>&ldquo;井字棋&rdquo;</a></p><p><img loading=lazy src=./Tictactoe1.gif alt></p><p>上面的棋局，使用 &lsquo;x&rsquo; 的一方率先完成连成一行的准则，故执&rsquo;x&rsquo;一方获胜。</p><h3 id=简单规则下的问题>简单规则下的问题<a hidden class=anchor aria-hidden=true href=#简单规则下的问题>#</a></h3><p>下面这个视频是我在Google上找到的一个小程序录的一段视频。</p><video src=./ttt.mov controls style=max-width:100%;display:block;margin-left:auto;margin-right:auto>
your browser does not support the video tag</video><p>可见，在高级的情况下，双方（我和AI）基本都没法获胜，也就是平局会大量出现，原因是，我们对这种棋的技巧掌握的都很熟练，无法在现行环境（规则）下战胜对方，通过这个观察我们也能发现，在规则相对简单的游戏（“博弈”）中，平局会大量出现。
那么问题就要来了，我们 - 也就是人，在这种简单的规则下，多以平局收场，那么这怎么训练agent呢？如果每局训练总是平局，agent就不知道往什么方向走了。这里我们就要做出一些修改，我们让与我们agent下棋的人或者另一个agent不那么高级，换句话说，我们在简单规则下，降低规则执行者的能力，进而模拟出更高级的博弈（所谓更高级的博弈，无非是我们能力不足才会觉得当前环境，或者规则很困难）。
在后面的训练里，agent会将平局和失败都当做失败来处理，agent的目标明确，就是胜利。</p><h3 id=非强化学习型的解决方法>非强化学习型的解决方法<a hidden class=anchor aria-hidden=true href=#非强化学习型的解决方法>#</a></h3><p>这个棋局太简单，但是在如此简单的规则下，传统方法（非学习方法）都有诸多问题：</p><ol><li>使用传统的“极大极小化”（minimax）方法，这个方法会假定对方按照某个既定方案下棋，而事实是对方可能无既定方案，或者既定方案我们永远无法得知。所以这个传统的博弈思想在此不适用。而且“极大极小化”（minimax）方法有个非常大的缺点：如果其认定当前状态必然失败的情况下，即使后面对手出现重大失误（可以逆转取胜的机会），其也不会采取任何致胜招数，而是按照既定失败的套路继续下去。</li><li>对于这种连续决策型问题的传统的优化方法，例如动态规划(dynamic programming)，可以计算出在对阵任何对手时候的最优解，但是前提是：对手的全部信息要作为输入提前提交给算法，其中包括在某特定情况（棋局）下，对手下一步棋下在哪的概率。如果我们提前不知道这些信息（大部分情况下，这个信息无法获得）</li><li>对于2中的动态规划方法，有另一种改进版就是在训练时和对手多次交手，从而记录学习出对手的信息，或者叫做给对手建模（model）然后再使用动态规划来寻找答案。</li></ol><p>上面3中方法中，1和2对解决问题基本没有帮助，3有帮助。3和我们后面会介绍的很多强化学习方法有着非常相似的思路。</p><h2 id=进化方法evolutionary-method学习-tic-tac-toe>进化方法(Evolutionary Method)学习 &ldquo;Tic-Tac-Toe&rdquo;<a hidden class=anchor aria-hidden=true href=#进化方法evolutionary-method学习-tic-tac-toe>#</a></h2><p><a href=https://face2ai.com/RL-RSAB-1-4-0-Limitations-and-Scope/>进化方法</a> 中讲解了进化方法的缺点，就是没有使用agent和环境之间的交互信息来改变agent。而这里如果把进化方法直接使用到“井字棋”中，其表策略（policy）的特点是：直接search全部可能的位置，找到获胜概率最大的位置，然后下棋。也就是，策略要考虑到当前盘面（ $3\times 3$ 的棋盘上 x和o的分布）并给出获胜概率最大的下棋位置。而这个概率的获得就需要和对手多次下棋，记录，学习出来的。有了这些信息，agent就知道接下来一步或者接下来很多步应该怎么走。
一个典型的进化方法就是在“策略空间”的 hill-climb ，这种方法的策略是逐渐找出能提高表现的策略（并不是一下找到最优的方法，而是像爬山一样，每一步都找到一个能提高agent表现的方案，一步一步的向上爬）。
遗传方法就更加直接暴力了，直接直接评估大量的策略(policies)，去除不合格的，留下好的，然后产生新一代的，以此迭代，直到问题解决。
解决“井字棋”问题，理论上存在很多种不同的优化方法。</p><h2 id=评价函数value-function学习-tic-tac-toe>评价函数(Value Function)学习 &ldquo;Tic-Tac-Toe&rdquo;<a hidden class=anchor aria-hidden=true href=#评价函数value-function学习-tic-tac-toe>#</a></h2><p>上面我们说了进化方法在井字棋中使用，下面我们就要看看另一个方向的方法 —— 评价函数(value Function)的方法了。</p><h3 id=设计评价函数>设计评价函数<a hidden class=anchor aria-hidden=true href=#设计评价函数>#</a></h3><p>我们列一个表，这个表中的每个格子对应一种状态（state），整张表就对应井字棋的全部情况，这个表的每一项都对应一个概率值，这个概率值表示当前状态下最后获胜的期望，注意两点，一个是当前的状态，第二是最终获胜的期望。这个期望，我们就把其当做评价函数的结果，value —— value值。这个表就是我们的评价函数(Value Function)了.
在井字棋中，这个表就包含下一步棋应该怎么走的评估。通过当前状态，我们可以缩小下一步可能出现的状态范围，然后比较所有可能的状态，如果A状态比B状态有更高的获胜期望，那么我们认为A状态比B状态好，所以我们倾向于把棋子走到A状态显示的位置。
对于这个状态表，假设我们执x，那么如果某状态中包含x在一行或者一列或者对角线，那么这个状态的获胜期望是1，相反，如果o在一行或者一列或者对角线，那么这个状态的获胜期望是0；同样，如果对应状态是棋盘下满，而没有获胜方，这时候期望同样是0 。除了上述这些情况，其他状态的<strong>初始化</strong>值都是0.5，即有一半的可能性会获胜。</p><h3 id=执行exploitation和探索exploration>执行(exploitation)和探索(exploration)<a hidden class=anchor aria-hidden=true href=#执行exploitation和探索exploration>#</a></h3><p>当我们有了这张表（上面的评价函数）我们就有了制胜法宝，但是具体执行也是有不同方案的，我们可以查找最大value的状态，然后执行贪心(greedily)选择，这是使得获胜期望最大的策略，也就是完全执行表（value function）中的指示，这个被称为exploitation。
但是我们有时候（偶尔，occasionally）在某些步骤中不选择最大value的状态，而是选择随机状态，因为这种方式可能带我们到一个前所未见的state下。</p><p>上面两段描述的评价函数，以及状态表在井字棋中可以表现为下图：</p><p><img loading=lazy src=./table.png alt=table></p><h3 id=学习评价函数value-function>学习评价函数（value function）<a hidden class=anchor aria-hidden=true href=#学习评价函数value-function>#</a></h3><p>在下棋的过程中，我们不断修改已有value function对于井字棋，也就是上面我们提到的那张表，我们的目标是试图使其更加准确，为了达到这个目的，我们增加了一步“返回”（back up） 过程（上图中的红色箭头），这个过程在每一步贪心选择后执行，这一步执行的实质是通过当前状态值（value）来适当修改上一步状态的value，使它更接近现在状态的值（评价函数的结果，value），比如图中的红箭头就是让e的值（评价函数的结果，value）更接近g的值（评价函数的结果，value）。
上面的数学表达为，让 $s$ 为贪心选择之前的状态的值（评价函数的结果，value）， $s&rsquo;$ 为贪心选择后的值（评价函数的结果，value），然后我们的back up更新 $s$ 值，方式是：
$$
V(s)\leftarrow V(s)+\alpha[V(s&rsquo;)-V(s)]
$$</p><p>其中 $\alpha$ 是一个小的正小数，叫做“步长”（step-size）参数，这个参数影响学习速率（the rate of learning）注意这里rate是速率的意思，而不是比率的意思，所以学习率这种翻译，我觉得欠妥。
这种基于 $[V(s&rsquo;)-V(s)]$ 的变化方式（update rule）被称为“时序差分学习”（temporal-difference learning），字面解释：基于两个不同时间点的值（评价函数的结果，value）的差值的方法，这个解释基本能反应这类方法的特点。
这类方法是我们后面要重点学习。</p><p>上述方法对于这个任务可以非常出色的得出不错的结果，例如，<strong>在步长参数（step-size）被精确递减后</strong>，这个算法对于固定的对手是收敛的，每一步都能给出胜算最高的走法。也就是说，这个方法对于一个固定的对手给出了最优策略。这里的一个关键就是 <strong>步长参数（step-size）</strong> 的调整，如果这个参数不调整，那么最后的结果也会收敛，但是速度会慢很多。</p><h2 id=进化方法与评价函数的区别>“进化方法”与“评价函数”的区别<a hidden class=anchor aria-hidden=true href=#进化方法与评价函数的区别>#</a></h2><p>上面这些细节也佐证了我们前面提到的：“进化方法”和“评价函数学习法”的区别：</p><ol><li>进化方法的为了学习一个策略，其根本原则是策略不变（不是永久不变，是在目前的短时间内），而去和对手进行多场游戏（就是和环境的交互，interaction），或者使用一个对手的模型，来模拟进行多场游戏。在进行多次游戏后，胜利的次数给出当前策略胜率的无偏估计，然后这个无偏估计被用来优化策略（根据一定的规则从多个策略中淘汰某些，或者其他办法生成新的策略）。
但是问题是每次策略进化都需要多场游戏来计算概率，而每场游戏内的信息被忽略掉了（因为计算概率只关心结果的 —— 胜利出现的次数）而且当一个player（也可以成为agent）获得了胜利，他本场比赛的所有行为都会被认为是正确的，且每一步给予相同的得分，但实际上并不是这样，首先并不是每一步的重要性都一样，其次是并不是每一步都是正确的选择。</li><li>对比之下“评价函数学习法”就有不同的表现了，每一步棋和环境的交互信息都会被利用来学习。</li></ol><p>总之，进化方法和“评价函数学习法”都是在搜索policy的空间，但是“评价函数学习法”搜索起来更高效，因为他利用了更多的有效信息。</p><h2 id=tic-tac-toe-中的强化学习>&ldquo;Tic-Tac-Toe&rdquo; 中的强化学习<a hidden class=anchor aria-hidden=true href=#tic-tac-toe-中的强化学习>#</a></h2><p>这个小小的游戏基本展现了强化学习的所有特性：</p><ol><li>首先强调和环境的交互，这里就是和对手下棋。</li><li>目标明确，正确的行为需要 <em>Planning</em> 或者 <em>Foresight</em> 来处理延迟出现的结果</li><li>另一个显著的特征是，RL形成有效的 <em>Planing</em> 以及 <em>lookahead</em> ，而且这个过程不需要使用对手模型（model of opponent），也不会对后面全部的可选操作序列进行搜索（减少policy的搜索空间）</li></ol><p>虽然RL的这几个有点很是吸引人，但是这不能掩盖其某些缺点：</p><ol><li>训练的时的对手，不可能是人，而还是程序，所以这个对手不是Nature的</li><li>学习过程也是打碎成不同的步骤（对于井字棋每一局都是一步），而不是连续的进行，只有下完了才会产生reward信号，而不是连续的。</li><li>同样对于某些连续的任务，我们也要把它拆成离散形式。</li></ol><p>井字棋的搜索范围很小，现在alpha go所面对搜索空间比井字棋大到不知道哪去了~，1992年的时候就有学者研究了比井字棋更复杂的游戏，空间大概规模是 $10^{20}$ 他们使用了神经网络作为模型，结合上面的方法得出了很好的结果，具体我们在后面16章学习。
强化学习面对如此巨大的policy空间的行为，主要取决于他对之前学习的信息的理解程度，理解的越好，其行为就更加可靠，反之则不可靠。</p><h2 id=先验知识prior-knowledge与模型model>先验知识(Prior Knowledge)与模型(Model)<a hidden class=anchor aria-hidden=true href=#先验知识prior-knowledge与模型model>#</a></h2><p>在井字棋游戏中，RL的agent只知道游戏规则毫无游戏经验或者先验知识，先验知识是锦上添花的元素，也就是，有或者没有这个要靠缘分，就算没有我们也要解决问题，而如果有，那么我们可以利用先验知识节省大量时间，或者大幅度提高结果。RL处理先验知识有多种方式，并且对学习结果。
我们目前处理的情况都是当前环境对agent有明确的反馈，且state明确，在有些问题中state可能是隐藏的或者有大量重复的state这种情况过于复杂，不在我们初级范围内。
Model同样是强化学习中的一个组成要素：当我们的RL（agent）学习应该怎么样应对不同的状况的时候，他需要思考的就是环境会怎么对他的action进行反应，有些问题确实如此，环境对action的反应就是一个模型，但是有的问题可能比这要更复杂一些，他们有的时候什么模型都没有，不知道环境会做出什么样的反应，面对这样的问题RL也有解决方案。同样的，有模型可以帮助RL更快的学习。
但是我们的井字棋就没有这个model，原因是对手怎么下棋是不可能有模型的，这就是个 Model-Free system。存在精确模型的系统由于模型被精确的使用，所以做起来相对简单，但是有的时候建立模型的过程会成为这个问题的瓶颈，本书中我们会讨论一些Model-Free的问题，同时组合这些问题成为更复杂的系统。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>本文通过研究Tic-Tac-Toe这个小游戏，从实际应用的角度分析了RL的各个方面，基本涵盖了大部分主要内容，后面我们进入第二章，开始分析具体算法，欢迎大家关注。</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>Sutton R S, Barto A G. Reinforcement learning: An introduction[J]. 2011.</p><hr><p>原文地址:<a href=https://face2ai.com/RL-RSAB-1-5-An-Extended-Example>https://face2ai.com/RL-RSAB-1-5-An-Extended-Example</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://go.face2ai.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 【强化学习】 1.5 强化学习的一个扩展举例 on twitter" href="https://twitter.com/intent/tweet/?text=%e3%80%90%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e3%80%91%201.5%20%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%b8%80%e4%b8%aa%e6%89%a9%e5%b1%95%e4%b8%be%e4%be%8b&url=https%3a%2f%2fgo.face2ai.com%2f%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%2frl-rsab-1-5-an-extended-example.zh%2f&hashtags=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【强化学习】 1.5 强化学习的一个扩展举例 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fgo.face2ai.com%2f%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%2frl-rsab-1-5-an-extended-example.zh%2f&title=%e3%80%90%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e3%80%91%201.5%20%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%b8%80%e4%b8%aa%e6%89%a9%e5%b1%95%e4%b8%be%e4%be%8b&summary=%e3%80%90%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e3%80%91%201.5%20%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%b8%80%e4%b8%aa%e6%89%a9%e5%b1%95%e4%b8%be%e4%be%8b&source=https%3a%2f%2fgo.face2ai.com%2f%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%2frl-rsab-1-5-an-extended-example.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【强化学习】 1.5 强化学习的一个扩展举例 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgo.face2ai.com%2f%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%2frl-rsab-1-5-an-extended-example.zh%2f&title=%e3%80%90%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e3%80%91%201.5%20%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%b8%80%e4%b8%aa%e6%89%a9%e5%b1%95%e4%b8%be%e4%be%8b"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【强化学习】 1.5 强化学习的一个扩展举例 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgo.face2ai.com%2f%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%2frl-rsab-1-5-an-extended-example.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【强化学习】 1.5 强化学习的一个扩展举例 on whatsapp" href="https://api.whatsapp.com/send?text=%e3%80%90%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e3%80%91%201.5%20%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%b8%80%e4%b8%aa%e6%89%a9%e5%b1%95%e4%b8%be%e4%be%8b%20-%20https%3a%2f%2fgo.face2ai.com%2f%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%2frl-rsab-1-5-an-extended-example.zh%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 【强化学习】 1.5 强化学习的一个扩展举例 on telegram" href="https://telegram.me/share/url?text=%e3%80%90%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e3%80%91%201.5%20%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%b8%80%e4%b8%aa%e6%89%a9%e5%b1%95%e4%b8%be%e4%be%8b&url=https%3a%2f%2fgo.face2ai.com%2f%25E5%25BC%25BA%25E5%258C%2596%25E5%25AD%25A6%25E4%25B9%25A0%2frl-rsab-1-5-an-extended-example.zh%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://go.face2ai.com>谭升的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>